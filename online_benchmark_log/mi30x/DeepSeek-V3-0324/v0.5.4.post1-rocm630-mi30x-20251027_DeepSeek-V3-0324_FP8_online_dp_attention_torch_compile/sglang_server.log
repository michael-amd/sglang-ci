INFO 10-27 11:15:42 __init__.py:179] Automatically detected platform rocm.
WARNING 10-27 11:15:42 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-27 11:15:43] WARNING server_args.py:1105: Attention backend not explicitly specified. Use aiter backend by default.
[2025-10-27 11:15:43] WARNING server_args.py:1294: DP attention is enabled. The chunked prefill size is adjusted to 16384 to avoid MoE kernel issues. 
[2025-10-27 11:15:43] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 11:15:44] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.7200000000000001, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=0.3, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=30494431, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=8, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=True, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
[2025-10-27 11:15:44] Using default HuggingFace chat template with detected content format: string
INFO 10-27 11:15:52 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 11:15:52 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-27 11:15:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 11:15:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
INFO 10-27 11:16:01 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 11:16:02 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 11:16:02 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 11:16:02 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 11:16:02 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 11:16:02 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 11:16:02 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 11:16:02 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-27 11:16:04] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 11:16:04] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 11:16:04 DP0 TP0] Process 815 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2025-10-27 11:16:04 DP7 TP7] Process 822 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-27 11:16:04 DP7 TP7] Init torch distributed begin.
[2025-10-27 11:16:04] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 11:16:04] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 11:16:04 DP2 TP2] Process 817 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
[2025-10-27 11:16:04 DP0 TP0] Init torch distributed begin.
[2025-10-27 11:16:04 DP6 TP6] Process 821 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-27 11:16:04] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-27 11:16:04 DP3 TP3] Process 818 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-10-27 11:16:04] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 11:16:04] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 11:16:04] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 11:16:04 DP1 TP1] Process 816 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2025-10-27 11:16:04 DP4 TP4] Process 819 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
[2025-10-27 11:16:04 DP5 TP5] Process 820 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-10-27 11:16:04 DP2 TP2] Init torch distributed begin.
[2025-10-27 11:16:04 DP6 TP6] Init torch distributed begin.
[2025-10-27 11:16:05 DP3 TP3] Init torch distributed begin.
[2025-10-27 11:16:05 DP1 TP1] Init torch distributed begin.
[2025-10-27 11:16:05 DP4 TP4] Init torch distributed begin.
[2025-10-27 11:16:05 DP5 TP5] Init torch distributed begin.
[2025-10-27 11:16:05 DP0 TP0] sglang is using nccl==2.21.5
[2025-10-27 11:16:06 DP0 TP0] Init torch distributed ends. mem usage=3.65 GB
[2025-10-27 11:16:06 DP7 TP7] Init torch distributed ends. mem usage=3.94 GB
[2025-10-27 11:16:06 DP6 TP6] Init torch distributed ends. mem usage=3.95 GB
[2025-10-27 11:16:06 DP5 TP5] Init torch distributed ends. mem usage=3.93 GB
[2025-10-27 11:16:06 DP4 TP4] Init torch distributed ends. mem usage=4.01 GB
[2025-10-27 11:16:06 DP3 TP3] Init torch distributed ends. mem usage=4.06 GB
[2025-10-27 11:16:06 DP1 TP1] Init torch distributed ends. mem usage=4.07 GB
[2025-10-27 11:16:06 DP2 TP2] Init torch distributed ends. mem usage=4.07 GB
[2025-10-27 11:16:08 DP6 TP6] Load weight begin. avail mem=187.31 GB
[2025-10-27 11:16:08 DP7 TP7] Load weight begin. avail mem=187.32 GB
[2025-10-27 11:16:08 DP4 TP4] Load weight begin. avail mem=187.25 GB
[2025-10-27 11:16:08 DP3 TP3] Load weight begin. avail mem=187.20 GB
[2025-10-27 11:16:08 DP1 TP1] Load weight begin. avail mem=187.19 GB
[2025-10-27 11:16:08 DP5 TP5] Load weight begin. avail mem=187.33 GB
[2025-10-27 11:16:08 DP0 TP0] Load weight begin. avail mem=187.61 GB
[2025-10-27 11:16:08 DP0 TP0] Detected fp8 checkpoint.
[2025-10-27 11:16:08 DP0 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
[2025-10-27 11:16:08 DP2 TP2] Load weight begin. avail mem=187.19 GB
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:31,  5.16it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:29,  5.35it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:00<00:27,  5.79it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:24,  6.41it/s]
Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:01<00:28,  5.58it/s]
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:01<00:27,  5.59it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:01<00:25,  6.04it/s]
Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:01<00:31,  4.86it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:01<00:27,  5.49it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:01<00:24,  6.13it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:02<00:51,  2.95it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:02<00:44,  3.37it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:03<00:37,  4.00it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:03<00:31,  4.71it/s]
Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:03<00:26,  5.56it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:03<00:23,  6.17it/s]
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:03<00:32,  4.53it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:03<00:27,  5.24it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:03<00:24,  5.79it/s]
Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:04<00:22,  6.44it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:04<00:23,  6.03it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:04<00:36,  3.84it/s]
Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:04<00:30,  4.55it/s]
Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:05<00:26,  5.20it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:05<00:14,  9.40it/s]
Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:05<00:10, 12.62it/s]
Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:05<00:07, 16.22it/s]
Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:05<00:11, 11.24it/s]
Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:06<00:20,  6.21it/s]
Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:06<00:16,  7.43it/s]
Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:06<00:13,  8.77it/s]
Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:06<00:11, 10.22it/s]
Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:06<00:10, 11.26it/s]
Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:07<00:09, 12.65it/s]
Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:07<00:07, 14.01it/s]
Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:07<00:06, 16.08it/s]
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:07<00:06, 17.56it/s]
Loading safetensors checkpoint shards:  36% Completed | 59/163 [00:07<00:07, 13.15it/s]
Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:07<00:07, 14.17it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:08<00:06, 16.28it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:08<00:05, 18.34it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:09<00:12,  7.29it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:09<00:11,  8.03it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:09<00:09,  9.32it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:09<00:07, 10.75it/s]
Loading safetensors checkpoint shards:  50% Completed | 81/163 [00:09<00:06, 12.89it/s]
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:09<00:05, 14.37it/s]
Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:10<00:04, 16.04it/s]
Loading safetensors checkpoint shards:  55% Completed | 90/163 [00:10<00:06, 11.96it/s]
Loading safetensors checkpoint shards:  57% Completed | 93/163 [00:10<00:04, 14.11it/s]
Loading safetensors checkpoint shards:  58% Completed | 95/163 [00:10<00:05, 13.06it/s]
Loading safetensors checkpoint shards:  60% Completed | 97/163 [00:10<00:04, 13.95it/s]
Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:10<00:04, 14.63it/s]
Loading safetensors checkpoint shards:  62% Completed | 101/163 [00:11<00:04, 15.05it/s]
Loading safetensors checkpoint shards:  63% Completed | 103/163 [00:11<00:04, 13.95it/s]
Loading safetensors checkpoint shards:  65% Completed | 106/163 [00:11<00:03, 16.52it/s]
Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:11<00:03, 16.69it/s]
Loading safetensors checkpoint shards:  67% Completed | 110/163 [00:12<00:08,  6.16it/s]
Loading safetensors checkpoint shards:  69% Completed | 113/163 [00:12<00:06,  8.20it/s]
Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:12<00:04, 10.61it/s]
Loading safetensors checkpoint shards:  72% Completed | 118/163 [00:12<00:03, 11.82it/s]
Loading safetensors checkpoint shards:  74% Completed | 120/163 [00:12<00:03, 11.49it/s]
Loading safetensors checkpoint shards:  75% Completed | 123/163 [00:13<00:03, 12.18it/s]
Loading safetensors checkpoint shards:  77% Completed | 126/163 [00:13<00:02, 14.74it/s]
Loading safetensors checkpoint shards:  79% Completed | 129/163 [00:13<00:02, 16.14it/s]
Loading safetensors checkpoint shards:  80% Completed | 131/163 [00:13<00:01, 16.73it/s]
Loading safetensors checkpoint shards:  82% Completed | 134/163 [00:13<00:01, 19.04it/s]
Loading safetensors checkpoint shards:  84% Completed | 137/163 [00:13<00:01, 16.52it/s]
Loading safetensors checkpoint shards:  85% Completed | 139/163 [00:14<00:01, 12.63it/s]
Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:14<00:01, 12.72it/s]
Loading safetensors checkpoint shards:  88% Completed | 143/163 [00:14<00:01, 13.97it/s]
Loading safetensors checkpoint shards:  90% Completed | 146/163 [00:14<00:01, 14.75it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:14<00:00, 16.26it/s]
Loading safetensors checkpoint shards:  93% Completed | 152/163 [00:14<00:00, 17.88it/s]
Loading safetensors checkpoint shards:  94% Completed | 154/163 [00:15<00:00, 13.27it/s]
Loading safetensors checkpoint shards:  96% Completed | 156/163 [00:15<00:00, 13.25it/s]
Loading safetensors checkpoint shards:  97% Completed | 158/163 [00:16<00:00,  5.51it/s]
Loading safetensors checkpoint shards:  98% Completed | 160/163 [00:16<00:00,  6.83it/s]
Loading safetensors checkpoint shards:  99% Completed | 162/163 [00:16<00:00,  8.13it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:16<00:00,  9.85it/s]

[2025-10-27 11:17:01 DP5 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.86 GB, mem usage=90.47 GB.
[2025-10-27 11:17:01 DP4 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.78 GB, mem usage=90.47 GB.
[2025-10-27 11:17:01 DP6 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.85 GB, mem usage=90.47 GB.
[2025-10-27 11:17:01 DP7 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.86 GB, mem usage=90.47 GB.
[2025-10-27 11:17:01 DP3 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.73 GB, mem usage=90.47 GB.
[2025-10-27 11:17:02 DP2 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.72 GB, mem usage=90.47 GB.
[2025-10-27 11:17:02 DP1 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.72 GB, mem usage=90.47 GB.
[2025-10-27 11:17:03 DP0 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=97.14 GB, mem usage=90.47 GB.
[2025-10-27 11:17:03 DP0 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-27 11:17:03 DP5 TP5] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-27 11:17:03 DP7 TP7] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-27 11:17:03 DP5 TP5] Memory pool end. avail mem=51.25 GB
[2025-10-27 11:17:03 DP7 TP7] Memory pool end. avail mem=51.24 GB
[2025-10-27 11:17:03 DP0 TP0] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-27 11:17:03 DP0 TP0] Memory pool end. avail mem=51.53 GB
[2025-10-27 11:17:03 DP2 TP2] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-27 11:17:03 DP2 TP2] Memory pool end. avail mem=51.11 GB
[2025-10-27 11:17:03 DP6 TP6] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-27 11:17:03 DP6 TP6] Memory pool end. avail mem=51.23 GB
[2025-10-27 11:17:03 DP4 TP4] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-27 11:17:03 DP4 TP4] Memory pool end. avail mem=51.17 GB
[2025-10-27 11:17:03 DP3 TP3] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-27 11:17:03 DP3 TP3] Memory pool end. avail mem=51.12 GB
[2025-10-27 11:17:03 DP1 TP1] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-27 11:17:03 DP1 TP1] Memory pool end. avail mem=51.11 GB
[2025-10-27 11:17:05 DP6 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=51.03 GB
[2025-10-27 11:17:05 DP3 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=50.91 GB
[2025-10-27 11:17:05 DP1 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=50.91 GB
[2025-10-27 11:17:05 DP0 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=51.32 GB
[2025-10-27 11:17:05 DP0 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-10-27 11:17:05 DP2 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=50.90 GB
[2025-10-27 11:17:05 DP4 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=50.96 GB
[2025-10-27 11:17:05 DP5 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=51.05 GB
[2025-10-27 11:17:05 DP7 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=51.04 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=50.68 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 11:17:06 DP1 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 11:17:06 DP3 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 11:17:06 DP2 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 11:17:06 DP0 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 11:17:06 DP4 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 11:17:06 DP5 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 11:17:06 DP6 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 11:17:06 DP7 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 11:17:07 DP4 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 11:17:07 DP6 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 11:17:07 DP3 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 11:17:08 DP2 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 11:17:08 DP1 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 11:17:08 DP5 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 11:17:08 DP0 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 11:17:08 DP7 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP4 TP4] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP4 TP4] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP3 TP3] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP3 TP3] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP6 TP6] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP6 TP6] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP2 TP2] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP2 TP2] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP1 TP1] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP1 TP1] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP5 TP5] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP5 TP5] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP0 TP0] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP0 TP0] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP7 TP7] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:08 DP7 TP7] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP1 TP1] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP0 TP0] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP6 TP6] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP2 TP2] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP4 TP4] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP5 TP5] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP1 TP1] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP3 TP3] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP7 TP7] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP0 TP0] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP6 TP6] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP4 TP4] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP2 TP2] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP5 TP5] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP3 TP3] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:09 DP7 TP7] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:09 DP1 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:09 DP0 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:09 DP2 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:09 DP3 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:09 DP6 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:09 DP7 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:09 DP4 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:09 DP5 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[rank0]:[W1027 11:17:09.104131979 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank4]:[W1027 11:17:09.104152623 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank6]:[W1027 11:17:09.104164982 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank7]:[W1027 11:17:09.104165084 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank2]:[W1027 11:17:09.104173141 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank3]:[W1027 11:17:09.104251195 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank1]:[W1027 11:17:09.104251213 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank5]:[W1027 11:17:09.104290836 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Capturing batches (bs=512 avail_mem=50.68 GB):   2%|         | 1/52 [00:04<03:50,  4.51s/it]Capturing batches (bs=496 avail_mem=42.68 GB):   2%|         | 1/52 [00:04<03:50,  4.51s/it]Capturing batches (bs=496 avail_mem=42.68 GB):   4%|         | 2/52 [00:05<01:53,  2.26s/it]Capturing batches (bs=480 avail_mem=42.68 GB):   4%|         | 2/52 [00:05<01:53,  2.26s/it]Capturing batches (bs=480 avail_mem=42.68 GB):   6%|         | 3/52 [00:05<01:16,  1.55s/it]Capturing batches (bs=464 avail_mem=42.67 GB):   6%|         | 3/52 [00:05<01:16,  1.55s/it]Capturing batches (bs=464 avail_mem=42.67 GB):   8%|         | 4/52 [00:06<00:58,  1.22s/it]Capturing batches (bs=448 avail_mem=42.66 GB):   8%|         | 4/52 [00:06<00:58,  1.22s/it]Capturing batches (bs=448 avail_mem=42.66 GB):  10%|         | 5/52 [00:07<00:48,  1.03s/it]Capturing batches (bs=432 avail_mem=42.65 GB):  10%|         | 5/52 [00:07<00:48,  1.03s/it]Capturing batches (bs=432 avail_mem=42.65 GB):  12%|        | 6/52 [00:07<00:40,  1.13it/s]Capturing batches (bs=416 avail_mem=42.65 GB):  12%|        | 6/52 [00:07<00:40,  1.13it/s]Capturing batches (bs=416 avail_mem=42.65 GB):  13%|        | 7/52 [00:08<00:35,  1.26it/s]Capturing batches (bs=400 avail_mem=42.64 GB):  13%|        | 7/52 [00:08<00:35,  1.26it/s]Capturing batches (bs=400 avail_mem=42.64 GB):  15%|        | 8/52 [00:09<00:32,  1.37it/s]Capturing batches (bs=384 avail_mem=42.63 GB):  15%|        | 8/52 [00:09<00:32,  1.37it/s]Capturing batches (bs=384 avail_mem=42.63 GB):  17%|        | 9/52 [00:09<00:28,  1.52it/s]Capturing batches (bs=368 avail_mem=42.62 GB):  17%|        | 9/52 [00:09<00:28,  1.52it/s]Capturing batches (bs=368 avail_mem=42.62 GB):  19%|        | 10/52 [00:10<00:26,  1.56it/s]Capturing batches (bs=352 avail_mem=42.62 GB):  19%|        | 10/52 [00:10<00:26,  1.56it/s]Capturing batches (bs=352 avail_mem=42.62 GB):  21%|        | 11/52 [00:10<00:25,  1.59it/s]Capturing batches (bs=336 avail_mem=42.61 GB):  21%|        | 11/52 [00:10<00:25,  1.59it/s]Capturing batches (bs=336 avail_mem=42.61 GB):  23%|       | 12/52 [00:11<00:24,  1.61it/s]Capturing batches (bs=320 avail_mem=42.60 GB):  23%|       | 12/52 [00:11<00:24,  1.61it/s]Capturing batches (bs=320 avail_mem=42.60 GB):  25%|       | 13/52 [00:12<00:23,  1.63it/s]Capturing batches (bs=304 avail_mem=42.60 GB):  25%|       | 13/52 [00:12<00:23,  1.63it/s]Capturing batches (bs=304 avail_mem=42.60 GB):  27%|       | 14/52 [00:12<00:20,  1.83it/s]Capturing batches (bs=288 avail_mem=42.59 GB):  27%|       | 14/52 [00:12<00:20,  1.83it/s]Capturing batches (bs=288 avail_mem=42.59 GB):  29%|       | 15/52 [00:12<00:18,  1.98it/s]Capturing batches (bs=272 avail_mem=42.58 GB):  29%|       | 15/52 [00:12<00:18,  1.98it/s]Capturing batches (bs=272 avail_mem=42.58 GB):  31%|       | 16/52 [00:13<00:19,  1.87it/s]Capturing batches (bs=256 avail_mem=42.58 GB):  31%|       | 16/52 [00:13<00:19,  1.87it/s][aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP1 TP1] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP1 TP1] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP1 TP1] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP1 TP1] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP2 TP2] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP2 TP2] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP2 TP2] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP2 TP2] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP3 TP3] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP3 TP3] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP3 TP3] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP3 TP3] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP0 TP0] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP0 TP0] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP0 TP0] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP0 TP0] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP6 TP6] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP6 TP6] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP6 TP6] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP6 TP6] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP4 TP4] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP4 TP4] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP4 TP4] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP4 TP4] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP5 TP5] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP5 TP5] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP5 TP5] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP5 TP5] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP7 TP7] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:19 DP7 TP7] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP7 TP7] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:19 DP7 TP7] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
Capturing batches (bs=256 avail_mem=42.58 GB):  33%|      | 17/52 [00:14<00:19,  1.80it/s]Capturing batches (bs=248 avail_mem=42.57 GB):  33%|      | 17/52 [00:14<00:19,  1.80it/s]Capturing batches (bs=248 avail_mem=42.57 GB):  35%|      | 18/52 [00:14<00:19,  1.76it/s]Capturing batches (bs=240 avail_mem=42.56 GB):  35%|      | 18/52 [00:14<00:19,  1.76it/s]Capturing batches (bs=240 avail_mem=42.56 GB):  37%|      | 19/52 [00:15<00:19,  1.73it/s]Capturing batches (bs=232 avail_mem=42.55 GB):  37%|      | 19/52 [00:15<00:19,  1.73it/s]Capturing batches (bs=232 avail_mem=42.55 GB):  38%|      | 20/52 [00:15<00:18,  1.70it/s]Capturing batches (bs=224 avail_mem=42.55 GB):  38%|      | 20/52 [00:15<00:18,  1.70it/s]Capturing batches (bs=224 avail_mem=42.55 GB):  40%|      | 21/52 [00:16<00:18,  1.70it/s]Capturing batches (bs=216 avail_mem=42.54 GB):  40%|      | 21/52 [00:16<00:18,  1.70it/s]Capturing batches (bs=216 avail_mem=42.54 GB):  42%|     | 22/52 [00:16<00:16,  1.78it/s]Capturing batches (bs=208 avail_mem=42.53 GB):  42%|     | 22/52 [00:16<00:16,  1.78it/s]Capturing batches (bs=208 avail_mem=42.53 GB):  44%|     | 23/52 [00:17<00:14,  1.95it/s]Capturing batches (bs=200 avail_mem=42.53 GB):  44%|     | 23/52 [00:17<00:14,  1.95it/s]Capturing batches (bs=200 avail_mem=42.53 GB):  46%|     | 24/52 [00:17<00:14,  1.96it/s]Capturing batches (bs=192 avail_mem=42.52 GB):  46%|     | 24/52 [00:17<00:14,  1.96it/s][aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP7 TP7] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP1 TP1] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP5 TP5] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP3 TP3] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP0 TP0] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP4 TP4] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP6 TP6] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP2 TP2] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:23 DP7 TP7] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:23 DP1 TP1] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:23 DP0 TP0] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:23 DP5 TP5] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:23 DP3 TP3] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:23 DP6 TP6] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:23 DP4 TP4] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:23 DP2 TP2] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP1 TP1] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP0 TP0] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP5 TP5] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP7 TP7] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP3 TP3] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP1 TP1] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP0 TP0] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP6 TP6] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP2 TP2] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP5 TP5] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP4 TP4] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP7 TP7] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP3 TP3] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP6 TP6] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP2 TP2] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:23 DP4 TP4] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
Capturing batches (bs=192 avail_mem=42.52 GB):  48%|     | 25/52 [00:18<00:12,  2.08it/s]Capturing batches (bs=184 avail_mem=42.52 GB):  48%|     | 25/52 [00:18<00:12,  2.08it/s]Capturing batches (bs=184 avail_mem=42.52 GB):  50%|     | 26/52 [00:18<00:11,  2.21it/s]Capturing batches (bs=176 avail_mem=42.51 GB):  50%|     | 26/52 [00:18<00:11,  2.21it/s]Capturing batches (bs=176 avail_mem=42.51 GB):  52%|    | 27/52 [00:19<00:10,  2.29it/s]Capturing batches (bs=168 avail_mem=42.51 GB):  52%|    | 27/52 [00:19<00:10,  2.29it/s]Capturing batches (bs=168 avail_mem=42.51 GB):  54%|    | 28/52 [00:19<00:10,  2.19it/s]Capturing batches (bs=160 avail_mem=42.50 GB):  54%|    | 28/52 [00:19<00:10,  2.19it/s]Capturing batches (bs=160 avail_mem=42.50 GB):  56%|    | 29/52 [00:20<00:10,  2.13it/s]Capturing batches (bs=152 avail_mem=42.49 GB):  56%|    | 29/52 [00:20<00:10,  2.13it/s]Capturing batches (bs=152 avail_mem=42.49 GB):  58%|    | 30/52 [00:20<00:10,  2.09it/s]Capturing batches (bs=144 avail_mem=42.49 GB):  58%|    | 30/52 [00:20<00:10,  2.09it/s]Capturing batches (bs=144 avail_mem=42.49 GB):  60%|    | 31/52 [00:20<00:09,  2.20it/s]Capturing batches (bs=136 avail_mem=42.48 GB):  60%|    | 31/52 [00:20<00:09,  2.20it/s]Capturing batches (bs=136 avail_mem=42.48 GB):  62%|   | 32/52 [00:21<00:08,  2.27it/s]Capturing batches (bs=128 avail_mem=42.47 GB):  62%|   | 32/52 [00:21<00:08,  2.27it/s][aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP1 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP3 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP7 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP0 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP5 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP2 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP6 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP4 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP1 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP3 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP7 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP0 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP5 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP2 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP6 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 11:17:26 DP4 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP1 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP5 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP1 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP0 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP7 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP3 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP2 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP6 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP5 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP4 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP0 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP7 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP3 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP2 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP6 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:26 DP4 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
Capturing batches (bs=128 avail_mem=42.47 GB):  63%|   | 33/52 [00:21<00:08,  2.34it/s]Capturing batches (bs=120 avail_mem=42.47 GB):  63%|   | 33/52 [00:21<00:08,  2.34it/s][aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP5 TP5] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP0 TP0] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP6 TP6] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP1 TP1] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP2 TP2] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP7 TP7] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP4 TP4] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP3 TP3] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=42.47 GB):  65%|   | 34/52 [00:22<00:08,  2.08it/s]Capturing batches (bs=112 avail_mem=42.46 GB):  65%|   | 34/52 [00:22<00:08,  2.08it/s][aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP1 TP1] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP0 TP0] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP5 TP5] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP7 TP7] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP3 TP3] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP2 TP2] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP6 TP6] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:27 DP4 TP4] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=42.46 GB):  67%|   | 35/52 [00:22<00:07,  2.20it/s]Capturing batches (bs=104 avail_mem=42.46 GB):  67%|   | 35/52 [00:22<00:07,  2.20it/s][aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP1 TP1] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP4 TP4] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP0 TP0] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP7 TP7] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP2 TP2] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP5 TP5] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP6 TP6] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP3 TP3] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=42.46 GB):  69%|   | 36/52 [00:23<00:07,  2.13it/s]Capturing batches (bs=96 avail_mem=42.45 GB):  69%|   | 36/52 [00:23<00:07,  2.13it/s] [aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP1 TP1] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP0 TP0] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP3 TP3] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP4 TP4] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP5 TP5] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP7 TP7] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP2 TP2] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:28 DP6 TP6] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=42.45 GB):  71%|   | 37/52 [00:23<00:06,  2.24it/s]Capturing batches (bs=88 avail_mem=42.45 GB):  71%|   | 37/52 [00:23<00:06,  2.24it/s][aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP4 TP4] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP0 TP0] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP6 TP6] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP3 TP3] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP5 TP5] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP7 TP7] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP2 TP2] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP1 TP1] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=42.45 GB):  73%|  | 38/52 [00:24<00:06,  2.16it/s]Capturing batches (bs=80 avail_mem=42.44 GB):  73%|  | 38/52 [00:24<00:06,  2.16it/s][aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP1 TP1] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP0 TP0] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP3 TP3] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP5 TP5] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP7 TP7] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP2 TP2] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP6 TP6] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:29 DP4 TP4] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=42.44 GB):  75%|  | 39/52 [00:24<00:05,  2.24it/s]Capturing batches (bs=72 avail_mem=42.43 GB):  75%|  | 39/52 [00:24<00:05,  2.24it/s][aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP5 TP5] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP6 TP6] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP4 TP4] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP7 TP7] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP1 TP1] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP2 TP2] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP0 TP0] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP3 TP3] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=42.43 GB):  77%|  | 40/52 [00:25<00:05,  2.16it/s]Capturing batches (bs=64 avail_mem=42.43 GB):  77%|  | 40/52 [00:25<00:05,  2.16it/s][aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP0 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP3 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP5 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP4 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP7 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP6 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP2 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP0 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP3 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP5 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP7 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP4 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP6 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP2 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP1 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP1 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP0 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP5 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP3 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP7 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP0 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP2 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP4 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP5 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP6 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP3 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP7 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP2 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP4 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP6 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP1 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP0 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP1 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:17:30 DP5 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP3 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP7 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP4 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP2 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP6 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:30 DP1 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=42.43 GB):  79%|  | 41/52 [00:25<00:04,  2.25it/s]Capturing batches (bs=56 avail_mem=42.42 GB):  79%|  | 41/52 [00:25<00:04,  2.25it/s][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP1 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP6 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP4 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP2 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP0 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP7 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP5 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP3 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=42.42 GB):  81%|  | 42/52 [00:26<00:04,  2.04it/s]Capturing batches (bs=48 avail_mem=42.41 GB):  81%|  | 42/52 [00:26<00:04,  2.04it/s][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP0 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP5 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP3 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP2 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP6 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP1 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP7 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:31 DP4 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=42.41 GB):  83%| | 43/52 [00:26<00:04,  2.15it/s]Capturing batches (bs=40 avail_mem=42.41 GB):  83%| | 43/52 [00:26<00:04,  2.15it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:32 DP0 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:32 DP1 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:32 DP2 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:32 DP3 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:32 DP6 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:32 DP7 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:32 DP4 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:17:32 DP5 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=42.41 GB):  85%| | 44/52 [00:27<00:04,  1.99it/s]Capturing batches (bs=32 avail_mem=42.40 GB):  85%| | 44/52 [00:27<00:04,  1.99it/s][rank3]:W1027 11:17:34.929000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:17:34.935000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:17:34.946000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1027 11:17:34.957000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1027 11:17:34.967000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1027 11:17:34.978000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1027 11:17:34.986000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1027 11:17:35.000000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1027 11:17:35.004000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:17:35.010000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1027 11:17:35.022000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1027 11:17:35.032000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:17:35.043000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:17:35.054000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:17:35.062000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:17:35.076000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:17:35.100000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:17:35.106000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:17:35.119000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:17:35.128000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:17:35.140000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:17:35.151000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:17:35.159000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:17:35.173000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1027 11:17:35.664000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:17:35.672000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:17:35.681000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:17:35.692000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:17:35.701000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:17:35.711000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:17:35.722000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:17:35.734000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:17:35.777000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:17:35.784000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:17:35.793000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:17:35.808000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:17:35.813000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:17:35.843000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:17:35.851000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:17:35.861000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:17:35.865000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:17:35.868000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:17:35.878000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:17:35.894000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:17:35.898000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:17:35.911000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:17:35.918000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:17:35.932000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:17:35.935000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:17:35.944000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:17:35.944000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:17:35.965000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:17:35.971000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:17:35.985000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:17:35.988000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:17:35.998000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:17:36.464000 822 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank3]:W1027 11:17:36.470000 818 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank1]:W1027 11:17:36.474000 816 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank4]:W1027 11:17:36.489000 819 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank6]:W1027 11:17:36.508000 821 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank2]:W1027 11:17:36.508000 817 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank5]:W1027 11:17:36.519000 820 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank0]:W1027 11:17:36.523000 815 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0109 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_23 0.0109 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0110 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0111 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0111 ms 94.9% 
  triton_bmm_15 0.0112 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0116 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0117 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0117 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2827 seconds and 0.6092 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0104 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0106 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0107 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_19 0.0107 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0107 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0111 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0112 ms 92.8% 
  triton_bmm_22 0.0114 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_16 0.0114 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0116 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2513 seconds and 0.6463 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0108 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0110 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_23 0.0110 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0111 ms 97.5% 
  triton_bmm_21 0.0111 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0112 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0115 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_9 0.0118 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0118 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_22 0.0119 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2940 seconds and 0.6214 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0106 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_23 0.0107 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0109 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0109 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0110 ms 95.6% 
  triton_bmm_15 0.0111 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0115 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_22 0.0115 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0117 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3022 seconds and 0.6421 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0106 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0107 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0108 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_25 0.0109 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0111 ms 94.9% 
  triton_bmm_15 0.0111 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0114 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_16 0.0115 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0116 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3332 seconds and 0.4384 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0107 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0110 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_23 0.0111 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0111 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0113 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0113 ms 94.3% 
  triton_bmm_15 0.0113 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_9 0.0116 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0117 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_22 0.0119 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2501 seconds and 0.8414 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0106 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0107 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0107 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0108 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_19 0.0108 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0110 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0114 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0114 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0115 ms 92.7% 
  triton_bmm_22 0.0117 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2589 seconds and 0.5282 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0104 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0106 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0107 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_19 0.0108 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0108 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0113 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0114 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_16 0.0115 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0116 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_20 0.0117 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3035 seconds and 0.6097 seconds precompiling for 27 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1027 11:17:48.121000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:17:48.267000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:17:48.281000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:17:48.466000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:17:48.644000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:17:48.734000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:17:48.787000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:17:48.799000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:17:48.974000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:17:49.111000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:17:49.249000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:17:49.359000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:17:49.365000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1027 11:17:49.618000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1027 11:17:49.877000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:17:49.886000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:17:50.029000 816 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1027 11:17:50.301000 822 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank5]:W1027 11:17:50.316000 820 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1027 11:17:50.497000 821 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1027 11:17:50.957000 817 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank4]:W1027 11:17:51.148000 819 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank3]:W1027 11:17:51.583000 818 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank0]:W1027 11:17:51.623000 815 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0104 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0104 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0104 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_37 0.0107 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0108 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0109 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0110 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0113 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0115 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3227 seconds and 0.6168 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0102 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_46 0.0103 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0104 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0107 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0107 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0108 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0109 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0109 ms 92.3% 
SingleProcess AUTOTUNE benchmarking takes 5.3900 seconds and 0.6159 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_37 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0103 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_31 0.0106 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_51 0.0107 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0107 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0107 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_45 0.0111 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3914 seconds and 0.6148 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0103 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_46 0.0103 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0103 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0105 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0108 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0108 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0109 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0112 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0112 ms 91.8% 
SingleProcess AUTOTUNE benchmarking takes 5.2505 seconds and 0.6134 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_46 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0101 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_37 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_41 0.0103 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0105 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0105 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0105 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0106 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_45 0.0109 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3495 seconds and 0.6119 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0104 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_48 0.0105 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0105 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0106 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0109 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0110 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0110 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0111 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0114 ms 90.8% 
SingleProcess AUTOTUNE benchmarking takes 5.3372 seconds and 0.6457 seconds precompiling for 27 choices
[rank1]:W1027 11:17:57.306000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:17:57.380000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:17:57.477000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0103 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0103 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0104 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_37 0.0105 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0107 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0109 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0109 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0109 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0112 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3041 seconds and 0.6426 seconds precompiling for 27 choices
[rank7]:W1027 11:17:57.651000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_46 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_41 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_37 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0103 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_31 0.0106 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_51 0.0106 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0107 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0107 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_45 0.0110 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3107 seconds and 0.6322 seconds precompiling for 27 choices
[rank7]:W1027 11:17:57.726000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:17:57.824000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:17:57.971000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:17:57.983000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:17:58.049000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:17:58.058000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:17:58.149000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:17:58.156000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:17:58.455000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:17:58.532000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:17:58.664000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:17:58.696000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:17:58.771000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:17:58.868000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:17:59.166000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:17:59.241000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:17:59.339000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:17:59.356000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:17:59.431000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:17:59.531000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:01.844000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:01.920000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:02.020000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:02 DP7 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1027 11:18:02.214000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:02.289000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:02.399000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:02 DP1 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1027 11:18:02.887000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:02.891000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:02.961000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:02.964000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:03.059000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:03.061000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:03 DP5 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:03 DP6 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1027 11:18:03.342000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:03.411000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:03.416000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:03.485000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:03.514000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:03 DP3 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank4]:W1027 11:18:03.582000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:03 DP4 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1027 11:18:03.707000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:03.782000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:03.880000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:03 DP2 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank0]:W1027 11:18:04.261000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:04.343000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:04.446000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:04 DP0 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank4]:W1027 11:18:05.017000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:05.029000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:05.039000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:05.055000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:05.065000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:05.080000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:05.091000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:05.092000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:05.102000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:05.113000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:05.130000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:05.140000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:05.155000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:05.166000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:05.188000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:05.199000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:05.211000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:05.228000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:05.239000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:05 DP4 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1027 11:18:05.256000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:05.265000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:05 DP1 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:05 DP6 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:05 DP2 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:05 DP7 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:05 DP5 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:05 DP3 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank0]:W1027 11:18:06.114000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:06.188000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:06.285000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:18:06 DP0 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1027 11:18:06.852000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:06.860000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:06.869000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:06.878000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:06.891000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:06.900000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:06.911000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:06.941000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:07.091000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:07.102000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:07.110000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:07.119000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:07.131000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:07.141000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:07.152000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:07.174000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:07.332000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:07.354000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:07.362000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:07.372000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:07.383000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:07.393000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:07.402000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:07.413000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:18:07 DP6 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:18:07 DP2 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:18:07 DP3 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:18:07 DP4 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:18:07 DP7 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:18:07 DP5 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:18:07 DP1 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:18:07 DP0 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1027 11:18:09.010000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:09.024000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:09.030000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:09.042000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:09.051000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:09.060000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:09.067000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:09.078000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:09.086000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:09.104000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:09.116000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:09.127000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:09.135000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:09.143000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:09.143000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:09.154000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:09.161000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:09.179000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:09.191000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:09.203000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:09.211000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:18:09 DP3 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[rank7]:W1027 11:18:09.219000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:09.219000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:09.230000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:18:09 DP4 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:18:09 DP1 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:18:09 DP6 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:18:09 DP5 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:18:09 DP7 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:18:09 DP2 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:18:09 DP0 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0146 ms 100.0% 
  triton_mm_55 0.0276 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0422 ms 34.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0455 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0583 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0584 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0610 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0753 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2570 seconds and 0.5654 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0145 ms 100.0% 
  triton_mm_55 0.0283 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 34.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0458 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0533 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0583 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0585 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0611 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0764 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2112 seconds and 0.6408 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0148 ms 100.0% 
  triton_mm_55 0.0275 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 35.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0456 ms 32.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_67 0.0582 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_69 0.0583 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_66 0.0610 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0752 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2465 seconds and 0.7708 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0148 ms 100.0% 
  triton_mm_55 0.0313 ms 47.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0419 ms 35.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0455 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0504 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0532 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0581 ms 25.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0582 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0609 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_59 0.0835 ms 17.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.2223 seconds and 0.5250 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0144 ms 100.0% 
  triton_mm_55 0.0283 ms 50.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0420 ms 34.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0456 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0505 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0533 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0584 ms 24.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0609 ms 23.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0757 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2289 seconds and 0.7890 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0148 ms 100.0% 
  triton_mm_55 0.0319 ms 46.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0423 ms 35.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0536 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_67 0.0582 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_69 0.0582 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_66 0.0610 ms 24.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_59 0.0837 ms 17.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.2137 seconds and 0.7377 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0148 ms 100.0% 
  triton_mm_55 0.0316 ms 46.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0422 ms 35.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_67 0.0582 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_69 0.0582 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_66 0.0609 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0834 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2094 seconds and 0.7277 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0137 ms 100.0% 
  triton_mm_55 0.0279 ms 49.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0422 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0505 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 25.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 23.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0582 ms 23.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0611 ms 22.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0754 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1889 seconds and 0.4607 seconds precompiling for 39 choices
[rank5]:W1027 11:18:20.179000 820 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef532bfc0>
[rank3]:W1027 11:18:20.200000 818 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ebbfc0>
[rank5]:W1027 11:18:20.211000 820 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef44bad30>
[rank4]:W1027 11:18:20.223000 819 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6730>
[rank3]:W1027 11:18:20.232000 818 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ff3f00>
[rank7]:W1027 11:18:20.264000 822 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f67434abfc0>
[rank0]:W1027 11:18:20.283000 815 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356ac40>
[rank7]:W1027 11:18:20.296000 822 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f673706ee20>
[rank4]:W1027 11:18:20.301000 819 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6e50>
[rank2]:W1027 11:18:20.304000 817 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5af70>
[rank1]:W1027 11:18:20.314000 816 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2bb0>
[rank0]:W1027 11:18:20.315000 815 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356aca0>
[rank2]:W1027 11:18:20.336000 817 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5b2a0>
[rank1]:W1027 11:18:20.346000 816 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2f40>
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:18:20 DP5 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:18:20 DP3 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:18:20 DP7 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:18:20 DP4 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:18:20 DP2 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:18:20 DP1 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:18:20 DP0 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1027 11:18:20.537000 821 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce10dac10>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1027 11:18:20.569000 821 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce2168ae0>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:18:20 DP6 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1027 11:18:22.154000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:22.164000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:22.173000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:22.183000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:22.197000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:22.243000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:22.258000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:22.267000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:22.402000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:22.411000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:22.421000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:22.431000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:22.440000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:22.483000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:22.503000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:22.513000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:22.649000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:22.657000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:22.665000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:22.674000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:22.684000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:22.722000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:22.742000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:22.755000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:22.895000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:22.904000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:22.912000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:22.921000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:22.931000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:22.962000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:22.978000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:22.991000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:23.139000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:23.149000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:23.159000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:23.170000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:23.179000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:23.203000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:23.217000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:23.282000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:23.415000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:23.424000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:23.433000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:23.442000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:23.450000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:23.462000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:23.470000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:23.524000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:23.654000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:23.666000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:23.674000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:23.687000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:23.697000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:23.707000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:23.717000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:23.763000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:23.898000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:23.910000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:23.931000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:23.941000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:23.952000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:23.962000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:23.971000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:23.999000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:24.141000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:24.153000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:24.175000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:24.186000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:24.196000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:24.205000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:24.216000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:24.235000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:24.386000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:24.424000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:24.438000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:24.450000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:24.458000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:24.475000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:24.486000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:24.498000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:24.630000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:24.668000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:24.698000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:24.706000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:24.717000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:24.729000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:24.741000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:24.750000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:24.912000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:24.926000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:24.945000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:24.954000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:24.964000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:24.976000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:24.987000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:24.998000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:25.169000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:25.189000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:25.197000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:25.209000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:25.219000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:25.231000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:25.241000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:25.251000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:25.413000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:25.433000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:25.441000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:25.455000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:25.465000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:25.477000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:25.489000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:25.497000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:25.657000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:25.677000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:25.685000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:25.700000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:25.709000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:25.721000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:25.733000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:25.741000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:25.901000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:25.921000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:25.929000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:25.943000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:25.954000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:25.965000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:25.977000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:25.985000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:26.145000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:26.165000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:26.173000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:26.187000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:26.198000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:26.209000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:26.221000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:26.229000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:26.389000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:26.409000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:26.417000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:26.427000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:26.438000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:26.450000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:26.462000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:26.470000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:26.635000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:26.655000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:26.665000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:26.682000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:26.701000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:26.712000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:26.721000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:26.751000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:26.891000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:26.900000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:26.909000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:26.941000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:26.953000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:26.961000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:26.972000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:26.993000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:27.135000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:27.144000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:27.153000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:27.185000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:27.197000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:27.205000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:27.213000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:27.233000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:27.379000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:27.391000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:27.429000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:27.441000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:27.449000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:27.459000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:27.468000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:27.478000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:27.627000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:27.639000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:27.677000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:27.688000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:27.696000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:27.707000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:27.716000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:27.726000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:27.872000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:27.926000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:27.935000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:27.945000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:27.956000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:27.969000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:27.993000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:28.021000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:28.155000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:28.173000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:28.195000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:28.204000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:28.213000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:28.233000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:28.248000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:28.265000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:28.430000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:28.444000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:28.453000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:28.463000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:28.472000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:28.486000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:28.498000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:28.509000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:28.690000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:28.699000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:28.707000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:28.719000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:28.731000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:28.743000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:28.753000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:28.763000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:28.937000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:28.946000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:28.954000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:28.965000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:28.976000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:28.987000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:28.998000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:29.007000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:29.185000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:29.193000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:29.202000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:29.212000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:29.223000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:29.234000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:29.246000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:29.254000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:29.433000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:29.441000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:29.450000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:29.461000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:29.471000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:29.482000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:29.493000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:29.502000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:29.684000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:29.693000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:29.703000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:29.712000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:29.721000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:29.731000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:29.744000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:29.751000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:29.927000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:29.936000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:29.947000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:29.956000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:29.964000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:29.973000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:29.985000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:29.994000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:30.171000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:30.181000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:30.191000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:30.199000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:30.208000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:30.218000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:30.230000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:30.238000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:30.418000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:30.426000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:30.435000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:30.464000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:30.474000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:30.483000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:30.494000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:30.511000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:30.665000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:30.673000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:30.681000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:30.707000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:30.718000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:30.728000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:30.739000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:30.751000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:30.913000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:30.921000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:30.929000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:30.947000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:30.963000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:30.972000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:30.983000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:30.996000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:31.162000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:31.174000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:31.204000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:31.213000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:31.230000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:31.240000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:31.250000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:31.262000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:31.411000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:31.421000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:31.446000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:31.459000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:31.476000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:31.484000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:31.494000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:31.504000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:31.668000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:31.703000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:31.719000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:31.729000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:31.738000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:31.747000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:31.757000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:31.805000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:31.943000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:31.963000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:31.975000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:31.985000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:31.993000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:32.003000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:32.015000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:32.048000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:32.187000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:32.207000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:32.219000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:32.233000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:32.241000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:32.251000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:32.263000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:32.292000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:32.455000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:32.467000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:32.481000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:32.489000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:32.502000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:32.510000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:32.522000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:32.537000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:32.699000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:32.715000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:32.743000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:32.751000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:32.767000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:32.775000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:32.792000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:32.803000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:32.944000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:32.959000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:32.983000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:32.997000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:33.011000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:33.020000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:33.036000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:33.049000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:33.187000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:33.203000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:33.223000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:33.241000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:33.255000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:33.265000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:33.280000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:33.293000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:33.432000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:33.447000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:33.463000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:33.497000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:33.507000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:33.517000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:33.537000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:33.550000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:33.687000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:33.697000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:33.706000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:33.741000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:33.752000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:33.761000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:33.781000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:33.794000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:33.931000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:33.941000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:33.951000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:33.986000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:33.996000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:34.005000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:34.025000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:34.037000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:34.175000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:34.185000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:34.195000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:34.230000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:34.240000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:34.249000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:34.269000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:34.282000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:34.419000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:34.430000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:34.439000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:34.482000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:34.495000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:34.514000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:34.531000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:34.543000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:34.677000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:34.685000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:34.693000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:34.726000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:34.739000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:34.758000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:34.775000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:34.787000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:34.925000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:34.933000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:34.941000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:34.970000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:34.983000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:34.999000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:35.015000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:35.027000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:35.186000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:35.194000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:35.218000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:35.236000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:35.245000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:35.254000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:35.267000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:35.279000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:35.439000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:35.454000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:35.465000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:35.478000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:35.491000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:35.499000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:35.510000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:35.520000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:35.688000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:35.699000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:35.711000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:35.721000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:35.735000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:35.745000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:35.755000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:35.765000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:35.946000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:35.958000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:35.978000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:36.006000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:36.016000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:36.027000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:36.044000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:36.067000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:36.202000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:36.226000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:36.258000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:36.268000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:36.279000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:36.290000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:36.300000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:36.311000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:37.056000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:37.065000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:37.076000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:37.095000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:37.095000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:37.112000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:37.115000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:37.151000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1771 ms 100.0% 
  triton_mm_127 0.2486 ms 71.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2564 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2621 ms 67.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2656 ms 66.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2676 ms 66.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2704 ms 65.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2747 ms 64.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2804 ms 63.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.3002 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9332 seconds and 0.5866 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1806 ms 100.0% 
  triton_mm_127 0.2452 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2523 ms 71.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2579 ms 70.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2612 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2640 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_107 0.2650 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_125 0.2651 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2758 ms 65.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_120 0.2931 ms 61.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.8271 seconds and 0.3449 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1824 ms 100.0% 
  triton_mm_127 0.2488 ms 73.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2562 ms 71.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2616 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2654 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2666 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2685 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2741 ms 66.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2858 ms 63.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2988 ms 61.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9493 seconds and 0.2850 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1805 ms 100.0% 
  triton_mm_127 0.2465 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2543 ms 71.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2594 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2639 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2646 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_124 0.2646 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2675 ms 67.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2745 ms 65.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2939 ms 61.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9556 seconds and 0.4924 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1829 ms 100.0% 
  triton_mm_127 0.2482 ms 73.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2567 ms 71.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2622 ms 69.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2655 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2667 ms 68.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2679 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2746 ms 66.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2838 ms 64.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_120 0.2986 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9511 seconds and 0.4277 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1787 ms 100.0% 
  triton_mm_127 0.2469 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2552 ms 70.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2613 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2642 ms 67.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2660 ms 67.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_107 0.2693 ms 66.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_125 0.2698 ms 66.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2795 ms 63.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_120 0.2965 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9560 seconds and 0.5553 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1806 ms 100.0% 
  triton_mm_127 0.2444 ms 73.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2522 ms 71.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2594 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2621 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2653 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2655 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_107 0.2695 ms 67.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2810 ms 64.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_121 0.2924 ms 61.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.8718 seconds and 0.3724 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1851 ms 100.0% 
  triton_mm_127 0.2496 ms 74.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2576 ms 71.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2619 ms 70.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2665 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_124 0.2667 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2714 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2740 ms 67.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2855 ms 64.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2998 ms 61.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9465 seconds and 0.4405 seconds precompiling for 39 choices
Capturing batches (bs=32 avail_mem=42.40 GB):  87%| | 45/52 [01:47<02:51, 24.54s/it]Capturing batches (bs=24 avail_mem=41.73 GB):  87%| | 45/52 [01:47<02:51, 24.54s/it][rank7]:W1027 11:18:54.630000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:54.647000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:54.660000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:54.675000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:54.687000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:54.695000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:54.697000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:54.700000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:54.714000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:54.727000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:54.742000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:54.756000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:54.767000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:54.771000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:54.776000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:54.785000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:54.798000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:54.823000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:54.836000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:54.845000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:54.848000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:55.124000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:55.191000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:55.257000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:55.262000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:55.273000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:55.285000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:55.314000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:55.323000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:55.333000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:55.337000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:55.345000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:55.353000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:55.368000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:55.393000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:55.405000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:55.405000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:55.411000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:55.422000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:55.427000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:55.436000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:55.462000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:55.472000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:55.474000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:55.480000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:18:55.489000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:55.496000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:18:55.503000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:55.529000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:18:55.542000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:18:55.547000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:18:55.564000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:55.753000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:55.832000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:18:55.899000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:18:55.932000 822 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank6]:W1027 11:18:55.950000 821 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank5]:W1027 11:18:55.964000 820 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank4]:W1027 11:18:55.966000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:18:55.991000 818 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank1]:W1027 11:18:56.005000 816 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank2]:W1027 11:18:56.007000 817 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank0]:W1027 11:18:56.027000 815 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank4]:W1027 11:18:56.474000 819 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_151 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0099 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0100 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_145 0.0101 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0101 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_144 0.0101 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_148 0.0101 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0102 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0102 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0104 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2942 seconds and 0.4252 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_144 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_145 0.0103 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_151 0.0103 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0104 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0104 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0104 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0105 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0107 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0109 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2950 seconds and 0.4128 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0104 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0105 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_151 0.0105 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_148 0.0105 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0106 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0107 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0107 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_149 0.0107 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0109 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3268 seconds and 0.3741 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_151 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0104 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0104 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0104 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0105 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0105 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0106 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0108 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0111 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2980 seconds and 0.3185 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_144 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_145 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_151 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_148 0.0102 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0103 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_153 0.0103 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_143 0.0103 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0103 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0104 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3031 seconds and 0.5099 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0101 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0101 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0101 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0102 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0104 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0105 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2369 seconds and 0.4009 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_144 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_145 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_151 0.0100 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_148 0.0101 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0101 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0101 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0101 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_149 0.0103 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0103 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2643 seconds and 0.3880 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0102 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0102 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_147 0.0103 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_151 0.0103 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0104 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_148 0.0104 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0105 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2790 seconds and 0.3682 seconds precompiling for 27 choices
[rank1]:W1027 11:19:06.793000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:06.898000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:06.988000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:07.187000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:07.306000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:07.321000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:07.409000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:07.491000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:07.692000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:07.791000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:07.819000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:08.114000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:08.291000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:08.584000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:08.611000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:08.744000 816 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank7]:W1027 11:19:08.782000 822 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank6]:W1027 11:19:09.088000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:09.162000 820 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank3]:W1027 11:19:09.294000 818 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank2]:W1027 11:19:09.404000 817 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank4]:W1027 11:19:09.623000 819 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank0]:W1027 11:19:09.948000 815 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank6]:W1027 11:19:10.563000 821 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_165 0.0104 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0105 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_174 0.0105 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_175 0.0107 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0108 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0109 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0111 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0114 ms 87.3% 
SingleProcess AUTOTUNE benchmarking takes 5.3209 seconds and 0.5326 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0098 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0101 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0101 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0102 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_179 0.0104 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_175 0.0105 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0105 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0107 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0110 ms 89.1% 
SingleProcess AUTOTUNE benchmarking takes 5.2889 seconds and 0.5347 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0099 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_165 0.0103 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0103 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_174 0.0104 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_175 0.0107 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0108 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0108 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_179 0.0108 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0110 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2902 seconds and 0.5253 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_165 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0103 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_176 0.0104 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_159 0.0107 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_175 0.0108 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0108 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0109 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_173 0.0111 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3058 seconds and 0.5435 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0099 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0100 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0100 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0102 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_159 0.0103 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_179 0.0104 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_175 0.0104 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0105 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_173 0.0107 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2871 seconds and 0.5486 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0102 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_176 0.0102 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0105 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_175 0.0107 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0108 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0110 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0112 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2454 seconds and 0.5496 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_165 0.0100 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0100 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_167 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_179 0.0103 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_176 0.0103 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_175 0.0104 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0104 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0107 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_173 0.0109 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2853 seconds and 0.5192 seconds precompiling for 27 choices
[rank1]:W1027 11:19:15.853000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:15.928000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:16.024000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:16.211000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:16.279000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:16.288000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:16.353000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:16.381000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:16.387000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0099 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_174 0.0102 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0104 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0105 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_175 0.0107 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0107 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0109 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0109 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0112 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2893 seconds and 0.5210 seconds precompiling for 27 choices
[rank7]:W1027 11:19:16.451000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:16.456000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:16.474000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:16.550000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:16.555000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:16.648000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:16.945000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:17.021000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:17.120000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:17.282000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:17.358000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:17.457000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:17.587000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:17.662000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:17.760000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:19.868000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:19.943000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:20.043000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:20.921000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:20.964000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:20.997000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:21.039000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:21.099000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:21.115000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:21.139000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:21.169000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:21.247000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:21.297000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:21.400000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:21.454000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:21.578000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:21.653000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:21.719000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:21.796000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:21.853000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:21.897000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:22.146000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:22.220000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:22.319000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:22.893000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:22.909000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:22.919000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:22.938000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:22.948000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:22.967000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:22.984000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:22.998000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:23.013000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:23.037000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:23.088000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:23.107000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:23.115000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:23.115000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:23.138000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:23.173000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:23.217000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:23.240000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:23.532000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:23.627000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:23.711000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:23.714000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:23.810000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:23.813000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:24.388000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:24.398000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:24.409000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:24.419000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:24.427000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:24.437000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:24.459000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:24.468000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:24.634000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:24.643000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:24.654000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:24.672000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:24.682000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:24.693000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:24.704000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:24.713000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:24.882000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:24.890000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:24.902000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:24.920000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:24.931000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:24.942000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:24.953000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:24.963000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:26.480000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:26.501000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:26.522000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:26.546000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:26.556000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:26.572000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:26.576000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:26.581000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:26.589000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:26.597000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:26.604000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:26.623000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:26.624000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:26.645000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:26.647000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:26.657000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:26.664000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:26.672000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:26.696000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:26.706000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:26.713000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:26.856000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:26.930000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:26.978000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0133 ms 100.0% 
  triton_mm_183 0.0276 ms 48.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0410 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0457 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0499 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0535 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0584 ms 22.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0598 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0756 ms 17.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2067 seconds and 0.6458 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0135 ms 100.0% 
  triton_mm_183 0.0276 ms 48.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0410 ms 32.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0499 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0583 ms 23.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0596 ms 22.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 22.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0754 ms 17.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2048 seconds and 0.7215 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0137 ms 100.0% 
  triton_mm_183 0.0276 ms 49.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 33.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0456 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0496 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 25.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 23.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0598 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0612 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0752 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1924 seconds and 0.5369 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0131 ms 100.0% 
  triton_mm_183 0.0276 ms 47.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0410 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0457 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 22.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0596 ms 22.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0612 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0753 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2381 seconds and 0.6456 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0133 ms 100.0% 
  triton_mm_183 0.0275 ms 48.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0533 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0596 ms 22.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0754 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1955 seconds and 0.7364 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0129 ms 100.0% 
  triton_mm_183 0.0274 ms 46.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0407 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0453 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0495 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0532 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0580 ms 22.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0596 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0611 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0754 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2457 seconds and 0.6698 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0132 ms 100.0% 
  triton_mm_183 0.0277 ms 47.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0410 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0457 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0498 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0583 ms 22.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0596 ms 22.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0755 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2304 seconds and 0.7083 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0132 ms 100.0% 
  triton_mm_183 0.0277 ms 47.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0409 ms 32.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0456 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0498 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0533 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0583 ms 22.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0598 ms 22.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0755 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1970 seconds and 0.4380 seconds precompiling for 39 choices
[rank1]:W1027 11:19:37.292000 816 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2bb0>
[rank1]:W1027 11:19:37.315000 816 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2f40>
[rank5]:W1027 11:19:37.361000 820 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef532bfc0>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:19:37 DP1 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1027 11:19:37.384000 820 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef44bad30>
[rank2]:W1027 11:19:37.401000 817 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5af70>
[rank3]:W1027 11:19:37.443000 818 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ebbfc0>
[rank4]:W1027 11:19:37.448000 819 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6730>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:19:37 DP5 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1027 11:19:37.457000 822 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f67434abfc0>
[rank3]:W1027 11:19:37.466000 818 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ff3f00>
[rank4]:W1027 11:19:37.471000 819 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6e50>
[rank7]:W1027 11:19:37.480000 822 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f673706ee20>
[rank0]:W1027 11:19:37.481000 815 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356ac40>
[rank0]:W1027 11:19:37.505000 815 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356aca0>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:19:37 DP3 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:19:37 DP4 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1027 11:19:37.539000 817 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5b2a0>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:19:37 DP7 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:19:37 DP2 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1027 11:19:37.704000 821 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce10dac10>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:19:37 DP0 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1027 11:19:37.727000 821 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce2168ae0>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:19:37 DP6 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1027 11:19:39.086000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:39.095000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:39.105000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:39.116000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:39.126000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:39.134000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:39.142000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:39.201000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:39.345000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:39.356000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:39.383000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:39.393000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:39.403000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:39.451000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:39.491000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:39.502000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:39.640000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:39.651000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:39.661000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:39.670000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:39.698000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:39.738000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:39.750000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:39.771000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:39.912000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:39.922000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:39.931000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:39.941000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:39.950000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:39.982000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:39.995000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:40.019000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:40.168000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:40.180000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:40.190000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:40.200000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:40.209000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:40.226000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:40.239000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:40.263000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:40.416000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:40.428000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:40.437000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:40.450000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:40.459000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:40.470000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:40.482000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:40.507000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:40.664000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:40.676000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:40.686000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:40.702000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:40.711000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:40.720000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:40.731000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:40.751000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:40.912000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:40.924000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:40.933000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:40.954000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:40.963000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:40.972000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:40.982000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:40.995000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:41.156000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:41.171000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:41.181000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:41.206000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:41.215000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:41.224000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:41.235000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:41.246000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:41.398000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:41.418000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:41.426000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:41.460000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:41.471000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:41.481000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:41.493000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:41.503000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:41.646000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:41.666000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:41.679000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:41.709000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:41.719000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:41.730000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:41.742000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:41.752000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:41.902000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:41.926000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:41.934000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:41.955000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:41.965000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:41.975000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:41.986000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:42.000000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:42.152000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:42.176000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:42.186000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:42.206000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:42.215000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:42.225000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:42.235000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:42.247000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:42.406000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:42.434000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:42.445000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:42.460000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:42.470000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:42.481000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:42.494000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:42.503000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:42.654000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:42.686000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:42.698000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:42.709000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:42.719000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:42.731000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:42.743000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:42.753000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:42.902000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:42.938000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:42.949000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:42.960000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:42.972000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:42.982000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:42.994000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:43.004000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:43.150000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:43.190000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:43.201000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:43.211000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:43.223000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:43.234000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:43.246000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:43.255000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:43.398000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:43.442000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:43.453000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:43.463000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:43.474000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:43.485000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:43.497000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:43.507000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:43.646000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:43.694000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:43.705000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:43.716000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:43.726000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:43.737000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:43.750000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:43.759000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:43.898000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:43.946000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:43.957000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:43.967000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:43.981000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:43.995000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:44.007000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:44.017000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:44.158000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:44.210000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:44.220000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:44.228000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:44.239000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:44.250000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:44.262000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:44.272000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:44.410000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:44.462000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:44.473000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:44.482000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:44.493000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:44.504000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:44.517000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:44.527000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:44.718000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:44.729000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:44.740000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:44.747000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:44.759000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:44.771000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:44.781000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:44.811000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:45.012000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:45.020000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:45.030000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:45.042000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:45.050000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:45.062000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:45.111000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:45.125000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:45.304000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:45.314000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:45.325000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:45.333000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:45.343000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:45.366000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:45.378000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:45.413000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:45.603000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:45.613000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:45.624000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:45.633000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:45.644000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:45.655000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:45.666000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:45.715000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:45.858000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:45.868000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:45.879000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:45.890000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:45.900000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:45.911000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:45.922000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:45.972000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:46.186000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:46.197000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:46.206000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:46.215000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:46.223000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:46.236000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:46.273000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:46.298000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:46.509000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:46.518000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:46.528000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:46.538000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:46.547000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:46.558000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:46.568000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:46.616000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:46.774000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:46.784000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:46.794000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:46.804000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:46.816000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:46.825000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:46.835000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:46.863000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:47.030000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:47.040000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:47.049000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:47.060000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:47.071000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:47.082000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:47.092000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:47.111000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:47.290000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:47.300000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:47.310000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:47.320000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:47.331000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:47.342000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:47.351000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:47.363000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:47.546000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:47.556000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:47.566000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:47.576000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:47.586000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:47.596000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:47.607000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:47.619000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:47.802000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:47.812000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:47.822000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:47.833000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:47.843000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:47.853000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:47.865000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:47.876000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:48.058000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:48.068000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:48.077000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:48.088000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:48.099000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:48.108000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:48.120000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:48.130000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:48.314000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:48.324000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:48.334000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:48.344000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:48.354000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:48.364000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:48.375000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:48.387000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:48.570000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:48.580000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:48.590000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:48.601000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:48.610000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:48.626000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:48.635000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:48.648000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:48.826000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:48.836000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:48.846000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:48.856000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:48.866000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:48.879000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:48.888000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:48.901000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:49.082000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:49.092000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:49.102000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:49.112000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:49.122000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:49.133000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:49.143000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:49.154000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:49.338000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:49.348000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:49.358000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:49.368000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:49.378000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:49.388000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:49.400000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:49.411000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:49.594000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:49.604000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:49.614000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:49.624000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:49.635000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:49.643000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:49.656000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:49.667000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:49.850000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:49.861000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:49.870000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:49.880000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:49.891000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:49.899000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:49.911000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:49.922000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:50.106000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:50.116000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:50.126000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:50.136000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:50.147000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:50.155000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:50.167000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:50.178000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:50.362000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:50.373000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:50.382000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:50.392000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:50.403000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:50.412000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:50.422000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:50.435000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:50.618000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:50.632000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:50.641000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:50.651000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:50.662000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:50.671000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:50.682000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:50.695000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:50.874000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:50.888000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:50.897000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:50.908000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:50.920000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:50.928000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:50.939000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:50.951000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:51.134000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:51.144000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:51.154000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:51.164000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:51.174000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:51.184000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:51.195000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:51.207000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:51.394000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:51.404000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:51.413000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:51.424000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:51.436000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:51.444000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:51.456000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:51.467000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:51.716000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:51.725000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:51.734000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:51.742000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:51.751000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:51.760000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:51.770000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:51.825000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:52.049000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:52.058000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:52.066000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:52.076000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:52.085000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:52.097000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:52.120000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:52.159000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:52.376000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:52.386000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:52.395000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:52.404000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:52.413000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:52.422000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:52.433000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:52.487000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:52.633000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:52.644000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:52.654000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:52.667000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:52.680000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:52.691000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:52.743000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:52.819000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:52.966000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:52.976000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:52.987000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:52.996000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:53.007000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:53.016000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:53.026000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:53.075000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:53.222000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:53.233000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:53.244000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:53.253000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:53.265000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:53.276000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:53.286000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:53.331000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:53.587000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:53.598000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:53.608000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:53.620000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:53.629000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:53.639000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:53.692000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:53.701000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:53.902000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:53.912000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:53.921000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:53.931000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:53.941000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:53.951000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:53.962000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:54.012000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:54.158000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:54.172000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:54.182000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:54.192000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:54.204000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:54.213000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:54.224000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:54.263000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:19:54.947000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:19:54.958000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:19:54.969000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:19:54.979000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:19:54.988000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:19:54.996000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:19:55.005000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:19:55.015000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1468 ms 100.0% 
  triton_mm_235 0.2258 ms 65.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2264 ms 64.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2426 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2491 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2509 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2534 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2560 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2562 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2672 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8190 seconds and 0.5787 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1473 ms 100.0% 
  triton_mm_234 0.2161 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_235 0.2185 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_233 0.2398 ms 61.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_227 0.2443 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_255 0.2448 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2484 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2507 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2528 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2628 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8120 seconds and 0.4732 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1476 ms 100.0% 
  triton_mm_235 0.2250 ms 65.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2253 ms 65.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2429 ms 60.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2479 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2504 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2533 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2544 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2553 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2650 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8977 seconds and 0.5929 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1492 ms 100.0% 
  triton_mm_235 0.2234 ms 66.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2234 ms 66.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2420 ms 61.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2465 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2502 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2527 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2553 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_227 0.2558 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_252 0.2680 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8282 seconds and 0.6897 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1470 ms 100.0% 
  triton_mm_235 0.2207 ms 66.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2223 ms 66.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2407 ms 61.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2432 ms 60.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2472 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2502 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2507 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_227 0.2525 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_252 0.2585 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8017 seconds and 0.5716 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1442 ms 100.0% 
  triton_mm_235 0.2215 ms 65.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2225 ms 64.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2418 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2471 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2493 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_227 0.2519 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2521 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2541 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2629 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8154 seconds and 0.3961 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1501 ms 100.0% 
  triton_mm_235 0.2227 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2250 ms 66.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2418 ms 62.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2473 ms 60.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2504 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2527 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2550 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_227 0.2553 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_252 0.2642 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.9307 seconds and 0.3803 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1465 ms 100.0% 
  triton_mm_234 0.2159 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_235 0.2160 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_233 0.2388 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_227 0.2436 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_255 0.2439 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2468 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2501 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2511 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2602 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7958 seconds and 0.3870 seconds precompiling for 39 choices
Capturing batches (bs=24 avail_mem=41.73 GB):  88%| | 46/52 [03:05<04:02, 40.40s/it]Capturing batches (bs=16 avail_mem=41.12 GB):  88%| | 46/52 [03:05<04:02, 40.40s/it][rank1]:W1027 11:20:12.054000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:12.068000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:12.083000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:12.092000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:12.103000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:12.111000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:12.122000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:12.124000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:12.132000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:12.137000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:12.152000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:12.159000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:12.172000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:12.179000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:12.192000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:12.194000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:12.200000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:12.210000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:12.225000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:12.231000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:12.244000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:12.251000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:12.265000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:12.271000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:12.686000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:12.702000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:12.721000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:12.729000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:12.737000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:12.746000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:12.755000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:12.763000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:12.767000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:12.786000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:12.804000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:12.812000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:12.822000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:12.826000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:12.836000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:12.837000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:12.845000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:12.855000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:12.873000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:12.881000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:12.892000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:12.895000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:12.904000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:12.906000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:12.913000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:12.923000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:12.941000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:12.949000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:12.960000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:12.963000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:12.974000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:12.981000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:13.370000 816 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank2]:W1027 11:20:13.392000 817 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank7]:W1027 11:20:13.402000 822 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank0]:W1027 11:20:13.407000 815 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank5]:W1027 11:20:13.416000 820 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank6]:W1027 11:20:13.429000 821 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank3]:W1027 11:20:13.432000 818 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank4]:W1027 11:20:13.434000 819 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0091 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0092 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0092 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_274 0.0092 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_276 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0093 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_277 0.0093 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_278 0.0093 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_279 0.0094 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8047 seconds and 0.2464 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0087 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0090 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0090 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_274 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_276 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0091 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_277 0.0091 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0091 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8288 seconds and 0.1995 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0090 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0092 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0093 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_277 0.0093 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0093 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_275 0.0093 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_278 0.0093 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0093 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_276 0.0093 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8628 seconds and 0.0905 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_275 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_276 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_274 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_279 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9161 seconds and 0.2525 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_271 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_270 0.0089 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_274 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_272 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_275 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_276 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_273 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_278 0.0092 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_279 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9157 seconds and 0.2034 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0091 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0091 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_279 0.0091 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_276 0.0091 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0091 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_278 0.0092 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0092 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9204 seconds and 0.0937 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0091 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0091 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_278 0.0093 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0093 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0093 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0093 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_272 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_275 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_276 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9187 seconds and 0.1890 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0091 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0091 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_275 0.0092 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_274 0.0092 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_279 0.0093 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_276 0.0093 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0093 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_278 0.0093 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9179 seconds and 0.0790 seconds precompiling for 25 choices
[rank1]:W1027 11:20:23.670000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:23.714000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:23.889000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:24.070000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:24.174000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:24.217000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:24.329000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:24.389000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:24.404000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:24.456000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:24.561000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:24.840000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:24.909000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:24.973000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:24.975000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:25.475000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:25.517000 819 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank5]:W1027 11:20:25.709000 820 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank6]:W1027 11:20:26.050000 821 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank1]:W1027 11:20:26.141000 816 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank0]:W1027 11:20:26.166000 815 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank7]:W1027 11:20:26.388000 822 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank3]:W1027 11:20:26.453000 818 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank2]:W1027 11:20:26.818000 817 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0094 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0094 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_303 0.0097 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0097 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_298 0.0099 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0099 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_285 0.0100 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0100 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8846 seconds and 0.4014 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0089 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0094 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0095 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_284 0.0098 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0098 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0098 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_298 0.0099 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0099 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_299 0.0100 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.0041 seconds and 0.4117 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0090 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_293 0.0095 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0095 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0098 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0098 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_299 0.0098 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_298 0.0099 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_284 0.0102 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0103 ms 87.5% 
SingleProcess AUTOTUNE benchmarking takes 4.8398 seconds and 0.4034 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0091 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0091 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_302 0.0093 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0093 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_285 0.0095 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0095 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_298 0.0095 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0095 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9928 seconds and 0.3991 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0087 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_303 0.0097 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0097 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_284 0.0098 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0099 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_298 0.0099 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0099 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.1173 seconds and 0.4039 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0088 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0096 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0096 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_303 0.0097 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0099 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_298 0.0099 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0100 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8889 seconds and 0.4065 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0091 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0091 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_303 0.0092 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0093 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_298 0.0096 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_284 0.0097 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0097 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_285 0.0099 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.1134 seconds and 0.4012 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0091 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_303 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_299 0.0095 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0096 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_298 0.0097 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0098 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7852 seconds and 0.4017 seconds precompiling for 25 choices
[rank4]:W1027 11:20:32.332000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:32.581000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:32.669000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:32.680000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:32.746000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:32.786000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:32.806000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:32.846000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:32.863000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:32.882000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:32.963000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:32.982000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:33.059000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:33.266000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:33.330000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:33.343000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:33.430000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:33.444000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:33.444000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:33.521000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:33.529000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:33.610000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:33.621000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:33.716000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:36.404000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:36.479000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:36.579000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:36 DP4 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1027 11:20:37.403000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:37.480000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:37.556000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:37.580000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:37.632000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:37 DP5 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank0]:W1027 11:20:37.695000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:37.731000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:37.773000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:37 DP6 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank0]:W1027 11:20:37.876000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:37 DP0 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1027 11:20:37.942000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:38.018000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:38.118000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:38.149000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:38 DP1 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank7]:W1027 11:20:38.225000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:38.232000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:38.308000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:38.325000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:38 DP7 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1027 11:20:38.408000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:38 DP3 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1027 11:20:38.498000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:38.576000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:38.677000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:38 DP2 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1027 11:20:39.256000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:39.280000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:39.285000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:39.301000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:39.332000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:39.356000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:39.362000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:39.377000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:39.432000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:39.459000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:39.466000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:39.478000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:39 DP6 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:39 DP5 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank7]:W1027 11:20:39.520000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:39 DP0 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:39 DP1 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank7]:W1027 11:20:39.596000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:39.597000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:39.673000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:39.696000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:39.724000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:39 DP7 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1027 11:20:39.774000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:39.799000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:39 DP3 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1027 11:20:39.859000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:39.898000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:39.935000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:39 DP4 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1027 11:20:40.034000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:20:40 DP2 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1027 11:20:40.596000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:40.608000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:40.617000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:40.627000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:40.636000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:40.648000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:40.661000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:40.678000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:40.852000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:40.864000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:40.877000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:40.886000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:40.896000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:40.908000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:40.921000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:40.934000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:41.108000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:41.120000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:41.138000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:41.146000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:41.156000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:41.168000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:41.180000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:41.190000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:20:41 DP6 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:20:41 DP5 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:20:41 DP3 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:20:41 DP0 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:20:41 DP1 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:20:41 DP4 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:20:41 DP7 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:20:41 DP2 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1027 11:20:42.629000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:42.706000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:42.728000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:42.742000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:42.784000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:42.789000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:42.803000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:42.807000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:42.816000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:42.819000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:42.826000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:42.839000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:20:42 DP3 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank1]:W1027 11:20:42.866000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:42.880000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:42.885000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:42.892000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:42.897000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:42.902000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:42.927000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:20:42 DP6 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:20:42 DP5 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank1]:W1027 11:20:42.954000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:42.968000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:42.976000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:42.978000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:43.005000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:20:43 DP1 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:20:43 DP7 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:20:43 DP0 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:20:43 DP4 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:20:43 DP2 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0132 ms 100.0% 
  triton_mm_307 0.0271 ms 48.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0419 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0453 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0496 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0531 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 22.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 22.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0742 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1374 seconds and 0.7031 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0125 ms 100.0% 
  triton_mm_307 0.0271 ms 46.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0419 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0454 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0495 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0613 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1517 seconds and 0.7327 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0123 ms 100.0% 
  triton_mm_307 0.0272 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0420 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0455 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0496 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0533 ms 23.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0583 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0602 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0613 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0744 ms 16.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2694 seconds and 0.7489 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0129 ms 100.0% 
  triton_mm_307 0.0271 ms 47.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0419 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0453 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0494 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0581 ms 22.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0742 ms 17.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1808 seconds and 0.4937 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0126 ms 100.0% 
  triton_mm_307 0.0270 ms 46.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0418 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0453 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0493 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0531 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0580 ms 21.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0598 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0611 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0742 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2216 seconds and 0.4853 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0125 ms 100.0% 
  triton_mm_307 0.0271 ms 46.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0420 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0454 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0497 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 23.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0584 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0742 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2358 seconds and 0.6445 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0128 ms 100.0% 
  triton_mm_307 0.0271 ms 47.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0420 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0454 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0497 ms 25.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0534 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0584 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0613 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0742 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2525 seconds and 0.4123 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0126 ms 100.0% 
  triton_mm_307 0.0271 ms 46.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0421 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0454 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0497 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0534 ms 23.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0584 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0613 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0742 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2721 seconds and 0.6247 seconds precompiling for 39 choices
[rank6]:W1027 11:20:53.529000 821 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce10dac10>
[rank6]:W1027 11:20:53.552000 821 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce2168ae0>
[rank5]:W1027 11:20:53.584000 820 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef532bfc0>
[rank5]:W1027 11:20:53.608000 820 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef44bad30>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:20:53 DP6 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1027 11:20:53.634000 817 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5af70>
[rank3]:W1027 11:20:53.646000 818 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ebbfc0>
[rank2]:W1027 11:20:53.657000 817 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5b2a0>
[rank4]:W1027 11:20:53.659000 819 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6730>
[rank3]:W1027 11:20:53.669000 818 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ff3f00>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:20:53 DP5 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1027 11:20:53.682000 819 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6e50>
[rank7]:W1027 11:20:53.687000 822 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f67434abfc0>
[rank0]:W1027 11:20:53.697000 815 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356ac40>
[rank7]:W1027 11:20:53.710000 822 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f673706ee20>
[rank0]:W1027 11:20:53.720000 815 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356aca0>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:20:53 DP2 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:20:53 DP3 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:20:53 DP4 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1027 11:20:53.752000 816 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2bb0>
[rank1]:W1027 11:20:53.775000 816 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2f40>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:20:53 DP7 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:20:53 DP0 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:20:53 DP1 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1027 11:20:54.998000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:55.007000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:55.019000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:55.031000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:55.041000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:55.052000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:55.111000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:55.266000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:55.411000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:55.419000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:55.431000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:55.440000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:55.523000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:55.537000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:55.549000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:55.561000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:55.709000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:55.721000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:55.783000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:55.800000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:55.809000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:55.823000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:55.888000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:55.898000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:56.043000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:56.054000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:56.065000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:56.076000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:56.085000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:56.095000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:56.144000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:56.156000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:56.452000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:56.460000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:56.469000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:56.478000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:56.487000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:56.497000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:56.564000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:56.577000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:56.723000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:56.733000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:56.742000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:56.752000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:56.762000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:56.773000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:56.821000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:56.836000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:56.982000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:57.004000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:57.013000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:57.022000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:57.032000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:57.043000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:57.076000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:57.092000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:57.238000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:57.260000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:57.271000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:57.279000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:57.290000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:57.301000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:57.328000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:57.348000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:57.494000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:57.516000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:57.530000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:57.539000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:57.549000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:57.560000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:57.580000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:57.604000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:57.750000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:57.772000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:57.790000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:57.799000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:57.810000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:57.820000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:57.833000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:57.856000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:58.006000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:58.028000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:58.050000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:58.059000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:58.070000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:58.080000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:58.093000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:58.112000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:58.262000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:58.284000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:58.310000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:58.319000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:58.330000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:58.340000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:58.353000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:58.365000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:58.518000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:58.540000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:58.567000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:58.576000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:58.590000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:58.601000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:58.613000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:58.625000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:58.774000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:58.796000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:58.826000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:58.835000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:58.847000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:58.857000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:58.870000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:58.882000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:59.034000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:59.064000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:59.086000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:59.095000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:59.106000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:59.124000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:59.136000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:59.149000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:59.294000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:59.320000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:59.346000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:59.355000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:59.365000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:59.379000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:59.392000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:59.405000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:59.554000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:59.576000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:59.606000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:59.615000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:59.625000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:59.636000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:59.648000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:59.664000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:20:59.810000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:20:59.836000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:20:59.866000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:20:59.875000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:20:59.885000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:20:59.896000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:20:59.909000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:20:59.921000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:00.070000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:00.092000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:00.126000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:00.135000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:00.146000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:00.155000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:00.168000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:00.181000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:00.330000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:00.348000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:00.386000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:00.395000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:00.405000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:00.416000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:00.428000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:00.441000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:00.590000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:00.604000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:00.646000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:00.655000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:00.665000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:00.677000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:00.688000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:00.701000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:00.853000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:00.862000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:00.908000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:00.919000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:00.930000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:00.940000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:00.950000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:00.961000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:01.112000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:01.121000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:01.164000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:01.175000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:01.188000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:01.198000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:01.207000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:01.218000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:01.368000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:01.378000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:01.420000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:01.431000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:01.444000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:01.457000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:01.466000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:01.477000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:01.628000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:01.637000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:01.677000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:01.687000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:01.700000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:01.717000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:01.727000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:01.738000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:01.887000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:01.898000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:01.931000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:01.940000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:01.954000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:01.979000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:01.991000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:02.003000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:02.150000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:02.161000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:02.191000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:02.200000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:02.214000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:02.236000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:02.248000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:02.260000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:02.410000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:02.421000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:02.451000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:02.460000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:02.474000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:02.495000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:02.508000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:02.521000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:02.670000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:02.681000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:02.710000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:02.719000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:02.734000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:02.751000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:02.764000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:02.777000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:02.930000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:02.941000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:02.970000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:02.979000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:02.994000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:03.007000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:03.020000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:03.033000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:03.190000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:03.201000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:03.231000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:03.240000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:03.255000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:03.265000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:03.278000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:03.290000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:03.450000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:03.468000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:03.490000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:03.499000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:03.514000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:03.524000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:03.537000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:03.550000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:03.710000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:03.724000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:03.750000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:03.759000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:03.774000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:03.784000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:03.797000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:03.810000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:03.972000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:03.981000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:04.012000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:04.023000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:04.036000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:04.044000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:04.055000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:04.066000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:04.232000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:04.250000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:04.272000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:04.283000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:04.295000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:04.314000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:04.324000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:04.335000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:04.492000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:04.510000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:04.533000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:04.543000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:04.554000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:04.573000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:04.584000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:04.595000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:04.752000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:04.770000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:04.792000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:04.803000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:04.815000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:04.834000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:04.844000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:04.854000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:05.225000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:05.237000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:05.248000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:05.258000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:05.334000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:05.348000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:05.359000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:05.371000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:05.522000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:05.531000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:05.542000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:05.550000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:05.593000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:05.609000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:05.621000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:05.637000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:05.933000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:05.942000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:05.953000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:05.963000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:05.973000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:05.982000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:06.039000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:06.050000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:06.207000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:06.232000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:06.241000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:06.252000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:06.299000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:06.312000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:06.479000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:06.490000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:06.642000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:06.652000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:06.662000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:06.672000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:06.683000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:06.694000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:06.739000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:06.750000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:06.916000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:06.924000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:06.935000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:06.944000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:06.957000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:06.969000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:06.998000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:07.014000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:07.176000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:07.190000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:07.200000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:07.210000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:07.222000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:07.234000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:07.258000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:07.274000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:07.435000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:07.456000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:07.465000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:07.477000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:07.487000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:07.498000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:07.520000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:07.540000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:07.698000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:07.720000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:07.729000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:07.741000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:07.755000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:07.764000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:07.780000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:07.800000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:07.962000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:07.987000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:07.996000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:08.008000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:08.019000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:08.029000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:08.040000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:08.064000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:08.226000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:08.252000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:08.261000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:08.273000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:08.284000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:08.293000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:08.305000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:08.324000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:08.490000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:08.512000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:08.523000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:08.536000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:08.551000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:08.559000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:08.576000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:08.596000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:08.754000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:08.776000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:08.785000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:08.796000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:08.811000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:08.823000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:08.836000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:08.856000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:09.018000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:09.046000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:09.056000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:09.069000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:09.078000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:09.088000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:09.100000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:09.116000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:09.282000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:09.307000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:09.320000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:09.331000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:09.341000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:09.353000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:09.363000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:09.376000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:09.546000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:09.566000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:09.580000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:09.591000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:09.601000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:09.615000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:09.625000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:09.638000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:09.810000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:09.826000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:09.840000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:09.851000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:09.863000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:09.879000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:09.889000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:09.902000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:10.074000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:10.086000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:10.099000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:10.111000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:10.123000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:10.142000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:10.153000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:10.166000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:10.338000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:10.347000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:10.364000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:10.375000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:10.386000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:10.406000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:10.417000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:10.430000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:10.602000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:10.611000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:10.627000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:10.640000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:10.649000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:10.671000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:10.682000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:10.694000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:11.398000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:11.408000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:11.419000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:11.428000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:11.436000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:11.446000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:11.456000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:11.467000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1047 ms 100.0% 
  triton_mm_359 0.1447 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1487 ms 70.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1632 ms 64.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1661 ms 63.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1765 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1766 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_351 0.1792 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1839 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2045 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6146 seconds and 0.4943 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1055 ms 100.0% 
  triton_mm_359 0.1474 ms 71.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1517 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1666 ms 63.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1689 ms 62.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1773 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1775 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1822 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1848 ms 57.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2061 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6750 seconds and 0.6012 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1078 ms 100.0% 
  triton_mm_359 0.1498 ms 72.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1545 ms 69.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1682 ms 64.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1697 ms 63.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1780 ms 60.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1784 ms 60.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1848 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1852 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2080 ms 51.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6480 seconds and 0.4263 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1037 ms 100.0% 
  triton_mm_359 0.1459 ms 71.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1483 ms 69.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1636 ms 63.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1643 ms 63.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1759 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1761 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1793 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1832 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2023 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6575 seconds and 0.5813 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1070 ms 100.0% 
  triton_mm_359 0.1500 ms 71.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1534 ms 69.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1692 ms 63.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1694 ms 63.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1780 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1783 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_351 0.1847 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1856 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2079 ms 51.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7086 seconds and 0.6729 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1054 ms 100.0% 
  triton_mm_359 0.1507 ms 69.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1535 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1676 ms 62.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1696 ms 62.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1781 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1785 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1845 ms 57.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1856 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2086 ms 50.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7033 seconds and 0.7212 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1063 ms 100.0% 
  triton_mm_359 0.1472 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1510 ms 70.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1631 ms 65.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1651 ms 64.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1762 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1763 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1826 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1838 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2075 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7023 seconds and 0.4185 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1073 ms 100.0% 
  triton_mm_359 0.1507 ms 71.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1545 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1679 ms 63.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1696 ms 63.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1779 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1786 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1848 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1859 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2080 ms 51.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7574 seconds and 0.4504 seconds precompiling for 39 choices
Capturing batches (bs=16 avail_mem=41.12 GB):  90%| | 47/52 [04:21<04:16, 51.34s/it]Capturing batches (bs=12 avail_mem=40.52 GB):  90%| | 47/52 [04:21<04:16, 51.34s/it][rank6]:W1027 11:21:28.912000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:28.930000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:28.940000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:28.952000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:28.964000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:28.976000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:28.981000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:28.985000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:28.997000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:28.999000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:29.009000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:29.021000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:29.033000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:29.045000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:29.053000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:29.054000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:29.065000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:29.072000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:29.082000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:29.094000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:29.106000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:29.119000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:29.126000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:29.147000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:29.553000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:29.579000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:29.587000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:29.599000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:29.607000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:29.616000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:29.630000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:29.638000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:29.643000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:29.663000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:29.674000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:29.681000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:29.691000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:29.700000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:29.709000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:29.714000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:29.725000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:29.734000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:29.745000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:29.752000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:29.762000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:29.770000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:29.779000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:29.785000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:29.795000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:29.804000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:29.814000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:29.821000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:29.830000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:29.838000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:29.855000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:29.864000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:30.445000 821 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank2]:W1027 11:21:30.471000 817 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank5]:W1027 11:21:30.477000 820 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank3]:W1027 11:21:30.485000 818 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank4]:W1027 11:21:30.494000 819 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank7]:W1027 11:21:30.500000 822 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank0]:W1027 11:21:30.520000 815 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank1]:W1027 11:21:30.522000 816 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0090 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0090 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0091 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0091 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_396 0.0091 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_398 0.0092 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_399 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_403 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9015 seconds and 0.1873 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_400 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_401 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_399 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_398 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_402 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_403 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8019 seconds and 0.1797 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_395 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_400 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_393 0.0090 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_392 0.0090 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_398 0.0090 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0090 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_403 0.0091 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9149 seconds and 0.2563 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_395 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_400 0.0089 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_396 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_399 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_401 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_403 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_398 0.0090 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9212 seconds and 0.2383 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0089 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_393 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_401 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_392 0.0090 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_396 0.0090 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0090 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_398 0.0091 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_402 0.0091 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9272 seconds and 0.2128 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_401 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_396 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_403 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_399 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_398 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9456 seconds and 0.1340 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_393 0.0089 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0089 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_396 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_401 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_392 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_399 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_398 0.0091 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9441 seconds and 0.1397 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_395 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0089 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_400 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_392 0.0092 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_402 0.0092 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_393 0.0092 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0092 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_398 0.0092 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9014 seconds and 0.1447 seconds precompiling for 25 choices
[rank2]:W1027 11:21:40.724000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:40.841000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:41.232000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:41.238000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:41.264000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:41.278000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:41.347000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:41.369000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:41.566000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:41.732000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:41.759000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:41.781000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:41.867000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:42.069000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:42.069000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:42.565000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:42.959000 817 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank0]:W1027 11:21:43.033000 815 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank6]:W1027 11:21:43.042000 821 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank7]:W1027 11:21:43.089000 822 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank1]:W1027 11:21:43.273000 816 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank3]:W1027 11:21:43.362000 818 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank5]:W1027 11:21:43.421000 820 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank4]:W1027 11:21:44.064000 819 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_415 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0087 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_426 0.0095 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0097 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_422 0.0098 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0098 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_425 0.0099 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_424 0.0100 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9235 seconds and 0.4401 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_415 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0090 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_427 0.0095 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0095 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_409 0.0096 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0096 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0097 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0097 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9050 seconds and 0.4225 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0088 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_427 0.0095 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0096 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_423 0.0099 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0099 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_408 0.0100 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0101 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9336 seconds and 0.4297 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0085 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0089 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0089 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_427 0.0092 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0092 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_423 0.0093 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0095 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_408 0.0096 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0097 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9290 seconds and 0.4249 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_415 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_416 0.0092 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0093 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_427 0.0095 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0096 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_422 0.0098 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0099 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0099 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_425 0.0101 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8546 seconds and 0.4179 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0085 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0092 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_427 0.0093 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0093 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_423 0.0095 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_409 0.0096 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0096 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_422 0.0097 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8736 seconds and 0.4135 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_427 0.0092 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0092 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_422 0.0094 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0094 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0096 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0096 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9125 seconds and 0.4039 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0088 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0092 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0092 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_426 0.0093 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0093 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_423 0.0097 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0097 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_425 0.0100 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_424 0.0101 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9063 seconds and 0.4056 seconds precompiling for 25 choices
[rank0]:W1027 11:21:49.586000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:49.664000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:49.766000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:49.790000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:49.847000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:49.853000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:49.867000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:49.925000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:49.930000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:49.938000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:49.966000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:49.971000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:49.979000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:50.015000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:50.025000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:50.030000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:50.048000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:50.055000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:50.115000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:50.148000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:50.154000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:50.916000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:50.993000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:51.093000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:53.876000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:53.952000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:53.975000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:54.052000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:54.053000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:54.153000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:54.470000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:54.475000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:54.547000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:54.551000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:54.622000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:54.648000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:54.650000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:54.652000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:54.662000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:54.698000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:54.724000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:54.739000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:54.799000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:54.824000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:54.839000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:54.904000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:54.981000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:55.081000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:55.672000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:55.690000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:55.750000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:55.765000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:55.850000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:55.852000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:55.862000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:55.866000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:55.927000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:55.942000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:55.986000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:56.026000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:56.028000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:56.036000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:56.044000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:56.063000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:56.103000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:56.117000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:56.164000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:56.205000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:56.223000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:56.268000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:56.346000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:56.451000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:57.299000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:57.308000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:57.325000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:57.339000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:57.352000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:57.365000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:57.381000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:57.639000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:57.809000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:57.819000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:57.830000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:57.839000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:57.851000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:57.906000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:57.915000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:57.926000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:58.080000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:58.091000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:58.102000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:58.113000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:58.124000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:58.174000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:58.184000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:58.194000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:59.722000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:59.757000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:59.762000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:59.775000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:59.799000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:59.815000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:59.827000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:59.835000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:59.835000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:59.839000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:21:59.849000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:59.852000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:21:59.885000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:21:59.889000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:59.889000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:59.892000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:21:59.901000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:59.904000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:59.913000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:21:59.941000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:21:59.962000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:21:59.965000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:21:59.974000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:00.014000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0114 ms 100.0% 
  triton_mm_431 0.0277 ms 41.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0417 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0451 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0491 ms 23.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0530 ms 21.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0581 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0608 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0753 ms 15.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2412 seconds and 0.6754 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0115 ms 100.0% 
  triton_mm_431 0.0280 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0418 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0492 ms 23.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0531 ms 21.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0581 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0598 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0756 ms 15.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2940 seconds and 0.7283 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0113 ms 100.0% 
  triton_mm_431 0.0278 ms 40.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0418 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0491 ms 23.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0531 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0581 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0594 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0610 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 15.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2134 seconds and 0.7769 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0113 ms 100.0% 
  triton_mm_431 0.0279 ms 40.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0419 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0491 ms 22.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0532 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0581 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0597 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0610 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0755 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2310 seconds and 0.7820 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0115 ms 100.0% 
  triton_mm_431 0.0278 ms 41.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0418 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0493 ms 23.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0531 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0581 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0597 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0755 ms 15.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2204 seconds and 0.6767 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0111 ms 100.0% 
  triton_mm_431 0.0279 ms 39.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0419 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0455 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0493 ms 22.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0532 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0756 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2553 seconds and 0.5840 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0111 ms 100.0% 
  triton_mm_431 0.0278 ms 40.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0420 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0493 ms 22.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0533 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0597 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0609 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0756 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2428 seconds and 0.5255 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0119 ms 100.0% 
  triton_mm_431 0.0279 ms 42.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0420 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0455 ms 26.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0492 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0534 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0598 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0756 ms 15.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2793 seconds and 0.4705 seconds precompiling for 39 choices
[rank7]:W1027 11:22:10.667000 822 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f67434abfc0>
[rank6]:W1027 11:22:10.678000 821 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce10dac10>
[rank5]:W1027 11:22:10.688000 820 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef532bfc0>
[rank7]:W1027 11:22:10.690000 822 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f673706ee20>
[rank6]:W1027 11:22:10.701000 821 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce2168ae0>
[rank5]:W1027 11:22:10.711000 820 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef44bad30>
[rank3]:W1027 11:22:10.736000 818 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ebbfc0>
[rank4]:W1027 11:22:10.754000 819 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6730>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:22:10 DP7 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1027 11:22:10.759000 818 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ff3f00>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:22:10 DP6 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1027 11:22:10.772000 816 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2bb0>
[rank4]:W1027 11:22:10.777000 819 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6e50>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:22:10 DP5 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1027 11:22:10.795000 816 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2f40>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:22:10 DP3 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:22:10 DP4 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1027 11:22:10.849000 817 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5af70>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:22:10 DP1 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1027 11:22:10.873000 817 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5b2a0>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:22:10 DP2 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1027 11:22:10.947000 815 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356ac40>
[rank0]:W1027 11:22:10.970000 815 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356aca0>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:22:11 DP0 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1027 11:22:12.449000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:12.459000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:12.469000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:12.478000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:12.487000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:12.497000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:12.508000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:12.555000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:12.713000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:12.725000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:12.735000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:12.745000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:12.756000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:12.767000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:12.778000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:12.815000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:12.975000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:12.987000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:13.001000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:13.013000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:13.022000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:13.036000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:13.045000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:13.077000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:13.239000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:13.255000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:13.266000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:13.278000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:13.287000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:13.300000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:13.311000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:13.336000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:13.503000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:13.523000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:13.534000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:13.546000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:13.556000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:13.568000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:13.578000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:13.596000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:13.767000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:13.791000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:13.802000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:13.814000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:13.824000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:13.836000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:13.846000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:13.863000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:14.046000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:14.063000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:14.077000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:14.087000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:14.098000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:14.108000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:14.121000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:14.133000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:14.309000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:14.331000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:14.345000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:14.356000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:14.365000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:14.376000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:14.389000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:14.400000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:14.571000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:14.600000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:14.618000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:14.630000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:14.641000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:14.652000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:14.663000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:14.674000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:14.838000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:14.865000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:14.887000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:14.898000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:14.910000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:14.920000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:14.932000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:14.943000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:15.103000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:15.129000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:15.154000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:15.166000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:15.178000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:15.188000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:15.200000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:15.210000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:15.367000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:15.392000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:15.423000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:15.434000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:15.446000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:15.457000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:15.468000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:15.479000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:15.633000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:15.655000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:15.697000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:15.706000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:15.717000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:15.729000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:15.739000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:15.752000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:15.905000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:15.923000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:15.966000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:15.976000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:15.986000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:15.997000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:16.008000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:16.021000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:16.181000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:16.191000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:16.237000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:16.248000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:16.258000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:16.268000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:16.279000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:16.292000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:16.445000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:16.459000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:16.505000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:16.515000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:16.526000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:16.537000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:16.549000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:16.561000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:16.717000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:16.727000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:16.773000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:16.783000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:16.793000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:16.804000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:16.816000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:16.829000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:16.989000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:16.998000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:17.041000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:17.052000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:17.061000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:17.072000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:17.085000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:17.096000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:17.257000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:17.267000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:17.309000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:17.320000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:17.329000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:17.340000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:17.353000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:17.364000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:17.525000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:17.539000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:17.585000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:17.595000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:17.604000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:17.615000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:17.626000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:17.639000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:17.793000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:17.807000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:17.853000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:17.863000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:17.873000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:17.883000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:17.899000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:17.913000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:18.065000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:18.075000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:18.120000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:18.131000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:18.141000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:18.151000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:18.163000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:18.177000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:18.333000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:18.343000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:18.389000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:18.399000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:18.409000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:18.419000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:18.430000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:18.444000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:18.601000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:18.611000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:18.657000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:18.668000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:18.677000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:18.688000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:18.701000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:18.712000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:18.869000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:18.879000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:18.929000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:18.939000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:18.950000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:18.959000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:18.972000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:18.984000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:19.141000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:19.150000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:19.201000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:19.211000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:19.223000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:19.232000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:19.244000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:19.258000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:19.749000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:19.758000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:19.770000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:19.781000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:19.794000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:19.827000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:19.855000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:19.868000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:20.247000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:20.258000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:20.268000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:20.277000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:20.289000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:20.301000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:20.311000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:20.355000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:20.803000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:20.812000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:20.820000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:20.829000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:20.841000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:20.853000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:20.908000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:20.921000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:21.339000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:21.351000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:21.360000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:21.369000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:21.380000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:21.391000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:21.400000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:21.453000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:21.612000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:21.624000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:21.634000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:21.643000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:21.655000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:21.667000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:21.677000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:21.721000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:21.885000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:21.896000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:21.907000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:21.916000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:21.928000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:21.941000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:21.950000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:21.989000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:22.156000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:22.168000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:22.183000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:22.192000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:22.204000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:22.216000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:22.226000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:22.257000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:22.432000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:22.444000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:22.459000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:22.468000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:22.480000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:22.491000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:22.502000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:22.529000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:22.704000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:22.716000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:22.735000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:22.744000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:22.756000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:22.767000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:22.778000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:22.796000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:22.976000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:22.988000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:23.011000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:23.021000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:23.032000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:23.044000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:23.053000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:23.066000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:23.248000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:23.260000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:23.287000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:23.297000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:23.308000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:23.319000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:23.330000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:23.343000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:23.520000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:23.532000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:23.563000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:23.573000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:23.584000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:23.596000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:23.606000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:23.619000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:23.790000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:23.800000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:23.837000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:23.849000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:23.859000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:23.872000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:23.884000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:23.896000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:24.062000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:24.071000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:24.109000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:24.121000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:24.132000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:24.143000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:24.156000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:24.167000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:24.334000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:24.343000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:24.381000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:24.394000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:24.404000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:24.418000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:24.430000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:24.441000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:24.614000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:24.623000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:24.653000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:24.665000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:24.676000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:24.688000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:24.700000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:24.712000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:24.888000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:24.900000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:24.923000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:24.933000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:24.946000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:24.956000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:24.967000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:24.985000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:25.164000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:25.177000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:25.195000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:25.205000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:25.216000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:25.229000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:25.238000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:25.252000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:25.444000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:25.456000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:25.467000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:25.477000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:25.488000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:25.499000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:25.510000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:25.523000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:25.712000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:25.724000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:25.743000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:25.752000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:25.763000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:25.775000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:25.785000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:25.798000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:25.980000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:25.991000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:26.015000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:26.025000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:26.036000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:26.047000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:26.057000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:26.070000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:26.248000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:26.260000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:26.287000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:26.298000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:26.309000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:26.320000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:26.330000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:26.343000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:26.516000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:26.527000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:26.563000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:26.573000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:26.584000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:26.595000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:26.605000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:26.618000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:26.784000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:26.795000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:26.835000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:26.845000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:26.856000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:26.867000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:26.878000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:26.891000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:27.052000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:27.063000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:27.111000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:27.121000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:27.131000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:27.143000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:27.153000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:27.166000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:27.332000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:27.345000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:27.387000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:27.397000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:27.408000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:27.420000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:27.429000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:27.442000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:27.605000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:27.616000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:27.663000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:27.673000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:27.684000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:27.695000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:27.705000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:27.719000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:27.881000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:27.892000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:27.939000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:27.949000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:27.960000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:27.971000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:27.982000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:27.997000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:28.156000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:28.168000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:28.215000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:28.225000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:28.236000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:28.247000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:28.258000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:28.271000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:28.428000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:28.440000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:28.491000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:28.501000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:28.512000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:28.523000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:28.534000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:28.547000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:28.704000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:28.716000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:28.767000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:28.777000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:28.788000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:28.800000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:28.810000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:28.824000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:29.534000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:29.545000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:29.554000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:29.564000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:29.578000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:29.590000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:29.600000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:29.610000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0906 ms 100.0% 
  triton_mm_483 0.1468 ms 61.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1516 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1638 ms 55.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1667 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1706 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1769 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1771 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1826 ms 49.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1841 ms 49.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.6391 seconds and 0.6110 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0905 ms 100.0% 
  triton_mm_483 0.1438 ms 62.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1486 ms 60.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1638 ms 55.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1645 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1678 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1762 ms 51.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1763 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_475 0.1793 ms 50.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1829 ms 49.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.6003 seconds and 0.5165 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0893 ms 100.0% 
  triton_mm_483 0.1442 ms 61.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1474 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1625 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1635 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1696 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1751 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1754 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1783 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1824 ms 48.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.6623 seconds and 0.6939 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0925 ms 100.0% 
  triton_mm_483 0.1499 ms 61.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1519 ms 60.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1669 ms 55.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1692 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1711 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1775 ms 52.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1777 ms 52.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1843 ms 50.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1844 ms 50.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7411 seconds and 0.7833 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0920 ms 100.0% 
  triton_mm_483 0.1499 ms 61.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1532 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1658 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1685 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1723 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1773 ms 51.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1777 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1841 ms 50.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_475 0.1848 ms 49.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7577 seconds and 0.7258 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0928 ms 100.0% 
  triton_mm_483 0.1491 ms 62.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1534 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1670 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1690 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1728 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1778 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1778 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_481 0.1849 ms 50.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_475 0.1853 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6757 seconds and 0.6393 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0925 ms 100.0% 
  triton_mm_483 0.1488 ms 62.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1538 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1660 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1678 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1740 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1771 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1776 ms 52.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1843 ms 50.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1843 ms 50.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.6805 seconds and 0.3778 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0908 ms 100.0% 
  triton_mm_483 0.1455 ms 62.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1514 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1636 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1645 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1708 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1755 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1758 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_475 0.1809 ms 50.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1830 ms 49.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7211 seconds and 0.4152 seconds precompiling for 39 choices
Capturing batches (bs=12 avail_mem=40.52 GB):  92%|| 48/52 [05:38<03:56, 59.06s/it]Capturing batches (bs=8 avail_mem=39.94 GB):  92%|| 48/52 [05:38<03:56, 59.06s/it] [rank4]:W1027 11:22:46.093000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:46.119000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:46.133000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:46.142000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:46.154000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:46.162000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:46.175000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:46.187000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:46.189000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:46.204000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:46.212000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:46.224000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:46.235000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:46.245000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:46.256000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:46.263000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:46.279000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:46.287000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:46.298000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:46.319000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:46.329000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:46.663000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:46.733000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:46.733000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:46.762000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:46.790000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:46.799000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:46.806000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:46.810000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:46.819000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:46.823000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:46.836000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:46.850000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:46.876000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:46.884000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:46.890000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:46.896000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:46.907000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:46.919000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:46.922000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:46.948000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:46.957000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:46.960000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:46.967000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:46.979000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:46.990000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:46.992000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:47.018000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:47.028000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:47.038000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:47.050000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:47.060000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:47.308000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:47.393000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:47.423000 819 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank5]:W1027 11:22:47.456000 820 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank2]:W1027 11:22:47.464000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:47.485000 822 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank6]:W1027 11:22:47.504000 821 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank0]:W1027 11:22:47.506000 815 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank1]:W1027 11:22:47.526000 816 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank3]:W1027 11:22:47.533000 818 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank2]:W1027 11:22:47.542000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:48.057000 817 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0087 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0087 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_520 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0089 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_526 0.0089 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_527 0.0089 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_522 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_523 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8124 seconds and 0.1599 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0088 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0088 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0088 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_521 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_525 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_524 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_520 0.0089 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_526 0.0089 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_527 0.0089 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8602 seconds and 0.2247 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0087 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_524 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_512 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_525 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_527 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_513 0.0088 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8208 seconds and 0.2020 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_516 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0086 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_527 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_520 0.0087 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_526 0.0087 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_523 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_521 0.0088 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_522 0.0088 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7832 seconds and 0.1259 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0089 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0089 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_521 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_520 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_526 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_522 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_512 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_523 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8381 seconds and 0.2008 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_523 0.0089 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_520 0.0089 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_527 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_516 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_521 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_526 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_517 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_522 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8455 seconds and 0.1535 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0087 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_520 0.0089 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0089 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_522 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_513 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_523 0.0090 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0090 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8878 seconds and 0.1322 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0086 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0087 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_527 0.0087 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_521 0.0087 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_526 0.0087 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_520 0.0087 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_524 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_522 0.0088 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8786 seconds and 0.1431 seconds precompiling for 25 choices
[rank4]:W1027 11:22:57.591000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:57.892000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:58.095000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:58.116000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:58.248000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:58.313000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:58.337000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:22:58.400000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:58.555000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:58.632000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:22:58.645000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:22:58.749000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:22:58.820000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:22:58.838000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:22:59.060000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:22:59.135000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:22:59.473000 819 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank5]:W1027 11:22:59.807000 820 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank0]:W1027 11:22:59.874000 815 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank1]:W1027 11:23:00.063000 816 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank7]:W1027 11:23:00.166000 822 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank6]:W1027 11:23:00.237000 821 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank2]:W1027 11:23:00.475000 817 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank3]:W1027 11:23:00.513000 818 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0086 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_541 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_551 0.0092 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_550 0.0093 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_546 0.0096 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_532 0.0096 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0097 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_533 0.0098 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9271 seconds and 0.4119 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_541 0.0088 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0089 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_550 0.0093 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_532 0.0093 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_551 0.0093 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_546 0.0094 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0095 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_547 0.0095 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9007 seconds and 0.4189 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_539 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_538 0.0083 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_540 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0090 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_547 0.0092 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0092 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_551 0.0093 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_548 0.0098 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_549 0.0099 ms 83.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9078 seconds and 0.4078 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_539 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_538 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0089 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_550 0.0094 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_546 0.0097 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_532 0.0098 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0098 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_533 0.0099 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8709 seconds and 0.4263 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0082 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_541 0.0087 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0087 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_551 0.0092 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_550 0.0092 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_546 0.0092 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0093 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0093 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0097 ms 83.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8966 seconds and 0.4217 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0089 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0090 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_551 0.0095 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_550 0.0096 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_547 0.0097 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_533 0.0099 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0099 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_532 0.0099 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9293 seconds and 0.4098 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_539 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_538 0.0084 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0088 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_550 0.0092 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0093 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_546 0.0093 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0094 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_533 0.0097 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_548 0.0098 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9026 seconds and 0.4094 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_541 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_551 0.0092 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_550 0.0093 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_546 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0097 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0099 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9054 seconds and 0.4147 seconds precompiling for 25 choices
[rank4]:W1027 11:23:06.360000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:06.380000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:06.436000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:06.458000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:06.536000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:06.561000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:06.585000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:06.662000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:06.704000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:06.752000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:06.772000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:06.781000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:06.823000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:06.830000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:06.883000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:06.900000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:06.932000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:07.001000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:07.011000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:07.057000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:07.090000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:07.135000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:07.192000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:07.236000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:11.035000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:11.107000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:11.112000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:11.124000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:11.186000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:11.201000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:11.214000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:11 DP3 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1027 11:23:11.289000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:11.302000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:11.338000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:11 DP0 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:11 DP4 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1027 11:23:11.377000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:11.416000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:11.454000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:11.516000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:11.517000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:11.557000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:11.571000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:11 DP5 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1027 11:23:11.594000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:11 DP1 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1027 11:23:11.649000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:11.697000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:11.716000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:11.752000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:11 DP7 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1027 11:23:11.795000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:11 DP6 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1027 11:23:11.906000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:11 DP2 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1027 11:23:12.474000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:12.492000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:12.505000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:12.550000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:12.570000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:12.583000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:12.652000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:12.674000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:12.686000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:23:12 DP3 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1027 11:23:12.716000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:23:12 DP0 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:23:12 DP4 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1027 11:23:12.754000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:12.796000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:12.831000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:12.893000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:12.901000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:12.933000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:23:12 DP5 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank7]:W1027 11:23:12.970000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:23:12 DP1 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank7]:W1027 11:23:13.073000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:13.107000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:23:13 DP7 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1027 11:23:13.184000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:13.285000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:23:13 DP2 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1027 11:23:13.399000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:13.475000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:13.575000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 11:23:13 DP6 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank4]:W1027 11:23:14.147000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:14.159000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:14.171000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:14.183000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:14.194000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:14.205000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:14.218000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:14.230000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:14.419000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:14.431000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:14.443000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:14.455000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:14.467000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:14.478000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:14.490000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:14.504000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:14.691000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:14.714000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:14.726000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:14.737000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:14.747000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:14.759000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:14.773000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:14.785000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:15 DP4 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:15 DP3 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:15 DP2 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:15 DP5 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:15 DP7 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:15 DP1 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:15 DP0 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:23:15 DP6 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1027 11:23:16.264000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:16.343000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:16.344000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:16.374000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:16.388000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:16.423000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:16.423000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:16.426000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:16.441000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:16.448000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:16.451000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:16.466000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:23:16 DP7 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank4]:W1027 11:23:16.501000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:16.504000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:16.519000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:16.525000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:16.529000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:16.544000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:23:16 DP4 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank1]:W1027 11:23:16.581000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:23:16 DP3 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank0]:W1027 11:23:16.598000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:23:16 DP2 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank6]:W1027 11:23:16.603000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:23:16 DP1 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:23:16 DP0 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:23:16 DP6 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank5]:W1027 11:23:16.871000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:16.963000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:17.058000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 11:23:17 DP5 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0108 ms 100.0% 
  triton_mm_555 0.0274 ms 39.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0404 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0451 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0493 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0531 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0580 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0597 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0616 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0743 ms 14.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.9530 seconds and 0.4547 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0111 ms 100.0% 
  triton_mm_555 0.0273 ms 40.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0434 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0452 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0495 ms 22.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0536 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0580 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0598 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0623 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0745 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 7.0392 seconds and 0.4366 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0108 ms 100.0% 
  triton_mm_555 0.0272 ms 39.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0405 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0453 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0497 ms 21.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0534 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0583 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0596 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0613 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0741 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 7.0179 seconds and 0.4342 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0107 ms 100.0% 
  triton_mm_555 0.0274 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0404 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0453 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0493 ms 21.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0533 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0581 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0597 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0615 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0745 ms 14.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.5137 seconds and 0.4436 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0106 ms 100.0% 
  triton_mm_555 0.0272 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0405 ms 26.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0453 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0497 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0534 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0582 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0597 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0612 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0742 ms 14.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.5191 seconds and 0.5413 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0108 ms 100.0% 
  triton_mm_555 0.0273 ms 39.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0402 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0452 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0494 ms 21.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0532 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0580 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0598 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0618 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0742 ms 14.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.5349 seconds and 0.5037 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0107 ms 100.0% 
  triton_mm_555 0.0275 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0415 ms 25.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0452 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0510 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0533 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0582 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0617 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0638 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0743 ms 14.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.6103 seconds and 0.5442 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0109 ms 100.0% 
  triton_mm_555 0.0271 ms 40.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0413 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0453 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0509 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0533 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0581 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0633 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0652 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0741 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.5954 seconds and 0.4734 seconds precompiling for 31 choices
[rank7]:W1027 11:23:25.447000 822 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f67434abfc0>
[rank7]:W1027 11:23:25.471000 822 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f673706ee20>
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:23:25 DP7 TP7] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1027 11:23:25.638000 816 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2bb0>
[rank1]:W1027 11:23:25.662000 816 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2f40>
[rank2]:W1027 11:23:25.712000 817 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5af70>
[rank4]:W1027 11:23:25.729000 819 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6730>
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:23:25 DP1 TP1] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1027 11:23:25.736000 817 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5b2a0>
[rank4]:W1027 11:23:25.753000 819 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6e50>
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:23:25 DP2 TP2] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1027 11:23:25.821000 815 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356ac40>
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:23:25 DP4 TP4] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1027 11:23:25.845000 815 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356aca0>
[rank6]:W1027 11:23:25.896000 821 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce10dac10>
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:23:25 DP0 TP0] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1027 11:23:25.919000 821 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce2168ae0>
[rank3]:W1027 11:23:25.925000 818 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ebbfc0>
[rank3]:W1027 11:23:25.949000 818 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ff3f00>
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:23:25 DP6 TP6] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:23:26 DP3 TP3] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1027 11:23:26.288000 820 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef532bfc0>
[rank5]:W1027 11:23:26.311000 820 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef44bad30>
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:23:26 DP5 TP5] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1027 11:23:27.539000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:27.548000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:27.558000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:27.569000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:27.580000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:27.592000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:27.604000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:27.663000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:27.825000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:27.837000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:27.849000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:27.858000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:27.868000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:27.879000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:27.891000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:27.941000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:28.101000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:28.112000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:28.125000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:28.134000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:28.145000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:28.155000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:28.166000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:28.213000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:28.372000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:28.383000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:28.395000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:28.408000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:28.420000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:28.433000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:28.447000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:28.484000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:28.643000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:28.659000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:28.669000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:28.681000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:28.697000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:28.710000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:28.723000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:28.760000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:28.920000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:28.931000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:28.943000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:28.954000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:28.968000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:28.982000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:28.996000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:29.036000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:29.195000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:29.205000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:29.215000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:29.226000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:29.240000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:29.254000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:29.267000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:29.314000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:29.477000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:29.488000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:29.501000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:29.510000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:29.521000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:29.531000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:29.542000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:29.593000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:29.752000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:29.762000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:29.772000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:29.785000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:29.797000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:29.810000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:29.824000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:29.864000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:30.028000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:30.037000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:30.048000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:30.060000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:30.072000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:30.086000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:30.100000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:30.140000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:30.299000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:30.311000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:30.321000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:30.337000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:30.357000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:30.370000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:30.383000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:30.416000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:30.575000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:30.585000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:30.595000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:30.607000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:30.629000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:30.641000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:30.655000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:30.691000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:30.853000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:30.865000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:30.877000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:30.887000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:30.910000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:30.921000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:30.935000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:30.969000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:31.129000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:31.141000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:31.153000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:31.163000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:31.182000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:31.193000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:31.207000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:31.241000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:31.400000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:31.409000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:31.423000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:31.437000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:31.460000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:31.472000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:31.486000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:31.512000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:31.675000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:31.685000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:31.695000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:31.709000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:31.733000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:31.746000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:31.759000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:31.788000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:31.951000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:31.961000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:31.970000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:31.983000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:32.005000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:32.017000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:32.031000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:32.064000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:32.227000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:32.236000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:32.247000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:32.259000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:32.277000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:32.289000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:32.303000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:32.340000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:32.510000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:32.522000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:32.534000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:32.543000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:32.553000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:32.564000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:32.576000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:32.617000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:32.779000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:32.791000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:32.803000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:32.817000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:32.830000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:32.842000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:32.856000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:32.888000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:33.052000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:33.063000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:33.075000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:33.089000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:33.105000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:33.118000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:33.132000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:33.164000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:33.327000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:33.337000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:33.347000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:33.369000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:33.381000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:33.395000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:33.409000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:33.440000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:33.603000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:33.613000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:33.623000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:33.641000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:33.653000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:33.666000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:33.681000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:33.715000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:33.882000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:33.893000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:33.905000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:33.916000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:33.926000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:33.937000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:33.951000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:33.993000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:34.153000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:34.165000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:34.177000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:34.187000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:34.202000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:34.213000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:34.227000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:34.265000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:34.424000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:34.436000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:34.447000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:34.461000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:34.480000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:34.492000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:34.505000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:34.540000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:34.703000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:34.713000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:34.723000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:34.734000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:34.752000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:34.765000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:34.779000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:34.816000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:34.980000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:34.989000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:34.999000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:35.011000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:35.032000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:35.049000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:35.065000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:35.092000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:35.256000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:35.265000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:35.275000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:35.287000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:35.304000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:35.321000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:35.337000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:35.367000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:35.533000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:35.545000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:35.557000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:35.567000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:35.577000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:35.591000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:35.607000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:35.645000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:35.809000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:35.821000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:35.833000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:35.843000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:35.855000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:35.865000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:35.883000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:35.921000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:36.081000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:36.097000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:36.109000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:36.118000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:36.130000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:36.141000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:36.159000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:36.197000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:36.357000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:36.369000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:36.381000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:36.395000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:36.406000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:36.422000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:36.435000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:36.474000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:36.637000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:36.649000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:36.661000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:36.675000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:36.686000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:36.697000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:36.711000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:36.749000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:36.910000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:36.921000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:36.933000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:36.951000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:36.963000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:36.973000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:36.987000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:37.025000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:37.189000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:37.200000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:37.213000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:37.223000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:37.238000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:37.249000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:37.263000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:37.301000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:37.465000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:37.477000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:37.489000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:37.499000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:37.522000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:37.532000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:37.547000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:37.577000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:37.741000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:37.753000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:37.765000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:37.775000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:37.798000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:37.809000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:37.823000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:37.853000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:38.015000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:38.025000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:38.035000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:38.053000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:38.077000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:38.089000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:38.103000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:38.128000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:38.291000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:38.301000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:38.311000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:38.329000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:38.352000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:38.365000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:38.379000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:38.408000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:38.573000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:38.586000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:38.597000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:38.607000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:38.626000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:38.637000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:38.651000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:38.690000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:38.853000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:38.865000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:38.877000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:38.887000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:38.903000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:38.913000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:38.927000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:38.965000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:39.129000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:39.141000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:39.153000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:39.175000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:39.194000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:39.204000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:39.215000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:39.241000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:39.405000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:39.417000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:39.429000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:39.463000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:39.482000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:39.492000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:39.504000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:39.517000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:39.681000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:39.693000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:39.705000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:39.739000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:39.759000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:39.768000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:39.783000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:39.797000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:39.960000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:39.970000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:39.980000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:40.017000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:40.037000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:40.049000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:40.063000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:40.076000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:40.240000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:40.250000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:40.260000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:40.293000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:40.313000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:40.325000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:40.339000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:40.356000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:40.519000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:40.529000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:40.539000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:40.569000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:40.588000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:40.601000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:40.615000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:40.636000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:40.800000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:40.809000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:40.819000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:40.845000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:40.864000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:40.878000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:40.892000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:40.916000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:41.396000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:41.405000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:41.417000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:41.429000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:41.440000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:41.452000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:41.463000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:41.508000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:41.675000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:41.685000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:41.697000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:41.709000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:41.721000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:41.734000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:41.746000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:41.784000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:42.422000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:42.431000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:42.441000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:42.450000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:42.460000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:42.530000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:42.541000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:42.554000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:43.052000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:43.061000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:43.072000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:43.082000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:43.094000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:43.106000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:43.152000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:43.165000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:43.340000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:43.357000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:43.367000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:43.380000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:43.393000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:43.432000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:43.445000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:43.845000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:44.007000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:44.017000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:44.029000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:44.039000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:44.050000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:44.063000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:44.074000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:44.116000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:44.658000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:44.667000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:44.677000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:44.687000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:44.699000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:44.712000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:44.720000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:44.774000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:44.937000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:44.947000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:44.960000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:44.969000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:44.982000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:44.995000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:45.006000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:45.053000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:23:45.771000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:23:45.786000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:23:45.797000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:23:45.810000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:23:45.820000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:23:45.829000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:23:45.839000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:23:45.848000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0750 ms 100.0% 
  triton_mm_598 0.1101 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1104 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1122 ms 66.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1244 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_585 0.1300 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_597 0.1300 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_606 0.1356 ms 55.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_596 0.1378 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_607 0.1383 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.6456 seconds and 0.5872 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0734 ms 100.0% 
  triton_mm_598 0.1099 ms 66.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1102 ms 66.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1119 ms 65.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1225 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_585 0.1293 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_597 0.1318 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_606 0.1363 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_596 0.1394 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_607 0.1397 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.6610 seconds and 0.5970 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0755 ms 100.0% 
  triton_mm_598 0.1104 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1109 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1127 ms 67.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1260 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_597 0.1302 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_585 0.1307 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_596 0.1385 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_606 0.1404 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_607 0.1435 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7476 seconds and 0.5606 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0751 ms 100.0% 
  triton_mm_598 0.1098 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1101 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1123 ms 66.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1224 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_597 0.1291 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_585 0.1300 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_596 0.1369 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_606 0.1379 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_607 0.1415 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7454 seconds and 0.4634 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0759 ms 100.0% 
  triton_mm_598 0.1103 ms 68.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1108 ms 68.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1126 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1263 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_585 0.1311 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_597 0.1337 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_606 0.1402 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_596 0.1413 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_607 0.1433 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7474 seconds and 0.6989 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0768 ms 100.0% 
  triton_mm_598 0.1107 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1111 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1126 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1264 ms 60.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_585 0.1314 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_597 0.1320 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_606 0.1389 ms 55.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_596 0.1395 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_607 0.1425 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7547 seconds and 0.4764 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0764 ms 100.0% 
  triton_mm_598 0.1103 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1108 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1126 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1265 ms 60.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_585 0.1315 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_597 0.1344 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_606 0.1411 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_596 0.1423 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_607 0.1435 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7622 seconds and 0.4992 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0751 ms 100.0% 
  triton_mm_598 0.1101 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1107 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1125 ms 66.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1248 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_585 0.1306 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_597 0.1313 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_606 0.1393 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_596 0.1397 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_607 0.1421 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7352 seconds and 0.4072 seconds precompiling for 31 choices
Capturing batches (bs=8 avail_mem=39.94 GB):  94%|| 49/52 [06:53<03:11, 63.69s/it]Capturing batches (bs=4 avail_mem=39.35 GB):  94%|| 49/52 [06:53<03:11, 63.69s/it][rank4]:W1027 11:24:00.612000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:00.627000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:00.636000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:00.649000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:00.666000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:00.674000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:00.682000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:00.683000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:00.698000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:00.699000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:00.706000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:00.719000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:00.737000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:00.744000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:00.753000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:00.756000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:00.770000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:00.773000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:00.781000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:00.794000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:00.812000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:00.818000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:00.827000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:00.845000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:01.258000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:01.274000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:01.282000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:01.294000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:01.320000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:01.332000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:01.344000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:01.344000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:01.355000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:01.363000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:01.367000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:01.380000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:01.405000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:01.416000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:01.416000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:01.428000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:01.435000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:01.439000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:01.440000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:01.452000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:01.478000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:01.486000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:01.488000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:01.499000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:01.506000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:01.509000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:01.513000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:01.523000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:01.549000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:01.558000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:01.569000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:01.583000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:02.158000 815 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank4]:W1027 11:24:02.162000 819 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank6]:W1027 11:24:02.177000 821 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank5]:W1027 11:24:02.180000 820 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank7]:W1027 11:24:02.197000 822 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank2]:W1027 11:24:02.219000 817 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank1]:W1027 11:24:02.234000 816 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank3]:W1027 11:24:02.236000 818 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_624 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_625 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0086 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_621 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_620 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_634 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_631 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9280 seconds and 0.1987 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_626 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_624 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_625 0.0085 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_629 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_628 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_620 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_621 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_634 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_635 0.0088 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9122 seconds and 0.1472 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_625 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_624 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_626 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0084 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_629 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_620 0.0087 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_621 0.0087 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_635 0.0087 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_630 0.0088 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9258 seconds and 0.1961 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_624 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_625 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_634 0.0085 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_628 0.0085 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_621 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_629 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_630 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_635 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9333 seconds and 0.2375 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_626 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_625 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_624 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0085 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_628 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_634 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_630 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_635 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_620 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9434 seconds and 0.1323 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_625 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_624 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_628 0.0086 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_629 0.0087 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_635 0.0087 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_634 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_630 0.0088 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_631 0.0088 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8775 seconds and 0.1514 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_625 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_624 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_626 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_629 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_635 0.0088 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_620 0.0088 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_630 0.0088 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_631 0.0088 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_634 0.0088 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8458 seconds and 0.1587 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_627 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_624 0.0084 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_626 0.0084 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_625 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0085 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_629 0.0085 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_630 0.0086 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_620 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_621 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_631 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8933 seconds and 0.1528 seconds precompiling for 25 choices
[rank2]:W1027 11:24:12.495000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:12.556000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:12.585000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:12.593000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:12.637000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:12.784000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:13.011000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:13.065000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:13.100000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:13.111000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:13.143000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:13.256000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:13.281000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:13.540000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:13.764000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:14.054000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:14.508000 819 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank7]:W1027 11:24:14.511000 822 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank6]:W1027 11:24:14.531000 821 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank0]:W1027 11:24:14.555000 815 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank2]:W1027 11:24:14.588000 817 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank1]:W1027 11:24:14.732000 816 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank3]:W1027 11:24:15.183000 818 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank5]:W1027 11:24:15.331000 820 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0086 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0087 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_658 0.0094 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_659 0.0095 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_654 0.0095 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0097 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_657 0.0099 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_641 0.0099 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8913 seconds and 0.4098 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0081 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0084 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0085 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_658 0.0089 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_659 0.0090 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_654 0.0092 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0093 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_640 0.0094 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_641 0.0094 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9291 seconds and 0.4059 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_647 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_646 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_648 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_659 0.0092 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_658 0.0093 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_655 0.0094 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_654 0.0095 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_640 0.0098 ms 86.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_641 0.0098 ms 86.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9057 seconds and 0.4255 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_649 0.0086 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0086 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_658 0.0091 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_659 0.0091 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_654 0.0092 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0092 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_640 0.0094 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_641 0.0095 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9106 seconds and 0.4013 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_647 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_646 0.0083 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_658 0.0090 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_659 0.0090 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_655 0.0092 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_654 0.0093 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_641 0.0096 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_640 0.0097 ms 84.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8132 seconds and 0.4142 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0083 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0087 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0087 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_659 0.0091 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_658 0.0092 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_655 0.0093 ms 87.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_654 0.0095 ms 86.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_641 0.0096 ms 85.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_640 0.0097 ms 84.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9016 seconds and 0.4066 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0085 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0088 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0088 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_659 0.0093 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_654 0.0094 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0094 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_658 0.0094 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_641 0.0098 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_640 0.0098 ms 85.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8873 seconds and 0.4112 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_647 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_646 0.0082 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_648 0.0086 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0086 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_659 0.0091 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_658 0.0092 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_655 0.0094 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_640 0.0095 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_641 0.0095 ms 85.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_654 0.0095 ms 85.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9189 seconds and 0.3989 seconds precompiling for 25 choices
[rank7]:W1027 11:24:21.079000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:21.097000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:21.113000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:21.138000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:21.157000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:21.176000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:21.191000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:21.207000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:21.217000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:21.226000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:21.241000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:21.267000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:21.274000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:21.351000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:21.365000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:21.401000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:21.442000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:21.492000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:21.703000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:21.781000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:21.832000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:22.176000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:22.258000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:22.309000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:25 DP3 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:25 DP7 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:25 DP6 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:25 DP0 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:25 DP1 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:25 DP2 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:26 DP4 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:26 DP5 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1027 11:24:26.913000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:26.923000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:26.936000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:26.947000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:26.956000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:26.970000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:26.983000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:26.984000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:26.993000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:27.007000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:27.018000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:27.027000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:27.040000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:27.054000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:27.055000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:27.063000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:27.078000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:27.089000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:27.097000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:27.111000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:27 DP4 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:27 DP1 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1027 11:24:27.126000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:27 DP6 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1027 11:24:27.138000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:27 DP7 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:27 DP0 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:27 DP3 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:27 DP2 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1027 11:24:27.209000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:27.280000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:27 DP5 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1027 11:24:27.835000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:27.847000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:27.858000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:27.872000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:27.883000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:27.895000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:27.908000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:27.924000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:28.120000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:28.142000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:28.154000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:28.166000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:28.178000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:28.191000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:28.204000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:28.216000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:28.400000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:28.422000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:28.434000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:28.446000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:28.458000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:28.472000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:28.484000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:28.496000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:28 DP6 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:28 DP2 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:28 DP0 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:28 DP7 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:28 DP3 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:28 DP1 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:28 DP4 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:28 DP5 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1027 11:24:29.072000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:29.098000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:29.110000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:29.123000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:29.133000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:29.144000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:29.146000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:29.162000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:29.170000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:29.176000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:29.181000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:29.195000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:29.205000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:29.215000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:29.217000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:29.232000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:29.241000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:29.247000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:29.252000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:29.266000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:29 DP6 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1027 11:24:29.276000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:29.287000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:29 DP2 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1027 11:24:29.302000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:29 DP0 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1027 11:24:29.318000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:29 DP7 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:29 DP3 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:29 DP1 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:29 DP4 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:24:29 DP5 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1027 11:24:29.758000 821 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce10dac10>
[rank6]:W1027 11:24:29.782000 821 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce2168ae0>
[rank2]:W1027 11:24:29.791000 817 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5af70>
[rank0]:W1027 11:24:29.799000 815 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356ac40>
[rank7]:W1027 11:24:29.812000 822 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f67434abfc0>
[rank2]:W1027 11:24:29.814000 817 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5b2a0>
[rank0]:W1027 11:24:29.822000 815 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356aca0>
[rank3]:W1027 11:24:29.825000 818 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ebbfc0>
[rank7]:W1027 11:24:29.835000 822 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f673706ee20>
[rank1]:W1027 11:24:29.836000 816 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2bb0>
[rank3]:W1027 11:24:29.848000 818 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ff3f00>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:24:29 DP6 TP6] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1027 11:24:29.851000 819 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6730>
[rank1]:W1027 11:24:29.859000 816 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2f40>
[rank5]:W1027 11:24:29.863000 820 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef532bfc0>
[rank4]:W1027 11:24:29.874000 819 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6e50>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:24:29 DP2 TP2] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1027 11:24:29.887000 820 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef44bad30>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:24:29 DP0 TP0] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:24:29 DP7 TP7] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:24:29 DP3 TP3] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:24:29 DP1 TP1] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:24:29 DP4 TP4] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:24:29 DP5 TP5] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1027 11:24:31.515000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:31.524000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:31.533000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:31.543000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:31.555000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:31.575000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:31.617000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:31.633000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:31.799000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:31.809000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:31.819000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:31.830000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:31.843000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:31.856000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:31.893000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:31.909000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:32.079000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:32.089000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:32.098000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:32.111000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:32.125000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:32.139000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:32.169000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:32.185000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:32.359000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:32.371000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:32.380000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:32.395000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:32.417000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:32.430000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:32.445000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:32.459000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:32.639000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:32.651000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:32.661000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:32.679000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:32.697000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:32.710000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:32.724000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:32.738000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:32.919000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:32.931000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:32.941000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:32.963000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:32.989000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:33.002000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:33.017000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:33.031000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:33.199000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:33.211000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:33.221000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:33.247000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:33.269000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:33.282000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:33.296000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:33.310000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:33.479000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:33.491000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:33.501000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:33.531000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:33.549000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:33.562000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:33.577000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:33.591000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:33.759000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:33.772000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:33.781000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:33.815000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:33.830000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:33.842000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:33.857000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:33.871000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:34.039000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:34.051000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:34.061000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:34.095000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:34.109000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:34.122000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:34.137000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:34.150000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:34.326000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:34.338000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:34.349000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:34.381000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:34.399000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:34.409000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:34.423000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:34.435000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:35.020000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:35.032000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:35.042000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:35.052000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:35.065000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:35.077000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:35.090000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:35.136000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:35.686000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:35.696000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:35.706000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:35.718000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:35.730000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:35.740000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:35.750000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:35.798000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:36.478000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:36.490000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:36.501000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:36.513000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:36.523000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:36.575000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:36.585000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:36.597000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:37.215000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:37.224000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:37.237000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:37.249000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:37.258000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:37.268000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:37.329000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:37.340000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:37.878000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:37.888000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:37.901000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:37.911000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:37.923000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:37.934000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:37.944000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:37.993000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:38.166000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:38.177000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:38.189000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:38.200000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:38.213000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:38.226000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:38.237000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:38.272000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:38.450000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:38.464000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:38.476000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:38.487000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:38.500000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:38.512000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:38.523000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:38.553000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:38.746000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:38.756000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:38.770000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:38.780000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:38.794000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:38.807000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:38.817000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:38.836000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:39.030000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:39.044000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:39.056000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:39.067000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:39.080000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:39.095000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:39.106000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:39.118000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:39.314000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:39.331000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:39.344000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:39.355000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:39.368000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:39.383000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:39.394000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:39.406000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:39.598000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:39.616000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:39.630000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:39.641000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:39.654000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:39.672000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:39.682000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:39.694000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:39.882000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:39.904000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:39.916000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:39.927000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:39.939000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:39.960000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:39.970000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:39.982000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:40.170000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:40.192000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:40.204000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:40.215000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:40.227000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:40.248000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:40.258000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:40.271000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:40.468000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:40.482000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:40.500000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:40.512000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:40.524000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:40.538000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:40.551000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:40.562000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:40.756000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:40.770000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:40.788000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:40.800000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:40.811000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:40.827000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:40.839000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:40.850000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:41.047000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:41.057000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:41.086000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:41.096000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:41.110000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:41.122000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:41.132000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:41.149000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:41.334000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:41.344000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:41.370000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:41.381000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:41.393000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:41.407000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:41.417000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:41.432000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:41.618000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:41.628000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:41.650000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:41.663000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:41.677000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:41.691000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:41.703000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:41.715000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:41.902000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:41.912000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:41.934000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:41.948000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:41.961000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:41.976000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:41.987000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:42.000000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:42.186000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:42.196000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:42.214000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:42.232000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:42.246000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:42.261000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:42.271000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:42.284000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:42.470000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:42.481000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:42.494000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:42.515000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:42.529000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:42.543000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:42.555000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:42.567000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:42.755000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:42.764000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:42.777000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:42.796000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:42.809000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:42.828000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:42.839000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:42.852000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:43.038000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:43.048000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:43.061000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:43.080000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:43.094000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:43.113000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:43.123000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:43.135000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:43.326000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:43.336000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:43.349000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:43.364000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:43.377000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:43.395000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:43.407000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:43.420000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:43.610000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:43.620000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:43.632000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:43.648000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:43.661000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:43.681000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:43.691000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:43.703000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:43.903000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:43.916000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:43.929000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:43.939000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:43.952000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:43.967000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:43.977000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:43.989000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:44.190000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:44.200000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:44.210000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:44.226000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:44.236000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:44.253000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:44.267000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:44.278000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:44.472000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:44.486000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:44.499000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:44.509000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:44.525000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:44.538000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:44.548000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:44.564000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:44.755000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:44.770000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:44.782000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:44.793000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:44.809000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:44.823000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:44.833000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:44.846000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:45.039000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:45.054000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:45.066000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:45.077000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:45.093000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:45.108000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:45.118000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:45.131000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:45.323000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:45.335000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:45.350000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:45.361000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:45.385000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:45.398000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:45.408000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:45.420000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:45.610000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:45.620000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:45.631000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:45.646000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:45.676000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:45.690000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:45.703000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:45.715000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:45.894000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:45.904000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:45.915000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:45.929000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:45.964000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:45.977000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:45.990000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:46.002000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:46.176000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:46.190000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:46.203000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:46.214000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:46.253000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:46.266000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:46.276000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:46.289000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:46.459000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:46.474000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:46.487000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:46.498000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:46.538000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:46.556000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:46.566000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:46.578000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:46.748000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:46.760000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:46.773000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:46.784000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:46.821000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:46.839000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:46.852000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:46.864000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:47.032000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:47.046000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:47.062000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:47.072000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:47.117000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:47.130000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:47.144000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:47.154000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:47.326000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:47.336000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:47.346000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:47.359000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:47.413000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:47.424000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:47.437000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:47.450000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:47.620000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:47.632000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:47.645000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:47.656000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:47.692000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:47.714000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:47.726000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:47.738000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:47.904000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:47.918000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:47.931000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:47.942000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:47.975000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:47.997000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:48.012000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:48.024000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:48.192000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:48.204000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:48.217000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:48.228000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:48.259000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:48.281000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:48.296000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:48.308000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:48.479000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:48.491000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:48.504000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:48.515000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:48.543000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:48.566000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:48.584000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:48.595000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:48.764000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:48.778000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:48.791000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:48.802000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:48.827000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:48.853000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:48.872000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:48.884000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:49.052000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:49.064000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:49.077000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:49.088000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:49.111000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:49.137000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:49.160000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:49.172000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:49.340000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:49.352000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:49.365000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:49.376000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:49.395000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:49.421000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:49.448000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:49.459000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:49.630000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:49.640000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:49.651000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:49.664000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:49.681000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:49.703000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:49.738000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:49.747000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:24:50.472000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:24:50.485000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:24:50.496000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:24:50.507000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:24:50.517000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:24:50.528000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:24:50.540000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:24:50.555000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0623 ms 100.0% 
  triton_mm_675 0.1053 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_663 0.1059 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1146 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1146 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1161 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1183 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1211 ms 51.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_681 0.1568 ms 39.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_680 0.1569 ms 39.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8081 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0621 ms 100.0% 
  triton_mm_663 0.1043 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1057 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1134 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1139 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1154 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1176 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1199 ms 51.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_680 0.1565 ms 39.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_681 0.1566 ms 39.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8003 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_663 0.1044 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1056 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1135 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1136 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1153 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1161 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1181 ms 51.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_672 0.1551 ms 39.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_681 0.1564 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.7751 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0628 ms 100.0% 
  triton_mm_663 0.1050 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1059 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1146 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1150 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1163 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1182 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1202 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_681 0.1567 ms 40.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_680 0.1571 ms 40.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8917 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0624 ms 100.0% 
  triton_mm_663 0.1052 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1055 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1143 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1146 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1160 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1173 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1196 ms 52.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_681 0.1565 ms 39.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_680 0.1567 ms 39.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8021 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0631 ms 100.0% 
  triton_mm_663 0.1046 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1057 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1146 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1149 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1161 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1192 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1215 ms 51.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_681 0.1565 ms 40.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_680 0.1566 ms 40.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.7696 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0620 ms 100.0% 
  triton_mm_663 0.1042 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1053 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1136 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1138 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1156 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1158 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1188 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_672 0.1549 ms 40.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_681 0.1564 ms 39.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8133 seconds and 0.0000 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0632 ms 100.0% 
  triton_mm_663 0.1052 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1055 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1149 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_683 0.1149 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_673 0.1164 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1177 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1217 ms 51.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_681 0.1566 ms 40.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_680 0.1568 ms 40.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.7828 seconds and 0.0001 seconds precompiling for 27 choices
Capturing batches (bs=4 avail_mem=39.35 GB):  96%|| 50/52 [07:56<02:06, 63.40s/it]Capturing batches (bs=2 avail_mem=38.88 GB):  96%|| 50/52 [07:56<02:06, 63.40s/it][rank3]:W1027 11:25:03.370000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:03.393000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:03.405000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:03.420000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:03.428000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:03.441000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:03.444000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:03.457000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:03.464000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:03.470000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:03.476000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:03.492000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:03.499000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:03.515000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:03.516000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:03.528000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:03.539000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:03.541000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:03.551000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:03.567000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:03.573000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:03.590000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:03.603000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:03.616000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:04.018000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:04.040000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:04.052000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:04.068000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:04.076000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:04.103000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:04.106000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:04.117000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:04.125000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:04.126000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:04.138000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:04.155000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:04.163000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:04.179000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:04.193000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:04.198000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:04.201000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:04.210000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:04.215000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:04.227000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:04.236000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:04.251000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:04.266000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:04.269000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:04.273000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:04.282000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:04.288000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:04.299000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:04.306000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:04.337000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:04.344000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:04.359000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:04.945000 821 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank3]:W1027 11:25:04.946000 818 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank0]:W1027 11:25:04.952000 815 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank7]:W1027 11:25:04.965000 822 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank5]:W1027 11:25:04.979000 820 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank1]:W1027 11:25:04.994000 816 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank4]:W1027 11:25:05.023000 819 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank2]:W1027 11:25:05.028000 817 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_698 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_701 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_699 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_700 0.0085 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_703 0.0085 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_708 0.0085 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_702 0.0085 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_709 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_705 0.0086 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_707 0.0086 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8494 seconds and 0.1570 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_699 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_698 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_700 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_701 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_708 0.0086 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_709 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_702 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_703 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_707 0.0088 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_695 0.0088 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8691 seconds and 0.2399 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_701 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_700 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_698 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_699 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_706 0.0084 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_708 0.0084 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_707 0.0084 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_709 0.0084 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_702 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_703 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8985 seconds and 0.2231 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_699 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_698 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_700 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_701 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_708 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_702 0.0087 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_703 0.0087 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_707 0.0087 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_694 0.0087 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_704 0.0087 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8727 seconds and 0.2133 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_699 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_700 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_698 0.0082 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_701 0.0082 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_702 0.0083 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_703 0.0083 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_709 0.0084 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_705 0.0084 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_707 0.0084 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_708 0.0084 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8173 seconds and 0.1597 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_698 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_700 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_701 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_699 0.0083 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_702 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_709 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_708 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_695 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_703 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_704 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8931 seconds and 0.2409 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_698 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_699 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_701 0.0083 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_700 0.0083 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_702 0.0084 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_703 0.0084 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_708 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_709 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_704 0.0085 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_706 0.0085 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8190 seconds and 0.1431 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_701 0.0080 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_700 0.0081 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_699 0.0081 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_698 0.0081 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_703 0.0083 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_702 0.0083 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_694 0.0083 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_708 0.0083 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_706 0.0083 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_709 0.0084 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9434 seconds and 0.1788 seconds precompiling for 25 choices
[rank2]:W1027 11:25:15.127000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:15.154000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:15.298000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:15.349000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:15.431000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:15.647000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:15.659000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:15.677000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:15.811000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:15.812000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:15.856000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:15.942000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:16.113000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:16.188000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:16.323000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:16.619000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:17.139000 818 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank2]:W1027 11:25:17.182000 817 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank1]:W1027 11:25:17.216000 816 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank4]:W1027 11:25:17.362000 819 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank0]:W1027 11:25:17.592000 815 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank6]:W1027 11:25:17.609000 821 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank5]:W1027 11:25:17.942000 820 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank7]:W1027 11:25:18.050000 822 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_720 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_721 0.0084 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0086 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0086 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_732 0.0093 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_733 0.0093 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_728 0.0095 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0096 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_714 0.0096 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_715 0.0098 ms 84.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9604 seconds and 0.4162 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_721 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0083 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0083 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_720 0.0083 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_733 0.0090 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_732 0.0090 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_728 0.0091 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0092 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_715 0.0097 ms 84.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_714 0.0098 ms 83.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9522 seconds and 0.4247 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_721 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_720 0.0084 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_722 0.0085 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0085 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_732 0.0091 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_733 0.0092 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_728 0.0095 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0096 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_715 0.0098 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_714 0.0099 ms 83.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9532 seconds and 0.4293 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_720 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_722 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_721 0.0085 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_723 0.0085 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_733 0.0091 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_732 0.0091 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_729 0.0093 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_728 0.0094 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_714 0.0095 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_715 0.0097 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8937 seconds and 0.4023 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_721 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_720 0.0083 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_722 0.0085 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0085 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_733 0.0090 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_732 0.0090 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_728 0.0092 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0092 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_730 0.0099 ms 83.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_731 0.0099 ms 82.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9170 seconds and 0.4223 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_720 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_721 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0087 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0087 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_733 0.0094 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_732 0.0094 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_729 0.0096 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_714 0.0096 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_728 0.0096 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_715 0.0097 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9390 seconds and 0.4252 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_721 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_720 0.0082 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_722 0.0085 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0085 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_733 0.0093 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_714 0.0095 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_732 0.0095 ms 84.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_728 0.0097 ms 83.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_715 0.0097 ms 83.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_729 0.0097 ms 82.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8489 seconds and 0.4060 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_721 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_720 0.0082 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_722 0.0083 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0083 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_732 0.0092 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_733 0.0093 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_728 0.0093 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0095 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_714 0.0095 ms 84.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_715 0.0096 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9782 seconds and 0.4081 seconds precompiling for 25 choices
[rank3]:W1027 11:25:23.765000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:23.798000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:23.831000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:23.844000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:23.879000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:23.894000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:23.910000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:23.931000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:23.961000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:24.165000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:24.212000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:24.243000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:24.250000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:24.290000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:24.309000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:24.321000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:24.340000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:24.372000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:24.438000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:24.517000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:24.568000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:24.693000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:24.772000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:24.824000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:28 DP3 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:28 DP2 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:28 DP1 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:28 DP4 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:28 DP0 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:28 DP6 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:29 DP5 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:29 DP7 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1027 11:25:29.370000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:29.381000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:29.395000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:29.405000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:29.414000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:29.430000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:29.442000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:29.445000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:29.453000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:29.467000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:29.476000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:29.485000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:29.486000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:29.496000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:29.501000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:29.510000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:29.516000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:29.520000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:29.528000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:29.544000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:29 DP6 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:29 DP4 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1027 11:25:29.559000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:29 DP2 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:29 DP1 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:29 DP0 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1027 11:25:29.596000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:29 DP3 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:29 DP5 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1027 11:25:29.667000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:29.711000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:29 DP7 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1027 11:25:30.272000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:30.284000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:30.297000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:30.307000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:30.317000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:30.331000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:30.345000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:30.360000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:30.571000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:30.584000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:30.596000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:30.611000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:30.623000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:30.634000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:30.646000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:30.658000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:30.858000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:30.872000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:30.886000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:30.899000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:30.911000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:30.924000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:30.936000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:30.948000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP1 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP6 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP2 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP3 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP0 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP5 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP4 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP7 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1027 11:25:31.542000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:31.566000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:31.576000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:31.589000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:31.602000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:31.614000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:31.619000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:31.629000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:31.639000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:31.641000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:31.648000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:31.660000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:31.673000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:31.685000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:31.692000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:31.701000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:31.710000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:31.714000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:31.720000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:31.731000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP1 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1027 11:25:31.744000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:31.764000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP6 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1027 11:25:31.773000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP2 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1027 11:25:31.785000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP3 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP0 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP5 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP4 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 11:25:31 DP7 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1027 11:25:32.229000 816 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2bb0>
[rank1]:W1027 11:25:32.252000 816 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2f40>
[rank6]:W1027 11:25:32.255000 821 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce10dac10>
[rank2]:W1027 11:25:32.269000 817 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5af70>
[rank6]:W1027 11:25:32.279000 821 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce2168ae0>
[rank3]:W1027 11:25:32.281000 818 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ebbfc0>
[rank2]:W1027 11:25:32.293000 817 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5b2a0>
[rank0]:W1027 11:25:32.293000 815 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356ac40>
[rank3]:W1027 11:25:32.304000 818 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ff3f00>
[rank5]:W1027 11:25:32.309000 820 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef532bfc0>
[rank0]:W1027 11:25:32.316000 815 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356aca0>
[rank4]:W1027 11:25:32.319000 819 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6730>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:25:32 DP1 TP1] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1027 11:25:32.331000 822 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f67434abfc0>
[rank5]:W1027 11:25:32.333000 820 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef44bad30>
[rank4]:W1027 11:25:32.343000 819 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6e50>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:25:32 DP6 TP6] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1027 11:25:32.354000 822 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f673706ee20>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:25:32 DP2 TP2] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:25:32 DP3 TP3] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:25:32 DP0 TP0] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:25:32 DP5 TP5] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:25:32 DP4 TP4] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:25:32 DP7 TP7] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1027 11:25:33.964000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:33.974000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:33.983000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:33.993000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:34.005000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:34.018000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:34.030000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:34.094000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:34.264000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:34.274000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:34.284000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:34.294000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:34.307000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:34.321000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:34.335000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:34.382000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:34.552000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:34.568000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:34.578000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:34.588000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:34.602000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:34.616000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:34.629000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:34.666000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:34.840000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:34.860000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:34.870000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:34.881000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:34.893000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:34.908000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:34.921000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:34.954000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:35.131000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:35.152000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:35.162000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:35.173000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:35.185000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:35.199000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:35.213000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:35.242000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:35.424000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:35.444000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:35.455000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:35.465000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:35.478000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:35.492000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:35.505000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:35.530000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:35.716000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:35.736000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:35.746000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:35.757000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:35.769000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:35.783000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:35.796000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:35.817000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:36.007000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:36.028000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:36.038000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:36.048000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:36.061000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:36.075000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:36.089000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:36.106000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:36.300000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:36.320000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:36.330000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:36.340000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:36.354000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:36.368000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:36.381000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:36.402000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:36.592000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:36.612000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:36.622000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:36.632000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:36.646000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:36.660000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:36.673000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:36.690000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:36.884000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:36.904000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:36.914000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:36.924000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:36.937000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:36.951000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:36.965000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:36.986000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:37.175000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:37.196000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:37.206000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:37.216000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:37.230000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:37.244000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:37.257000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:37.274000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:37.467000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:37.488000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:37.498000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:37.510000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:37.522000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:37.537000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:37.550000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:37.565000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:37.760000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:37.780000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:37.790000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:37.800000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:37.813000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:37.828000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:37.841000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:37.856000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:38.052000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:38.072000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:38.082000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:38.092000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:38.105000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:38.120000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:38.133000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:38.148000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:38.344000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:38.364000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:38.374000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:38.385000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:38.398000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:38.413000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:38.426000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:38.441000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:38.636000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:38.656000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:38.666000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:38.677000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:38.690000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:38.705000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:38.718000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:38.733000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:38.928000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:38.948000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:38.959000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:38.969000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:38.990000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:39.004000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:39.017000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:39.034000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:39.220000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:39.240000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:39.250000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:39.260000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:39.282000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:39.298000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:39.313000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:39.334000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:39.512000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:39.536000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:39.546000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:39.556000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:39.569000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:39.584000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:39.600000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:39.622000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:40.411000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:40.423000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:40.433000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:40.443000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:40.453000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:40.465000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:40.522000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:40.536000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:41.282000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:41.295000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:41.307000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:41.320000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:41.330000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:41.341000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:41.396000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:41.407000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:42.095000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:42.107000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:42.118000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:42.128000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:42.138000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:42.151000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:42.164000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:42.209000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:42.891000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:42.901000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:42.914000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:42.926000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:42.938000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:42.950000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:42.960000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:43.004000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:43.675000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:43.685000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:43.695000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:43.705000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:43.717000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:43.728000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:43.741000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:43.794000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:44.524000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:44.536000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:44.547000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:44.559000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:44.571000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:44.582000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:44.592000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:44.638000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:44.824000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:44.836000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:44.847000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:44.860000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:44.873000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:44.885000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:44.897000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:44.926000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:45.124000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:45.137000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:45.147000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:45.160000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:45.173000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:45.186000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:45.197000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:45.214000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:45.424000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:45.437000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:45.447000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:45.461000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:45.473000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:45.486000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:45.498000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:45.512000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:45.724000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:45.737000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:45.748000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:45.761000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:45.773000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:45.786000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:45.798000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:45.812000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:46.024000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:46.042000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:46.053000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:46.070000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:46.083000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:46.095000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:46.107000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:46.121000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:46.324000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:46.336000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:46.348000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:46.366000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:46.379000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:46.392000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:46.403000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:46.416000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:46.624000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:46.642000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:46.653000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:46.674000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:46.687000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:46.698000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:46.712000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:46.725000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:46.924000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:46.938000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:46.949000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:46.970000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:46.982000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:46.994000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:47.007000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:47.021000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:47.224000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:47.236000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:47.247000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:47.270000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:47.283000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:47.294000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:47.308000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:47.321000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:47.526000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:47.537000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:47.547000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:47.566000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:47.579000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:47.590000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:47.605000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:47.617000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:47.819000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:47.836000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:47.846000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:47.862000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:47.874000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:47.886000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:47.900000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:47.913000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:48.110000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:48.136000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:48.146000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:48.159000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:48.171000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:48.183000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:48.196000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:48.210000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:48.402000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:48.436000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:48.446000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:48.459000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:48.471000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:48.482000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:48.496000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:48.509000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:48.694000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:48.736000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:48.746000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:48.759000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:48.771000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:48.783000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:48.795000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:48.809000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:48.986000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:49.036000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:49.046000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:49.059000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:49.072000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:49.083000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:49.095000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:49.109000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:49.286000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:49.336000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:49.346000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:49.359000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:49.371000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:49.382000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:49.396000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:49.409000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:49.586000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:49.636000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:49.646000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:49.659000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:49.670000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:49.683000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:49.697000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:49.709000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:49.886000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:49.936000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:49.946000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:49.960000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:49.971000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:49.983000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:49.997000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:50.010000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:50.186000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:50.236000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:50.246000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:50.259000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:50.271000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:50.283000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:50.297000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:50.310000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:50.490000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:50.536000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:50.546000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:50.559000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:50.571000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:50.582000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:50.596000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:50.608000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:50.782000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:50.836000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:50.846000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:50.859000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:50.872000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:50.883000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:50.897000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:50.909000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:51.095000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:51.136000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:51.146000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:51.166000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:51.177000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:51.191000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:51.202000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:51.217000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:51.390000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:51.436000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:51.446000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:51.462000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:51.474000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:51.486000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:51.499000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:51.513000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:51.686000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:51.736000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:51.746000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:51.759000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:51.771000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:51.783000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:51.796000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:51.810000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:51.987000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:52.036000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:52.046000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:52.059000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:52.071000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:52.084000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:52.098000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:52.111000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:52.287000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:52.336000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:52.346000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:52.360000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:52.371000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:52.384000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:52.396000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:52.411000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:52.594000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:52.636000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:52.646000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:52.664000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:52.677000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:52.690000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:52.703000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:52.717000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:52.898000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:52.936000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:52.946000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:52.957000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:52.974000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:52.987000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:53.001000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:53.013000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:53.190000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:53.236000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:53.246000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:53.257000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:53.270000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:53.283000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:53.296000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:53.310000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:53.486000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:53.536000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:53.546000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:53.557000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:53.570000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:53.583000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:53.597000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:53.609000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:53.786000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:53.836000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:53.846000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:53.857000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:53.870000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:53.883000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:53.897000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:53.910000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:25:54.651000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:25:54.662000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:25:54.673000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:25:54.687000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:25:54.698000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:25:54.710000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:25:54.720000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:25:54.734000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0623 ms 100.0% 
  triton_mm_736 0.1036 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_737 0.1037 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_746 0.1043 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_747 0.1043 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_755 0.1093 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1094 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1130 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1132 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_740 0.1483 ms 42.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2232 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0621 ms 100.0% 
  triton_mm_736 0.1035 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_737 0.1036 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_747 0.1044 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_746 0.1044 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1093 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1094 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1129 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1130 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_741 0.1482 ms 41.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2157 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_736 0.1036 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_737 0.1037 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_746 0.1044 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_747 0.1045 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_755 0.1089 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1090 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_744 0.1124 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1125 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_741 0.1478 ms 41.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1795 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0618 ms 100.0% 
  triton_mm_736 0.1037 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_737 0.1037 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_747 0.1046 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_746 0.1046 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_754 0.1090 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1090 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1124 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1125 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_740 0.1475 ms 41.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2311 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0619 ms 100.0% 
  triton_mm_736 0.1037 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_737 0.1037 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_747 0.1045 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_746 0.1047 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1093 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1094 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1133 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1134 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_741 0.1484 ms 41.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2090 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0621 ms 100.0% 
  triton_mm_737 0.1037 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_736 0.1037 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_747 0.1046 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_746 0.1047 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1093 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1094 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_744 0.1132 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1134 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_740 0.1482 ms 41.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1171 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0619 ms 100.0% 
  triton_mm_737 0.1037 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_736 0.1037 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_746 0.1045 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_747 0.1045 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1091 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1091 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1125 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1126 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_741 0.1476 ms 41.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1709 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0622 ms 100.0% 
  triton_mm_737 0.1037 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_736 0.1037 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_746 0.1047 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_747 0.1047 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1094 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1095 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1132 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1132 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_741 0.1483 ms 41.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2451 seconds and 0.0001 seconds precompiling for 25 choices
Capturing batches (bs=2 avail_mem=38.88 GB):  98%|| 51/52 [08:59<01:03, 63.45s/it]Capturing batches (bs=1 avail_mem=38.41 GB):  98%|| 51/52 [08:59<01:03, 63.45s/it][rank5]:W1027 11:26:06.956000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:06.964000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:06.975000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:07.003000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:07.013000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:07.028000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:07.029000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:07.036000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:07.046000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:07.075000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:07.084000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:07.094000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:07.102000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:07.105000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:07.113000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:07.122000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:07.153000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:07.161000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:07.166000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:07.188000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:07.243000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:07.393000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:07.465000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:07.542000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:07.607000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:07.619000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:07.640000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:07.669000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:07.685000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:07.696000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:07.699000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:07.709000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:07.728000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:07.744000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:07.755000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:07.775000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:07.775000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:07.783000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:07.784000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:07.801000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:07.828000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:07.833000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:07.848000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:07.849000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:07.857000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:07.858000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:07.873000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:07.900000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:07.907000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:07.922000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:07.931000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:07.980000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:08.047000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:08.136000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:08.209000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:08.282000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:13.914000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:14.307000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:14.337000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:14.420000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:14.425000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:14.442000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:14.456000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:14.503000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:14.674000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:14.823000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:14.849000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:14.925000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:14.954000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:14.978000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:15.001000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:15.182000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:16.427000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:16.510000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:16.563000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:17.046000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:17.128000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:17.181000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:17.211000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:17.255000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:17.293000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:17.328000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:17.338000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:17.346000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:17.375000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:17.390000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:17.409000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:17.455000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:17.460000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:17.499000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:17.507000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:17.542000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:17.578000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:17.624000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:17.630000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:17.676000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:21.301000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:21.317000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:21.328000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:21.345000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:21.355000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:21.362000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:21.372000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:21.375000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:21.388000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:21.399000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:21.416000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:21.417000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:21.429000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:21.431000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:21.434000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:21.442000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:21.449000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:21.460000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:21.473000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:21.478000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:21.493000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:21.524000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:21.594000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:21.637000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:22.200000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:22.214000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:22.230000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:22.242000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:22.255000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:22.268000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:22.282000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:22.294000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:22.498000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:22.511000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:22.528000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:22.541000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:22.552000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:22.567000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:22.585000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:22.598000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:22.792000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:22.822000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:22.833000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:22.846000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:22.861000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:22.872000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:22.884000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:22.899000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:23.492000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:23.526000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:23.533000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:23.550000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:23.558000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:23.565000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:23.571000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:23.585000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:23.600000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:23.601000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:23.605000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:23.625000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:23.631000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:23.637000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:23.643000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:23.657000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:23.672000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:23.673000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:23.677000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:23.698000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:23.703000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:23.715000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:23.729000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:23.744000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:24.163000 822 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f67434abfc0>
[rank7]:W1027 11:26:24.187000 822 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f673706ee20>
[rank4]:W1027 11:26:24.199000 819 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6730>
[rank0]:W1027 11:26:24.211000 815 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356ac40>
[rank4]:W1027 11:26:24.223000 819 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8dc58a6e50>
[rank2]:W1027 11:26:24.226000 817 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5af70>
[rank0]:W1027 11:26:24.234000 815 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7eba7356aca0>
[rank1]:W1027 11:26:24.238000 816 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2bb0>
[rank2]:W1027 11:26:24.249000 817 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f89c9c5b2a0>
[rank6]:W1027 11:26:24.252000 821 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce10dac10>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:26:24 DP7 TP7] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1027 11:26:24.261000 816 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fa7d6bc2f40>
[rank5]:W1027 11:26:24.264000 820 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef532bfc0>
[rank6]:W1027 11:26:24.275000 821 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7edce2168ae0>
[rank3]:W1027 11:26:24.276000 818 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ebbfc0>
[rank5]:W1027 11:26:24.287000 820 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7faef44bad30>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:26:24 DP4 TP4] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1027 11:26:24.299000 818 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f39a9ff3f00>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:26:24 DP0 TP0] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:26:24 DP2 TP2] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:26:24 DP1 TP1] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:26:24 DP6 TP6] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:26:24 DP5 TP5] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:26:24 DP3 TP3] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1027 11:26:25.664000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:25.687000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:25.718000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:25.731000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:25.750000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:25.762000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:25.774000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:25.788000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:25.964000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:25.984000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:26.022000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:26.035000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:26.049000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:26.061000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:26.074000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:26.087000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:26.260000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:26.280000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:26.330000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:26.343000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:26.357000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:26.369000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:26.381000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:26.395000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:26.574000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:26.587000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:26.632000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:26.641000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:26.653000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:26.667000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:26.683000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:26.693000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:26.872000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:26.883000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:26.938000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:26.951000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:26.964000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:26.976000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:26.988000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:27.003000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:27.180000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:27.190000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:27.242000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:27.256000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:27.269000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:27.281000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:27.293000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:27.307000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:27.484000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:27.494000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:27.546000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:27.559000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:27.572000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:27.585000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:27.598000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:27.610000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:27.788000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:27.798000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:27.850000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:27.863000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:27.878000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:27.890000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:27.902000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:27.916000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:28.092000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:28.102000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:28.166000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:28.179000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:28.192000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:28.205000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:28.218000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:28.230000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:28.416000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:28.426000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:28.483000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:28.494000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:28.509000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:28.520000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:28.532000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:28.546000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:28.720000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:28.730000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:28.787000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:28.799000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:28.813000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:28.825000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:28.837000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:28.851000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:29.028000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:29.039000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:29.095000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:29.108000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:29.121000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:29.133000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:29.145000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:29.159000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:29.336000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:29.346000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:29.398000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:29.412000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:29.425000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:29.437000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:29.449000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:29.463000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:29.643000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:29.655000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:29.700000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:29.711000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:29.721000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:29.736000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:29.751000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:29.762000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:29.941000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:29.952000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:30.007000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:30.019000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:30.032000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:30.045000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:30.057000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:30.071000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:30.248000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:30.258000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:30.310000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:30.324000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:30.337000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:30.349000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:30.362000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:30.375000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:30.555000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:30.567000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:30.612000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:30.622000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:30.634000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:30.648000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:30.664000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:30.675000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:30.862000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:30.875000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:30.916000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:30.927000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:30.937000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:30.952000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:30.967000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:30.978000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:31.162000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:31.175000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:31.220000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:31.231000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:31.241000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:31.256000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:31.271000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:31.282000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:31.463000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:31.475000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:31.524000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:31.534000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:31.545000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:31.559000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:31.575000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:31.585000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:31.766000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:31.779000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:31.828000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:31.839000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:31.849000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:31.864000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:31.878000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:31.890000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:32.070000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:32.083000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:32.132000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:32.142000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:32.153000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:32.167000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:32.181000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:32.194000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:32.375000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:32.387000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:32.435000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:32.446000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:32.457000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:32.471000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:32.485000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:32.497000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:32.677000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:32.687000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:32.743000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:32.755000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:32.769000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:32.781000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:32.793000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:32.807000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:32.984000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:32.994000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:33.046000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:33.059000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:33.073000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:33.085000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:33.099000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:33.111000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:33.288000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:33.298000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:33.350000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:33.363000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:33.377000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:33.390000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:33.403000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:33.415000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:33.592000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:33.602000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:33.654000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:33.667000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:33.681000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:33.694000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:33.707000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:33.719000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:33.896000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:33.906000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:33.959000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:33.971000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:33.986000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:33.998000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:34.011000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:34.023000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:34.200000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:34.211000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:34.274000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:34.288000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:34.301000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:34.313000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:34.327000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:34.339000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:34.516000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:34.526000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:34.579000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:34.592000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:34.605000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:34.618000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:34.631000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:34.644000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:34.824000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:34.834000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:34.886000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:34.900000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:34.913000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:34.925000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:34.939000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:34.951000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:35.131000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:35.144000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:35.192000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:35.202000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:35.214000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:35.228000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:35.244000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:35.255000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:35.438000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:35.451000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:35.500000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:35.510000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:35.521000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:35.536000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:35.553000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:35.564000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:35.749000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:35.759000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:35.819000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:35.832000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:35.847000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:35.858000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:35.871000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:35.886000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:36.071000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:36.083000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:36.132000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:36.143000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:36.154000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:36.169000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:36.183000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:36.196000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:36.382000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:36.395000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:36.444000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:36.455000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:36.465000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:36.480000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:36.495000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:36.507000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:36.691000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:36.703000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:36.756000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:36.765000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:36.777000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:36.790000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:36.805000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:36.817000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:36.999000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:37.011000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:37.064000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:37.074000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:37.084000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:37.099000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:37.114000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:37.125000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:37.306000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:37.319000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:37.372000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:37.382000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:37.392000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:37.406000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:37.422000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:37.432000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:37.614000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:37.627000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:37.680000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:37.689000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:37.700000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:37.715000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:37.731000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:37.741000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:38.492000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:38.503000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:38.513000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:38.526000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:38.539000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:38.552000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:38.564000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:38.609000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:39.560000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:39.570000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:39.588000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:39.598000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:39.662000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:39.676000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:39.690000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:39.703000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:40.439000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:40.449000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:40.460000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:40.470000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:40.482000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:40.496000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:40.508000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:40.552000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:40.747000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:40.757000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:40.767000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:40.780000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:40.794000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:40.807000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:40.821000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:40.852000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:41.051000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:41.064000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:41.074000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:41.087000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:41.102000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:41.116000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:41.130000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:41.152000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:41.355000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:41.368000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:41.378000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:41.396000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:41.410000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:41.424000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:41.438000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:41.452000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:41.659000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:41.672000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:41.684000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:41.704000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:41.719000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:41.732000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:41.746000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:41.759000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:41.963000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:41.980000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:41.990000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:42.012000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:42.027000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:42.040000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:42.054000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:42.067000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:42.267000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:42.288000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:42.298000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:42.320000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:42.338000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:42.352000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:42.366000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:42.379000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:42.571000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:42.596000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:42.606000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:42.628000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:42.646000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:42.660000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:42.674000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:42.687000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:42.875000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:42.904000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:42.914000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:42.936000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:42.955000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:42.968000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:42.982000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:42.996000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:43.179000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:43.212000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:43.222000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:43.244000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:43.263000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:43.276000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:43.290000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:43.304000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:43.491000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:43.520000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:43.530000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:43.552000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:43.575000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:43.588000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:43.602000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:43.616000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:43.799000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:43.828000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:43.838000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:43.860000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:43.883000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:43.896000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:43.910000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:43.924000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:44.111000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:44.136000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:44.146000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:44.168000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:44.191000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:44.204000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:44.218000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:44.232000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:44.419000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:44.440000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:44.452000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:44.476000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:44.499000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:44.512000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:44.526000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:44.540000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:44.727000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:44.748000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:44.759000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:44.784000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:44.806000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:44.820000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:44.834000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:44.848000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1027 11:26:45.597000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1027 11:26:45.612000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1027 11:26:45.623000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1027 11:26:45.635000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1027 11:26:45.646000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1027 11:26:45.658000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1027 11:26:45.670000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1027 11:26:45.681000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_761 0.1041 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_760 0.1042 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_770 0.1042 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1042 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1082 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1082 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1120 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1120 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_764 0.1399 ms 43.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2411 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_760 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_761 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_770 0.1047 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1047 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_779 0.1086 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1086 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1121 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1121 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_764 0.1405 ms 43.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2496 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0617 ms 100.0% 
  triton_mm_761 0.1039 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_760 0.1040 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_770 0.1043 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1045 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1086 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1087 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_769 0.1123 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1124 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_764 0.1402 ms 44.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2285 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0619 ms 100.0% 
  triton_mm_761 0.1040 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_760 0.1041 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_771 0.1043 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_770 0.1043 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_778 0.1086 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1088 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1125 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1126 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_765 0.1407 ms 44.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2671 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0618 ms 100.0% 
  triton_mm_760 0.1041 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_761 0.1042 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_770 0.1045 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1046 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1090 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1090 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1127 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1127 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_764 0.1411 ms 43.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2399 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0617 ms 100.0% 
  triton_mm_760 0.1040 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_761 0.1041 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_770 0.1043 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1043 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_779 0.1086 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1086 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_768 0.1124 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1125 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_765 0.1406 ms 43.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2762 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_761 0.1041 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_760 0.1042 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_770 0.1043 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1043 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1085 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1086 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1120 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1121 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_764 0.1399 ms 43.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2424 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0616 ms 100.0% 
  triton_mm_761 0.1045 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_760 0.1045 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_771 0.1045 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_770 0.1046 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_778 0.1086 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1087 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1123 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1124 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_765 0.1403 ms 43.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2398 seconds and 0.0001 seconds precompiling for 25 choices
Capturing batches (bs=1 avail_mem=38.41 GB): 100%|| 52/52 [09:50<00:00, 59.56s/it]Capturing batches (bs=1 avail_mem=38.41 GB): 100%|| 52/52 [09:50<00:00, 11.35s/it]
[2025-10-27 11:26:55 DP0 TP0] Registering 22 cuda graph addresses
[2025-10-27 11:26:57 DP3 TP3] Capture cuda graph end. Time elapsed: 592.00 s. mem usage=13.66 GB. avail mem=37.25 GB.
[2025-10-27 11:26:57 DP1 TP1] Capture cuda graph end. Time elapsed: 591.95 s. mem usage=13.62 GB. avail mem=37.29 GB.
[2025-10-27 11:26:57 DP4 TP4] Capture cuda graph end. Time elapsed: 591.93 s. mem usage=13.62 GB. avail mem=37.34 GB.
[2025-10-27 11:26:57 DP7 TP7] Capture cuda graph end. Time elapsed: 591.85 s. mem usage=13.48 GB. avail mem=37.55 GB.
[2025-10-27 11:26:57 DP6 TP6] Capture cuda graph end. Time elapsed: 592.08 s. mem usage=13.53 GB. avail mem=37.50 GB.
[2025-10-27 11:26:57 DP0 TP0] Capture cuda graph end. Time elapsed: 592.04 s. mem usage=13.26 GB. avail mem=38.06 GB.
[2025-10-27 11:26:57 DP5 TP5] Capture cuda graph end. Time elapsed: 591.95 s. mem usage=13.55 GB. avail mem=37.50 GB.
[2025-10-27 11:26:57 DP2 TP2] Capture cuda graph end. Time elapsed: 592.06 s. mem usage=13.66 GB. avail mem=37.24 GB.
[2025-10-27 11:26:57 DP0 TP0] max_total_num_tokens=676049, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=2112, context_len=163840, available_gpu_mem=38.06 GB
[2025-10-27 11:26:57] INFO:     Started server process [42]
[2025-10-27 11:26:57] INFO:     Waiting for application startup.
[2025-10-27 11:26:57] INFO:     Application startup complete.
[2025-10-27 11:26:57] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-10-27 11:26:58] INFO:     127.0.0.1:40588 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-27 11:26:58 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:26:58 DP4 TP4] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:26:58 DP7 TP7] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:26:58 DP6 TP6] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:26:58 DP3 TP3] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:26:58 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:26:58 DP5 TP5] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:26:58 DP2 TP2] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:26:59 DP1 TP1] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:26:59 DP0 TP0] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:00 DP3 TP3] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:00 DP7 TP7] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:00 DP4 TP4] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:00 DP2 TP2] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:00 DP5 TP5] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:00 DP6 TP6] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:00] INFO:     127.0.0.1:40600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:00] The server is fired up and ready to roll!
[2025-10-27 11:27:02] INFO:     127.0.0.1:40614 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-27 11:27:09] INFO:     127.0.0.1:52718 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-27 11:27:09 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 666, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP0 TP0] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP7 TP7] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP6 TP6] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP4 TP4] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP5 TP5] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP1 TP1] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP3 TP3] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP2 TP2] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09] INFO:     127.0.0.1:52722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:09 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 733, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP5 TP5] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP7 TP7] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP6 TP6] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP4 TP4] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP3 TP3] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP2 TP2] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP1 TP1] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP0 TP0] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:09 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 1445, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:27:09 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 1435, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:27:09 DP2 TP2] Prefill batch, #new-seq: 3, #new-token: 2143, #cached-token: 3, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:27:09 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 1443, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:27:09 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 1504, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:27:09 DP3 TP3] Prefill batch, #new-seq: 3, #new-token: 2141, #cached-token: 3, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:27:09 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 5791, #cached-token: 8, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-27 11:27:09 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 487, #cached-token: 4669, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[2025-10-27 11:27:10 DP4 TP4] Prefill batch, #new-seq: 22, #new-token: 16059, #cached-token: 22, token usage: 0.00, #running-req: 2, #queue-req: 15, 
[2025-10-27 11:27:10 DP6 TP6] Prefill batch, #new-seq: 22, #new-token: 16072, #cached-token: 22, token usage: 0.00, #running-req: 2, #queue-req: 15, 
[2025-10-27 11:27:10 DP2 TP2] Prefill batch, #new-seq: 22, #new-token: 16183, #cached-token: 22, token usage: 0.00, #running-req: 3, #queue-req: 15, 
[2025-10-27 11:27:10 DP7 TP7] Prefill batch, #new-seq: 22, #new-token: 15994, #cached-token: 22, token usage: 0.00, #running-req: 2, #queue-req: 15, 
[2025-10-27 11:27:10 DP5 TP5] Prefill batch, #new-seq: 22, #new-token: 16003, #cached-token: 22, token usage: 0.00, #running-req: 2, #queue-req: 15, 
[2025-10-27 11:27:10 DP3 TP3] Prefill batch, #new-seq: 22, #new-token: 16098, #cached-token: 22, token usage: 0.00, #running-req: 3, #queue-req: 14, 
[2025-10-27 11:27:10 DP0 TP0] Prefill batch, #new-seq: 32, #new-token: 1920, #cached-token: 21344, token usage: 0.00, #running-req: 7, #queue-req: 0, 
[2025-10-27 11:27:10 DP1 TP1] Prefill batch, #new-seq: 31, #new-token: 1760, #cached-token: 20740, token usage: 0.01, #running-req: 9, #queue-req: 0, 
[2025-10-27 11:27:19 DP1 TP1] Prefill batch, #new-seq: 125, #new-token: 7425, #cached-token: 83634, token usage: 0.00, #running-req: 40, #queue-req: 0, 
[2025-10-27 11:27:19 DP0 TP0] Prefill batch, #new-seq: 125, #new-token: 8155, #cached-token: 83639, token usage: 0.00, #running-req: 39, #queue-req: 0, 
[2025-10-27 11:27:19 DP2 TP2] Prefill batch, #new-seq: 140, #new-token: 8499, #cached-token: 93677, token usage: 0.03, #running-req: 25, #queue-req: 0, 
[2025-10-27 11:27:19 DP4 TP4] Prefill batch, #new-seq: 141, #new-token: 8357, #cached-token: 94335, token usage: 0.02, #running-req: 24, #queue-req: 0, 
[2025-10-27 11:27:19 DP6 TP6] Prefill batch, #new-seq: 141, #new-token: 8703, #cached-token: 94329, token usage: 0.02, #running-req: 24, #queue-req: 0, 
[2025-10-27 11:27:19 DP5 TP5] Prefill batch, #new-seq: 141, #new-token: 8226, #cached-token: 94329, token usage: 0.02, #running-req: 24, #queue-req: 0, 
[2025-10-27 11:27:19 DP7 TP7] Prefill batch, #new-seq: 141, #new-token: 8753, #cached-token: 94337, token usage: 0.02, #running-req: 24, #queue-req: 0, 
[2025-10-27 11:27:19 DP3 TP3] Prefill batch, #new-seq: 140, #new-token: 8283, #cached-token: 93660, token usage: 0.03, #running-req: 25, #queue-req: 0, 
[2025-10-27 11:27:26] INFO:     127.0.0.1:55700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:27 DP4 TP4] Decode batch, #running-req: 165, #token: 15879, token usage: 0.02, cuda graph: True, gen throughput (token/s): 177.51, #queue-req: 0, 
[2025-10-27 11:27:27 DP0 TP0] Decode batch, #running-req: 163, #token: 16409, token usage: 0.02, cuda graph: True, gen throughput (token/s): 176.26, #queue-req: 0, 
[2025-10-27 11:27:27 DP5 TP5] Decode batch, #running-req: 165, #token: 15760, token usage: 0.02, cuda graph: True, gen throughput (token/s): 177.51, #queue-req: 0, 
[2025-10-27 11:27:27 DP1 TP1] Decode batch, #running-req: 165, #token: 15763, token usage: 0.02, cuda graph: True, gen throughput (token/s): 177.51, #queue-req: 0, 
[2025-10-27 11:27:27 DP3 TP3] Decode batch, #running-req: 165, #token: 15879, token usage: 0.02, cuda graph: True, gen throughput (token/s): 177.51, #queue-req: 0, 
[2025-10-27 11:27:27 DP6 TP6] Decode batch, #running-req: 165, #token: 16236, token usage: 0.02, cuda graph: True, gen throughput (token/s): 177.51, #queue-req: 0, 
[2025-10-27 11:27:27 DP7 TP7] Decode batch, #running-req: 165, #token: 16095, token usage: 0.02, cuda graph: True, gen throughput (token/s): 177.50, #queue-req: 0, 
[2025-10-27 11:27:27 DP2 TP2] Decode batch, #running-req: 165, #token: 16197, token usage: 0.02, cuda graph: True, gen throughput (token/s): 177.50, #queue-req: 0, 
[2025-10-27 11:27:27] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:27] INFO:     127.0.0.1:35898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:27] INFO:     127.0.0.1:55730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:53318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:56340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:57324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:33082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:35512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:36020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:54090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:56944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:60406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:54240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:34782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:34312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:32936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:53150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:33744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:57452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:53498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:54976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:35876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:56058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:33206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:33056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:34118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:35684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:54422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:58608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:57656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:56472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:60600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:55960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:28] INFO:     127.0.0.1:35088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:34922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:34690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:35930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:56052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:53562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:60754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:59400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:58506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:33138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:35310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:58870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:34216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:54936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:33520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:55666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:60288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:52736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:57748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:53718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:34736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:34464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:59722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:34650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:34766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:59958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:60260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:56874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:60844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:53786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:55332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:57570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:54122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:33606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:57002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:35542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:60708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:34268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:58844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:57300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:34334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:54770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:34894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:57134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:56626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:55196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:60546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:34380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:54606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:59006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:56782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:58490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:60618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:33772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:34434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:59064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:59560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:35476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:53530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:57862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:29] INFO:     127.0.0.1:35910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:33676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:54038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:54130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:54474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:56954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:58950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:58934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:53778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:60906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:35042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:54958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:33280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:54568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:56110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:58762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:34456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:53626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:53364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:54734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:59050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:55084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:55548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:60422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:60550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:58558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:54284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:59878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:35180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:55778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:35312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:59328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:32860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:53168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:57144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:58468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:58688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:59174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:35576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:32954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:33462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:35916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:59220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:36168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:59708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:60868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:34878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:36178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:54592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:34846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:54906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:34308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:53010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:35844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:58190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:33916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:35664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:34356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:59978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:58864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:33048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:60772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:57972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:34418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:57762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:32922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:35786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:36030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:33736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:59474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:35722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:59604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:60224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:60644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:30] INFO:     127.0.0.1:34752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:58072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:54192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:54996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:54036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:59482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:58626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:34672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:57190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:60680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:36158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:35960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:59894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:56720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:53296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:34534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:34498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:60506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:57726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:53234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:59416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:53750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:59850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:56106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:34452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:56448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:53518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:54938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:57238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:35122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:53998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:60964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:54404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:60978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:35678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:53680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:60374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:34862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:54310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:34500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:58408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:60672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:60004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:34188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:54986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:35330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:57528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:58928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:60818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:33940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:52926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:56238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:35720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:54652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:58318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:54490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:60478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:57294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:59738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:56114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:58138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:58700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:35466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:35590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:59436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:33762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:59250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:54688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:57480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:34810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:59264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:32880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:34354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:58568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:34020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:36066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:56434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:33384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:59932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:34536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:35802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:57696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:59644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:54960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:33956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:57080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:33300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:34518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:35428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:56640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:59462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:36142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:59082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:35568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:34098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:57010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:54556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:54838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:59798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:33980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:53698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:33626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:35304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31 DP1 TP1] Decode batch, #running-req: 116, #token: 16119, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1290.49, #queue-req: 0, 
[2025-10-27 11:27:31 DP0 TP0] Decode batch, #running-req: 130, #token: 18388, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1326.14, #queue-req: 0, 
[2025-10-27 11:27:31 DP6 TP6] Decode batch, #running-req: 119, #token: 16809, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1311.42, #queue-req: 0, 
[2025-10-27 11:27:31 DP7 TP7] Decode batch, #running-req: 127, #token: 17881, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1351.16, #queue-req: 0, 
[2025-10-27 11:27:31 DP3 TP3] Decode batch, #running-req: 118, #token: 16255, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1284.44, #queue-req: 0, 
[2025-10-27 11:27:31 DP2 TP2] Decode batch, #running-req: 116, #token: 16438, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1291.96, #queue-req: 0, 
[2025-10-27 11:27:31 DP4 TP4] Decode batch, #running-req: 127, #token: 17060, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1325.76, #queue-req: 0, 
[2025-10-27 11:27:31] INFO:     127.0.0.1:53304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31 DP5 TP5] Decode batch, #running-req: 119, #token: 16250, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1299.03, #queue-req: 0, 
[2025-10-27 11:27:31] INFO:     127.0.0.1:60924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:35220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:54620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:55152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:35102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:60526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:59314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:33440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:33920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:56236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:57354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:33448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:31] INFO:     127.0.0.1:34328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:33238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:56828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:54272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:33002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:56252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:35078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:56226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:35504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:34546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:56882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:36238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:35138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:58412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:54316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:55982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:33850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:54420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:55016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:36180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:56922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:56966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:55106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:33182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:36266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:34818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:54500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:59672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:58660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:54798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:55526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:59516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:35884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:35574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:53714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:32920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:52976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:56558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:59818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:32772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:35276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:36248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:55916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:34036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:54228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:52992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:56744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:35414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:33548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:53094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:33930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:35290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:59530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:35776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:54758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:34254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:33196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:55334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:53108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:55564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:56564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:53426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:53658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:35048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:35258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:55364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:55906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:58972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:52892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:59036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:34612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:36210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:55738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:58738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:36222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:54706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:54634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:58210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:35976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:35958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:59766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:35472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:58206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:36094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:56584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:60164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:52788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:56996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:35862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:54830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:55422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:59572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:32] INFO:     127.0.0.1:34712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:54014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:58818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:35364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:54876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:33936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:52776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:32870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:57664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:58474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:54766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:60110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:35346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:52822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:55838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:60590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:33828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:57960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:33414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:54940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:60302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:55136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:55728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:60362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:35832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:35894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:58788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:35538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:36202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:33210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:35060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:36100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:33996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:54922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:36046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:60578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:32820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:35442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:33016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:60696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:57814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:33358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:57772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:55412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:60426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:55644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:54328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:55214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:58120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:60954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:54204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:35866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:33574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:33814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:60920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:54082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:60394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:57746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:35236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:56380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:57646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:57798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:34990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:33] INFO:     127.0.0.1:35860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:55944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:55774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:57062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:57204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:35878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:32902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:35274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:35936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:36004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:53784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:60044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:32986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:53130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:54844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:53844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:54660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:57160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:57318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:59660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:59034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:35522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:52940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:53012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:34160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:60800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:35196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:60160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:34104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:35092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:52980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:59636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:54104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:32846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:35108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:60408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:34824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:35710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:54296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:54026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:32836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:53810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:58914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:35164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:35990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:36054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:57340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:34988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:58296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:34398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:58308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:55394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:54068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:53942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:60148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:53186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:53070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:60194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:58534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:34084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:57124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:60892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:35680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:58886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:59386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:60368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:58954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:54954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:55892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:54256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:33660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:34] INFO:     127.0.0.1:59298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:57506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:57720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:33868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:34004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:36116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:33342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:60316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:32786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:33292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:34946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:33966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:33692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:53384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:59946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:33504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:32892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:57292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:33430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:53772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:32968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:60496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:53720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:60992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:53334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:33314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:33870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:34600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:33096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:33110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:53956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:54480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35 DP4 TP4] Decode batch, #running-req: 56, #token: 10868, token usage: 0.02, cuda graph: True, gen throughput (token/s): 983.79, #queue-req: 0, 
[2025-10-27 11:27:35 DP6 TP6] Decode batch, #running-req: 51, #token: 9593, token usage: 0.01, cuda graph: True, gen throughput (token/s): 917.16, #queue-req: 0, 
[2025-10-27 11:27:35 DP3 TP3] Decode batch, #running-req: 63, #token: 11056, token usage: 0.02, cuda graph: True, gen throughput (token/s): 946.94, #queue-req: 0, 
[2025-10-27 11:27:35 DP0 TP0] Decode batch, #running-req: 64, #token: 12655, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1062.18, #queue-req: 0, 
[2025-10-27 11:27:35 DP5 TP5] Decode batch, #running-req: 53, #token: 9908, token usage: 0.01, cuda graph: True, gen throughput (token/s): 984.86, #queue-req: 0, 
[2025-10-27 11:27:35 DP1 TP1] Decode batch, #running-req: 59, #token: 11148, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1010.46, #queue-req: 0, 
[2025-10-27 11:27:35 DP7 TP7] Decode batch, #running-req: 62, #token: 11750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1010.84, #queue-req: 0, 
[2025-10-27 11:27:35] INFO:     127.0.0.1:54332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35 DP2 TP2] Decode batch, #running-req: 59, #token: 11170, token usage: 0.02, cuda graph: True, gen throughput (token/s): 928.97, #queue-req: 0, 
[2025-10-27 11:27:35] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:59286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:57012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:60602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:33788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:53262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:33578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:53192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:59158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:34720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:59546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:59618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:36050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:53976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:33004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:53442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:60182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:33566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:59670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:52856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:54344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:53932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:60104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:60858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:34000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:60262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:53274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:54750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:54596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:34594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:52880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:32976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:34368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:60398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:56392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:60950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:57302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:60660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:35] INFO:     127.0.0.1:35534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:58686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:57162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:36190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:55858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:58832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:33116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:35640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:58508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:58996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:60236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:52828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:58670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:60054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:56354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:33070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:60880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:59704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:54504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:58642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:55582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:58902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:56760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:59356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:55370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:35250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:35388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:53440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:55458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:53762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:33906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:59970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:53454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:60080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:60756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:32972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:33836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:55714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:60332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:35698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:57086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:56028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:60208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:54892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:33316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:57410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:53232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:60320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:60348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:32812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:57586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:57386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:52744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:58754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:33296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:35682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:33156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:35494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:33652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:54118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:53880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:57412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:35572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:36176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:35992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:56574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:35922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:56718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:59150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:60834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:35170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:33560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:59376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:60494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:60698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:34260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:58096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:36] INFO:     127.0.0.1:57616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:58402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:57490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:58794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:60020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:36076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:34046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:55952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:55244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:60278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:53026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:59130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:34664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:58802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:60436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:59588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:34054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:58086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:34206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:59856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:58786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:59388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:33088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:55642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:34874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:58342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:34132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:54682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:60066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:35728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:53166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:33478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:57904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:35752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:53138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:59448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:57372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:59206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:52908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:36254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:34100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:36012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:35232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:60370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:35650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:53812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:33364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:36234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:54220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:54780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:56936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:60524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:33894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:34238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:34402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:56186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:60246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:35594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:55316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:57912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:55234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:56378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:58224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:58680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:37] INFO:     127.0.0.1:33032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:34900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:53634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:54144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:35806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:56726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38 DP0 TP0] Decode batch, #running-req: 28, #token: 7152, token usage: 0.01, cuda graph: True, gen throughput (token/s): 690.29, #queue-req: 0, 
[2025-10-27 11:27:38 DP4 TP4] Decode batch, #running-req: 22, #token: 5460, token usage: 0.01, cuda graph: True, gen throughput (token/s): 581.48, #queue-req: 0, 
[2025-10-27 11:27:38 DP5 TP5] Decode batch, #running-req: 16, #token: 4108, token usage: 0.01, cuda graph: True, gen throughput (token/s): 411.13, #queue-req: 0, 
[2025-10-27 11:27:38 DP1 TP1] Decode batch, #running-req: 16, #token: 3496, token usage: 0.01, cuda graph: True, gen throughput (token/s): 482.99, #queue-req: 0, 
[2025-10-27 11:27:38 DP6 TP6] Decode batch, #running-req: 17, #token: 4445, token usage: 0.01, cuda graph: True, gen throughput (token/s): 458.47, #queue-req: 0, 
[2025-10-27 11:27:38 DP3 TP3] Decode batch, #running-req: 15, #token: 3740, token usage: 0.01, cuda graph: True, gen throughput (token/s): 472.01, #queue-req: 0, 
[2025-10-27 11:27:38] INFO:     127.0.0.1:56844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38 DP7 TP7] Decode batch, #running-req: 24, #token: 5757, token usage: 0.01, cuda graph: True, gen throughput (token/s): 601.42, #queue-req: 0, 
[2025-10-27 11:27:38 DP2 TP2] Decode batch, #running-req: 20, #token: 5127, token usage: 0.01, cuda graph: True, gen throughput (token/s): 494.00, #queue-req: 0, 
[2025-10-27 11:27:38] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:58848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:58968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:56680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:36092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:55002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:35154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:33746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:55656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:54702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:32924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:56458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:34168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:33804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:35794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:54468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:33722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:59872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:34636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:35490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:58524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:56292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:55282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:35404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:55030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:59834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:52804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:58262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:35316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:55742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:57812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:54536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:55928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:34360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:53860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:33586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:53290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:33924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:33882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:57262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:35072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:33600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:54944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:38] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:34492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:57804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:60084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:36126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:35172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:35024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:59140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:33194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:34854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:52850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:54832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:58124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:54360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:56612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:59808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:57512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:53246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:35638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:36118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:53686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:55386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:54722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:53436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:56926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:54004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:55758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:53816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:33614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:54244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:59310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:39] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:33268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:34290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40 DP4 TP4] Decode batch, #running-req: 11, #token: 3704, token usage: 0.01, cuda graph: True, gen throughput (token/s): 323.02, #queue-req: 0, 
[2025-10-27 11:27:40 DP5 TP5] Decode batch, #running-req: 4, #token: 1840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 154.49, #queue-req: 0, 
[2025-10-27 11:27:40 DP1 TP1] Decode batch, #running-req: 6, #token: 2315, token usage: 0.00, cuda graph: True, gen throughput (token/s): 189.86, #queue-req: 0, 
[2025-10-27 11:27:40 DP0 TP0] Decode batch, #running-req: 12, #token: 3252, token usage: 0.00, cuda graph: True, gen throughput (token/s): 367.18, #queue-req: 0, 
[2025-10-27 11:27:40 DP7 TP7] Decode batch, #running-req: 12, #token: 3995, token usage: 0.01, cuda graph: True, gen throughput (token/s): 341.78, #queue-req: 0, 
[2025-10-27 11:27:40 DP2 TP2] Decode batch, #running-req: 7, #token: 2599, token usage: 0.00, cuda graph: True, gen throughput (token/s): 230.96, #queue-req: 0, 
[2025-10-27 11:27:40 DP6 TP6] Decode batch, #running-req: 7, #token: 2457, token usage: 0.00, cuda graph: True, gen throughput (token/s): 217.40, #queue-req: 0, 
[2025-10-27 11:27:40 DP3 TP3] Decode batch, #running-req: 5, #token: 1497, token usage: 0.00, cuda graph: True, gen throughput (token/s): 186.20, #queue-req: 0, 
[2025-10-27 11:27:40] INFO:     127.0.0.1:55486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:60722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:60530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:57786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:57734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:53510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:55974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:34932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:60938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:58400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:33490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:58038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:53608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:55800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:57584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:32914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:34502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:57902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:34226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:35814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:57176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:52872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:60910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:59910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:53970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:40] INFO:     127.0.0.1:34350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:53218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:58992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:34916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:34580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:35226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:57066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:53578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:34152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:60988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41 DP4 TP4] Decode batch, #running-req: 2, #token: 979, token usage: 0.00, cuda graph: True, gen throughput (token/s): 145.18, #queue-req: 0, 
[2025-10-27 11:27:41 DP0 TP0] Decode batch, #running-req: 3, #token: 1683, token usage: 0.00, cuda graph: True, gen throughput (token/s): 130.97, #queue-req: 0, 
[2025-10-27 11:27:41 DP1 TP1] Decode batch, #running-req: 3, #token: 1619, token usage: 0.00, cuda graph: True, gen throughput (token/s): 97.42, #queue-req: 0, 
[2025-10-27 11:27:41 DP7 TP7] Decode batch, #running-req: 2, #token: 1294, token usage: 0.00, cuda graph: True, gen throughput (token/s): 158.72, #queue-req: 0, 
[2025-10-27 11:27:41 DP5 TP5] Decode batch, #running-req: 1, #token: 967, token usage: 0.00, cuda graph: True, gen throughput (token/s): 55.48, #queue-req: 0, 
[2025-10-27 11:27:41 DP3 TP3] Decode batch, #running-req: 1, #token: 984, token usage: 0.00, cuda graph: True, gen throughput (token/s): 54.20, #queue-req: 0, 
[2025-10-27 11:27:41 DP6 TP6] Decode batch, #running-req: 2, #token: 989, token usage: 0.00, cuda graph: True, gen throughput (token/s): 90.97, #queue-req: 0, 
[2025-10-27 11:27:41 DP2 TP2] Decode batch, #running-req: 3, #token: 1630, token usage: 0.00, cuda graph: True, gen throughput (token/s): 111.60, #queue-req: 0, 
[2025-10-27 11:27:41] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:55634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:59080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:35004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:41] INFO:     127.0.0.1:56382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:42] INFO:     127.0.0.1:33376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:42] INFO:     127.0.0.1:53906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:42] INFO:     127.0.0.1:33544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:42] INFO:     127.0.0.1:34562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:42] INFO:     127.0.0.1:59914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:42 DP0 TP0] Decode batch, #running-req: 1, #token: 1082, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.68, #queue-req: 0, 
[2025-10-27 11:27:42 DP7 TP7] Decode batch, #running-req: 1, #token: 1024, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.54, #queue-req: 0, 
[2025-10-27 11:27:42 DP1 TP1] Decode batch, #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 52.24, #queue-req: 0, 
[2025-10-27 11:27:42 DP2 TP2] Decode batch, #running-req: 1, #token: 1030, token usage: 0.00, cuda graph: True, gen throughput (token/s): 63.39, #queue-req: 0, 
[2025-10-27 11:27:42] INFO:     127.0.0.1:54382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:42] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:43] INFO:     127.0.0.1:33398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:43 DP7 TP7] Decode batch, #running-req: 1, #token: 1064, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.73, #queue-req: 0, 
[2025-10-27 11:27:44 DP7 TP7] Decode batch, #running-req: 1, #token: 1104, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.97, #queue-req: 0, 
[2025-10-27 11:27:45] INFO:     127.0.0.1:54646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:58] INFO:     127.0.0.1:38480 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-27 11:27:58 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP6 TP6] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP7 TP7] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP4 TP4] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP3 TP3] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP5 TP5] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP1 TP1] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP2 TP2] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP0 TP0] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58] INFO:     127.0.0.1:38486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:27:58 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:27:58 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 58, #cached-token: 1373, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:27:58 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 56, #cached-token: 1406, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:27:58 DP2 TP2] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1424, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:27:58 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 42, #cached-token: 1388, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:27:58 DP0 TP0] Prefill batch, #new-seq: 2, #new-token: 66, #cached-token: 1430, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:27:58 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1504, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:27:58 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 317, #cached-token: 3350, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-27 11:27:58 DP3 TP3] Prefill batch, #new-seq: 2, #new-token: 64, #cached-token: 1388, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP1 TP1] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP7 TP7] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP5 TP5] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP6 TP6] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP4 TP4] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP0 TP0] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP3 TP3] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP2 TP2] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:27:58 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 489, #cached-token: 6068, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:27:58 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 413, #cached-token: 6794, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:27:58 DP1 TP1] Prefill batch, #new-seq: 6, #new-token: 455, #cached-token: 4017, token usage: 0.00, #running-req: 6, #queue-req: 0, 
[2025-10-27 11:27:58 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 453, #cached-token: 6119, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:27:58 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 427, #cached-token: 6823, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:27:58 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 627, #cached-token: 6723, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:27:58 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 486, #cached-token: 6835, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:27:58 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 354, #cached-token: 6157, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:27:58 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 461, #cached-token: 6097, token usage: 0.00, #running-req: 11, #queue-req: 0, 
[2025-10-27 11:27:58 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 392, #cached-token: 6128, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:27:58 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 527, #cached-token: 5412, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:27:58 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 519, #cached-token: 6028, token usage: 0.00, #running-req: 11, #queue-req: 0, 
[2025-10-27 11:27:58 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 566, #cached-token: 5356, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:27:58 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 442, #cached-token: 6062, token usage: 0.00, #running-req: 11, #queue-req: 0, 
[2025-10-27 11:27:58 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 548, #cached-token: 5413, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:27:58 DP1 TP1] Prefill batch, #new-seq: 9, #new-token: 481, #cached-token: 6070, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:27:58 DP4 TP4] Prefill batch, #new-seq: 17, #new-token: 870, #cached-token: 11563, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:27:58 DP2 TP2] Prefill batch, #new-seq: 16, #new-token: 774, #cached-token: 10861, token usage: 0.00, #running-req: 21, #queue-req: 0, 
[2025-10-27 11:27:58 DP6 TP6] Prefill batch, #new-seq: 17, #new-token: 925, #cached-token: 11527, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:27:58 DP0 TP0] Prefill batch, #new-seq: 17, #new-token: 832, #cached-token: 11602, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:27:58 DP7 TP7] Prefill batch, #new-seq: 17, #new-token: 713, #cached-token: 11711, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:27:58 DP1 TP1] Prefill batch, #new-seq: 16, #new-token: 684, #cached-token: 10804, token usage: 0.00, #running-req: 21, #queue-req: 0, 
[2025-10-27 11:27:58 DP5 TP5] Prefill batch, #new-seq: 17, #new-token: 734, #cached-token: 11527, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:27:58 DP3 TP3] Prefill batch, #new-seq: 17, #new-token: 747, #cached-token: 11532, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:27:59 DP0 TP0] Prefill batch, #new-seq: 16, #new-token: 413, #cached-token: 11407, token usage: 0.01, #running-req: 37, #queue-req: 0, 
[2025-10-27 11:27:59 DP2 TP2] Prefill batch, #new-seq: 17, #new-token: 428, #cached-token: 11993, token usage: 0.00, #running-req: 37, #queue-req: 0, 
[2025-10-27 11:27:59 DP4 TP4] Prefill batch, #new-seq: 17, #new-token: 374, #cached-token: 12083, token usage: 0.01, #running-req: 37, #queue-req: 0, 
[2025-10-27 11:27:59 DP6 TP6] Prefill batch, #new-seq: 16, #new-token: 258, #cached-token: 11411, token usage: 0.01, #running-req: 37, #queue-req: 0, 
[2025-10-27 11:27:59 DP3 TP3] Prefill batch, #new-seq: 17, #new-token: 539, #cached-token: 11946, token usage: 0.01, #running-req: 37, #queue-req: 0, 
[2025-10-27 11:27:59 DP7 TP7] Prefill batch, #new-seq: 16, #new-token: 303, #cached-token: 11369, token usage: 0.01, #running-req: 37, #queue-req: 0, 
[2025-10-27 11:27:59 DP1 TP1] Prefill batch, #new-seq: 17, #new-token: 320, #cached-token: 11902, token usage: 0.00, #running-req: 37, #queue-req: 0, 
[2025-10-27 11:27:59 DP5 TP5] Prefill batch, #new-seq: 17, #new-token: 391, #cached-token: 12055, token usage: 0.01, #running-req: 37, #queue-req: 0, 
[2025-10-27 11:27:59 DP2 TP2] Prefill batch, #new-seq: 23, #new-token: 905, #cached-token: 15806, token usage: 0.01, #running-req: 54, #queue-req: 0, 
[2025-10-27 11:27:59 DP6 TP6] Prefill batch, #new-seq: 24, #new-token: 815, #cached-token: 16434, token usage: 0.01, #running-req: 53, #queue-req: 0, 
[2025-10-27 11:27:59 DP7 TP7] Prefill batch, #new-seq: 24, #new-token: 784, #cached-token: 16632, token usage: 0.01, #running-req: 53, #queue-req: 0, 
[2025-10-27 11:27:59 DP3 TP3] Prefill batch, #new-seq: 23, #new-token: 809, #cached-token: 15921, token usage: 0.01, #running-req: 54, #queue-req: 0, 
[2025-10-27 11:27:59 DP4 TP4] Prefill batch, #new-seq: 23, #new-token: 676, #cached-token: 16005, token usage: 0.01, #running-req: 54, #queue-req: 0, 
[2025-10-27 11:27:59 DP0 TP0] Prefill batch, #new-seq: 23, #new-token: 976, #cached-token: 15894, token usage: 0.01, #running-req: 53, #queue-req: 0, 
[2025-10-27 11:27:59 DP1 TP1] Prefill batch, #new-seq: 23, #new-token: 1013, #cached-token: 15752, token usage: 0.01, #running-req: 54, #queue-req: 0, 
[2025-10-27 11:27:59 DP5 TP5] Prefill batch, #new-seq: 23, #new-token: 963, #cached-token: 15867, token usage: 0.01, #running-req: 54, #queue-req: 0, 
[2025-10-27 11:27:59 DP4 TP4] Prefill batch, #new-seq: 14, #new-token: 280, #cached-token: 9918, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-27 11:27:59 DP6 TP6] Prefill batch, #new-seq: 14, #new-token: 224, #cached-token: 10013, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-27 11:27:59 DP2 TP2] Prefill batch, #new-seq: 15, #new-token: 115, #cached-token: 10804, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-27 11:27:59 DP7 TP7] Prefill batch, #new-seq: 13, #new-token: 365, #cached-token: 9045, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-27 11:27:59 DP3 TP3] Prefill batch, #new-seq: 14, #new-token: 312, #cached-token: 9913, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-27 11:27:59 DP5 TP5] Prefill batch, #new-seq: 14, #new-token: 205, #cached-token: 9972, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-27 11:27:59 DP1 TP1] Prefill batch, #new-seq: 15, #new-token: 412, #cached-token: 10631, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-27 11:27:59 DP0 TP0] Prefill batch, #new-seq: 15, #new-token: 491, #cached-token: 10376, token usage: 0.01, #running-req: 76, #queue-req: 0, 
[2025-10-27 11:28:00 DP2 TP2] Prefill batch, #new-seq: 25, #new-token: 858, #cached-token: 17315, token usage: 0.01, #running-req: 92, #queue-req: 0, 
[2025-10-27 11:28:00 DP6 TP6] Prefill batch, #new-seq: 26, #new-token: 725, #cached-token: 18272, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[2025-10-27 11:28:00 DP4 TP4] Prefill batch, #new-seq: 26, #new-token: 640, #cached-token: 18202, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[2025-10-27 11:28:00 DP3 TP3] Prefill batch, #new-seq: 26, #new-token: 579, #cached-token: 18418, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[2025-10-27 11:28:00 DP0 TP0] Prefill batch, #new-seq: 25, #new-token: 373, #cached-token: 17766, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[2025-10-27 11:28:00 DP7 TP7] Prefill batch, #new-seq: 27, #new-token: 773, #cached-token: 19059, token usage: 0.01, #running-req: 90, #queue-req: 0, 
[2025-10-27 11:28:00 DP1 TP1] Prefill batch, #new-seq: 25, #new-token: 655, #cached-token: 17471, token usage: 0.01, #running-req: 92, #queue-req: 0, 
[2025-10-27 11:28:00 DP5 TP5] Prefill batch, #new-seq: 26, #new-token: 674, #cached-token: 18441, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[2025-10-27 11:28:00 DP6 TP6] Prefill batch, #new-seq: 12, #new-token: 417, #cached-token: 8361, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-27 11:28:00 DP2 TP2] Prefill batch, #new-seq: 13, #new-token: 489, #cached-token: 9041, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-27 11:28:00 DP7 TP7] Prefill batch, #new-seq: 12, #new-token: 518, #cached-token: 8272, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-27 11:28:00 DP3 TP3] Prefill batch, #new-seq: 13, #new-token: 516, #cached-token: 8919, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-27 11:28:00 DP4 TP4] Prefill batch, #new-seq: 13, #new-token: 438, #cached-token: 9038, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-27 11:28:00 DP0 TP0] Prefill batch, #new-seq: 13, #new-token: 726, #cached-token: 9050, token usage: 0.01, #running-req: 116, #queue-req: 0, 
[2025-10-27 11:28:00 DP1 TP1] Prefill batch, #new-seq: 13, #new-token: 676, #cached-token: 8968, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-27 11:28:00 DP5 TP5] Prefill batch, #new-seq: 13, #new-token: 499, #cached-token: 8924, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-27 11:28:00 DP2 TP2] Prefill batch, #new-seq: 21, #new-token: 671, #cached-token: 14679, token usage: 0.01, #running-req: 130, #queue-req: 0, 
[2025-10-27 11:28:00 DP6 TP6] Prefill batch, #new-seq: 22, #new-token: 503, #cached-token: 15659, token usage: 0.01, #running-req: 129, #queue-req: 0, 
[2025-10-27 11:28:00 DP4 TP4] Prefill batch, #new-seq: 21, #new-token: 576, #cached-token: 14673, token usage: 0.01, #running-req: 130, #queue-req: 0, 
[2025-10-27 11:28:00 DP7 TP7] Prefill batch, #new-seq: 22, #new-token: 622, #cached-token: 15778, token usage: 0.01, #running-req: 129, #queue-req: 0, 
[2025-10-27 11:28:00 DP3 TP3] Prefill batch, #new-seq: 21, #new-token: 504, #cached-token: 14625, token usage: 0.01, #running-req: 130, #queue-req: 0, 
[2025-10-27 11:28:00 DP0 TP0] Prefill batch, #new-seq: 22, #new-token: 831, #cached-token: 15428, token usage: 0.01, #running-req: 129, #queue-req: 0, 
[2025-10-27 11:28:00 DP5 TP5] Prefill batch, #new-seq: 21, #new-token: 460, #cached-token: 14750, token usage: 0.01, #running-req: 130, #queue-req: 0, 
[2025-10-27 11:28:00 DP1 TP1] Prefill batch, #new-seq: 22, #new-token: 607, #cached-token: 15208, token usage: 0.01, #running-req: 130, #queue-req: 0, 
[2025-10-27 11:28:00 DP4 TP4] Prefill batch, #new-seq: 11, #new-token: 346, #cached-token: 7548, token usage: 0.01, #running-req: 151, #queue-req: 0, 
[2025-10-27 11:28:00 DP6 TP6] Prefill batch, #new-seq: 11, #new-token: 139, #cached-token: 7882, token usage: 0.02, #running-req: 151, #queue-req: 0, 
[2025-10-27 11:28:00 DP2 TP2] Prefill batch, #new-seq: 11, #new-token: 606, #cached-token: 7564, token usage: 0.01, #running-req: 151, #queue-req: 0, 
[2025-10-27 11:28:00 DP5 TP5] Prefill batch, #new-seq: 11, #new-token: 226, #cached-token: 7773, token usage: 0.02, #running-req: 151, #queue-req: 0, 
[2025-10-27 11:28:00 DP7 TP7] Prefill batch, #new-seq: 11, #new-token: 87, #cached-token: 7981, token usage: 0.02, #running-req: 151, #queue-req: 0, 
[2025-10-27 11:28:00 DP1 TP1] Prefill batch, #new-seq: 11, #new-token: 556, #cached-token: 7613, token usage: 0.01, #running-req: 152, #queue-req: 0, 
[2025-10-27 11:28:00 DP3 TP3] Prefill batch, #new-seq: 11, #new-token: 365, #cached-token: 7653, token usage: 0.01, #running-req: 151, #queue-req: 0, 
[2025-10-27 11:28:00 DP0 TP0] Prefill batch, #new-seq: 11, #new-token: 266, #cached-token: 7890, token usage: 0.02, #running-req: 151, #queue-req: 0, 
[2025-10-27 11:28:01 DP4 TP4] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2255, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-27 11:28:01 DP6 TP6] Prefill batch, #new-seq: 3, #new-token: 125, #cached-token: 2062, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-27 11:28:01 DP2 TP2] Prefill batch, #new-seq: 3, #new-token: 63, #cached-token: 2137, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-27 11:28:01 DP1 TP1] Prefill batch, #new-seq: 2, #new-token: 110, #cached-token: 1340, token usage: 0.02, #running-req: 163, #queue-req: 0, 
[2025-10-27 11:28:01 DP7 TP7] Prefill batch, #new-seq: 3, #new-token: 38, #cached-token: 2143, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-27 11:28:01 DP5 TP5] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2160, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-27 11:28:01 DP0 TP0] Prefill batch, #new-seq: 2, #new-token: 115, #cached-token: 1339, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-27 11:28:01 DP3 TP3] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2150, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[aiter] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:01 DP6 TP6] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:01 DP7 TP7] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:01 DP4 TP4] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:01 DP5 TP5] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:01 DP1 TP1] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:01 DP3 TP3] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:01 DP0 TP0] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:01 DP2 TP2] [fused_moe] using default for (460, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:02 DP7 TP7] Decode batch, #running-req: 165, #token: 12456, token usage: 0.02, cuda graph: True, gen throughput (token/s): 76.87, #queue-req: 0, 
[2025-10-27 11:28:02 DP6 TP6] Decode batch, #running-req: 165, #token: 12345, token usage: 0.02, cuda graph: True, gen throughput (token/s): 79.85, #queue-req: 0, 
[2025-10-27 11:28:03 DP5 TP5] Decode batch, #running-req: 165, #token: 13873, token usage: 0.02, cuda graph: True, gen throughput (token/s): 142.69, #queue-req: 0, 
[2025-10-27 11:28:04 DP0 TP0] Decode batch, #running-req: 164, #token: 15272, token usage: 0.02, cuda graph: True, gen throughput (token/s): 183.42, #queue-req: 0, 
[2025-10-27 11:28:04] INFO:     127.0.0.1:41456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:04 DP4 TP4] Decode batch, #running-req: 165, #token: 15003, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-27 11:28:05] INFO:     127.0.0.1:47114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:05 DP2 TP2] Decode batch, #running-req: 165, #token: 16270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 246.96, #queue-req: 0, 
[2025-10-27 11:28:05] INFO:     127.0.0.1:41472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:05] INFO:     127.0.0.1:50070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:05 DP3 TP3] Decode batch, #running-req: 165, #token: 16842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 257.17, #queue-req: 0, 
[2025-10-27 11:28:06] INFO:     127.0.0.1:38822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06 DP1 TP1] Decode batch, #running-req: 165, #token: 16873, token usage: 0.02, cuda graph: True, gen throughput (token/s): 275.74, #queue-req: 0, 
[2025-10-27 11:28:06] INFO:     127.0.0.1:42980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:42160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:39620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:38530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:50198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:42984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:48932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:46334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:39276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:48950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:39916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:40946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:44272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:41338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:38982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:39052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:48422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:41458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:44296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:47104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:38802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:43220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:38734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:43304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:47714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:41008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:43478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:47750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:39148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:44574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:47364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:44578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:47244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:39488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:41832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:48212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:49834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:43610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:43790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:42318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:41726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:46504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:49266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:43080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:50098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:49096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:43248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:06] INFO:     127.0.0.1:46726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:46654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:41968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:43394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:45854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:38756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:38896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:44316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:45386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:43522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:47310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:50066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07 DP7 TP7] Decode batch, #running-req: 162, #token: 18705, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1377.97, #queue-req: 0, 
[2025-10-27 11:28:07] INFO:     127.0.0.1:46188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:40842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:47634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:38508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:46434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:41446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:44856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:48614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:44480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:40282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:48912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:45700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:45934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:46156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07 DP6 TP6] Decode batch, #running-req: 153, #token: 17776, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1366.99, #queue-req: 0, 
[2025-10-27 11:28:07] INFO:     127.0.0.1:40080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:41828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:47094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:43298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:44190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:40044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:42496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:43524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:49498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:43082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:48340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:44812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:47494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:49652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:42754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:40540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:44072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:48594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:44360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:48458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:43688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:39802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:39118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:41162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:41618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:44454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:43198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:44978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:47552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:39674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:46476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:48542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:46530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:49060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:43806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:47892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:40656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:49588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:42688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:40002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:42132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:50084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:40318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:49206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:40150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:45516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:38864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:42948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:40368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:43172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:44708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:42032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:44154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:07] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:48604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:49706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:46828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:38500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:45046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:39774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:40816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:39418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:40520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:41848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:45440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:47414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:39284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:44880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:45030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:41900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:46342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:41308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:44442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:40604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:44886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:40450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:48584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:39496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:40822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:45568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:39048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:44546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:49364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:41176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:40232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:41522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:43012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:46294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:45296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:39588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08 DP5 TP5] Decode batch, #running-req: 142, #token: 17708, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1332.69, #queue-req: 0, 
[2025-10-27 11:28:08] INFO:     127.0.0.1:45154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:40904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:42894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:46680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:43022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:45692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:45196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:50342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:40070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:49048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:40998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:43336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:49612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:42908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:44636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:46764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:49002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:46488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:43604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:47006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:44266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:48406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:50136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:49798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:39366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:43158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:49464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:40194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:43594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:45876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:45948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:44830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:38634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:40154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:41112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:42012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:44144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:44200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:38794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:39928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:39632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:45514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:46130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:46548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:41156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:44906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:46854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:49940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:50210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:46612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:42074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:39172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:41304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:41984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:46046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:39784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:44602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:48852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:44008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:46898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:41400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:46588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:08] INFO:     127.0.0.1:50312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09 DP0 TP0] Decode batch, #running-req: 136, #token: 18351, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1325.09, #queue-req: 0, 
[2025-10-27 11:28:09] INFO:     127.0.0.1:39658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:41578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:42610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:46406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:39252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:41554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:46600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:45838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:43108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:46376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:39204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:40832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:46884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:39244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:39856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:41550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:43654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:40846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:39924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:42284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:43150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:47532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:43704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:39908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:49302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:41454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:39354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:41202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:49038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:41282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:38718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09 DP4 TP4] Decode batch, #running-req: 133, #token: 17803, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1333.70, #queue-req: 0, 
[2025-10-27 11:28:09] INFO:     127.0.0.1:41144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:41568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:42048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:44354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:38668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:40726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:46352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:41258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:45436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:49506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:39524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:40090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:43260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:49492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:44870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:46574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:46708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:40032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:49832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:44290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:45410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:39596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:49562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:40300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:40438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:43240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:43560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:43622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:44262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:40632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:41902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:44116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:44186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:45746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:47756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:42752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:40934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:42416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:45966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:47038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:40024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:42470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:42942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:44558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:47420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:50246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:39236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:39470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:40470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:47572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:44204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:44394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:41850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:49550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:40672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:44892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:45628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:42276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:43944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:47566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:41314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:44544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:40974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:41870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:42566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:39786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:41632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:43140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:45434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:45820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:42042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:45068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:49690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:38808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:39846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:44024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:45246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:41040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:42508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:49710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:44726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:40370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:44610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:40406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:42976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:47592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:41670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:45294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:46466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:47812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:49298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:42330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:45718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:42058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:47178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:42760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:48142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:39484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:39560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:49624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:44638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:09] INFO:     127.0.0.1:49248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:42448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:49310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10 DP2 TP2] Decode batch, #running-req: 125, #token: 17668, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1322.41, #queue-req: 0, 
[2025-10-27 11:28:10] INFO:     127.0.0.1:38798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:40260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:41764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:44692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:50068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:50422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:44362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:47352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:50092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:39372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:44714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:50428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:39710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:43320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:42190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:47388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:49440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:41126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:41274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:43376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:41786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:42818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:40582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:41532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:48978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:44374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:43388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:43656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:44618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:39446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:43904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:49454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:40296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:43490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:47068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10 DP3 TP3] Decode batch, #running-req: 115, #token: 16829, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1278.80, #queue-req: 0, 
[2025-10-27 11:28:10] INFO:     127.0.0.1:38678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:49378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:43644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:40994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10 DP1 TP1] Decode batch, #running-req: 100, #token: 14888, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1218.66, #queue-req: 0, 
[2025-10-27 11:28:10] INFO:     127.0.0.1:39146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:47358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:43542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:49442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:43884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:44346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:40914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:49540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:40064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:43236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:48182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:39818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:47642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:48338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:47904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:43268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:49928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:39088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:39510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:42454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:43752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:50384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:38642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:40778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:50124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:38776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:39950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:41648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:44996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:48786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:40088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:38994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:43836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:44176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:47332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:42622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:40500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:41012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:40648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:50152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:39642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:39760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:40386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:38632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:41930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:49578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:46236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:50400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:41090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:41080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:39016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:50036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:44764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:49210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:41206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:42482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:38968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:48778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:44950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:45680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:44988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:48894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:39868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:42848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:38928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:42024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:10] INFO:     127.0.0.1:42162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:38560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:44168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:39844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:47544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:41566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:46200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:47022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:47466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:46054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:38574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:42400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:42714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:46518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:42112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:40560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:50390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:49646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:50276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:41222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:42366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:41412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:41460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:41948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:47782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:50058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:40794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:41826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:42344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:38782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:47376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:40294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11 DP7 TP7] Decode batch, #running-req: 99, #token: 16009, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1282.11, #queue-req: 0, 
[2025-10-27 11:28:11] INFO:     127.0.0.1:43466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:41788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:40056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:38682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:40880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:46698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:50214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:44656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:42148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:46650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:45406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:45454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:46968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:45392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:46610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:39064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:40402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:42642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:47318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:40746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11 DP6 TP6] Decode batch, #running-req: 80, #token: 12937, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1140.84, #queue-req: 0, 
[2025-10-27 11:28:11] INFO:     127.0.0.1:44648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:41466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:46356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:39334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:41990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:44540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:40310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:45432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:49502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:46546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:45212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:39260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:39546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:41932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:49416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:38866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:40860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:49132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:39532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:41806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:44058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:44660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:50040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:42806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:50192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:40210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:46938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:38534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:49226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:45956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:46866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:44632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:47910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:46304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:46116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:50006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:42534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:49420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:45186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:40218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:47768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:41700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:45502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:42720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:38546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:49264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:48028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:42516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:41508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:43928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:11] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:42524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:43064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:49732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:42828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:39182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:43724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:48054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:47166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:46018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:43186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:43380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:44572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:49424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:40384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:47398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:40870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:40104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:45358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:46990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:42090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:44326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:41716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:44040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:43092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:42634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:40524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:47044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:39574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:41348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:41540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:44018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:48990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:38674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:42452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:45022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:48256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12 DP5 TP5] Decode batch, #running-req: 81, #token: 13392, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1124.14, #queue-req: 0, 
[2025-10-27 11:28:12] INFO:     127.0.0.1:47932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:48196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:40292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:47740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:48346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:41356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:39138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:40436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:39812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:47798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:47298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:43408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:42426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:40808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:49854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:41914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:46084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:50374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:45594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:44468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:42860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:38694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:44876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:39694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:42350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:40338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:47972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:49324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:50160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:50230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:40010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:47192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:42240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:48296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:44256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:41886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:43276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:47474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:39766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:45146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:45226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:38588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:40038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:41752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:43038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:45060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:39078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:40538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:43310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:46316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:38836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:46494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:39102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:49542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:47176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12 DP0 TP0] Decode batch, #running-req: 67, #token: 12367, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1075.51, #queue-req: 0, 
[2025-10-27 11:28:12] INFO:     127.0.0.1:46802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:45004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:41230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:48226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:49808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:43962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:48176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:49554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:40696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:38728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:42060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:44242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:47610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:38880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:48570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:41638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:39384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:42738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:43648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:42260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:46254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:49148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:43282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:44860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:47758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:40978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:44494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:45276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:48098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:47776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:43370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:43606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:46484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:46194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:49150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:43182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:44800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:46414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:12] INFO:     127.0.0.1:48138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13 DP4 TP4] Decode batch, #running-req: 62, #token: 11676, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1007.75, #queue-req: 0, 
[2025-10-27 11:28:13] INFO:     127.0.0.1:49188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:45896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:46076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:46214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:47964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:38660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:42118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:43046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:47722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:40172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:41342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:41964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:42956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:45614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:46918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:49772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:45272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:45324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:45920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:40060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:40354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:47578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:47824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:43758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:46888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:49666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:50148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:47052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:47854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:42266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:40712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:43926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:46100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:39192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:40734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:49176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:47692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:39834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:48880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:47194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:39104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:44104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:44226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:39036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:49762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:44564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:48770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:38704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:41236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:39744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:41072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:50128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:44508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:46172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:47510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:39424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:47266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:41364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:39326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:42872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:43570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:49504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:43526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:41822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:47886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:45604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:46786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:42594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:46134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:42808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13 DP2 TP2] Decode batch, #running-req: 55, #token: 10487, token usage: 0.02, cuda graph: True, gen throughput (token/s): 956.39, #queue-req: 0, 
[2025-10-27 11:28:13] INFO:     127.0.0.1:40576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:43956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:44592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:45560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:49386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:41462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:41330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:45524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:39132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:41104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:43062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:50220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:49922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:47256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:45142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:40910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:46104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:44430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:45490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:38912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:39680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:40244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:45334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:39220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:44836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:46522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:49284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:38600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:46036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:41592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:49368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:41654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:44130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:43000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:45262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:47920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:48134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:40196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:42518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:42540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:46330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:39298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:44416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:48904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:43124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:44530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:48826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:39730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:41576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13 DP3 TP3] Decode batch, #running-req: 50, #token: 9744, token usage: 0.01, cuda graph: True, gen throughput (token/s): 948.71, #queue-req: 0, 
[2025-10-27 11:28:13] INFO:     127.0.0.1:42198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:44842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:46624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:47702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:38636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:49758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:45654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13 DP1 TP1] Decode batch, #running-req: 45, #token: 8951, token usage: 0.01, cuda graph: True, gen throughput (token/s): 811.30, #queue-req: 0, 
[2025-10-27 11:28:13] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:39202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:43224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:41492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:48146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:49522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:13] INFO:     127.0.0.1:43584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:44792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:49978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:49170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:40430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:39962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:41058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:44922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:44966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:42440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:49064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:49124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:44232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:42776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:40862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:47280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:45990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:41250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:42296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:46876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:40148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:43708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:45712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:45304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:47288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:38874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:44848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:46292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:44604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:40930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:44152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:48886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:38594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:40266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:47140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:45946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:49030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:44626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:45280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:42702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:49042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:45548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:48756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:44382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:46226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:46120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:48558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:41420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:47156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:47522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:42672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14 DP7 TP7] Decode batch, #running-req: 46, #token: 10102, token usage: 0.01, cuda graph: True, gen throughput (token/s): 878.11, #queue-req: 0, 
[2025-10-27 11:28:14] INFO:     127.0.0.1:41324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:42972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:44014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:47448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:46000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:46244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:41432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:41600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:48172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:41772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:38518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:39070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:42698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:43554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:40366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:47496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14 DP6 TP6] Decode batch, #running-req: 27, #token: 6057, token usage: 0.01, cuda graph: True, gen throughput (token/s): 640.92, #queue-req: 0, 
[2025-10-27 11:28:14] INFO:     127.0.0.1:38562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:41676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:39058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:44330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:47226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:46234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:48266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:39024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:44022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:47336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:46742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:46030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:43414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:49352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:49596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:38744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:39860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:39882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:47836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:48816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:43842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:41804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:50174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:49158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:41838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:42002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:46390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:48360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:45134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:46952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:44132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:42498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:48242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:42134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:42924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:50248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:42204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:48162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:49426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:14] INFO:     127.0.0.1:48200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:48836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:40892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:43486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:46362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:49516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:39986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:47618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:46358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:39150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:42254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:44212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:44278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:45546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:41386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:44676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:46672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:45372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:39722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:41270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:45852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:49340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:38962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:48312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15 DP5 TP5] Decode batch, #running-req: 24, #token: 5227, token usage: 0.01, cuda graph: True, gen throughput (token/s): 642.37, #queue-req: 0, 
[2025-10-27 11:28:15] INFO:     127.0.0.1:41024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:47108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:44746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:45790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:45310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:46026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:40486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:49896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:43774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:38648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:46056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:42310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:43860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:49406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:45188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:48186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:49912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:40610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:45422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:50436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:41738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:42790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:49538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:47080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:47208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:42928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:48158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15 DP0 TP0] Decode batch, #running-req: 28, #token: 6714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 680.34, #queue-req: 0, 
[2025-10-27 11:28:15] INFO:     127.0.0.1:42840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:50414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:40958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:39392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:40542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:42186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:44628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:48004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:40550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:39396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:39896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:43352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:46930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:44742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:46432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15 DP4 TP4] Decode batch, #running-req: 21, #token: 5002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 623.04, #queue-req: 0, 
[2025-10-27 11:28:15] INFO:     127.0.0.1:47386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:46142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:50344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:40972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:43868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:40716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:43270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:42734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:41862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:42104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:45314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:41684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:42658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:39084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:41108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:49964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:39938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:38960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:15] INFO:     127.0.0.1:50250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:40458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:47484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:44822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:49074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:44778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:47664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:39010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:40568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16 DP2 TP2] Decode batch, #running-req: 22, #token: 5700, token usage: 0.01, cuda graph: True, gen throughput (token/s): 552.34, #queue-req: 0, 
[2025-10-27 11:28:16] INFO:     127.0.0.1:44738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:45092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:48814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:49956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:43672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:39454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:49742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:44400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:45866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:40762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:48336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:41612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:47988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:41800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:42288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:48258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:39344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:38944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16 DP3 TP3] Decode batch, #running-req: 15, #token: 3948, token usage: 0.01, cuda graph: True, gen throughput (token/s): 510.11, #queue-req: 0, 
[2025-10-27 11:28:16] INFO:     127.0.0.1:41428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:48566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:42110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16 DP1 TP1] Decode batch, #running-req: 15, #token: 3921, token usage: 0.01, cuda graph: True, gen throughput (token/s): 456.46, #queue-req: 0, 
[2025-10-27 11:28:16] INFO:     127.0.0.1:42888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:41478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:49610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:40920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:43754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:44300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:45830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:49318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:48538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:47848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:40770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:49232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:39250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:44522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:44056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:44206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:42006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:48628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:39310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:42178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:44002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:49190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16 DP7 TP7] Decode batch, #running-req: 22, #token: 6005, token usage: 0.01, cuda graph: True, gen throughput (token/s): 569.18, #queue-req: 0, 
[2025-10-27 11:28:16] INFO:     127.0.0.1:43698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:40680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:42494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:45806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:42812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:40588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16 DP6 TP6] Decode batch, #running-req: 9, #token: 2770, token usage: 0.00, cuda graph: True, gen throughput (token/s): 300.10, #queue-req: 0, 
[2025-10-27 11:28:16] INFO:     127.0.0.1:45082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:45130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:50286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:49642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:16] INFO:     127.0.0.1:38848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:44088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:38564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:38596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:47878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:39870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:47528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:43736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:40034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:43440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:41056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17 DP5 TP5] Decode batch, #running-req: 7, #token: 2510, token usage: 0.00, cuda graph: True, gen throughput (token/s): 268.93, #queue-req: 0, 
[2025-10-27 11:28:17] INFO:     127.0.0.1:47368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:45802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:49086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:40170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:40506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:46028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:48372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:43894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17 DP0 TP0] Decode batch, #running-req: 11, #token: 3777, token usage: 0.01, cuda graph: True, gen throughput (token/s): 293.10, #queue-req: 0, 
[2025-10-27 11:28:17] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:43040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:39972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:41496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:48388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17 DP4 TP4] Decode batch, #running-req: 10, #token: 2873, token usage: 0.00, cuda graph: True, gen throughput (token/s): 279.89, #queue-req: 0, 
[2025-10-27 11:28:17] INFO:     127.0.0.1:46908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:44196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:40120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:46632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:49276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:49016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:43970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:41046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:46450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:17] INFO:     127.0.0.1:43856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:48392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:44758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18 DP2 TP2] Decode batch, #running-req: 10, #token: 3287, token usage: 0.00, cuda graph: True, gen throughput (token/s): 306.68, #queue-req: 0, 
[2025-10-27 11:28:18] INFO:     127.0.0.1:44328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:39212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:42630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:49112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:47596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:48328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18 DP3 TP3] Decode batch, #running-req: 4, #token: 1522, token usage: 0.00, cuda graph: True, gen throughput (token/s): 167.30, #queue-req: 0, 
[2025-10-27 11:28:18] INFO:     127.0.0.1:41370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:47074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:46944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18 DP1 TP1] Decode batch, #running-req: 3, #token: 1464, token usage: 0.00, cuda graph: True, gen throughput (token/s): 171.60, #queue-req: 0, 
[2025-10-27 11:28:18] INFO:     127.0.0.1:39610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:40178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:43550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:42370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:39404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:41266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:42578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:45668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:45584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18 DP7 TP7] Decode batch, #running-req: 8, #token: 3051, token usage: 0.00, cuda graph: True, gen throughput (token/s): 314.02, #queue-req: 0, 
[2025-10-27 11:28:18] INFO:     127.0.0.1:48662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:48740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:45892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:49992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18 DP6 TP6] Decode batch, #running-req: 4, #token: 1959, token usage: 0.00, cuda graph: True, gen throughput (token/s): 143.47, #queue-req: 0, 
[2025-10-27 11:28:18] INFO:     127.0.0.1:38616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:39160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:48472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:38772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:46004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:18] INFO:     127.0.0.1:44936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:19 DP5 TP5] Decode batch, #running-req: 3, #token: 1579, token usage: 0.00, cuda graph: True, gen throughput (token/s): 109.27, #queue-req: 0, 
[2025-10-27 11:28:19] INFO:     127.0.0.1:49482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:19] INFO:     127.0.0.1:46842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:19] INFO:     127.0.0.1:45366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:19] INFO:     127.0.0.1:45762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:19] INFO:     127.0.0.1:49396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:19] INFO:     127.0.0.1:46872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:19] INFO:     127.0.0.1:47430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:19 DP0 TP0] Decode batch, #running-req: 2, #token: 1346, token usage: 0.00, cuda graph: True, gen throughput (token/s): 121.54, #queue-req: 0, 
[2025-10-27 11:28:19 DP4 TP4] Decode batch, #running-req: 5, #token: 2238, token usage: 0.00, cuda graph: True, gen throughput (token/s): 176.74, #queue-req: 0, 
[2025-10-27 11:28:19 DP2 TP2] Decode batch, #running-req: 4, #token: 1976, token usage: 0.00, cuda graph: True, gen throughput (token/s): 148.80, #queue-req: 0, 
[2025-10-27 11:28:19] INFO:     127.0.0.1:47730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:19] INFO:     127.0.0.1:48244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:19] INFO:     127.0.0.1:39436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:19] INFO:     127.0.0.1:45058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:19] INFO:     127.0.0.1:40134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:19 DP1 TP1] Decode batch, #running-req: 2, #token: 1266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 62.83, #queue-req: 0, 
[2025-10-27 11:28:19] INFO:     127.0.0.1:48020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:19] INFO:     127.0.0.1:43358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:20] INFO:     127.0.0.1:47648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:20] INFO:     127.0.0.1:41294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:20] INFO:     127.0.0.1:43342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:20] INFO:     127.0.0.1:40324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:20 DP7 TP7] Decode batch, #running-req: 1, #token: 1004, token usage: 0.00, cuda graph: True, gen throughput (token/s): 91.24, #queue-req: 0, 
[2025-10-27 11:28:20 DP6 TP6] Decode batch, #running-req: 2, #token: 1341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 91.86, #queue-req: 0, 
[2025-10-27 11:28:20] INFO:     127.0.0.1:42226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:20] INFO:     127.0.0.1:47816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:20 DP0 TP0] Decode batch, #running-req: 2, #token: 1426, token usage: 0.00, cuda graph: True, gen throughput (token/s): 59.42, #queue-req: 0, 
[2025-10-27 11:28:20] INFO:     127.0.0.1:42554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:20 DP4 TP4] Decode batch, #running-req: 1, #token: 1035, token usage: 0.00, cuda graph: True, gen throughput (token/s): 103.44, #queue-req: 0, 
[2025-10-27 11:28:20] INFO:     127.0.0.1:45534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:20] INFO:     127.0.0.1:48736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:20] INFO:     127.0.0.1:45904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:20] INFO:     127.0.0.1:42220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:21] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:21] INFO:     127.0.0.1:47540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:21 DP6 TP6] Decode batch, #running-req: 1, #token: 1042, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.48, #queue-req: 0, 
[2025-10-27 11:28:21 DP4 TP4] Decode batch, #running-req: 1, #token: 1075, token usage: 0.00, cuda graph: True, gen throughput (token/s): 36.56, #queue-req: 0, 
[2025-10-27 11:28:21] INFO:     127.0.0.1:41534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:22 DP6 TP6] Decode batch, #running-req: 1, #token: 1082, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.68, #queue-req: 0, 
[2025-10-27 11:28:23 DP6 TP6] Decode batch, #running-req: 1, #token: 1122, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.80, #queue-req: 0, 
[2025-10-27 11:28:23] INFO:     127.0.0.1:40416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:36] INFO:     127.0.0.1:51624 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-27 11:28:36 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:28:36] INFO:     127.0.0.1:51638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:36 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:28:36 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 69, #cached-token: 1373, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:28:36 DP2 TP2] Prefill batch, #new-seq: 2, #new-token: 58, #cached-token: 1365, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:28:36 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 60, #cached-token: 1393, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:28:36 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 734, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:28:36 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 64, #cached-token: 1446, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:28:36 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1428, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:28:36 DP3 TP3] Prefill batch, #new-seq: 2, #new-token: 60, #cached-token: 1389, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:28:36 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 155, #cached-token: 3500, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:36 DP1 TP1] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:36 DP3 TP3] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:36 DP4 TP4] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:36 DP6 TP6] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:36 DP2 TP2] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:36 DP0 TP0] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:36 DP5 TP5] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:36 DP7 TP7] [fused_moe] using default for (469, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:36 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 460, #cached-token: 6794, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:28:36 DP0 TP0] Prefill batch, #new-seq: 10, #new-token: 471, #cached-token: 6863, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-27 11:28:36 DP6 TP6] Prefill batch, #new-seq: 10, #new-token: 334, #cached-token: 6892, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:28:36 DP1 TP1] Prefill batch, #new-seq: 6, #new-token: 338, #cached-token: 4066, token usage: 0.00, #running-req: 6, #queue-req: 0, 
[2025-10-27 11:28:36 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 437, #cached-token: 6836, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:28:36 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 523, #cached-token: 6756, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:28:36 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 416, #cached-token: 6873, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:28:36 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 446, #cached-token: 6820, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:28:36 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 234, #cached-token: 5631, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:28:36 DP6 TP6] Prefill batch, #new-seq: 7, #new-token: 237, #cached-token: 4801, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:28:36 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 280, #cached-token: 5437, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:28:36 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 509, #cached-token: 4720, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:28:36 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 357, #cached-token: 5547, token usage: 0.00, #running-req: 11, #queue-req: 0, 
[2025-10-27 11:28:36 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 270, #cached-token: 5643, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:28:36 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 336, #cached-token: 5400, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:28:36 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 358, #cached-token: 5616, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:28:36 DP7 TP7] Prefill batch, #new-seq: 15, #new-token: 830, #cached-token: 10154, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-27 11:28:36 DP3 TP3] Prefill batch, #new-seq: 15, #new-token: 398, #cached-token: 10385, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:28:36 DP2 TP2] Prefill batch, #new-seq: 15, #new-token: 568, #cached-token: 10450, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:28:36 DP6 TP6] Prefill batch, #new-seq: 15, #new-token: 572, #cached-token: 10351, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-27 11:28:36 DP4 TP4] Prefill batch, #new-seq: 15, #new-token: 701, #cached-token: 10326, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:28:36 DP0 TP0] Prefill batch, #new-seq: 15, #new-token: 516, #cached-token: 10382, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-27 11:28:36 DP1 TP1] Prefill batch, #new-seq: 15, #new-token: 498, #cached-token: 10398, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:28:36 DP5 TP5] Prefill batch, #new-seq: 15, #new-token: 433, #cached-token: 10446, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:28:37 DP4 TP4] Prefill batch, #new-seq: 13, #new-token: 226, #cached-token: 9241, token usage: 0.00, #running-req: 35, #queue-req: 0, 
[2025-10-27 11:28:37 DP3 TP3] Prefill batch, #new-seq: 13, #new-token: 245, #cached-token: 9248, token usage: 0.00, #running-req: 35, #queue-req: 0, 
[2025-10-27 11:28:37 DP0 TP0] Prefill batch, #new-seq: 13, #new-token: 279, #cached-token: 9276, token usage: 0.00, #running-req: 34, #queue-req: 0, 
[2025-10-27 11:28:37 DP7 TP7] Prefill batch, #new-seq: 13, #new-token: 172, #cached-token: 9327, token usage: 0.01, #running-req: 34, #queue-req: 0, 
[2025-10-27 11:28:37 DP2 TP2] Prefill batch, #new-seq: 13, #new-token: 170, #cached-token: 9188, token usage: 0.00, #running-req: 35, #queue-req: 0, 
[2025-10-27 11:28:37 DP6 TP6] Prefill batch, #new-seq: 14, #new-token: 264, #cached-token: 9991, token usage: 0.00, #running-req: 34, #queue-req: 0, 
[2025-10-27 11:28:37 DP1 TP1] Prefill batch, #new-seq: 13, #new-token: 221, #cached-token: 9141, token usage: 0.00, #running-req: 35, #queue-req: 0, 
[2025-10-27 11:28:37 DP5 TP5] Prefill batch, #new-seq: 13, #new-token: 274, #cached-token: 9225, token usage: 0.00, #running-req: 35, #queue-req: 0, 
[2025-10-27 11:28:37 DP3 TP3] Prefill batch, #new-seq: 19, #new-token: 526, #cached-token: 13316, token usage: 0.01, #running-req: 48, #queue-req: 0, 
[2025-10-27 11:28:37 DP7 TP7] Prefill batch, #new-seq: 20, #new-token: 600, #cached-token: 13940, token usage: 0.01, #running-req: 47, #queue-req: 0, 
[2025-10-27 11:28:37 DP0 TP0] Prefill batch, #new-seq: 20, #new-token: 652, #cached-token: 13979, token usage: 0.01, #running-req: 47, #queue-req: 0, 
[2025-10-27 11:28:37 DP4 TP4] Prefill batch, #new-seq: 19, #new-token: 494, #cached-token: 13384, token usage: 0.01, #running-req: 48, #queue-req: 0, 
[2025-10-27 11:28:37 DP2 TP2] Prefill batch, #new-seq: 20, #new-token: 774, #cached-token: 13911, token usage: 0.01, #running-req: 48, #queue-req: 0, 
[2025-10-27 11:28:37 DP6 TP6] Prefill batch, #new-seq: 19, #new-token: 356, #cached-token: 13411, token usage: 0.01, #running-req: 48, #queue-req: 0, 
[2025-10-27 11:28:37 DP5 TP5] Prefill batch, #new-seq: 20, #new-token: 438, #cached-token: 13938, token usage: 0.01, #running-req: 48, #queue-req: 0, 
[2025-10-27 11:28:37 DP1 TP1] Prefill batch, #new-seq: 20, #new-token: 854, #cached-token: 13845, token usage: 0.01, #running-req: 48, #queue-req: 0, 
[2025-10-27 11:28:37 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 65, #cached-token: 6441, token usage: 0.01, #running-req: 67, #queue-req: 0, 
[2025-10-27 11:28:37 DP3 TP3] Prefill batch, #new-seq: 9, #new-token: 247, #cached-token: 6309, token usage: 0.01, #running-req: 67, #queue-req: 0, 
[2025-10-27 11:28:37 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 154, #cached-token: 6491, token usage: 0.01, #running-req: 67, #queue-req: 0, 
[2025-10-27 11:28:37 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 122, #cached-token: 6413, token usage: 0.01, #running-req: 67, #queue-req: 0, 
[2025-10-27 11:28:37 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 292, #cached-token: 6318, token usage: 0.01, #running-req: 68, #queue-req: 0, 
[2025-10-27 11:28:37 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 156, #cached-token: 6371, token usage: 0.01, #running-req: 67, #queue-req: 0, 
[2025-10-27 11:28:37 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 148, #cached-token: 5648, token usage: 0.01, #running-req: 68, #queue-req: 0, 
[2025-10-27 11:28:37 DP1 TP1] Prefill batch, #new-seq: 9, #new-token: 156, #cached-token: 6396, token usage: 0.01, #running-req: 68, #queue-req: 0, 
[2025-10-27 11:28:37 DP4 TP4] Prefill batch, #new-seq: 20, #new-token: 276, #cached-token: 14361, token usage: 0.01, #running-req: 76, #queue-req: 0, 
[2025-10-27 11:28:37 DP2 TP2] Prefill batch, #new-seq: 20, #new-token: 430, #cached-token: 14183, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-27 11:28:37 DP6 TP6] Prefill batch, #new-seq: 20, #new-token: 365, #cached-token: 14184, token usage: 0.01, #running-req: 76, #queue-req: 0, 
[2025-10-27 11:28:37 DP5 TP5] Prefill batch, #new-seq: 20, #new-token: 253, #cached-token: 14263, token usage: 0.01, #running-req: 76, #queue-req: 0, 
[2025-10-27 11:28:37 DP3 TP3] Prefill batch, #new-seq: 21, #new-token: 400, #cached-token: 14910, token usage: 0.01, #running-req: 76, #queue-req: 0, 
[2025-10-27 11:28:37 DP1 TP1] Prefill batch, #new-seq: 21, #new-token: 353, #cached-token: 14970, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-27 11:28:37 DP7 TP7] Prefill batch, #new-seq: 20, #new-token: 306, #cached-token: 14202, token usage: 0.01, #running-req: 76, #queue-req: 0, 
[2025-10-27 11:28:37 DP0 TP0] Prefill batch, #new-seq: 21, #new-token: 307, #cached-token: 14898, token usage: 0.01, #running-req: 76, #queue-req: 0, 
[2025-10-27 11:28:37 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 142, #cached-token: 5088, token usage: 0.01, #running-req: 97, #queue-req: 0, 
[2025-10-27 11:28:37 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 300, #cached-token: 6315, token usage: 0.01, #running-req: 96, #queue-req: 0, 
[2025-10-27 11:28:37 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 102, #cached-token: 5737, token usage: 0.01, #running-req: 97, #queue-req: 0, 
[2025-10-27 11:28:37 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 266, #cached-token: 6337, token usage: 0.01, #running-req: 96, #queue-req: 0, 
[2025-10-27 11:28:37 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 128, #cached-token: 5759, token usage: 0.01, #running-req: 97, #queue-req: 0, 
[2025-10-27 11:28:37 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 285, #cached-token: 6449, token usage: 0.01, #running-req: 96, #queue-req: 0, 
[2025-10-27 11:28:37 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 39, #cached-token: 5012, token usage: 0.01, #running-req: 98, #queue-req: 0, 
[2025-10-27 11:28:37 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 180, #cached-token: 6382, token usage: 0.01, #running-req: 96, #queue-req: 0, 
[2025-10-27 11:28:38 DP4 TP4] Prefill batch, #new-seq: 13, #new-token: 195, #cached-token: 9237, token usage: 0.01, #running-req: 105, #queue-req: 0, 
[2025-10-27 11:28:38 DP3 TP3] Prefill batch, #new-seq: 13, #new-token: 227, #cached-token: 9174, token usage: 0.01, #running-req: 105, #queue-req: 0, 
[2025-10-27 11:28:38 DP7 TP7] Prefill batch, #new-seq: 13, #new-token: 260, #cached-token: 9303, token usage: 0.01, #running-req: 105, #queue-req: 0, 
[2025-10-27 11:28:38 DP0 TP0] Prefill batch, #new-seq: 13, #new-token: 266, #cached-token: 9212, token usage: 0.01, #running-req: 104, #queue-req: 0, 
[2025-10-27 11:28:38 DP6 TP6] Prefill batch, #new-seq: 13, #new-token: 197, #cached-token: 9195, token usage: 0.01, #running-req: 105, #queue-req: 0, 
[2025-10-27 11:28:38 DP2 TP2] Prefill batch, #new-seq: 13, #new-token: 47, #cached-token: 9314, token usage: 0.01, #running-req: 105, #queue-req: 0, 
[2025-10-27 11:28:38 DP1 TP1] Prefill batch, #new-seq: 13, #new-token: 235, #cached-token: 9225, token usage: 0.01, #running-req: 105, #queue-req: 0, 
[2025-10-27 11:28:38 DP5 TP5] Prefill batch, #new-seq: 13, #new-token: 248, #cached-token: 9323, token usage: 0.01, #running-req: 105, #queue-req: 0, 
[2025-10-27 11:28:38 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 284, #cached-token: 6241, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-27 11:28:38 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 231, #cached-token: 6394, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-27 11:28:38 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 174, #cached-token: 6348, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-27 11:28:38 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 378, #cached-token: 6308, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-27 11:28:38 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 218, #cached-token: 6454, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-27 11:28:38 DP3 TP3] Prefill batch, #new-seq: 9, #new-token: 340, #cached-token: 6233, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-27 11:28:38 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 266, #cached-token: 6383, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-27 11:28:38 DP1 TP1] Prefill batch, #new-seq: 9, #new-token: 349, #cached-token: 6350, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-27 11:28:38 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 354, #cached-token: 6265, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-27 11:28:38 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 430, #cached-token: 6435, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-27 11:28:38 DP0 TP0] Prefill batch, #new-seq: 10, #new-token: 134, #cached-token: 7171, token usage: 0.01, #running-req: 126, #queue-req: 0, 
[2025-10-27 11:28:38 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 290, #cached-token: 6189, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-27 11:28:38 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 178, #cached-token: 6414, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-27 11:28:38 DP3 TP3] Prefill batch, #new-seq: 9, #new-token: 247, #cached-token: 6254, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-27 11:28:38 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 290, #cached-token: 6188, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-27 11:28:38 DP1 TP1] Prefill batch, #new-seq: 10, #new-token: 260, #cached-token: 7017, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-27 11:28:38 DP6 TP6] Prefill batch, #new-seq: 12, #new-token: 162, #cached-token: 8544, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-27 11:28:38 DP2 TP2] Prefill batch, #new-seq: 12, #new-token: 260, #cached-token: 8623, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-27 11:28:38 DP4 TP4] Prefill batch, #new-seq: 12, #new-token: 109, #cached-token: 8578, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-27 11:28:38 DP7 TP7] Prefill batch, #new-seq: 11, #new-token: 100, #cached-token: 8010, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-27 11:28:38 DP0 TP0] Prefill batch, #new-seq: 11, #new-token: 170, #cached-token: 7772, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-27 11:28:38 DP1 TP1] Prefill batch, #new-seq: 11, #new-token: 353, #cached-token: 7590, token usage: 0.01, #running-req: 137, #queue-req: 0, 
[2025-10-27 11:28:38 DP5 TP5] Prefill batch, #new-seq: 12, #new-token: 169, #cached-token: 8546, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-27 11:28:38 DP3 TP3] Prefill batch, #new-seq: 12, #new-token: 203, #cached-token: 8455, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-27 11:28:38 DP4 TP4] Prefill batch, #new-seq: 12, #new-token: 261, #cached-token: 8484, token usage: 0.01, #running-req: 148, #queue-req: 0, 
[2025-10-27 11:28:38 DP3 TP3] Prefill batch, #new-seq: 12, #new-token: 312, #cached-token: 8413, token usage: 0.01, #running-req: 148, #queue-req: 0, 
[2025-10-27 11:28:38 DP7 TP7] Prefill batch, #new-seq: 13, #new-token: 276, #cached-token: 9215, token usage: 0.02, #running-req: 147, #queue-req: 0, 
[2025-10-27 11:28:38 DP5 TP5] Prefill batch, #new-seq: 12, #new-token: 117, #cached-token: 8591, token usage: 0.01, #running-req: 148, #queue-req: 0, 
[2025-10-27 11:28:38 DP6 TP6] Prefill batch, #new-seq: 12, #new-token: 332, #cached-token: 8416, token usage: 0.01, #running-req: 148, #queue-req: 0, 
[2025-10-27 11:28:38 DP2 TP2] Prefill batch, #new-seq: 12, #new-token: 264, #cached-token: 8626, token usage: 0.02, #running-req: 148, #queue-req: 0, 
[2025-10-27 11:28:38 DP0 TP0] Prefill batch, #new-seq: 12, #new-token: 47, #cached-token: 8837, token usage: 0.02, #running-req: 147, #queue-req: 0, 
[2025-10-27 11:28:38 DP1 TP1] Prefill batch, #new-seq: 12, #new-token: 412, #cached-token: 8565, token usage: 0.02, #running-req: 148, #queue-req: 0, 
[2025-10-27 11:28:38 DP4 TP4] Prefill batch, #new-seq: 5, #new-token: 92, #cached-token: 3484, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-27 11:28:38 DP2 TP2] Prefill batch, #new-seq: 5, #new-token: 68, #cached-token: 3605, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-27 11:28:38 DP6 TP6] Prefill batch, #new-seq: 5, #new-token: 157, #cached-token: 3515, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-27 11:28:38 DP7 TP7] Prefill batch, #new-seq: 5, #new-token: 175, #cached-token: 3435, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-27 11:28:38 DP0 TP0] Prefill batch, #new-seq: 5, #new-token: 65, #cached-token: 3596, token usage: 0.02, #running-req: 159, #queue-req: 0, 
[2025-10-27 11:28:38 DP3 TP3] Prefill batch, #new-seq: 5, #new-token: 139, #cached-token: 3544, token usage: 0.01, #running-req: 160, #queue-req: 0, 
[2025-10-27 11:28:38 DP5 TP5] Prefill batch, #new-seq: 5, #new-token: 201, #cached-token: 3456, token usage: 0.01, #running-req: 160, #queue-req: 0, 
[2025-10-27 11:28:38 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 75, #cached-token: 3561, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:39 DP7 TP7] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:39 DP6 TP6] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:39 DP0 TP0] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:39 DP1 TP1] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:39 DP3 TP3] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:39 DP4 TP4] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:39 DP2 TP2] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:39 DP5 TP5] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:28:39 DP5 TP5] Decode batch, #running-req: 165, #token: 10636, token usage: 0.02, cuda graph: True, gen throughput (token/s): 19.64, #queue-req: 0, 
[2025-10-27 11:28:39 DP3 TP3] Decode batch, #running-req: 165, #token: 10878, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.14, #queue-req: 0, 
[2025-10-27 11:28:39 DP2 TP2] Decode batch, #running-req: 165, #token: 11464, token usage: 0.02, cuda graph: True, gen throughput (token/s): 29.01, #queue-req: 0, 
[2025-10-27 11:28:40 DP0 TP0] Decode batch, #running-req: 164, #token: 13540, token usage: 0.02, cuda graph: True, gen throughput (token/s): 123.20, #queue-req: 0, 
[2025-10-27 11:28:40 DP7 TP7] Decode batch, #running-req: 165, #token: 13506, token usage: 0.02, cuda graph: True, gen throughput (token/s): 120.57, #queue-req: 0, 
[2025-10-27 11:28:42] INFO:     127.0.0.1:54420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:42 DP1 TP1] Decode batch, #running-req: 165, #token: 15473, token usage: 0.02, cuda graph: True, gen throughput (token/s): 198.85, #queue-req: 0, 
[2025-10-27 11:28:42] INFO:     127.0.0.1:55794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:42] INFO:     127.0.0.1:60082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:42 DP4 TP4] Decode batch, #running-req: 165, #token: 15940, token usage: 0.02, cuda graph: True, gen throughput (token/s): 250.58, #queue-req: 0, 
[2025-10-27 11:28:43 DP6 TP6] Decode batch, #running-req: 165, #token: 16198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 278.19, #queue-req: 0, 
[2025-10-27 11:28:43] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:34758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:51694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:56220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:52082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:34880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:34320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:33652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:56022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:33668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:57122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:52718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:43] INFO:     127.0.0.1:54292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44 DP5 TP5] Decode batch, #running-req: 162, #token: 16874, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1382.61, #queue-req: 0, 
[2025-10-27 11:28:44] INFO:     127.0.0.1:58970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:33124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:52612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:57136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:60050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44 DP3 TP3] Decode batch, #running-req: 161, #token: 17086, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1381.17, #queue-req: 0, 
[2025-10-27 11:28:44] INFO:     127.0.0.1:51916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44 DP2 TP2] Decode batch, #running-req: 161, #token: 17498, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1383.87, #queue-req: 0, 
[2025-10-27 11:28:44] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:52164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:51920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:52130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:54778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:53316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:60282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:56472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:60152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:32932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:34532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:56626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:60172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:54688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:59508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:33952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:34794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:57450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:52426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:53826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:33800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:52068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:59312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:57158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:33594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:59642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:53122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:54902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:58338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:60214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:34738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:53948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:34142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:60404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:59180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:33064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:59462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:51662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:44] INFO:     127.0.0.1:60662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:52792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:53390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:52186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:33634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:56570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:59154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:33304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:56364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:56012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:53302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:55390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:57006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:34160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:33166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:55584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:56102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:53852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:34386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:56874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:59372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:54596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:60834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:60904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:32988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:33230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:57302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:57902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:60380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:33764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:53372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:55046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:59964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:52634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:54024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:33256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:53536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:34278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:33894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:57606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:52760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:58300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:52806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45 DP7 TP7] Decode batch, #running-req: 153, #token: 18788, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1347.77, #queue-req: 0, 
[2025-10-27 11:28:45] INFO:     127.0.0.1:60342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45 DP0 TP0] Decode batch, #running-req: 150, #token: 18483, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1339.27, #queue-req: 0, 
[2025-10-27 11:28:45] INFO:     127.0.0.1:53410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:33302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:53452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:58416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:58468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:53904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:57960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:53578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:51646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:56946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:59338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:33760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:52466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:53680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:54822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:53938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:55936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:53184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:54998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:55872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:53748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:54508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:45] INFO:     127.0.0.1:58738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:59276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:56154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:58764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:59488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:34420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:35030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:59686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:55830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:54976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:58144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:60444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:58104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:59782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:58234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:35032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:53422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:33724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:58592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:34700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:53996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:56958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:52998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:55788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:58840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:56424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:59798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:60960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:33134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:57010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:33184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:51814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:54068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:59098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:52902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:56586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:60022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:57736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:52006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:53236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:59536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:34630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:34570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:52674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:54918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:54362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:59576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:52642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:33580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:33986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:52040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:53932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:33638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:34772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:34842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:53296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:33362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:54566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:52622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:53256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:59426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:55506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:58344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:56200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:33290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:52970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:54528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:35008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:53882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:55210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:52158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:54812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:52496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:52624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:59880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:53298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:32774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:54172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:54228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:56810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:33336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:34522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46 DP1 TP1] Decode batch, #running-req: 136, #token: 17845, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1354.57, #queue-req: 0, 
[2025-10-27 11:28:46] INFO:     127.0.0.1:57204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:59332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:56320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:51850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:34250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:34562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:54204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:54562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:59572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:46] INFO:     127.0.0.1:32972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:53976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:33248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:34434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:59418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:60636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:53326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:57984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:52116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:53530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:57094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:59730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:60806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:33746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:33696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:55650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:59974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:53554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:33180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:55352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:57404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:33506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:34918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:52954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:57012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:57248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:32852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:34234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:52030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:55206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:60480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:54794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:54282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:33408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:34658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:32794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:53732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:34380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:34140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:53398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:59754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:34396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:53104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:54618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:60712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:52244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:54990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47 DP4 TP4] Decode batch, #running-req: 128, #token: 16865, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1330.89, #queue-req: 0, 
[2025-10-27 11:28:47] INFO:     127.0.0.1:33424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:34988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:52550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:54646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:55250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:60966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:33150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:55542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:33970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:54620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:59446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:60754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47 DP6 TP6] Decode batch, #running-req: 115, #token: 16162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1303.86, #queue-req: 0, 
[2025-10-27 11:28:47] INFO:     127.0.0.1:34308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:57544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:60822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:60122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:60540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:34184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:52142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:32848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:60136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:52826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:54732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:51936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:33378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:34748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:35100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:57220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:34008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:60874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:34030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:55984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:60278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:35128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:57610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:35044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:54062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:59694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:60322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:52446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:53900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:59796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:57240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:34502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:54510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:33686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:54748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:55740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:55932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:57502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:59910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:51878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:52662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:34410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:59438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:53116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:47] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:54840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:33434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:35108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:35064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:53320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:52078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:33026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:57470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:32886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:55526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:60300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:53606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:58440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:60766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:60620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:33900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:51950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:53696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:52266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:57910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:52866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:57558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:33498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:52574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:60000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:35078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48 DP5 TP5] Decode batch, #running-req: 99, #token: 14615, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1241.83, #queue-req: 0, 
[2025-10-27 11:28:48] INFO:     127.0.0.1:34624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:54856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:53566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:57362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:52586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:53208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:58650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48 DP3 TP3] Decode batch, #running-req: 103, #token: 14779, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1256.71, #queue-req: 0, 
[2025-10-27 11:28:48] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48 DP2 TP2] Decode batch, #running-req: 95, #token: 14781, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1243.44, #queue-req: 0, 
[2025-10-27 11:28:48] INFO:     127.0.0.1:34122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:53024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:58494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:55138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:57846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:52044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:54178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:52404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:55386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:58852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:33630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:51888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:54960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:32790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:51948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:33494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:55554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:33322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:51728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:57294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:58976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:54564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:55096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:58664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:60872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:53994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:54946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:54770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:51802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:54386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:54456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:34742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:55282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:35050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:52212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:53776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:52478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:53670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:55266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:33904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:60314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:33460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:52070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:60992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:54908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:55376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:58778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:32814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:54506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:55082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:56800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:52896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:53598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:55892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:48] INFO:     127.0.0.1:59936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:51756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:59956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:52304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:58002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:53278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:55174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:33268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:33682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:52074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:60352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:60732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:53804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:54666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:59352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:33284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:34014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:52528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:59318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:33678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:53842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:58388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:33158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:52286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:56906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:58162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:52918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:52084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:54760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:34092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:56886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:57908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:59946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:33812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:56070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:60068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:34716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:60436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:58478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:51710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:52600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:33386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:53694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:51996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:55846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:56556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:55470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:59302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:33096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:52700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:57182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:59588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:34470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:34690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:56600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:34704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49 DP0 TP0] Decode batch, #running-req: 91, #token: 15871, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1258.42, #queue-req: 0, 
[2025-10-27 11:28:49] INFO:     127.0.0.1:60262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49 DP7 TP7] Decode batch, #running-req: 81, #token: 13728, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1214.38, #queue-req: 0, 
[2025-10-27 11:28:49] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:55310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:34616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:60946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:52336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:56718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:34732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:58512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:60978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:55750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:57068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:57414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:60108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:60594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:57128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:34108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:55622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:34462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:34806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:34854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:53518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:54940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:55408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:58718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:56084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:60330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:54460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:58558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:57970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:57160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:54016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:57712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:34786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:59290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:34850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:52008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:55342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:57934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:32956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:60726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:49] INFO:     127.0.0.1:60560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:32902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:58924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:59256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:33708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:56358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:59076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:56032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:59012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:55002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:53616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:33992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:34550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:56144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:53078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:54846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:33924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:53724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:34246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:59698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:34908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:57054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:34336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:54708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:51782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:55644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:58080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:54156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:55866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:53094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:33250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:56222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:59502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:55938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:57376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:59792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:58120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:34516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:58534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:58214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:51718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:57774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:33828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:55860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:32796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:55012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:54124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:56532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:59190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:59406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:32820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:33864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50 DP1 TP1] Decode batch, #running-req: 67, #token: 11995, token usage: 0.02, cuda graph: True, gen throughput (token/s): 994.32, #queue-req: 0, 
[2025-10-27 11:28:50] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:51848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:54312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:55810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:54610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:33878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:33200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:34366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:59212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:55192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:56624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:57334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:59856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:58290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:56748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:34828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:55024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:60586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:54288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:59998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:33884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:34852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:57050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:51906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:34816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:34252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:50] INFO:     127.0.0.1:57406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:33486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:52566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:60646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:52228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:60898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:52960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:59166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51 DP4 TP4] Decode batch, #running-req: 69, #token: 12622, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1042.18, #queue-req: 0, 
[2025-10-27 11:28:51] INFO:     127.0.0.1:52462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:53730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:59908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:33512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51 DP6 TP6] Decode batch, #running-req: 50, #token: 9839, token usage: 0.01, cuda graph: True, gen throughput (token/s): 860.41, #queue-req: 0, 
[2025-10-27 11:28:51] INFO:     127.0.0.1:54192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:53854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:56768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:34902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:58518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:33808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:51988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:57748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:33622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:58444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:53310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:34056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:34194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:33962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:34602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:51772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:59248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:34694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:58654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:59864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:34216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:52732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:58972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:60504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:52222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:57510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:32976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:34032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:57336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:56932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:53436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:60574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:34678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:32810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:33550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:52062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:34460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:35076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:52650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:56440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:53340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:59762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:60204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:58564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:32832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:33936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:59556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:56282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:34202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:53734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:53444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:32880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:34870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:53504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:59112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:34540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51 DP5 TP5] Decode batch, #running-req: 28, #token: 5939, token usage: 0.01, cuda graph: True, gen throughput (token/s): 714.67, #queue-req: 0, 
[2025-10-27 11:28:51] INFO:     127.0.0.1:57486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:60546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:60134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:60166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:34478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51 DP3 TP3] Decode batch, #running-req: 39, #token: 8123, token usage: 0.01, cuda graph: True, gen throughput (token/s): 809.23, #queue-req: 0, 
[2025-10-27 11:28:51 DP2 TP2] Decode batch, #running-req: 42, #token: 9105, token usage: 0.01, cuda graph: True, gen throughput (token/s): 739.48, #queue-req: 0, 
[2025-10-27 11:28:51] INFO:     127.0.0.1:55686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:60982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:59262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:54308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:55324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:60186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:51] INFO:     127.0.0.1:33608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:52290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:58274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:54786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:58946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:51754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:51892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:33456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:33470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:33766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:56174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:32968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:33730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:34920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:55908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:33244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:54446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:33602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:59082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:59658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:60406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:53912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:60890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:33016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:56410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:54286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:51678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:34018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:54410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:57234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:60370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:60246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:55058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:33752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:56834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:34118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:52318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:57196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:34292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:35092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:34038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:59530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:60426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:54652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:59610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:34530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:59360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:59590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:33836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:52410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:52052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:33040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:33208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:54810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:32920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:56266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:53754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52 DP0 TP0] Decode batch, #running-req: 33, #token: 7304, token usage: 0.01, cuda graph: True, gen throughput (token/s): 751.80, #queue-req: 0, 
[2025-10-27 11:28:52] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52 DP7 TP7] Decode batch, #running-req: 36, #token: 8148, token usage: 0.01, cuda graph: True, gen throughput (token/s): 711.72, #queue-req: 0, 
[2025-10-27 11:28:52] INFO:     127.0.0.1:53384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:33074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:56656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:32858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:58136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:60140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:52702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:33564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:59358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:52100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:58496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:32864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:57628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:58334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:60178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:32984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:54370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:52776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:32918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:32936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:52] INFO:     127.0.0.1:60066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:57524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:52524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:34584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:58940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:53692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:60006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:56042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:52240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:53428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:59934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:34600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:52512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:60034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:53966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:54244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:33910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:34492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:55140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:60704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:53646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:55168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:33092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:60938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:53666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:53064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53 DP1 TP1] Decode batch, #running-req: 24, #token: 5916, token usage: 0.01, cuda graph: True, gen throughput (token/s): 605.03, #queue-req: 0, 
[2025-10-27 11:28:53] INFO:     127.0.0.1:53608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:34446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:54112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:59138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:56694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:53636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:52924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:54586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:59000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:34660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53 DP4 TP4] Decode batch, #running-req: 29, #token: 7143, token usage: 0.01, cuda graph: True, gen throughput (token/s): 670.29, #queue-req: 0, 
[2025-10-27 11:28:53] INFO:     127.0.0.1:60768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:53990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:57022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:57726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:58264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:33782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:35118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53 DP6 TP6] Decode batch, #running-req: 18, #token: 4307, token usage: 0.01, cuda graph: True, gen throughput (token/s): 500.55, #queue-req: 0, 
[2025-10-27 11:28:53] INFO:     127.0.0.1:52880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:53550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:55468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:52326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:60800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:52502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:35018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:58754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:33520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:53884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:57350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:34646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:34218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:52390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:54398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:53] INFO:     127.0.0.1:52208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:54754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:54008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:55640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:54486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:58702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:53920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:55038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:34270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54 DP5 TP5] Decode batch, #running-req: 7, #token: 2438, token usage: 0.00, cuda graph: True, gen throughput (token/s): 272.62, #queue-req: 0, 
[2025-10-27 11:28:54] INFO:     127.0.0.1:58688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:56610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:60494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54 DP3 TP3] Decode batch, #running-req: 12, #token: 3253, token usage: 0.00, cuda graph: True, gen throughput (token/s): 391.59, #queue-req: 0, 
[2025-10-27 11:28:54 DP2 TP2] Decode batch, #running-req: 16, #token: 4374, token usage: 0.01, cuda graph: True, gen throughput (token/s): 451.19, #queue-req: 0, 
[2025-10-27 11:28:54] INFO:     127.0.0.1:53388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:60930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:56852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:34936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:60298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:54140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:33216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:54930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:52992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:56820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:51742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:33886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:34344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:56704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:58698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:55714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:34080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:57144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:34964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:58000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:32970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:33728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:60420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:56894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54 DP0 TP0] Decode batch, #running-req: 11, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 349.53, #queue-req: 0, 
[2025-10-27 11:28:54 DP7 TP7] Decode batch, #running-req: 15, #token: 4546, token usage: 0.01, cuda graph: True, gen throughput (token/s): 475.27, #queue-req: 0, 
[2025-10-27 11:28:54] INFO:     127.0.0.1:34488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:60784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:56118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:52050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:59396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:34992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:52360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:52442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:54] INFO:     127.0.0.1:33082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:34004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:52592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:52558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:53286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:53764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:54504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55 DP1 TP1] Decode batch, #running-req: 9, #token: 3091, token usage: 0.00, cuda graph: True, gen throughput (token/s): 311.99, #queue-req: 0, 
[2025-10-27 11:28:55] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:56996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:52870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:60668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:33112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:56554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55 DP4 TP4] Decode batch, #running-req: 13, #token: 4300, token usage: 0.01, cuda graph: True, gen throughput (token/s): 383.13, #queue-req: 0, 
[2025-10-27 11:28:55] INFO:     127.0.0.1:59482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55 DP6 TP6] Decode batch, #running-req: 7, #token: 2756, token usage: 0.00, cuda graph: True, gen throughput (token/s): 209.78, #queue-req: 0, 
[2025-10-27 11:28:55] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:52098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:60678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:59904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:57170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:33804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:54334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:60780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:59922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:33350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:52722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:55] INFO:     127.0.0.1:56384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:56680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56 DP5 TP5] Decode batch, #running-req: 2, #token: 1280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 95.20, #queue-req: 0, 
[2025-10-27 11:28:56] INFO:     127.0.0.1:58312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:52946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:55484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56 DP3 TP3] Decode batch, #running-req: 2, #token: 1239, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.74, #queue-req: 0, 
[2025-10-27 11:28:56 DP2 TP2] Decode batch, #running-req: 7, #token: 2718, token usage: 0.00, cuda graph: True, gen throughput (token/s): 237.99, #queue-req: 0, 
[2025-10-27 11:28:56] INFO:     127.0.0.1:58568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:56054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:55224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:34156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:53318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:54526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:55294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:58638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:33170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56 DP7 TP7] Decode batch, #running-req: 9, #token: 3070, token usage: 0.00, cuda graph: True, gen throughput (token/s): 276.51, #queue-req: 0, 
[2025-10-27 11:28:56 DP0 TP0] Decode batch, #running-req: 1, #token: 955, token usage: 0.00, cuda graph: True, gen throughput (token/s): 129.96, #queue-req: 0, 
[2025-10-27 11:28:56] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:57874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:33796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:34054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:59812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:59848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:56] INFO:     127.0.0.1:55926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:57 DP1 TP1] Decode batch, #running-req: 1, #token: 943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 86.99, #queue-req: 0, 
[2025-10-27 11:28:57] INFO:     127.0.0.1:56784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:57 DP4 TP4] Decode batch, #running-req: 5, #token: 2326, token usage: 0.00, cuda graph: True, gen throughput (token/s): 190.99, #queue-req: 0, 
[2025-10-27 11:28:57 DP6 TP6] Decode batch, #running-req: 4, #token: 2065, token usage: 0.00, cuda graph: True, gen throughput (token/s): 133.79, #queue-req: 0, 
[2025-10-27 11:28:57] INFO:     127.0.0.1:53222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:57] INFO:     127.0.0.1:32954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:57] INFO:     127.0.0.1:33002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:57] INFO:     127.0.0.1:58910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:57] INFO:     127.0.0.1:55152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:57] INFO:     127.0.0.1:33852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:57 DP5 TP5] Decode batch, #running-req: 2, #token: 1360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 50.75, #queue-req: 0, 
[2025-10-27 11:28:57] INFO:     127.0.0.1:51800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:57 DP3 TP3] Decode batch, #running-req: 1, #token: 993, token usage: 0.00, cuda graph: True, gen throughput (token/s): 46.49, #queue-req: 0, 
[2025-10-27 11:28:57 DP2 TP2] Decode batch, #running-req: 5, #token: 2284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 131.81, #queue-req: 0, 
[2025-10-27 11:28:57] INFO:     127.0.0.1:55454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:57] INFO:     127.0.0.1:55176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:57] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:57] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:58 DP7 TP7] Decode batch, #running-req: 2, #token: 1348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 110.45, #queue-req: 0, 
[2025-10-27 11:28:58] INFO:     127.0.0.1:60532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:58] INFO:     127.0.0.1:60940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:58] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:58] INFO:     127.0.0.1:52744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:58] INFO:     127.0.0.1:33440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:58 DP4 TP4] Decode batch, #running-req: 2, #token: 1423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 100.48, #queue-req: 0, 
[2025-10-27 11:28:58] INFO:     127.0.0.1:58480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:58 DP6 TP6] Decode batch, #running-req: 1, #token: 1025, token usage: 0.00, cuda graph: True, gen throughput (token/s): 39.12, #queue-req: 0, 
[2025-10-27 11:28:58] INFO:     127.0.0.1:60860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:58 DP5 TP5] Decode batch, #running-req: 1, #token: 1092, token usage: 0.00, cuda graph: True, gen throughput (token/s): 42.29, #queue-req: 0, 
[2025-10-27 11:28:59] INFO:     127.0.0.1:60464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:59] INFO:     127.0.0.1:60610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:59 DP7 TP7] Decode batch, #running-req: 1, #token: 1051, token usage: 0.00, cuda graph: True, gen throughput (token/s): 54.50, #queue-req: 0, 
[2025-10-27 11:28:59] INFO:     127.0.0.1:58780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:59] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:28:59 DP6 TP6] Decode batch, #running-req: 1, #token: 1065, token usage: 0.00, cuda graph: True, gen throughput (token/s): 36.90, #queue-req: 0, 
[2025-10-27 11:29:00 DP6 TP6] Decode batch, #running-req: 1, #token: 1105, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.84, #queue-req: 0, 
[2025-10-27 11:29:01 DP6 TP6] Decode batch, #running-req: 1, #token: 1145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.77, #queue-req: 0, 
[2025-10-27 11:29:02 DP6 TP6] Decode batch, #running-req: 1, #token: 1185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.72, #queue-req: 0, 
[2025-10-27 11:29:03 DP6 TP6] Decode batch, #running-req: 1, #token: 1225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.76, #queue-req: 0, 
[2025-10-27 11:29:04] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:16] INFO:     127.0.0.1:48472 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-27 11:29:16 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:16] INFO:     127.0.0.1:48474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:16 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:17 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1440, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:17 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1451, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:17 DP2 TP2] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1421, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:17 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 771, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:17 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 92, #cached-token: 1379, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:17 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1508, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:17 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 91, #cached-token: 3494, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-27 11:29:17 DP3 TP3] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1447, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:17 DP0 TP0] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:17 DP1 TP1] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:17 DP3 TP3] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:17 DP5 TP5] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:17 DP7 TP7] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:17 DP6 TP6] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:17 DP4 TP4] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:17 DP2 TP2] [fused_moe] using default for (194, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:17 DP1 TP1] Prefill batch, #new-seq: 6, #new-token: 238, #cached-token: 4146, token usage: 0.00, #running-req: 6, #queue-req: 0, 
[2025-10-27 11:29:17 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 252, #cached-token: 6999, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:29:17 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 229, #cached-token: 7020, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:29:17 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 162, #cached-token: 6999, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:29:17 DP6 TP6] Prefill batch, #new-seq: 10, #new-token: 292, #cached-token: 7016, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:29:17 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 370, #cached-token: 6879, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:29:17 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 393, #cached-token: 6872, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:29:17 DP0 TP0] Prefill batch, #new-seq: 11, #new-token: 557, #cached-token: 7499, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-27 11:29:17 DP6 TP6] Prefill batch, #new-seq: 7, #new-token: 363, #cached-token: 4782, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:29:17 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 346, #cached-token: 4880, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:29:17 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 260, #cached-token: 5534, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:29:17 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 438, #cached-token: 5354, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:29:17 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 314, #cached-token: 4859, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:29:17 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 282, #cached-token: 5487, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:29:17 DP5 TP5] Prefill batch, #new-seq: 7, #new-token: 352, #cached-token: 4779, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:29:17 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 319, #cached-token: 4784, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-27 11:29:17 DP3 TP3] Prefill batch, #new-seq: 13, #new-token: 473, #cached-token: 9037, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:29:17 DP7 TP7] Prefill batch, #new-seq: 13, #new-token: 190, #cached-token: 9315, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-27 11:29:17 DP6 TP6] Prefill batch, #new-seq: 13, #new-token: 450, #cached-token: 9059, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-27 11:29:17 DP2 TP2] Prefill batch, #new-seq: 13, #new-token: 276, #cached-token: 9145, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:29:17 DP1 TP1] Prefill batch, #new-seq: 13, #new-token: 571, #cached-token: 8991, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:29:17 DP4 TP4] Prefill batch, #new-seq: 14, #new-token: 727, #cached-token: 9581, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-27 11:29:17 DP0 TP0] Prefill batch, #new-seq: 13, #new-token: 437, #cached-token: 9031, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-27 11:29:17 DP5 TP5] Prefill batch, #new-seq: 13, #new-token: 525, #cached-token: 8907, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-27 11:29:17 DP7 TP7] Prefill batch, #new-seq: 13, #new-token: 106, #cached-token: 9327, token usage: 0.00, #running-req: 32, #queue-req: 0, 
[2025-10-27 11:29:17 DP3 TP3] Prefill batch, #new-seq: 12, #new-token: 250, #cached-token: 8508, token usage: 0.00, #running-req: 33, #queue-req: 0, 
[2025-10-27 11:29:17 DP2 TP2] Prefill batch, #new-seq: 12, #new-token: 221, #cached-token: 8409, token usage: 0.00, #running-req: 33, #queue-req: 0, 
[2025-10-27 11:29:17 DP0 TP0] Prefill batch, #new-seq: 12, #new-token: 181, #cached-token: 8556, token usage: 0.00, #running-req: 32, #queue-req: 0, 
[2025-10-27 11:29:17 DP4 TP4] Prefill batch, #new-seq: 12, #new-token: 222, #cached-token: 8617, token usage: 0.01, #running-req: 33, #queue-req: 0, 
[2025-10-27 11:29:17 DP1 TP1] Prefill batch, #new-seq: 12, #new-token: 197, #cached-token: 8452, token usage: 0.00, #running-req: 33, #queue-req: 0, 
[2025-10-27 11:29:17 DP5 TP5] Prefill batch, #new-seq: 13, #new-token: 118, #cached-token: 9396, token usage: 0.00, #running-req: 32, #queue-req: 0, 
[2025-10-27 11:29:17 DP6 TP6] Prefill batch, #new-seq: 13, #new-token: 103, #cached-token: 9469, token usage: 0.01, #running-req: 32, #queue-req: 0, 
[2025-10-27 11:29:17 DP3 TP3] Prefill batch, #new-seq: 17, #new-token: 419, #cached-token: 12059, token usage: 0.01, #running-req: 45, #queue-req: 0, 
[2025-10-27 11:29:17 DP7 TP7] Prefill batch, #new-seq: 17, #new-token: 395, #cached-token: 12087, token usage: 0.01, #running-req: 45, #queue-req: 0, 
[2025-10-27 11:29:17 DP6 TP6] Prefill batch, #new-seq: 17, #new-token: 341, #cached-token: 12074, token usage: 0.01, #running-req: 45, #queue-req: 0, 
[2025-10-27 11:29:17 DP2 TP2] Prefill batch, #new-seq: 17, #new-token: 336, #cached-token: 12109, token usage: 0.01, #running-req: 45, #queue-req: 0, 
[2025-10-27 11:29:17 DP0 TP0] Prefill batch, #new-seq: 18, #new-token: 218, #cached-token: 12917, token usage: 0.01, #running-req: 44, #queue-req: 0, 
[2025-10-27 11:29:17 DP4 TP4] Prefill batch, #new-seq: 17, #new-token: 350, #cached-token: 12089, token usage: 0.01, #running-req: 45, #queue-req: 0, 
[2025-10-27 11:29:17 DP1 TP1] Prefill batch, #new-seq: 18, #new-token: 318, #cached-token: 12723, token usage: 0.01, #running-req: 45, #queue-req: 0, 
[2025-10-27 11:29:17 DP5 TP5] Prefill batch, #new-seq: 17, #new-token: 249, #cached-token: 12031, token usage: 0.01, #running-req: 45, #queue-req: 0, 
[2025-10-27 11:29:18 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 305, #cached-token: 5448, token usage: 0.01, #running-req: 62, #queue-req: 0, 
[2025-10-27 11:29:18 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 101, #cached-token: 6389, token usage: 0.01, #running-req: 62, #queue-req: 0, 
[2025-10-27 11:29:18 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 248, #cached-token: 5557, token usage: 0.01, #running-req: 62, #queue-req: 0, 
[2025-10-27 11:29:18 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 375, #cached-token: 5389, token usage: 0.01, #running-req: 62, #queue-req: 0, 
[2025-10-27 11:29:18 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 255, #cached-token: 5469, token usage: 0.01, #running-req: 62, #queue-req: 0, 
[2025-10-27 11:29:18 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 246, #cached-token: 5577, token usage: 0.01, #running-req: 62, #queue-req: 0, 
[2025-10-27 11:29:18 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 273, #cached-token: 4913, token usage: 0.01, #running-req: 62, #queue-req: 0, 
[2025-10-27 11:29:18 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 151, #cached-token: 5604, token usage: 0.01, #running-req: 63, #queue-req: 0, 
[2025-10-27 11:29:18 DP7 TP7] Prefill batch, #new-seq: 13, #new-token: 59, #cached-token: 9353, token usage: 0.01, #running-req: 69, #queue-req: 0, 
[2025-10-27 11:29:18 DP3 TP3] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8696, token usage: 0.01, #running-req: 70, #queue-req: 0, 
[2025-10-27 11:29:18 DP4 TP4] Prefill batch, #new-seq: 12, #new-token: 68, #cached-token: 8635, token usage: 0.01, #running-req: 70, #queue-req: 0, 
[2025-10-27 11:29:18 DP0 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8816, token usage: 0.01, #running-req: 70, #queue-req: 0, 
[2025-10-27 11:29:18 DP2 TP2] Prefill batch, #new-seq: 11, #new-token: 63, #cached-token: 7992, token usage: 0.01, #running-req: 71, #queue-req: 0, 
[2025-10-27 11:29:18 DP6 TP6] Prefill batch, #new-seq: 12, #new-token: 107, #cached-token: 8540, token usage: 0.01, #running-req: 70, #queue-req: 0, 
[2025-10-27 11:29:18 DP1 TP1] Prefill batch, #new-seq: 12, #new-token: 81, #cached-token: 8781, token usage: 0.01, #running-req: 71, #queue-req: 0, 
[2025-10-27 11:29:18 DP5 TP5] Prefill batch, #new-seq: 12, #new-token: 97, #cached-token: 8660, token usage: 0.01, #running-req: 70, #queue-req: 0, 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP0 TP0] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP7 TP7] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP5 TP5] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP4 TP4] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP1 TP1] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP3 TP3] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP6 TP6] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP2 TP2] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 50, #cached-token: 6499, token usage: 0.01, #running-req: 82, #queue-req: 0, 
[2025-10-27 11:29:18 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 113, #cached-token: 7163, token usage: 0.01, #running-req: 82, #queue-req: 0, 
[2025-10-27 11:29:18 DP1 TP1] Prefill batch, #new-seq: 9, #new-token: 111, #cached-token: 6448, token usage: 0.01, #running-req: 83, #queue-req: 0, 
[2025-10-27 11:29:18 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 199, #cached-token: 6360, token usage: 0.01, #running-req: 82, #queue-req: 0, 
[2025-10-27 11:29:18 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 237, #cached-token: 7109, token usage: 0.01, #running-req: 82, #queue-req: 0, 
[2025-10-27 11:29:18 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 139, #cached-token: 6469, token usage: 0.01, #running-req: 82, #queue-req: 0, 
[2025-10-27 11:29:18 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 50, #cached-token: 7232, token usage: 0.01, #running-req: 82, #queue-req: 0, 
[2025-10-27 11:29:18 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 109, #cached-token: 6419, token usage: 0.01, #running-req: 82, #queue-req: 0, 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP3 TP3] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP2 TP2] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP6 TP6] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP7 TP7] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP1 TP1] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP0 TP0] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP4 TP4] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP5 TP5] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP3 TP3] Prefill batch, #new-seq: 6, #new-token: 177, #cached-token: 4288, token usage: 0.01, #running-req: 92, #queue-req: 0, 
[2025-10-27 11:29:18 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 103, #cached-token: 4927, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[2025-10-27 11:29:18 DP2 TP2] Prefill batch, #new-seq: 6, #new-token: 132, #cached-token: 4298, token usage: 0.01, #running-req: 92, #queue-req: 0, 
[2025-10-27 11:29:18 DP6 TP6] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5011, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[2025-10-27 11:29:18 DP1 TP1] Prefill batch, #new-seq: 6, #new-token: 71, #cached-token: 4228, token usage: 0.01, #running-req: 92, #queue-req: 0, 
[2025-10-27 11:29:18 DP5 TP5] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5150, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[2025-10-27 11:29:18 DP4 TP4] Prefill batch, #new-seq: 6, #new-token: 54, #cached-token: 4323, token usage: 0.01, #running-req: 92, #queue-req: 0, 
[2025-10-27 11:29:18 DP0 TP0] Prefill batch, #new-seq: 6, #new-token: 98, #cached-token: 4289, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP2 TP2] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP3 TP3] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP6 TP6] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP4 TP4] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP5 TP5] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP0 TP0] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP1 TP1] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP7 TP7] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 229, #cached-token: 5002, token usage: 0.01, #running-req: 98, #queue-req: 0, 
[2025-10-27 11:29:18 DP3 TP3] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5096, token usage: 0.01, #running-req: 98, #queue-req: 0, 
[2025-10-27 11:29:18 DP5 TP5] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5145, token usage: 0.01, #running-req: 98, #queue-req: 0, 
[2025-10-27 11:29:18 DP2 TP2] Prefill batch, #new-seq: 7, #new-token: 73, #cached-token: 5048, token usage: 0.01, #running-req: 98, #queue-req: 0, 
[2025-10-27 11:29:18 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 156, #cached-token: 4912, token usage: 0.01, #running-req: 98, #queue-req: 0, 
[2025-10-27 11:29:18 DP6 TP6] Prefill batch, #new-seq: 7, #new-token: 64, #cached-token: 5164, token usage: 0.01, #running-req: 98, #queue-req: 0, 
[2025-10-27 11:29:18 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 256, #cached-token: 4869, token usage: 0.01, #running-req: 97, #queue-req: 0, 
[2025-10-27 11:29:18 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5104, token usage: 0.01, #running-req: 98, #queue-req: 0, 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP1 TP1] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP0 TP0] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP2 TP2] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP6 TP6] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP3 TP3] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP5 TP5] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP7 TP7] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP4 TP4] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP7 TP7] Prefill batch, #new-seq: 6, #new-token: 128, #cached-token: 4270, token usage: 0.01, #running-req: 105, #queue-req: 0, 
[2025-10-27 11:29:18 DP3 TP3] Prefill batch, #new-seq: 6, #new-token: 92, #cached-token: 4230, token usage: 0.01, #running-req: 105, #queue-req: 0, 
[2025-10-27 11:29:18 DP5 TP5] Prefill batch, #new-seq: 6, #new-token: 139, #cached-token: 4185, token usage: 0.01, #running-req: 105, #queue-req: 0, 
[2025-10-27 11:29:18 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 63, #cached-token: 5029, token usage: 0.01, #running-req: 105, #queue-req: 0, 
[2025-10-27 11:29:18 DP2 TP2] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5039, token usage: 0.01, #running-req: 105, #queue-req: 0, 
[2025-10-27 11:29:18 DP6 TP6] Prefill batch, #new-seq: 6, #new-token: 109, #cached-token: 4286, token usage: 0.01, #running-req: 105, #queue-req: 0, 
[2025-10-27 11:29:18 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 287, #cached-token: 4777, token usage: 0.01, #running-req: 105, #queue-req: 0, 
[2025-10-27 11:29:18 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 107, #cached-token: 4907, token usage: 0.01, #running-req: 104, #queue-req: 0, 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP7 TP7] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP2 TP2] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP3 TP3] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP6 TP6] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP4 TP4] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP5 TP5] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP0 TP0] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP1 TP1] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:18 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 277, #cached-token: 4871, token usage: 0.01, #running-req: 111, #queue-req: 0, 
[2025-10-27 11:29:18 DP3 TP3] Prefill batch, #new-seq: 7, #new-token: 64, #cached-token: 5035, token usage: 0.01, #running-req: 111, #queue-req: 0, 
[2025-10-27 11:29:18 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 94, #cached-token: 5089, token usage: 0.01, #running-req: 112, #queue-req: 0, 
[2025-10-27 11:29:18 DP5 TP5] Prefill batch, #new-seq: 7, #new-token: 97, #cached-token: 5033, token usage: 0.01, #running-req: 111, #queue-req: 0, 
[2025-10-27 11:29:18 DP2 TP2] Prefill batch, #new-seq: 6, #new-token: 88, #cached-token: 4219, token usage: 0.01, #running-req: 112, #queue-req: 0, 
[2025-10-27 11:29:18 DP6 TP6] Prefill batch, #new-seq: 7, #new-token: 48, #cached-token: 5000, token usage: 0.01, #running-req: 111, #queue-req: 0, 
[2025-10-27 11:29:18 DP4 TP4] Prefill batch, #new-seq: 6, #new-token: 174, #cached-token: 4217, token usage: 0.01, #running-req: 112, #queue-req: 0, 
[2025-10-27 11:29:18 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 242, #cached-token: 4950, token usage: 0.01, #running-req: 111, #queue-req: 0, 
[2025-10-27 11:29:18 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 207, #cached-token: 5046, token usage: 0.01, #running-req: 119, #queue-req: 0, 
[2025-10-27 11:29:18 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 216, #cached-token: 5648, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-27 11:29:18 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 156, #cached-token: 5609, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-27 11:29:18 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 173, #cached-token: 5620, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-27 11:29:18 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 127, #cached-token: 4976, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-27 11:29:18 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 64, #cached-token: 5758, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-27 11:29:18 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 192, #cached-token: 5676, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-27 11:29:18 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 187, #cached-token: 5665, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-27 11:29:19 DP3 TP3] Prefill batch, #new-seq: 9, #new-token: 178, #cached-token: 6429, token usage: 0.01, #running-req: 126, #queue-req: 0, 
[2025-10-27 11:29:19 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 481, #cached-token: 6402, token usage: 0.01, #running-req: 126, #queue-req: 0, 
[2025-10-27 11:29:19 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 320, #cached-token: 6336, token usage: 0.01, #running-req: 126, #queue-req: 0, 
[2025-10-27 11:29:19 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 112, #cached-token: 6359, token usage: 0.01, #running-req: 126, #queue-req: 0, 
[2025-10-27 11:29:19 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 261, #cached-token: 6315, token usage: 0.01, #running-req: 126, #queue-req: 0, 
[2025-10-27 11:29:19 DP0 TP0] Prefill batch, #new-seq: 10, #new-token: 299, #cached-token: 7077, token usage: 0.01, #running-req: 125, #queue-req: 0, 
[2025-10-27 11:29:19 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 227, #cached-token: 6373, token usage: 0.01, #running-req: 126, #queue-req: 0, 
[2025-10-27 11:29:19 DP1 TP1] Prefill batch, #new-seq: 10, #new-token: 293, #cached-token: 6966, token usage: 0.01, #running-req: 126, #queue-req: 0, 
[2025-10-27 11:29:19 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 120, #cached-token: 5015, token usage: 0.01, #running-req: 135, #queue-req: 0, 
[2025-10-27 11:29:19 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 79, #cached-token: 5709, token usage: 0.01, #running-req: 135, #queue-req: 0, 
[2025-10-27 11:29:19 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 169, #cached-token: 5847, token usage: 0.01, #running-req: 135, #queue-req: 0, 
[2025-10-27 11:29:19 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 186, #cached-token: 5691, token usage: 0.01, #running-req: 135, #queue-req: 0, 
[2025-10-27 11:29:19 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 55, #cached-token: 5644, token usage: 0.01, #running-req: 135, #queue-req: 0, 
[2025-10-27 11:29:19 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5781, token usage: 0.01, #running-req: 135, #queue-req: 0, 
[2025-10-27 11:29:19 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 58, #cached-token: 5000, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-27 11:29:19 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 68, #cached-token: 5690, token usage: 0.01, #running-req: 135, #queue-req: 0, 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP2 TP2] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP3 TP3] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP7 TP7] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP4 TP4] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP5 TP5] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP1 TP1] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP6 TP6] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP0 TP0] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP3 TP3] Prefill batch, #new-seq: 11, #new-token: 343, #cached-token: 7597, token usage: 0.01, #running-req: 143, #queue-req: 0, 
[2025-10-27 11:29:19 DP7 TP7] Prefill batch, #new-seq: 11, #new-token: 253, #cached-token: 7745, token usage: 0.02, #running-req: 143, #queue-req: 0, 
[2025-10-27 11:29:19 DP5 TP5] Prefill batch, #new-seq: 11, #new-token: 313, #cached-token: 7664, token usage: 0.01, #running-req: 143, #queue-req: 0, 
[2025-10-27 11:29:19 DP1 TP1] Prefill batch, #new-seq: 11, #new-token: 136, #cached-token: 7756, token usage: 0.01, #running-req: 143, #queue-req: 0, 
[2025-10-27 11:29:19 DP6 TP6] Prefill batch, #new-seq: 11, #new-token: 255, #cached-token: 7739, token usage: 0.01, #running-req: 143, #queue-req: 0, 
[2025-10-27 11:29:19 DP4 TP4] Prefill batch, #new-seq: 11, #new-token: 391, #cached-token: 7640, token usage: 0.01, #running-req: 143, #queue-req: 0, 
[2025-10-27 11:29:19 DP2 TP2] Prefill batch, #new-seq: 11, #new-token: 221, #cached-token: 8045, token usage: 0.01, #running-req: 143, #queue-req: 0, 
[2025-10-27 11:29:19 DP0 TP0] Prefill batch, #new-seq: 11, #new-token: 184, #cached-token: 8009, token usage: 0.01, #running-req: 142, #queue-req: 0, 
[2025-10-27 11:29:19 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 63, #cached-token: 5797, token usage: 0.02, #running-req: 154, #queue-req: 0, 
[2025-10-27 11:29:19 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 44, #cached-token: 5723, token usage: 0.02, #running-req: 154, #queue-req: 0, 
[2025-10-27 11:29:19 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 92, #cached-token: 5670, token usage: 0.02, #running-req: 154, #queue-req: 0, 
[2025-10-27 11:29:19 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 158, #cached-token: 5806, token usage: 0.02, #running-req: 153, #queue-req: 0, 
[2025-10-27 11:29:19 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 86, #cached-token: 5770, token usage: 0.01, #running-req: 154, #queue-req: 0, 
[2025-10-27 11:29:19 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5938, token usage: 0.02, #running-req: 154, #queue-req: 0, 
[2025-10-27 11:29:19 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 164, #cached-token: 5649, token usage: 0.02, #running-req: 154, #queue-req: 0, 
[2025-10-27 11:29:19 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5913, token usage: 0.02, #running-req: 154, #queue-req: 0, 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP4 TP4] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP2 TP2] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP3 TP3] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP6 TP6] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP0 TP0] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP5 TP5] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP7 TP7] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP1 TP1] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP7 TP7] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2189, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-27 11:29:19 DP3 TP3] Prefill batch, #new-seq: 3, #new-token: 58, #cached-token: 2201, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-27 11:29:19 DP6 TP6] Prefill batch, #new-seq: 3, #new-token: 50, #cached-token: 2099, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-27 11:29:19 DP1 TP1] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2171, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-27 11:29:19 DP2 TP2] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2197, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-27 11:29:19 DP5 TP5] Prefill batch, #new-seq: 3, #new-token: 135, #cached-token: 2042, token usage: 0.01, #running-req: 162, #queue-req: 0, 
[2025-10-27 11:29:19 DP4 TP4] Prefill batch, #new-seq: 3, #new-token: 117, #cached-token: 2049, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-27 11:29:19 DP0 TP0] Prefill batch, #new-seq: 3, #new-token: 67, #cached-token: 2137, token usage: 0.02, #running-req: 161, #queue-req: 0, 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP3 TP3] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP2 TP2] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP1 TP1] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP0 TP0] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP4 TP4] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP7 TP7] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP5 TP5] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:19 DP6 TP6] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:20 DP1 TP1] Decode batch, #running-req: 165, #token: 11857, token usage: 0.02, cuda graph: True, gen throughput (token/s): 57.11, #queue-req: 0, 
[2025-10-27 11:29:20 DP4 TP4] Decode batch, #running-req: 165, #token: 12278, token usage: 0.02, cuda graph: True, gen throughput (token/s): 69.06, #queue-req: 0, 
[2025-10-27 11:29:20 DP2 TP2] Decode batch, #running-req: 165, #token: 12220, token usage: 0.02, cuda graph: True, gen throughput (token/s): 74.38, #queue-req: 0, 
[2025-10-27 11:29:22] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:23 DP0 TP0] Decode batch, #running-req: 163, #token: 15825, token usage: 0.02, cuda graph: True, gen throughput (token/s): 178.87, #queue-req: 0, 
[2025-10-27 11:29:23 DP6 TP6] Decode batch, #running-req: 165, #token: 15493, token usage: 0.02, cuda graph: True, gen throughput (token/s): 247.47, #queue-req: 0, 
[2025-10-27 11:29:23] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:23] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:23 DP5 TP5] Decode batch, #running-req: 165, #token: 15838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 219.73, #queue-req: 0, 
[2025-10-27 11:29:23 DP7 TP7] Decode batch, #running-req: 164, #token: 17041, token usage: 0.03, cuda graph: True, gen throughput (token/s): 233.99, #queue-req: 0, 
[2025-10-27 11:29:24] INFO:     127.0.0.1:59788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24 DP3 TP3] Decode batch, #running-req: 165, #token: 16761, token usage: 0.02, cuda graph: True, gen throughput (token/s): 230.20, #queue-req: 0, 
[2025-10-27 11:29:24] INFO:     127.0.0.1:49254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:48518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:59882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:52726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:58580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:50012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:59312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:50056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:53982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:49910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:51908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:53154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:49538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:51258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:50082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:53998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:49174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:48828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:52988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:53256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:48770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:48910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:24] INFO:     127.0.0.1:53168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:56996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:51602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:53382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:56844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:49836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:53530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:50764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:58914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:51538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:52042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:52796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:59818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:53088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:58736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:49006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:54318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:56356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:55068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:51706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:55796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:56914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:49272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:50480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:54192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:56268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25 DP1 TP1] Decode batch, #running-req: 158, #token: 17290, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1374.70, #queue-req: 0, 
[2025-10-27 11:29:25] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:48870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:59128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:55854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:51232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:48484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:50488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:54296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25 DP4 TP4] Decode batch, #running-req: 160, #token: 18225, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1374.22, #queue-req: 0, 
[2025-10-27 11:29:25] INFO:     127.0.0.1:54972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:58264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:55340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:58568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:55846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:49526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25 DP2 TP2] Decode batch, #running-req: 151, #token: 17432, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1366.77, #queue-req: 0, 
[2025-10-27 11:29:25] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:59152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:52588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:53284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:57210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:49498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:52714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:53014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:52372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:54546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:51592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:54064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:58720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:50774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:50944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:53460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:25] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:58172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:49924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:54576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:57574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:58008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:51436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:54166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:56126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:50202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:50360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:49752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:54672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:49308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:57510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:56168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:58840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:49668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:58228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:49038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:51882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:59276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:52722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:54430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:48666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:54306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:54626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:55018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:58260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:50874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:48496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:49464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:54742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:54618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:50310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:50248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:53860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:51084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:54158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:51638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:55988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:56110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:50782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:50678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:55472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:51328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:49382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:51606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:55310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:50174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:55492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:56608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:60058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:60050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:54842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:50694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:58972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:50148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:51756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:54894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:58698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:53352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:56434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:58656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:54384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:56380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:49740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:50114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:26] INFO:     127.0.0.1:52926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:54020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:55584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:50730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:59596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:50028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:50606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:59474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:48616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:54622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:59356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:59694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:59900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:48932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:55180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:54564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:55222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:51070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:59782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:51632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:54338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:55100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:57696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:60016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:48954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:51392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:55184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:58052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:58292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:52292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:50826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:50308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:52626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:58244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:55462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:51354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:51712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:57160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:48966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:60028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:51020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:50564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:51042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:58684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:59498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:58950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:51782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:54070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:50646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:57934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:48672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:51380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:55344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:49320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:59262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:55086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:57490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:57170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:59402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:51644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:58180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27 DP0 TP0] Decode batch, #running-req: 129, #token: 17925, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1285.85, #queue-req: 0, 
[2025-10-27 11:29:27] INFO:     127.0.0.1:52822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27 DP6 TP6] Decode batch, #running-req: 127, #token: 16619, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1309.67, #queue-req: 0, 
[2025-10-27 11:29:27] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:55618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:50286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:50342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:50406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:52128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:52158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:53932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:59924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:52012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:54986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:56610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:52406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:58644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:52772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:54116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:27] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:51180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:51618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:57186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:51552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:55266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:57712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:53416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:57062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:54926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:57836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:58338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:51460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:54766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:49628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:52258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:49118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:52960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:52232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:53770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:49812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:52760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:57658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:56086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:58942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:57304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:49536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28 DP5 TP5] Decode batch, #running-req: 114, #token: 16101, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1265.09, #queue-req: 0, 
[2025-10-27 11:29:28] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:52434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:49094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:52864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:55342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:51780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:57010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:51792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:54386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:48680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:53336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:56416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:58966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:49666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:55934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:54872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:56954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:54082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:52940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:58322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:51746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:57558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28 DP7 TP7] Decode batch, #running-req: 119, #token: 17227, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1315.24, #queue-req: 0, 
[2025-10-27 11:29:28] INFO:     127.0.0.1:53004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:50840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:49846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:54816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:60140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:51934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:56302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:49822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:50054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:50816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:52482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:53626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:54440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:54924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:52838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:54100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:51064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:58630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:51558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:52138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:53294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:51314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:54352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28 DP3 TP3] Decode batch, #running-req: 111, #token: 16098, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1253.38, #queue-req: 0, 
[2025-10-27 11:29:28] INFO:     127.0.0.1:52602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:56670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:48778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:54424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:49470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:53426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:56262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:51494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:52650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:56076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:49154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:58216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:56362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:49426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:53616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:53442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:56950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:48840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:50898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:48774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:55144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:48764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:55834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:50526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:50166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:57284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:54642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:50598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:53504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:56282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:59830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:51470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:53182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:53516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:28] INFO:     127.0.0.1:58544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:53434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:54702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:48636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:50272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:60104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:49208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:50348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:53906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:54514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:48924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:56398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:50126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:49180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:59856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:52306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:60106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:48846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:50212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:59106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:59710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:50546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:50966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:52826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:59942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:49434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:58892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:49066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:51918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:49146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:53402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:48554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:52318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:57462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:52116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:49406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:51742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:52572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:53756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:59974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:51702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:56630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:50850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:51680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:57198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:52092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:53634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:54450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:59324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:58850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:48792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:48940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:52076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:56936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:48572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:51844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:50036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:51658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:60094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:50370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:56968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:59196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:52638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29 DP1 TP1] Decode batch, #running-req: 85, #token: 13128, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1168.56, #queue-req: 0, 
[2025-10-27 11:29:29] INFO:     127.0.0.1:48714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:53690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:56266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:49450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:56114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:57212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:59904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:49040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:56320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:49548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:53414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:56576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:51912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:54404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:54774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29 DP4 TP4] Decode batch, #running-req: 95, #token: 15160, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1205.81, #queue-req: 0, 
[2025-10-27 11:29:29] INFO:     127.0.0.1:56004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:56532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:51166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:59228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:56216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:58238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:57116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:49680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29 DP2 TP2] Decode batch, #running-req: 86, #token: 13827, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1186.21, #queue-req: 0, 
[2025-10-27 11:29:29] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:50476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:50708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:51190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:55094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:51566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:29] INFO:     127.0.0.1:59986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:48990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:51576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:49032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:50070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:54764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:49586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:49784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:59248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:51276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:53072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:48532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:57970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:54060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:59682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:55602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:49602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:53456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:57316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:59054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:55966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:53558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:51974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:54878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:53582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:59744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:48542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:51520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:51734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:53962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:53656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:58946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:48816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:56760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:59828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:59880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:49552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:49940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:55674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:57888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:50890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:59576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:49532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:50302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:51526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:53970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:48662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:57334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:51132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:51342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:53482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:49198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:56078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:57846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:58652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:51652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:58014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:49578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:51804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:52684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:54180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:55764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:49238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:51590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:49260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:55182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:55242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:59858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:30] INFO:     127.0.0.1:59914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:52082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:59322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:48584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:52412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:54830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:52106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:55010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:49164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:49990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:51548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:56584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:58766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:48800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:51570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:59486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:48602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:52002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:54234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:49956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:55212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:55816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:48896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:54542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:54606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:51634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:55734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:55974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:58802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:51996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:55526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:56622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:49322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:51820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:51446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:58898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:48652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:52670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:55746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:55872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:51854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:56768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:59366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:59814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:49376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:55006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:52134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:56512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:49350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31 DP6 TP6] Decode batch, #running-req: 53, #token: 9829, token usage: 0.01, cuda graph: True, gen throughput (token/s): 939.04, #queue-req: 0, 
[2025-10-27 11:29:31 DP0 TP0] Decode batch, #running-req: 60, #token: 11416, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1020.82, #queue-req: 0, 
[2025-10-27 11:29:31] INFO:     127.0.0.1:54326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:54610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:56660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:49974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:55868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:56058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:52142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:58408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:49996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:59832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:49388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:56816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:56886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:51694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:54418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:55852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:49110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:52136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:55256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:58450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:59570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:56012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:59020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:52890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31 DP5 TP5] Decode batch, #running-req: 46, #token: 8564, token usage: 0.01, cuda graph: True, gen throughput (token/s): 917.74, #queue-req: 0, 
[2025-10-27 11:29:31] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:52344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:51588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:54132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:52528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:55228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:59426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:51192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:53672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:54218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:54822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:57494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:49122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:50418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:52282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:55824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:31] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32 DP7 TP7] Decode batch, #running-req: 53, #token: 9776, token usage: 0.01, cuda graph: True, gen throughput (token/s): 950.70, #queue-req: 0, 
[2025-10-27 11:29:32] INFO:     127.0.0.1:59176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:48696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:54580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:58158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:50632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:59030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:49898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:58928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:54928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:49372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:53840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:48574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:57350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:59666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:49988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:57238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:59650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32 DP3 TP3] Decode batch, #running-req: 55, #token: 10591, token usage: 0.02, cuda graph: True, gen throughput (token/s): 927.88, #queue-req: 0, 
[2025-10-27 11:29:32] INFO:     127.0.0.1:57756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:48754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:49312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:58458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:58564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:52516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:54028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:52454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:58398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:48976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:54572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:48624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:53876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:56378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:57770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:59182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:52046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:53262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:54856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:49618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:50800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:52034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:54530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:60086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:50978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:50740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:52360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:56190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:53348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:54660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:50462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:52448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:58750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:56854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:59448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:49310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:54592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:58034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:54358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:56466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:49168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:52336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:53322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:52612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:56172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:48738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:56394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:48568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:49280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:54088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:49078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:53598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:57448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:59934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:59526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:57052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:50296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:53122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:57278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32 DP1 TP1] Decode batch, #running-req: 30, #token: 6692, token usage: 0.01, cuda graph: True, gen throughput (token/s): 695.86, #queue-req: 0, 
[2025-10-27 11:29:32] INFO:     127.0.0.1:57570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:53732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:51106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:53446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:32] INFO:     127.0.0.1:55638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33 DP4 TP4] Decode batch, #running-req: 36, #token: 7357, token usage: 0.01, cuda graph: True, gen throughput (token/s): 838.32, #queue-req: 0, 
[2025-10-27 11:29:33] INFO:     127.0.0.1:57262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:50428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:53314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:50098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:52392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:50432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:58984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33 DP2 TP2] Decode batch, #running-req: 39, #token: 8505, token usage: 0.01, cuda graph: True, gen throughput (token/s): 802.90, #queue-req: 0, 
[2025-10-27 11:29:33] INFO:     127.0.0.1:48564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:49694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:58028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:48504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:49008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:53742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:59458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:54056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:49550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:53106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:57860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:51514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:56872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:51896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:57176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:58006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:50914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:51624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:52196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:56050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:58646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:58792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:53208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:54478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:59300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:53844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:53946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:55002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:48646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:51056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:56232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:56024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:51992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:50434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:56752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:55046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:49358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:53988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:56288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:59560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:59206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:53590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:56646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:50612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:57806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:59048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:54888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:59332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:60038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:55092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:52462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:56980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:59472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:50058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:60110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:33] INFO:     127.0.0.1:57158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:59964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:49166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:54498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:55200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:51728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:58734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:50238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:51960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:53100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:50542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34 DP6 TP6] Decode batch, #running-req: 21, #token: 4828, token usage: 0.01, cuda graph: True, gen throughput (token/s): 503.77, #queue-req: 0, 
[2025-10-27 11:29:34 DP0 TP0] Decode batch, #running-req: 28, #token: 7186, token usage: 0.01, cuda graph: True, gen throughput (token/s): 633.37, #queue-req: 0, 
[2025-10-27 11:29:34] INFO:     127.0.0.1:49520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:51408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:60136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:48916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:53676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:57916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:59622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:59076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:52428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:59948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:50754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:54548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:59190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34 DP5 TP5] Decode batch, #running-req: 13, #token: 3608, token usage: 0.01, cuda graph: True, gen throughput (token/s): 392.63, #queue-req: 0, 
[2025-10-27 11:29:34] INFO:     127.0.0.1:52256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:49248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:57020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:58186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:52244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:49338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:49044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:54462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:58806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34 DP7 TP7] Decode batch, #running-req: 24, #token: 6141, token usage: 0.01, cuda graph: True, gen throughput (token/s): 534.25, #queue-req: 0, 
[2025-10-27 11:29:34] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:50498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:55478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:58468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:59506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34 DP3 TP3] Decode batch, #running-req: 18, #token: 4480, token usage: 0.01, cuda graph: True, gen throughput (token/s): 557.81, #queue-req: 0, 
[2025-10-27 11:29:34] INFO:     127.0.0.1:51216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:50660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:52028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:49872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:59604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:48726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:53240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:58138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:50868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:55000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:54014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:55454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:58976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:51498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:51866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:53508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:54780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:56966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:52674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:34] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:51288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:58828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:49056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:49920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:50018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:50230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:58278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35 DP1 TP1] Decode batch, #running-req: 11, #token: 3214, token usage: 0.00, cuda graph: True, gen throughput (token/s): 339.38, #queue-req: 0, 
[2025-10-27 11:29:35] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:55078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35 DP4 TP4] Decode batch, #running-req: 11, #token: 2909, token usage: 0.00, cuda graph: True, gen throughput (token/s): 333.75, #queue-req: 0, 
[2025-10-27 11:29:35] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:50400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:52468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:54782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35 DP2 TP2] Decode batch, #running-req: 14, #token: 4000, token usage: 0.01, cuda graph: True, gen throughput (token/s): 443.19, #queue-req: 0, 
[2025-10-27 11:29:35] INFO:     127.0.0.1:55418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:54770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:52300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:50386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:48598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:53466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:59290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:52170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:54124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:56100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:58672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:60000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:48706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:49390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:49482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:53610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:54246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:49762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:59418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:35] INFO:     127.0.0.1:50570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:51304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:54946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36 DP6 TP6] Decode batch, #running-req: 6, #token: 2311, token usage: 0.00, cuda graph: True, gen throughput (token/s): 196.63, #queue-req: 0, 
[2025-10-27 11:29:36] INFO:     127.0.0.1:56246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36 DP0 TP0] Decode batch, #running-req: 12, #token: 4081, token usage: 0.01, cuda graph: True, gen throughput (token/s): 385.40, #queue-req: 0, 
[2025-10-27 11:29:36] INFO:     127.0.0.1:58064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:49644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:57080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:49584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36 DP5 TP5] Decode batch, #running-req: 4, #token: 1755, token usage: 0.00, cuda graph: True, gen throughput (token/s): 141.64, #queue-req: 0, 
[2025-10-27 11:29:36] INFO:     127.0.0.1:54494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36 DP7 TP7] Decode batch, #running-req: 11, #token: 3892, token usage: 0.01, cuda graph: True, gen throughput (token/s): 317.87, #queue-req: 0, 
[2025-10-27 11:29:36] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:57218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36 DP3 TP3] Decode batch, #running-req: 7, #token: 2404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 234.92, #queue-req: 0, 
[2025-10-27 11:29:36] INFO:     127.0.0.1:58762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:49914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:54208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:48610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:56548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:51340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:59638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:50930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:53302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:58318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:52270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:52742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:51032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:36] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37 DP1 TP1] Decode batch, #running-req: 5, #token: 2193, token usage: 0.00, cuda graph: True, gen throughput (token/s): 160.25, #queue-req: 0, 
[2025-10-27 11:29:37] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37 DP4 TP4] Decode batch, #running-req: 3, #token: 1534, token usage: 0.00, cuda graph: True, gen throughput (token/s): 105.87, #queue-req: 0, 
[2025-10-27 11:29:37 DP2 TP2] Decode batch, #running-req: 6, #token: 2400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 203.94, #queue-req: 0, 
[2025-10-27 11:29:37] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37] INFO:     127.0.0.1:50042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37] INFO:     127.0.0.1:52102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37] INFO:     127.0.0.1:49656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37] INFO:     127.0.0.1:57972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37] INFO:     127.0.0.1:58102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37] INFO:     127.0.0.1:57904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37] INFO:     127.0.0.1:58722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37] INFO:     127.0.0.1:54654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37] INFO:     127.0.0.1:59038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:37 DP0 TP0] Decode batch, #running-req: 4, #token: 1986, token usage: 0.00, cuda graph: True, gen throughput (token/s): 206.37, #queue-req: 0, 
[2025-10-27 11:29:37 DP6 TP6] Decode batch, #running-req: 2, #token: 1302, token usage: 0.00, cuda graph: True, gen throughput (token/s): 78.74, #queue-req: 0, 
[2025-10-27 11:29:37] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38 DP5 TP5] Decode batch, #running-req: 1, #token: 943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.70, #queue-req: 0, 
[2025-10-27 11:29:38] INFO:     127.0.0.1:55644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38] INFO:     127.0.0.1:57042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38 DP7 TP7] Decode batch, #running-req: 2, #token: 1358, token usage: 0.00, cuda graph: True, gen throughput (token/s): 151.84, #queue-req: 0, 
[2025-10-27 11:29:38] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38 DP3 TP3] Decode batch, #running-req: 2, #token: 1334, token usage: 0.00, cuda graph: True, gen throughput (token/s): 95.47, #queue-req: 0, 
[2025-10-27 11:29:38] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38] INFO:     127.0.0.1:48560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38] INFO:     127.0.0.1:51986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38] INFO:     127.0.0.1:58804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38] INFO:     127.0.0.1:49980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38 DP1 TP1] Decode batch, #running-req: 2, #token: 983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 74.29, #queue-req: 0, 
[2025-10-27 11:29:38 DP4 TP4] Decode batch, #running-req: 1, #token: 1003, token usage: 0.00, cuda graph: True, gen throughput (token/s): 47.86, #queue-req: 0, 
[2025-10-27 11:29:38 DP2 TP2] Decode batch, #running-req: 4, #token: 1952, token usage: 0.00, cuda graph: True, gen throughput (token/s): 146.98, #queue-req: 0, 
[2025-10-27 11:29:38] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38] INFO:     127.0.0.1:51150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38] INFO:     127.0.0.1:58364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38] INFO:     127.0.0.1:48850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38] INFO:     127.0.0.1:57108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:38] INFO:     127.0.0.1:55536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:39] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:39 DP0 TP0] Decode batch, #running-req: 1, #token: 1020, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.42, #queue-req: 0, 
[2025-10-27 11:29:39 DP6 TP6] Decode batch, #running-req: 1, #token: 1021, token usage: 0.00, cuda graph: True, gen throughput (token/s): 47.22, #queue-req: 0, 
[2025-10-27 11:29:39] INFO:     127.0.0.1:55190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:39 DP7 TP7] Decode batch, #running-req: 1, #token: 1085, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-10-27 11:29:39 DP3 TP3] Decode batch, #running-req: 1, #token: 1047, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.28, #queue-req: 0, 
[2025-10-27 11:29:39] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:39] INFO:     127.0.0.1:57370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:39 DP2 TP2] Decode batch, #running-req: 1, #token: 1014, token usage: 0.00, cuda graph: True, gen throughput (token/s): 65.89, #queue-req: 0, 
[2025-10-27 11:29:39] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:40 DP6 TP6] Decode batch, #running-req: 1, #token: 1061, token usage: 0.00, cuda graph: True, gen throughput (token/s): 37.84, #queue-req: 0, 
[2025-10-27 11:29:40 DP3 TP3] Decode batch, #running-req: 1, #token: 1087, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.18, #queue-req: 0, 
[2025-10-27 11:29:41 DP6 TP6] Decode batch, #running-req: 1, #token: 1101, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.41, #queue-req: 0, 
[2025-10-27 11:29:41 DP3 TP3] Decode batch, #running-req: 1, #token: 1127, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.45, #queue-req: 0, 
[2025-10-27 11:29:41] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:42] INFO:     127.0.0.1:52532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:54] INFO:     127.0.0.1:53530 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-27 11:29:54 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:55] INFO:     127.0.0.1:53532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:29:55 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 27, #cached-token: 670, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP0 TP0] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP3 TP3] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP4 TP4] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP5 TP5] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP6 TP6] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP7 TP7] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP2 TP2] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP1 TP1] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 51, #cached-token: 1401, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:55 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 50, #cached-token: 1395, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:55 DP2 TP2] Prefill batch, #new-seq: 2, #new-token: 65, #cached-token: 1395, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:55 DP0 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1494, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:55 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 59, #cached-token: 1379, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:55 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 68, #cached-token: 1447, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:55 DP3 TP3] Prefill batch, #new-seq: 2, #new-token: 34, #cached-token: 1400, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 11:29:55 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 39, #cached-token: 3615, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP1 TP1] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP0 TP0] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP5 TP5] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP3 TP3] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP6 TP6] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP4 TP4] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP7 TP7] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP2 TP2] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 229, #cached-token: 3399, token usage: 0.00, #running-req: 6, #queue-req: 0, 
[2025-10-27 11:29:55 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 320, #cached-token: 5495, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:29:55 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 162, #cached-token: 5536, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:29:55 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 361, #cached-token: 5492, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:29:55 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 453, #cached-token: 5463, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:29:55 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 158, #cached-token: 5566, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:29:55 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 237, #cached-token: 6268, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:29:55 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 129, #cached-token: 5616, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-27 11:29:55 DP4 TP4] Prefill batch, #new-seq: 1, #new-token: 90, #cached-token: 669, token usage: 0.00, #running-req: 10, #queue-req: 0, 
[2025-10-27 11:29:55 DP3 TP3] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 746, token usage: 0.00, #running-req: 10, #queue-req: 0, 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP4 TP4] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP3 TP3] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP7 TP7] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP6 TP6] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP5 TP5] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP1 TP1] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP0 TP0] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP2 TP2] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:55 DP1 TP1] Prefill batch, #new-seq: 9, #new-token: 442, #cached-token: 6110, token usage: 0.00, #running-req: 11, #queue-req: 0, 
[2025-10-27 11:29:55 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 453, #cached-token: 6209, token usage: 0.00, #running-req: 10, #queue-req: 0, 
[2025-10-27 11:29:55 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 172, #cached-token: 5694, token usage: 0.00, #running-req: 11, #queue-req: 0, 
[2025-10-27 11:29:55 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 404, #cached-token: 6203, token usage: 0.00, #running-req: 10, #queue-req: 0, 
[2025-10-27 11:29:55 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 381, #cached-token: 6173, token usage: 0.00, #running-req: 11, #queue-req: 0, 
[2025-10-27 11:29:55 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 541, #cached-token: 6136, token usage: 0.00, #running-req: 10, #queue-req: 0, 
[2025-10-27 11:29:55 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 362, #cached-token: 6235, token usage: 0.00, #running-req: 10, #queue-req: 0, 
[2025-10-27 11:29:55 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 294, #cached-token: 5660, token usage: 0.00, #running-req: 11, #queue-req: 0, 
[2025-10-27 11:29:55 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 231, #cached-token: 4833, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:29:55 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 73, #cached-token: 5688, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-27 11:29:55 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 42, #cached-token: 5040, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-27 11:29:55 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 92, #cached-token: 5799, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-27 11:29:55 DP2 TP2] Prefill batch, #new-seq: 7, #new-token: 383, #cached-token: 4846, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-27 11:29:55 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 106, #cached-token: 5729, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-27 11:29:55 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 221, #cached-token: 5539, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-27 11:29:55 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 109, #cached-token: 5771, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-27 11:29:56 DP1 TP1] Prefill batch, #new-seq: 22, #new-token: 624, #cached-token: 15314, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-27 11:29:56 DP5 TP5] Prefill batch, #new-seq: 22, #new-token: 589, #cached-token: 15419, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-27 11:29:56 DP4 TP4] Prefill batch, #new-seq: 22, #new-token: 302, #cached-token: 15631, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-27 11:29:56 DP0 TP0] Prefill batch, #new-seq: 22, #new-token: 307, #cached-token: 15607, token usage: 0.00, #running-req: 26, #queue-req: 0, 
[2025-10-27 11:29:56 DP7 TP7] Prefill batch, #new-seq: 21, #new-token: 425, #cached-token: 14823, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-27 11:29:56 DP3 TP3] Prefill batch, #new-seq: 22, #new-token: 466, #cached-token: 15666, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-27 11:29:56 DP2 TP2] Prefill batch, #new-seq: 22, #new-token: 440, #cached-token: 15509, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-27 11:29:56 DP6 TP6] Prefill batch, #new-seq: 22, #new-token: 413, #cached-token: 15788, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-27 11:29:56 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 97, #cached-token: 5704, token usage: 0.01, #running-req: 49, #queue-req: 0, 
[2025-10-27 11:29:56 DP1 TP1] Prefill batch, #new-seq: 9, #new-token: 98, #cached-token: 6497, token usage: 0.01, #running-req: 49, #queue-req: 0, 
[2025-10-27 11:29:56 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 106, #cached-token: 6511, token usage: 0.01, #running-req: 49, #queue-req: 0, 
[2025-10-27 11:29:56 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 53, #cached-token: 6635, token usage: 0.01, #running-req: 48, #queue-req: 0, 
[2025-10-27 11:29:56 DP3 TP3] Prefill batch, #new-seq: 9, #new-token: 331, #cached-token: 6325, token usage: 0.01, #running-req: 49, #queue-req: 0, 
[2025-10-27 11:29:56 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 59, #cached-token: 6548, token usage: 0.01, #running-req: 48, #queue-req: 0, 
[2025-10-27 11:29:56 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 205, #cached-token: 5555, token usage: 0.01, #running-req: 49, #queue-req: 0, 
[2025-10-27 11:29:56 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 185, #cached-token: 6408, token usage: 0.01, #running-req: 49, #queue-req: 0, 
[2025-10-27 11:29:56 DP4 TP4] Prefill batch, #new-seq: 14, #new-token: 413, #cached-token: 9698, token usage: 0.01, #running-req: 58, #queue-req: 0, 
[2025-10-27 11:29:56 DP3 TP3] Prefill batch, #new-seq: 14, #new-token: 461, #cached-token: 9680, token usage: 0.01, #running-req: 58, #queue-req: 0, 
[2025-10-27 11:29:56 DP7 TP7] Prefill batch, #new-seq: 14, #new-token: 270, #cached-token: 9769, token usage: 0.01, #running-req: 57, #queue-req: 0, 
[2025-10-27 11:29:56 DP1 TP1] Prefill batch, #new-seq: 15, #new-token: 251, #cached-token: 10577, token usage: 0.01, #running-req: 58, #queue-req: 0, 
[2025-10-27 11:29:56 DP5 TP5] Prefill batch, #new-seq: 15, #new-token: 528, #cached-token: 10369, token usage: 0.01, #running-req: 57, #queue-req: 0, 
[2025-10-27 11:29:56 DP6 TP6] Prefill batch, #new-seq: 15, #new-token: 369, #cached-token: 10497, token usage: 0.01, #running-req: 57, #queue-req: 0, 
[2025-10-27 11:29:56 DP0 TP0] Prefill batch, #new-seq: 15, #new-token: 381, #cached-token: 10650, token usage: 0.01, #running-req: 57, #queue-req: 0, 
[2025-10-27 11:29:56 DP2 TP2] Prefill batch, #new-seq: 15, #new-token: 361, #cached-token: 10615, token usage: 0.01, #running-req: 58, #queue-req: 0, 
[2025-10-27 11:29:56 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 65, #cached-token: 6522, token usage: 0.01, #running-req: 72, #queue-req: 0, 
[2025-10-27 11:29:56 DP1 TP1] Prefill batch, #new-seq: 9, #new-token: 98, #cached-token: 6508, token usage: 0.01, #running-req: 73, #queue-req: 0, 
[2025-10-27 11:29:56 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 113, #cached-token: 7176, token usage: 0.01, #running-req: 72, #queue-req: 0, 
[2025-10-27 11:29:56 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 63, #cached-token: 6434, token usage: 0.01, #running-req: 72, #queue-req: 0, 
[2025-10-27 11:29:56 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 138, #cached-token: 6434, token usage: 0.01, #running-req: 72, #queue-req: 0, 
[2025-10-27 11:29:56 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6456, token usage: 0.01, #running-req: 73, #queue-req: 0, 
[2025-10-27 11:29:56 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 126, #cached-token: 7189, token usage: 0.01, #running-req: 71, #queue-req: 0, 
[2025-10-27 11:29:56 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 139, #cached-token: 7148, token usage: 0.01, #running-req: 72, #queue-req: 0, 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:56 DP1 TP1] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:56 DP5 TP5] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:56 DP6 TP6] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:56 DP4 TP4] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:56 DP3 TP3] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:56 DP2 TP2] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:56 DP7 TP7] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:56 DP0 TP0] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:56 DP4 TP4] Prefill batch, #new-seq: 12, #new-token: 68, #cached-token: 8705, token usage: 0.01, #running-req: 82, #queue-req: 0, 
[2025-10-27 11:29:56 DP1 TP1] Prefill batch, #new-seq: 12, #new-token: 194, #cached-token: 8630, token usage: 0.01, #running-req: 82, #queue-req: 0, 
[2025-10-27 11:29:56 DP5 TP5] Prefill batch, #new-seq: 13, #new-token: 193, #cached-token: 9267, token usage: 0.01, #running-req: 81, #queue-req: 0, 
[2025-10-27 11:29:56 DP2 TP2] Prefill batch, #new-seq: 12, #new-token: 144, #cached-token: 8572, token usage: 0.01, #running-req: 82, #queue-req: 0, 
[2025-10-27 11:29:56 DP6 TP6] Prefill batch, #new-seq: 13, #new-token: 150, #cached-token: 9386, token usage: 0.01, #running-req: 81, #queue-req: 0, 
[2025-10-27 11:29:56 DP0 TP0] Prefill batch, #new-seq: 13, #new-token: 205, #cached-token: 9250, token usage: 0.01, #running-req: 81, #queue-req: 0, 
[2025-10-27 11:29:56 DP7 TP7] Prefill batch, #new-seq: 13, #new-token: 120, #cached-token: 9326, token usage: 0.01, #running-req: 81, #queue-req: 0, 
[2025-10-27 11:29:56 DP3 TP3] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8697, token usage: 0.01, #running-req: 82, #queue-req: 0, 
[2025-10-27 11:29:56 DP5 TP5] Prefill batch, #new-seq: 6, #new-token: 52, #cached-token: 4296, token usage: 0.01, #running-req: 94, #queue-req: 0, 
[2025-10-27 11:29:56 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5008, token usage: 0.01, #running-req: 94, #queue-req: 0, 
[2025-10-27 11:29:56 DP6 TP6] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4354, token usage: 0.01, #running-req: 94, #queue-req: 0, 
[2025-10-27 11:29:56 DP2 TP2] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5104, token usage: 0.01, #running-req: 94, #queue-req: 0, 
[2025-10-27 11:29:56 DP0 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4346, token usage: 0.01, #running-req: 94, #queue-req: 0, 
[2025-10-27 11:29:56 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 194, #cached-token: 5026, token usage: 0.01, #running-req: 94, #queue-req: 0, 
[2025-10-27 11:29:56 DP7 TP7] Prefill batch, #new-seq: 6, #new-token: 129, #cached-token: 4282, token usage: 0.01, #running-req: 94, #queue-req: 0, 
[2025-10-27 11:29:56 DP3 TP3] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5125, token usage: 0.01, #running-req: 94, #queue-req: 0, 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP1 TP1] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP3 TP3] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP0 TP0] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP6 TP6] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP4 TP4] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP5 TP5] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP7 TP7] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP2 TP2] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 84, #cached-token: 4985, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-27 11:29:57 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 58, #cached-token: 5768, token usage: 0.01, #running-req: 100, #queue-req: 0, 
[2025-10-27 11:29:57 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 105, #cached-token: 4995, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-27 11:29:57 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 126, #cached-token: 5102, token usage: 0.01, #running-req: 100, #queue-req: 0, 
[2025-10-27 11:29:57 DP2 TP2] Prefill batch, #new-seq: 7, #new-token: 79, #cached-token: 5030, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-27 11:29:57 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 51, #cached-token: 5768, token usage: 0.01, #running-req: 100, #queue-req: 0, 
[2025-10-27 11:29:57 DP3 TP3] Prefill batch, #new-seq: 7, #new-token: 121, #cached-token: 4938, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-27 11:29:57 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 84, #cached-token: 5833, token usage: 0.01, #running-req: 100, #queue-req: 0, 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP3 TP3] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP1 TP1] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP4 TP4] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP0 TP0] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP2 TP2] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP7 TP7] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP5 TP5] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP6 TP6] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP5 TP5] Prefill batch, #new-seq: 6, #new-token: 42, #cached-token: 4272, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-27 11:29:57 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5019, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-27 11:29:57 DP4 TP4] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4379, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-27 11:29:57 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5047, token usage: 0.01, #running-req: 107, #queue-req: 0, 
[2025-10-27 11:29:57 DP6 TP6] Prefill batch, #new-seq: 6, #new-token: 39, #cached-token: 4288, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-27 11:29:57 DP3 TP3] Prefill batch, #new-seq: 6, #new-token: 142, #cached-token: 4163, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-27 11:29:57 DP7 TP7] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4366, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-27 11:29:57 DP2 TP2] Prefill batch, #new-seq: 7, #new-token: 246, #cached-token: 4856, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP5 TP5] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP3 TP3] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP6 TP6] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP0 TP0] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP4 TP4] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP2 TP2] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP7 TP7] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP1 TP1] [fused_moe] using default for (495, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP4 TP4] Prefill batch, #new-seq: 6, #new-token: 49, #cached-token: 4337, token usage: 0.01, #running-req: 114, #queue-req: 0, 
[2025-10-27 11:29:57 DP7 TP7] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4441, token usage: 0.01, #running-req: 114, #queue-req: 0, 
[2025-10-27 11:29:57 DP5 TP5] Prefill batch, #new-seq: 6, #new-token: 49, #cached-token: 4374, token usage: 0.01, #running-req: 114, #queue-req: 0, 
[2025-10-27 11:29:57 DP1 TP1] Prefill batch, #new-seq: 6, #new-token: 42, #cached-token: 4446, token usage: 0.01, #running-req: 115, #queue-req: 0, 
[2025-10-27 11:29:57 DP3 TP3] Prefill batch, #new-seq: 7, #new-token: 62, #cached-token: 5041, token usage: 0.01, #running-req: 114, #queue-req: 0, 
[2025-10-27 11:29:57 DP6 TP6] Prefill batch, #new-seq: 6, #new-token: 53, #cached-token: 4294, token usage: 0.01, #running-req: 114, #queue-req: 0, 
[2025-10-27 11:29:57 DP2 TP2] Prefill batch, #new-seq: 6, #new-token: 86, #cached-token: 4349, token usage: 0.01, #running-req: 115, #queue-req: 0, 
[2025-10-27 11:29:57 DP0 TP0] Prefill batch, #new-seq: 6, #new-token: 89, #cached-token: 4385, token usage: 0.01, #running-req: 114, #queue-req: 0, 
[2025-10-27 11:29:57 DP1 TP1] Prefill batch, #new-seq: 6, #new-token: 176, #cached-token: 4334, token usage: 0.01, #running-req: 121, #queue-req: 0, 
[2025-10-27 11:29:57 DP5 TP5] Prefill batch, #new-seq: 7, #new-token: 195, #cached-token: 4931, token usage: 0.01, #running-req: 120, #queue-req: 0, 
[2025-10-27 11:29:57 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 94, #cached-token: 5003, token usage: 0.01, #running-req: 120, #queue-req: 0, 
[2025-10-27 11:29:57 DP3 TP3] Prefill batch, #new-seq: 6, #new-token: 256, #cached-token: 4121, token usage: 0.01, #running-req: 121, #queue-req: 0, 
[2025-10-27 11:29:57 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 113, #cached-token: 5105, token usage: 0.01, #running-req: 120, #queue-req: 0, 
[2025-10-27 11:29:57 DP2 TP2] Prefill batch, #new-seq: 6, #new-token: 139, #cached-token: 4269, token usage: 0.01, #running-req: 121, #queue-req: 0, 
[2025-10-27 11:29:57 DP0 TP0] Prefill batch, #new-seq: 6, #new-token: 41, #cached-token: 4289, token usage: 0.01, #running-req: 120, #queue-req: 0, 
[2025-10-27 11:29:57 DP6 TP6] Prefill batch, #new-seq: 7, #new-token: 49, #cached-token: 5123, token usage: 0.01, #running-req: 120, #queue-req: 0, 
[2025-10-27 11:29:57 DP5 TP5] Prefill batch, #new-seq: 6, #new-token: 91, #cached-token: 4314, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-27 11:29:57 DP1 TP1] Prefill batch, #new-seq: 6, #new-token: 350, #cached-token: 4064, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-27 11:29:57 DP4 TP4] Prefill batch, #new-seq: 6, #new-token: 144, #cached-token: 4253, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-27 11:29:57 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 209, #cached-token: 4830, token usage: 0.01, #running-req: 126, #queue-req: 0, 
[2025-10-27 11:29:57 DP6 TP6] Prefill batch, #new-seq: 6, #new-token: 132, #cached-token: 4204, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-27 11:29:57 DP2 TP2] Prefill batch, #new-seq: 6, #new-token: 202, #cached-token: 4224, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-27 11:29:57 DP3 TP3] Prefill batch, #new-seq: 6, #new-token: 247, #cached-token: 4167, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-27 11:29:57 DP7 TP7] Prefill batch, #new-seq: 6, #new-token: 137, #cached-token: 4229, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-27 11:29:57 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 157, #cached-token: 6347, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-27 11:29:57 DP1 TP1] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6473, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-27 11:29:57 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 140, #cached-token: 6355, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-27 11:29:57 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 83, #cached-token: 5895, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-27 11:29:57 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 87, #cached-token: 5919, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-27 11:29:57 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 124, #cached-token: 6503, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-27 11:29:57 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 91, #cached-token: 6344, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-27 11:29:57 DP3 TP3] Prefill batch, #new-seq: 9, #new-token: 171, #cached-token: 6411, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP1 TP1] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP7 TP7] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP5 TP5] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP6 TP6] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP0 TP0] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP4 TP4] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP3 TP3] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP2 TP2] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 188, #cached-token: 5630, token usage: 0.01, #running-req: 142, #queue-req: 0, 
[2025-10-27 11:29:57 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5841, token usage: 0.01, #running-req: 141, #queue-req: 0, 
[2025-10-27 11:29:57 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 62, #cached-token: 5771, token usage: 0.01, #running-req: 142, #queue-req: 0, 
[2025-10-27 11:29:57 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 190, #cached-token: 5565, token usage: 0.01, #running-req: 142, #queue-req: 0, 
[2025-10-27 11:29:57 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 98, #cached-token: 5730, token usage: 0.01, #running-req: 142, #queue-req: 0, 
[2025-10-27 11:29:57 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 275, #cached-token: 6408, token usage: 0.01, #running-req: 141, #queue-req: 0, 
[2025-10-27 11:29:57 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 173, #cached-token: 5604, token usage: 0.01, #running-req: 142, #queue-req: 0, 
[2025-10-27 11:29:57 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 77, #cached-token: 5796, token usage: 0.01, #running-req: 142, #queue-req: 0, 
[2025-10-27 11:29:57 DP5 TP5] Prefill batch, #new-seq: 7, #new-token: 126, #cached-token: 4980, token usage: 0.01, #running-req: 150, #queue-req: 0, 
[2025-10-27 11:29:57 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 165, #cached-token: 5093, token usage: 0.01, #running-req: 150, #queue-req: 0, 
[2025-10-27 11:29:57 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 148, #cached-token: 5029, token usage: 0.02, #running-req: 149, #queue-req: 0, 
[2025-10-27 11:29:57 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5046, token usage: 0.01, #running-req: 150, #queue-req: 0, 
[2025-10-27 11:29:57 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 117, #cached-token: 4976, token usage: 0.02, #running-req: 150, #queue-req: 0, 
[2025-10-27 11:29:57 DP3 TP3] Prefill batch, #new-seq: 7, #new-token: 171, #cached-token: 5017, token usage: 0.01, #running-req: 150, #queue-req: 0, 
[2025-10-27 11:29:57 DP2 TP2] Prefill batch, #new-seq: 7, #new-token: 82, #cached-token: 4926, token usage: 0.01, #running-req: 150, #queue-req: 0, 
[2025-10-27 11:29:57 DP6 TP6] Prefill batch, #new-seq: 7, #new-token: 104, #cached-token: 5031, token usage: 0.01, #running-req: 150, #queue-req: 0, 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP1 TP1] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP5 TP5] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP6 TP6] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP7 TP7] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP3 TP3] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP4 TP4] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP0 TP0] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP2 TP2] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:57 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 96, #cached-token: 5625, token usage: 0.02, #running-req: 157, #queue-req: 0, 
[2025-10-27 11:29:57 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 53, #cached-token: 5768, token usage: 0.02, #running-req: 157, #queue-req: 0, 
[2025-10-27 11:29:57 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 198, #cached-token: 5689, token usage: 0.02, #running-req: 157, #queue-req: 0, 
[2025-10-27 11:29:57 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 169, #cached-token: 5704, token usage: 0.02, #running-req: 157, #queue-req: 0, 
[2025-10-27 11:29:57 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5832, token usage: 0.02, #running-req: 157, #queue-req: 0, 
[2025-10-27 11:29:57 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 97, #cached-token: 5702, token usage: 0.02, #running-req: 157, #queue-req: 0, 
[2025-10-27 11:29:57 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 137, #cached-token: 5804, token usage: 0.02, #running-req: 157, #queue-req: 0, 
[2025-10-27 11:29:57 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 115, #cached-token: 5803, token usage: 0.02, #running-req: 156, #queue-req: 0, 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:58 DP7 TP7] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:58 DP0 TP0] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:58 DP5 TP5] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:58 DP1 TP1] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:58 DP4 TP4] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:58 DP6 TP6] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:58 DP3 TP3] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:58 DP2 TP2] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 11:29:58 DP3 TP3] Decode batch, #running-req: 165, #token: 11224, token usage: 0.02, cuda graph: True, gen throughput (token/s): 23.47, #queue-req: 0, 
[2025-10-27 11:29:59 DP5 TP5] Decode batch, #running-req: 165, #token: 12450, token usage: 0.02, cuda graph: True, gen throughput (token/s): 87.84, #queue-req: 0, 
[2025-10-27 11:30:00 DP4 TP4] Decode batch, #running-req: 165, #token: 13282, token usage: 0.02, cuda graph: True, gen throughput (token/s): 132.57, #queue-req: 0, 
[2025-10-27 11:30:00 DP7 TP7] Decode batch, #running-req: 165, #token: 14973, token usage: 0.02, cuda graph: True, gen throughput (token/s): 172.64, #queue-req: 0, 
[2025-10-27 11:30:00 DP6 TP6] Decode batch, #running-req: 165, #token: 14366, token usage: 0.02, cuda graph: True, gen throughput (token/s): 197.08, #queue-req: 0, 
[2025-10-27 11:30:01 DP0 TP0] Decode batch, #running-req: 164, #token: 15172, token usage: 0.02, cuda graph: True, gen throughput (token/s): 189.88, #queue-req: 0, 
[2025-10-27 11:30:01 DP1 TP1] Decode batch, #running-req: 165, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 185.61, #queue-req: 0, 
[2025-10-27 11:30:01] INFO:     127.0.0.1:56446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:01 DP2 TP2] Decode batch, #running-req: 165, #token: 15652, token usage: 0.02, cuda graph: True, gen throughput (token/s): 228.33, #queue-req: 0, 
[2025-10-27 11:30:01] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:01] INFO:     127.0.0.1:33928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:54124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:36776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:53562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:53960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:54028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:36918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:33910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:53938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:55558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:57146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:55250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:02] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03 DP3 TP3] Decode batch, #running-req: 162, #token: 17518, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1382.55, #queue-req: 0, 
[2025-10-27 11:30:03] INFO:     127.0.0.1:54444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:54700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:59224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:33974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:35262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:58332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:58402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:53534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:55174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:56866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:58502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:34014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:53878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:54172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:54224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:34150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:35042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:58700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:34022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:56726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:33340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:35966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:57276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:36760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:35824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:60896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:58624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:33876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:36840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:59228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:59368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:35634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:32912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:33154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:33476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:55798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:60304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:60810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:34062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:34342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:59812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:34382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:56424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:59358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:56118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:59770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:33290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:36214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:03] INFO:     127.0.0.1:35426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04 DP5 TP5] Decode batch, #running-req: 157, #token: 18075, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1362.15, #queue-req: 0, 
[2025-10-27 11:30:04] INFO:     127.0.0.1:32984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:35678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:60668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:58236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:35698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:57834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:59090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:59298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:35308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:53766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:34716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:33574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:58966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:36412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:59728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:33206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:34562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:54534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:34554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:33362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:33434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:59894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:59830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:34762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:35200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:33292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:35354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:36330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:58704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:54070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:35418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:35800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:34280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:57140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:36792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:54652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:36358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:54692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:35422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:60266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:36216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:56916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:54008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:33668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:60408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:34214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:58038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:59968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:59030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:59508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:33166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:56276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:33304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:54990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04 DP4 TP4] Decode batch, #running-req: 136, #token: 16743, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1325.93, #queue-req: 0, 
[2025-10-27 11:30:04] INFO:     127.0.0.1:55648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:57906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:54056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:54592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:53846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:55182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:04] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:60830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:37050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:34004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:59952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:33114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:53890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:32942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:36464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:58486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:33696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:36006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:57894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:60122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:57748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:60658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:32794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:53918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:37046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:58024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:59568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:33618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:54042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:60084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:60936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:59174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:33508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:34252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:36714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:53906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:55728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:60924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:55108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:59040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:59106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:59834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05 DP7 TP7] Decode batch, #running-req: 141, #token: 18435, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1354.08, #queue-req: 0, 
[2025-10-27 11:30:05] INFO:     127.0.0.1:36656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:60486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:36932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:54408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:58808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:36612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:33406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05 DP6 TP6] Decode batch, #running-req: 137, #token: 17223, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1344.75, #queue-req: 0, 
[2025-10-27 11:30:05] INFO:     127.0.0.1:56272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:53756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:57016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:36326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:33748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:55812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:58904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:37026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:33364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:33824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:33424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:56590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:56556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:33238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05 DP1 TP1] Decode batch, #running-req: 134, #token: 17919, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1344.87, #queue-req: 0, 
[2025-10-27 11:30:05] INFO:     127.0.0.1:55734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05 DP0 TP0] Decode batch, #running-req: 139, #token: 18464, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1334.83, #queue-req: 0, 
[2025-10-27 11:30:05] INFO:     127.0.0.1:35694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:54372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:34406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:33736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:55704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:56548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:58586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:55306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:56712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:60798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:54964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:56440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:60226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:56216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:54862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:55062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:56256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:58638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:59290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:36062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:60396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:33180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:55144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:60926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:36596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:36874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:54140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:58396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:33572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:33390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:54344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:55686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:05] INFO:     127.0.0.1:35106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:54060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:55442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:36480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:55282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:58144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:33212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:56936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:33228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:35362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:54460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:35318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:35592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:36562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06 DP2 TP2] Decode batch, #running-req: 118, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1312.13, #queue-req: 0, 
[2025-10-27 11:30:06] INFO:     127.0.0.1:36312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:55810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:35498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:36260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:36660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:37012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:56394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:58548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:58682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:58830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:36566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:58956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:36190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:55064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:36436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:56910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:35520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:56070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:33832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:36450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:55394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:56740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:33284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:35980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:33956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:33174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:37132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:54902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:54846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:36362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:58474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:36806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:36778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:35466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:36002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:54664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:53966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:37056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:33644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:55168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:55616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:54072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:36956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:58082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:34120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:37162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:33588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:55622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:56782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:55028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:53740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:33810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:53776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:35718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:36546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:60772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:56232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:59542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:33888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:54392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:58802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:06] INFO:     127.0.0.1:57854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:37138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:55078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:60144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:37092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:56524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:35276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:32894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:53586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:55106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:59324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:34122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:35184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:34724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:32964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:35654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:56290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:60146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:35156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:60424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:59918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:32820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:53600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:34454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:35586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:55222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07 DP3 TP3] Decode batch, #running-req: 111, #token: 16288, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1338.56, #queue-req: 0, 
[2025-10-27 11:30:07] INFO:     127.0.0.1:55514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:60960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:59700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:60716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:53636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:57838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:59932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:57544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:37106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:59592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:32860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:56116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:59844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:56110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:56162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:53998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:60920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:59050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:59872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:60648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:35572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:57152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:57422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:59360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:57558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:57628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:35542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:56010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:34790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:60724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:32806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:35346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:57354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:34908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:56852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:34448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:56462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:35402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:57984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:56954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:35436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:35514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:55774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:57306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:35940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:37088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:34160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:34848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:60860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:53582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:59994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:34926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:36924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:57150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:58180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:60344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:60436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:07] INFO:     127.0.0.1:33794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:54290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:60334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:34610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:60090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:35700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:35710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:55938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:33778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:34200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:54940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:35938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:36698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:56398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:56806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:56126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:36392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:54378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:53840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:35414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:33376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08 DP5 TP5] Decode batch, #running-req: 94, #token: 14978, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1246.27, #queue-req: 0, 
[2025-10-27 11:30:08] INFO:     127.0.0.1:54314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:37010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:55208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:60130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:56816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:54924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:35858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:58970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:34304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:35332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:36118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:54974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:60470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:35302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:34740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:36896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:36728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:58614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:36104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:33418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:34576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:35252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:32896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:33142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:59564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:60372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:33684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:35472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:36704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:58716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:34114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:53762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:58510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:58202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:56502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:35080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:54786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:35984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:33846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:58640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:59130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:32770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:36166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:33938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:34396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:58816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:34824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:36748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:36824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:54266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:60254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08 DP4 TP4] Decode batch, #running-req: 72, #token: 12080, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1068.57, #queue-req: 0, 
[2025-10-27 11:30:08] INFO:     127.0.0.1:35060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:54096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:59434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:34190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:60620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:58926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:33126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:59946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:34632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:33804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:54518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:34270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:53646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:34092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:34916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:53984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:34076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:35054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:36048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:08] INFO:     127.0.0.1:60110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:33762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:36592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:54988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:32850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:35950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:56952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:54600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:33520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:35140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:60596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:36888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:59800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:36942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09 DP7 TP7] Decode batch, #running-req: 77, #token: 13919, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1111.48, #queue-req: 0, 
[2025-10-27 11:30:09] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:58260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:54476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:59146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:35382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:56756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:59984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:60064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:34326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:55130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:56926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:57638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:54112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:57312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:54272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:36400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09 DP6 TP6] Decode batch, #running-req: 62, #token: 10550, token usage: 0.02, cuda graph: True, gen throughput (token/s): 964.77, #queue-req: 0, 
[2025-10-27 11:30:09] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:33972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:34618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:35114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:59740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:33330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:57376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:35732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:59934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:34998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:58464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:34596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:34756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:60692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:35070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:32868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:33638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:36548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09 DP1 TP1] Decode batch, #running-req: 70, #token: 13101, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1074.42, #queue-req: 0, 
[2025-10-27 11:30:09] INFO:     127.0.0.1:33080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09 DP0 TP0] Decode batch, #running-req: 69, #token: 11591, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1079.17, #queue-req: 0, 
[2025-10-27 11:30:09] INFO:     127.0.0.1:60282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:56636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:34434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:34910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:58600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:60178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:35882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:34052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:53800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:58840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:34464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:57878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:36494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:56340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:33952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:60030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:34508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:34814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:54178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:56550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:57244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:33036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:56330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:56838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:56992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:35934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:36638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:35868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:54542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:59782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:36324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:36858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:53690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:60866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:34648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:58684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:34684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:36424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:56060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:33444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:34546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:36274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:36812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:36878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:56314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:33934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:33870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:60166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09 DP2 TP2] Decode batch, #running-req: 58, #token: 10546, token usage: 0.02, cuda graph: True, gen throughput (token/s): 882.33, #queue-req: 0, 
[2025-10-27 11:30:09] INFO:     127.0.0.1:56624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:34350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:59380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:59466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:53866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:56206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:58996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:33006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:53812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:35554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:36528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:33986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:36842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:55052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:09] INFO:     127.0.0.1:60610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:35664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:53660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:56364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:59142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:59614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:36258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:54798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:54220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:54156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:58196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:59764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:32986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:60506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:60560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:35624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:60068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:33724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:34288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:36938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:53720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:59348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:36074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:36630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:58942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:54080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:35850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:35972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:53872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:34994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:36694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:54180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:60638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:34030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:32796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:34474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:33602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:34928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:56594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:36676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:54642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:58650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:34880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:35564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:34940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:36510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:36276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:35604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:32782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:37072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:59720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:33064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:56652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:33074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:59848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:35820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:35124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:56056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:35094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:32956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:35012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:58478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:36430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:35908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:34058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10 DP3 TP3] Decode batch, #running-req: 55, #token: 9999, token usage: 0.01, cuda graph: True, gen throughput (token/s): 941.14, #queue-req: 0, 
[2025-10-27 11:30:10] INFO:     127.0.0.1:58008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:56348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:34024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:58784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:36514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:37120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:33720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:56410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:34268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:35246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:32968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:60660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:35650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:33092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:56390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:10] INFO:     127.0.0.1:59518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:60912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:53578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:60534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:36586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:60188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:35640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:57870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:35760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:53790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:35232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:35616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:32928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:34776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:56150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:33048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:33438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:33202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:35026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:60974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:58458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:57598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:55406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:34590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:34100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:54304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11 DP5 TP5] Decode batch, #running-req: 28, #token: 5781, token usage: 0.01, cuda graph: True, gen throughput (token/s): 705.73, #queue-req: 0, 
[2025-10-27 11:30:11] INFO:     127.0.0.1:59710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:36378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:35766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:59272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:54706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:58940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:60042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:58304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:60010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:59560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:34522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:36034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:54562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:57268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:35890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:54998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:33222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:56894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:34972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:35194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:34832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:59024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:34678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:33786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:34934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:56378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:34980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:56826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:59334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:35622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:35910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:57194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11 DP4 TP4] Decode batch, #running-req: 22, #token: 5089, token usage: 0.01, cuda graph: True, gen throughput (token/s): 527.95, #queue-req: 0, 
[2025-10-27 11:30:11] INFO:     127.0.0.1:59122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:55954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:33186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:60576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:56226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:36954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:57212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:33652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:55368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:33936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:33502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:35216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:53724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:55980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:55492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:60740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:56092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:11] INFO:     127.0.0.1:36620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:56212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:32774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:34410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:58772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:34386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:36292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:59078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:57618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:57694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:33346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12 DP7 TP7] Decode batch, #running-req: 40, #token: 9146, token usage: 0.01, cuda graph: True, gen throughput (token/s): 787.54, #queue-req: 0, 
[2025-10-27 11:30:12] INFO:     127.0.0.1:37150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:58876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:54014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:36634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12 DP6 TP6] Decode batch, #running-req: 13, #token: 3169, token usage: 0.00, cuda graph: True, gen throughput (token/s): 430.04, #queue-req: 0, 
[2025-10-27 11:30:12] INFO:     127.0.0.1:36540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:33256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:34702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12 DP0 TP0] Decode batch, #running-req: 22, #token: 5831, token usage: 0.01, cuda graph: True, gen throughput (token/s): 528.01, #queue-req: 0, 
[2025-10-27 11:30:12 DP1 TP1] Decode batch, #running-req: 25, #token: 6300, token usage: 0.01, cuda graph: True, gen throughput (token/s): 638.86, #queue-req: 0, 
[2025-10-27 11:30:12] INFO:     127.0.0.1:59664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:54024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:56066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:33776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:53980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:53774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:57102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:55644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:55650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:53608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:33994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:35894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12 DP2 TP2] Decode batch, #running-req: 15, #token: 3958, token usage: 0.01, cuda graph: True, gen throughput (token/s): 459.17, #queue-req: 0, 
[2025-10-27 11:30:12] INFO:     127.0.0.1:34168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:54410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:36662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:36738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:32834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:35806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:53750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:33902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:60018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:34806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:55466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:55120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:60818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:37036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:12] INFO:     127.0.0.1:59120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:35618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:36658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:35170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:55000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:56612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:35020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:35958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:53854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:35368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:55538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:60790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:55320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:36208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:34318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:34238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:36020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:56790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13 DP3 TP3] Decode batch, #running-req: 18, #token: 4888, token usage: 0.01, cuda graph: True, gen throughput (token/s): 492.30, #queue-req: 0, 
[2025-10-27 11:30:13] INFO:     127.0.0.1:34532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:36576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:36960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:59422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:60756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:56488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:34498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:55226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:35434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:57540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:57954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:60294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:55192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:36150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:36348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:35348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:34378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:53676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:36084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:36986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:36044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13 DP5 TP5] Decode batch, #running-req: 10, #token: 3279, token usage: 0.00, cuda graph: True, gen throughput (token/s): 306.24, #queue-req: 0, 
[2025-10-27 11:30:13] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:55522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:55552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:53616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:35102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:58910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:58980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:60358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:54284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:56678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:34138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:36526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:13] INFO:     127.0.0.1:37000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:58660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:55836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14 DP4 TP4] Decode batch, #running-req: 7, #token: 2500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 302.73, #queue-req: 0, 
[2025-10-27 11:30:14] INFO:     127.0.0.1:34364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:35762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:55750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14 DP7 TP7] Decode batch, #running-req: 16, #token: 5051, token usage: 0.01, cuda graph: True, gen throughput (token/s): 493.49, #queue-req: 0, 
[2025-10-27 11:30:14] INFO:     127.0.0.1:54504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:32784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:36680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14 DP6 TP6] Decode batch, #running-req: 4, #token: 1835, token usage: 0.00, cuda graph: True, gen throughput (token/s): 126.59, #queue-req: 0, 
[2025-10-27 11:30:14] INFO:     127.0.0.1:56044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:33272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:36482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:54892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14 DP0 TP0] Decode batch, #running-req: 11, #token: 3514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 303.46, #queue-req: 0, 
[2025-10-27 11:30:14 DP1 TP1] Decode batch, #running-req: 7, #token: 2461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 305.37, #queue-req: 0, 
[2025-10-27 11:30:14] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:33914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:54624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14 DP2 TP2] Decode batch, #running-req: 5, #token: 1977, token usage: 0.00, cuda graph: True, gen throughput (token/s): 205.54, #queue-req: 0, 
[2025-10-27 11:30:14] INFO:     127.0.0.1:54926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:34526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:34430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:55244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:14] INFO:     127.0.0.1:35834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:33780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:55706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:54554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:58442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:59406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:33708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:60270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15 DP3 TP3] Decode batch, #running-req: 6, #token: 2062, token usage: 0.00, cuda graph: True, gen throughput (token/s): 218.92, #queue-req: 0, 
[2025-10-27 11:30:15] INFO:     127.0.0.1:35458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:33674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:60640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:57504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:54428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:34226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:60844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:35538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:59258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:60580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15 DP5 TP5] Decode batch, #running-req: 6, #token: 2473, token usage: 0.00, cuda graph: True, gen throughput (token/s): 172.71, #queue-req: 0, 
[2025-10-27 11:30:15] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:60988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:57332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:35316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:35160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15 DP4 TP4] Decode batch, #running-req: 3, #token: 1284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 137.16, #queue-req: 0, 
[2025-10-27 11:30:15] INFO:     127.0.0.1:58792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:59864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:34420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:58758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15 DP7 TP7] Decode batch, #running-req: 6, #token: 2197, token usage: 0.00, cuda graph: True, gen throughput (token/s): 250.15, #queue-req: 0, 
[2025-10-27 11:30:15] INFO:     127.0.0.1:36100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:15] INFO:     127.0.0.1:35822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:16 DP6 TP6] Decode batch, #running-req: 1, #token: 1080, token usage: 0.00, cuda graph: True, gen throughput (token/s): 53.16, #queue-req: 0, 
[2025-10-27 11:30:16 DP0 TP0] Decode batch, #running-req: 1, #token: 982, token usage: 0.00, cuda graph: True, gen throughput (token/s): 145.92, #queue-req: 0, 
[2025-10-27 11:30:16 DP1 TP1] Decode batch, #running-req: 2, #token: 1317, token usage: 0.00, cuda graph: True, gen throughput (token/s): 97.69, #queue-req: 0, 
[2025-10-27 11:30:16] INFO:     127.0.0.1:60700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:16 DP2 TP2] Decode batch, #running-req: 2, #token: 1278, token usage: 0.00, cuda graph: True, gen throughput (token/s): 84.37, #queue-req: 0, 
[2025-10-27 11:30:16] INFO:     127.0.0.1:33768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:16] INFO:     127.0.0.1:35072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:16] INFO:     127.0.0.1:54878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:16] INFO:     127.0.0.1:59960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:16 DP3 TP3] Decode batch, #running-req: 2, #token: 1315, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.02, #queue-req: 0, 
[2025-10-27 11:30:16] INFO:     127.0.0.1:57918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:16] INFO:     127.0.0.1:54610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:16] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:16] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:16 DP5 TP5] Decode batch, #running-req: 1, #token: 1061, token usage: 0.00, cuda graph: True, gen throughput (token/s): 115.79, #queue-req: 0, 
[2025-10-27 11:30:16] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:17] INFO:     127.0.0.1:34586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:17 DP4 TP4] Decode batch, #running-req: 2, #token: 1363, token usage: 0.00, cuda graph: True, gen throughput (token/s): 58.85, #queue-req: 0, 
[2025-10-27 11:30:17] INFO:     127.0.0.1:35956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:17 DP7 TP7] Decode batch, #running-req: 2, #token: 1363, token usage: 0.00, cuda graph: True, gen throughput (token/s): 114.38, #queue-req: 0, 
[2025-10-27 11:30:17] INFO:     127.0.0.1:57488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:17] INFO:     127.0.0.1:57218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:17] INFO:     127.0.0.1:60526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:17 DP2 TP2] Decode batch, #running-req: 2, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 62.51, #queue-req: 0, 
[2025-10-27 11:30:17] INFO:     127.0.0.1:35532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:17] INFO:     127.0.0.1:56540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:17] INFO:     127.0.0.1:60862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:18] INFO:     127.0.0.1:34870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:18 DP7 TP7] Decode batch, #running-req: 1, #token: 1054, token usage: 0.00, cuda graph: True, gen throughput (token/s): 59.19, #queue-req: 0, 
[2025-10-27 11:30:19 DP7 TP7] Decode batch, #running-req: 1, #token: 1094, token usage: 0.00, cuda graph: True, gen throughput (token/s): 39.04, #queue-req: 0, 
[2025-10-27 11:30:20] INFO:     127.0.0.1:55434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 11:30:25] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-27 11:30:29] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
