INFO 11-20 13:45:17 __init__.py:179] Automatically detected platform rocm.
WARNING 11-20 13:45:17 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-20 13:45:17] WARNING server_args.py:1280: Attention backend not explicitly specified. Use aiter backend by default.
[2025-11-20 13:45:18] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.68, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=503760551, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, mm_process_config={}, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=16, cuda_graph_bs=[1, 2, 4, 8, 12, 16], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, decrypted_config_file=None, decrypted_draft_config_file=None, mm_enable_dp_encoder=False, hooks=None)
[2025-11-20 13:45:18] Using default HuggingFace chat template with detected content format: string
INFO 11-20 13:45:29 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-20 13:45:29 TP1] Process 276 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
INFO 11-20 13:45:29 __init__.py:179] Automatically detected platform rocm.
INFO 11-20 13:45:29 __init__.py:179] Automatically detected platform rocm.
INFO 11-20 13:45:29 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-20 13:45:29 TP0] Process 275 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2025-11-20 13:45:29 TP5] Process 280 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-11-20 13:45:29 TP4] Process 279 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
INFO 11-20 13:45:30 __init__.py:179] Automatically detected platform rocm.
INFO 11-20 13:45:30 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
INFO 11-20 13:45:30 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
INFO 11-20 13:45:30 __init__.py:179] Automatically detected platform rocm.
INFO 11-20 13:45:30 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-20 13:45:30 TP7] Process 282 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
[2025-11-20 13:45:30 TP1] Init torch distributed begin.
[2025-11-20 13:45:30 TP3] Process 278 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-11-20 13:45:30 TP2] Process 277 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
[2025-11-20 13:45:30 TP6] Process 281 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
[2025-11-20 13:45:30 TP5] Init torch distributed begin.
[2025-11-20 13:45:30 TP4] Init torch distributed begin.
[2025-11-20 13:45:30 TP0] Init torch distributed begin.
[2025-11-20 13:45:30 TP3] Init torch distributed begin.
[2025-11-20 13:45:30 TP2] Init torch distributed begin.
[2025-11-20 13:45:31 TP7] Init torch distributed begin.
[2025-11-20 13:45:31 TP6] Init torch distributed begin.
[2025-11-20 13:45:31 TP0] sglang is using nccl==2.21.5
[2025-11-20 13:45:32 TP5] Aiter custom all-reduce not available (optional dependency missing); falling back to sglang CustomAllreduce. Details: No module named 'aiter.dist.device_communicators'
[2025-11-20 13:45:32 TP4] Aiter custom all-reduce not available (optional dependency missing); falling back to sglang CustomAllreduce. Details: No module named 'aiter.dist.device_communicators'
[2025-11-20 13:45:32 TP2] Aiter custom all-reduce not available (optional dependency missing); falling back to sglang CustomAllreduce. Details: No module named 'aiter.dist.device_communicators'
[2025-11-20 13:45:32 TP3] Aiter custom all-reduce not available (optional dependency missing); falling back to sglang CustomAllreduce. Details: No module named 'aiter.dist.device_communicators'
[2025-11-20 13:45:32 TP6] Aiter custom all-reduce not available (optional dependency missing); falling back to sglang CustomAllreduce. Details: No module named 'aiter.dist.device_communicators'
[2025-11-20 13:45:32 TP7] Aiter custom all-reduce not available (optional dependency missing); falling back to sglang CustomAllreduce. Details: No module named 'aiter.dist.device_communicators'
[2025-11-20 13:45:32 TP1] Aiter custom all-reduce not available (optional dependency missing); falling back to sglang CustomAllreduce. Details: No module named 'aiter.dist.device_communicators'
[2025-11-20 13:45:32 TP0] Aiter custom all-reduce not available (optional dependency missing); falling back to sglang CustomAllreduce. Details: No module named 'aiter.dist.device_communicators'
[2025-11-20 13:45:33 TP7] Init torch distributed ends. mem usage=3.94 GB
[2025-11-20 13:45:33 TP5] Init torch distributed ends. mem usage=3.93 GB
[2025-11-20 13:45:33 TP6] Init torch distributed ends. mem usage=3.95 GB
[2025-11-20 13:45:33 TP0] Init torch distributed ends. mem usage=3.65 GB
[2025-11-20 13:45:33 TP4] Init torch distributed ends. mem usage=4.01 GB
[2025-11-20 13:45:33 TP2] Init torch distributed ends. mem usage=4.07 GB
[2025-11-20 13:45:33 TP3] Init torch distributed ends. mem usage=4.06 GB
[2025-11-20 13:45:33 TP1] Init torch distributed ends. mem usage=4.07 GB
[2025-11-20 13:45:33 TP2] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-20 13:45:33 TP6] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-20 13:45:33 TP3] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-20 13:45:33 TP4] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-20 13:45:33 TP0] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-20 13:45:33 TP7] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-20 13:45:33 TP5] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-20 13:45:33 TP1] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-20 13:45:35 TP7] Load weight begin. avail mem=187.32 GB
[2025-11-20 13:45:35 TP4] Load weight begin. avail mem=187.25 GB
[2025-11-20 13:45:35 TP5] Load weight begin. avail mem=187.33 GB
[2025-11-20 13:45:35 TP6] Load weight begin. avail mem=187.31 GB
[2025-11-20 13:45:35 TP3] Load weight begin. avail mem=187.20 GB
[2025-11-20 13:45:35 TP2] Load weight begin. avail mem=187.19 GB
[2025-11-20 13:45:35 TP1] Load weight begin. avail mem=187.19 GB
[2025-11-20 13:45:35 TP0] Load weight begin. avail mem=187.61 GB
[2025-11-20 13:45:35 TP0] Detected fp8 checkpoint.
[2025-11-20 13:45:35 TP0] Shared experts fusion optimization enabled.
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:36,  4.47it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:28,  5.57it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:24,  6.52it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:00<00:27,  5.75it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:24,  6.45it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:00<00:13, 11.59it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:01<00:25,  6.02it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:01<00:20,  7.38it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:01<00:16,  9.08it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:02<00:13, 10.54it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:02<00:12, 11.95it/s]
Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:02<00:12, 11.35it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:02<00:10, 12.75it/s]
Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:02<00:09, 14.11it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:02<00:09, 14.32it/s]
Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:02<00:08, 15.44it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:03<00:07, 18.50it/s]
Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:03<00:06, 19.30it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:04<00:15,  7.80it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:04<00:13,  9.06it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:04<00:11, 10.33it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:04<00:11, 10.03it/s]
Loading safetensors checkpoint shards:  29% Completed | 48/163 [00:04<00:09, 12.58it/s]
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:04<00:08, 13.38it/s]
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:04<00:08, 12.89it/s]
Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:05<00:08, 12.22it/s]
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:05<00:08, 12.59it/s]
Loading safetensors checkpoint shards:  36% Completed | 59/163 [00:05<00:08, 12.06it/s]
Loading safetensors checkpoint shards:  37% Completed | 61/163 [00:05<00:07, 13.44it/s]
Loading safetensors checkpoint shards:  39% Completed | 64/163 [00:05<00:06, 14.33it/s]
Loading safetensors checkpoint shards:  40% Completed | 66/163 [00:05<00:06, 14.81it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:06<00:06, 14.55it/s]
Loading safetensors checkpoint shards:  43% Completed | 70/163 [00:06<00:13,  6.78it/s]
Loading safetensors checkpoint shards:  44% Completed | 72/163 [00:06<00:11,  8.16it/s]
Loading safetensors checkpoint shards:  45% Completed | 74/163 [00:06<00:09,  9.61it/s]
Loading safetensors checkpoint shards:  47% Completed | 76/163 [00:07<00:08, 10.02it/s]
Loading safetensors checkpoint shards:  48% Completed | 79/163 [00:07<00:06, 12.73it/s]
Loading safetensors checkpoint shards:  50% Completed | 81/163 [00:07<00:06, 12.41it/s]
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:07<00:05, 14.11it/s]
Loading safetensors checkpoint shards:  53% Completed | 86/163 [00:07<00:05, 13.05it/s]
Loading safetensors checkpoint shards:  55% Completed | 89/163 [00:07<00:05, 14.76it/s]
Loading safetensors checkpoint shards:  56% Completed | 92/163 [00:08<00:05, 13.13it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:08<00:04, 13.88it/s]
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:08<00:04, 14.65it/s]
Loading safetensors checkpoint shards:  60% Completed | 98/163 [00:08<00:04, 15.24it/s]
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:08<00:04, 15.24it/s]
Loading safetensors checkpoint shards:  63% Completed | 102/163 [00:08<00:03, 16.03it/s]
Loading safetensors checkpoint shards:  64% Completed | 104/163 [00:09<00:05, 11.11it/s]
Loading safetensors checkpoint shards:  66% Completed | 107/163 [00:09<00:04, 13.29it/s]
Loading safetensors checkpoint shards:  67% Completed | 109/163 [00:10<00:08,  6.29it/s]
Loading safetensors checkpoint shards:  68% Completed | 111/163 [00:10<00:06,  7.72it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:10<00:04, 10.02it/s]
Loading safetensors checkpoint shards:  72% Completed | 117/163 [00:10<00:03, 12.35it/s]
Loading safetensors checkpoint shards:  73% Completed | 119/163 [00:10<00:03, 12.28it/s]
Loading safetensors checkpoint shards:  74% Completed | 121/163 [00:10<00:04, 10.45it/s]
Loading safetensors checkpoint shards:  76% Completed | 124/163 [00:11<00:03, 12.73it/s]
Loading safetensors checkpoint shards:  78% Completed | 127/163 [00:11<00:02, 14.75it/s]
Loading safetensors checkpoint shards:  80% Completed | 130/163 [00:11<00:02, 14.65it/s]
Loading safetensors checkpoint shards:  82% Completed | 133/163 [00:11<00:01, 15.82it/s]
Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:11<00:01, 16.60it/s]
Loading safetensors checkpoint shards:  84% Completed | 137/163 [00:11<00:02, 12.85it/s]
Loading safetensors checkpoint shards:  85% Completed | 139/163 [00:12<00:01, 13.96it/s]
Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:12<00:01, 14.61it/s]
Loading safetensors checkpoint shards:  88% Completed | 143/163 [00:12<00:01, 11.75it/s]
Loading safetensors checkpoint shards:  90% Completed | 146/163 [00:12<00:01, 14.14it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:12<00:00, 16.05it/s]
Loading safetensors checkpoint shards:  93% Completed | 152/163 [00:12<00:00, 17.31it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:13<00:00, 12.53it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:13<00:00,  6.72it/s]
Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:14<00:00,  8.04it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:14<00:00,  9.39it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:14<00:00,  9.88it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:14<00:00, 11.37it/s]

[2025-11-20 13:46:28 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.56 GB, mem usage=79.62 GB.
[2025-11-20 13:46:28 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.57 GB, mem usage=79.62 GB.
[2025-11-20 13:46:28 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.57 GB, mem usage=79.62 GB.
[2025-11-20 13:46:28 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.98 GB, mem usage=79.62 GB.
[2025-11-20 13:46:43 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.69 GB, mem usage=79.62 GB.
[2025-11-20 13:46:44 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.62 GB, mem usage=79.62 GB.
[2025-11-20 13:46:44 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.71 GB, mem usage=79.62 GB.
[2025-11-20 13:46:45 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.70 GB, mem usage=79.62 GB.
[2025-11-20 13:46:45 TP0] Using KV cache dtype: torch.bfloat16
[2025-11-20 13:46:45 TP7] KV Cache is allocated. #tokens: 727202, KV size: 47.59 GB
[2025-11-20 13:46:45 TP7] Memory pool end. avail mem=59.42 GB
[2025-11-20 13:46:45 TP6] KV Cache is allocated. #tokens: 727202, KV size: 47.59 GB
[2025-11-20 13:46:45 TP6] Memory pool end. avail mem=59.41 GB
[2025-11-20 13:46:45 TP5] KV Cache is allocated. #tokens: 727202, KV size: 47.59 GB
[2025-11-20 13:46:45 TP5] Memory pool end. avail mem=59.42 GB
[2025-11-20 13:46:45 TP0] KV Cache is allocated. #tokens: 727202, KV size: 47.59 GB
[2025-11-20 13:46:45 TP0] Memory pool end. avail mem=59.70 GB
[2025-11-20 13:46:45 TP4] KV Cache is allocated. #tokens: 727202, KV size: 47.59 GB
[2025-11-20 13:46:45 TP4] Memory pool end. avail mem=59.34 GB
[2025-11-20 13:46:45 TP2] KV Cache is allocated. #tokens: 727202, KV size: 47.59 GB
[2025-11-20 13:46:45 TP3] KV Cache is allocated. #tokens: 727202, KV size: 47.59 GB
[2025-11-20 13:46:45 TP2] Memory pool end. avail mem=59.28 GB
[2025-11-20 13:46:45 TP3] Memory pool end. avail mem=59.29 GB
[2025-11-20 13:46:45 TP1] KV Cache is allocated. #tokens: 727202, KV size: 47.59 GB
[2025-11-20 13:46:45 TP1] Memory pool end. avail mem=59.28 GB
[2025-11-20 13:46:46 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=59.21 GB
[2025-11-20 13:46:46 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=59.20 GB
[2025-11-20 13:46:46 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=59.49 GB
[2025-11-20 13:46:46 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16]
[2025-11-20 13:46:46 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=59.08 GB
[2025-11-20 13:46:47 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=59.07 GB
[2025-11-20 13:46:47 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=59.08 GB
[2025-11-20 13:46:47 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=59.13 GB
[2025-11-20 13:46:47 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=59.22 GB
  0%|          | 0/6 [00:00<?, ?it/s]Capturing batches (bs=16 avail_mem=59.47 GB):   0%|          | 0/6 [00:00<?, ?it/s][rank3]:W1120 13:46:54.027000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:46:54.123000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:46:54.124000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:46:54.142000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:46:54.149000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:46:54.221000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:46:54.240000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:46:54.251000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:46:54.316000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:46:54.385000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:46:54.414000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-20 13:46:54 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[rank2]:W1120 13:46:54.423000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1120 13:46:54.481000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-20 13:46:54 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[rank2]:W1120 13:46:54.518000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-20 13:46:54 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[rank3]:W1120 13:46:54.522000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-20 13:46:54 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1120 13:46:54.613000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:46:54.631000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:46:54.646000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-20 13:46:54 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[rank7]:W1120 13:46:54.742000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-20 13:46:54 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-20 13:46:54 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[rank1]:W1120 13:46:54.815000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1120 13:46:54.841000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1120 13:46:54.882000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:46:54.917000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-20 13:46:55 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1120 13:46:55.251000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:46:57.341000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:46:57.436000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:46:57.445000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:46:57.456000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:46:57.649000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:46:57.705000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:46:57.725000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:46:58.069000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:46:58.964000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:46:59.038000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:46:59.152000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:46:59.300000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:46:59.375000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:46:59.414000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:46:59.434000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:46:59.488000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:46:59.490000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:46:59.492000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:46:59.510000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:46:59.563000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:46:59.611000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:46:59.626000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:46:59.678000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:46:59.758000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:46:59.838000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:46:59.959000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:00.010000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:00.084000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:00.185000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:00.198000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:00.260000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:00.377000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:00.805000 278 torch/_inductor/utils.py:1349] [38/0] Please pip install Composable Kernel package
[rank0]:W1120 13:47:01.176000 275 torch/_inductor/utils.py:1349] [38/0] Please pip install Composable Kernel package
[rank6]:W1120 13:47:01.295000 281 torch/_inductor/utils.py:1349] [38/0] Please pip install Composable Kernel package
[rank4]:W1120 13:47:01.311000 279 torch/_inductor/utils.py:1349] [38/0] Please pip install Composable Kernel package
[rank1]:W1120 13:47:01.413000 276 torch/_inductor/utils.py:1349] [38/0] Please pip install Composable Kernel package
[rank5]:W1120 13:47:01.730000 280 torch/_inductor/utils.py:1349] [38/0] Please pip install Composable Kernel package
[rank2]:W1120 13:47:01.901000 277 torch/_inductor/utils.py:1349] [38/0] Please pip install Composable Kernel package
[rank7]:W1120 13:47:02.062000 282 torch/_inductor/utils.py:1349] [38/0] Please pip install Composable Kernel package
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_12 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_4 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_13 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_14 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_18 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_22 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_3 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_5 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_7 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.1703 seconds and 0.2313 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_2 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_4 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_10 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_11 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_3 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_5 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.0872 seconds and 0.2369 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_14 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_16 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_18 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_19 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_20 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_23 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_5 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_15 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.3189 seconds and 0.2316 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_3 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_5 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_16 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_20 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_22 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_1 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_2 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_4 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.3058 seconds and 0.2368 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_1 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_0 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_2 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_4 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_6 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_7 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_10 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_13 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.2215 seconds and 0.2352 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_7 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_11 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_0 0.0067 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_1 0.0067 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_2 0.0067 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_4 0.0067 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_12 0.0067 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_13 0.0067 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_14 0.0067 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.2740 seconds and 0.2337 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_0 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_3 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_4 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_5 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_7 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_10 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_1 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_9 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.2833 seconds and 0.2375 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_0 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_1 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_2 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_3 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_11 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_18 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_19 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.3868 seconds and 0.2331 seconds precompiling for 25 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1120 13:47:12.646000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:13.106000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:13.149000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:13.186000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:13.198000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:13.608000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:13.686000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:13.716000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:13.785000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:13.941000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:14.148000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:14.232000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:14.296000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:14.449000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:14.689000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:14.742000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[rank3]:W1120 13:47:15.792000 278 torch/_inductor/utils.py:1349] [50/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[rank0]:W1120 13:47:16.380000 275 torch/_inductor/utils.py:1349] [50/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[rank1]:W1120 13:47:16.956000 276 torch/_inductor/utils.py:1349] [50/0_1] Please pip install Composable Kernel package
[rank2]:W1120 13:47:17.351000 277 torch/_inductor/utils.py:1349] [50/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[rank4]:W1120 13:47:18.344000 279 torch/_inductor/utils.py:1349] [50/0_1] Please pip install Composable Kernel package
[rank6]:W1120 13:47:18.574000 281 torch/_inductor/utils.py:1349] [50/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[rank7]:W1120 13:47:20.037000 282 torch/_inductor/utils.py:1349] [50/0_1] Please pip install Composable Kernel package
[rank5]:W1120 13:47:20.074000 280 torch/_inductor/utils.py:1349] [50/0_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_27 0.0068 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_26 0.0068 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_36 0.0071 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0071 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_35 0.0077 ms 85.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_34 0.0078 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_30 0.0081 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_31 0.0081 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_46 0.0085 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9384 seconds and 0.3952 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_26 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_27 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_36 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_34 0.0077 ms 83.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_35 0.0077 ms 83.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0082 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_30 0.0083 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_46 0.0086 ms 75.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8491 seconds and 0.3965 seconds precompiling for 25 choices
[rank3]:W1120 13:47:22.206000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:22.281000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_26 0.0067 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_27 0.0068 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_37 0.0068 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_36 0.0069 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_35 0.0072 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_34 0.0073 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_31 0.0081 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_30 0.0081 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_46 0.0084 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8744 seconds and 0.4017 seconds precompiling for 25 choices
[rank3]:W1120 13:47:22.378000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:22.696000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_26 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_27 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_36 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_34 0.0072 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_35 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_30 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_31 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_28 0.0083 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9092 seconds and 0.4038 seconds precompiling for 25 choices
[rank0]:W1120 13:47:22.771000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:22.870000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:23.301000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:23.376000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:23.474000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_26 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_27 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_36 0.0071 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0072 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_35 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_34 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_30 0.0081 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_31 0.0081 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_46 0.0087 ms 73.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8936 seconds and 0.4012 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_27 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_26 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_37 0.0069 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_36 0.0069 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_34 0.0073 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_35 0.0074 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_30 0.0080 ms 83.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_31 0.0081 ms 83.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_46 0.0083 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_47 0.0084 ms 79.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8774 seconds and 0.3900 seconds precompiling for 25 choices
[rank2]:W1120 13:47:24.151000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:24.225000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:24.320000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:24.702000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:24.777000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:24.874000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:25.022000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:25.098000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:25.196000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_26 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_27 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_37 0.0068 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_36 0.0068 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_34 0.0071 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_35 0.0072 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0073 ms 90.7% 
  triton_bmm_30 0.0079 ms 83.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_31 0.0080 ms 82.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_47 0.0083 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7076 seconds and 0.4111 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_26 0.0067 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_27 0.0067 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_36 0.0068 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0069 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_34 0.0071 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_35 0.0073 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_30 0.0081 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_31 0.0081 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_46 0.0083 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9025 seconds and 0.4091 seconds precompiling for 25 choices
[rank7]:W1120 13:47:26.326000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:26.403000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:26.455000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:26.503000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:26.566000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:26.665000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:30.167000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:30.270000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:30.283000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:30.347000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:30.386000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:30.490000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:31.375000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:31.450000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:31.593000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:32.207000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:32.282000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:32.423000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:32.699000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:32.778000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:32.947000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:33.939000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:34.015000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:34.157000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:34.172000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:34.633000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:34.717000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:34.789000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:34.910000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:35.015000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:35.074000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:35.163000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:35.307000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:35.369000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:35.384000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:35.487000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:35.607000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:35.835000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:36.274000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:36.590000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:36.775000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:37.040000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:37.245000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:37.483000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:37.682000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:38.372000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:38.811000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:38.837000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:39.271000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:39.279000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:39.706000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:39.710000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:40.210000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:40.654000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_60 0.0327 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_50 0.0327 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_51 0.0329 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_61 0.0331 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_59 0.0491 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0509 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0516 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_68 0.0520 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_54 0.0594 ms 15.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0267 seconds and 0.2092 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_51 0.0323 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_50 0.0327 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_60 0.0336 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_61 0.0336 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_59 0.0507 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0509 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0516 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_68 0.0519 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_54 0.0602 ms 15.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0824 seconds and 0.2130 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_51 0.0289 ms 32.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_61 0.0306 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_60 0.0306 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_50 0.0317 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_59 0.0462 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0463 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0519 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0519 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0540 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0180 seconds and 0.2145 seconds precompiling for 25 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_61 0.0305 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_60 0.0306 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_51 0.0314 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_50 0.0329 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_59 0.0465 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0468 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0518 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0518 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0543 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0036 seconds and 0.2186 seconds precompiling for 25 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_50 0.0327 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_51 0.0334 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_60 0.0335 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_61 0.0342 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_68 0.0498 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_59 0.0503 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_69 0.0508 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0511 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_54 0.0613 ms 15.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0768 seconds and 0.2176 seconds precompiling for 25 choices
[rank0]:W1120 13:47:44.185000 275 torch/_dynamo/variables/builtin.py:1091] [78/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7745647fc0>
[rank0]:W1120 13:47:44.218000 275 torch/_dynamo/variables/builtin.py:1091] [79/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7745647cc0>
[aiter] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:47:44 TP0] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[rank3]:W1120 13:47:44.810000 278 torch/_dynamo/variables/builtin.py:1091] [78/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f109c633c60>
[rank3]:W1120 13:47:44.843000 278 torch/_dynamo/variables/builtin.py:1091] [79/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f109c633360>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:47:44 TP3] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_51 0.0319 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_50 0.0332 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_61 0.0334 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_60 0.0334 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_59 0.0495 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0507 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0513 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_68 0.0518 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_54 0.0615 ms 15.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0211 seconds and 0.2121 seconds precompiling for 25 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1120 13:47:45.302000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_51 0.0296 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_61 0.0305 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_60 0.0305 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_50 0.0317 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_59 0.0461 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0462 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0514 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_68 0.0514 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_54 0.0540 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0421 seconds and 0.2122 seconds precompiling for 25 choices
[rank1]:W1120 13:47:45.681000 276 torch/_dynamo/variables/builtin.py:1091] [78/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ee62bcd01b0>
[rank1]:W1120 13:47:45.715000 276 torch/_dynamo/variables/builtin.py:1091] [79/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ee62bcd0b40>
[rank0]:W1120 13:47:45.798000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:47:45 TP1] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1120 13:47:45.903000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1120 13:47:46.230000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[rank3]:W1120 13:47:46.397000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1120 13:47:46.673000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:46.810000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:46.843000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:46.859000 277 torch/_dynamo/variables/builtin.py:1091] [78/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef3ce43f300>
[rank2]:W1120 13:47:46.893000 277 torch/_dynamo/variables/builtin.py:1091] [79/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef3d0088ae0>
[rank4]:W1120 13:47:46.985000 279 torch/_dynamo/variables/builtin.py:1091] [78/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f99e78db390>
[aiter] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:47:47 TP2] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1120 13:47:47.019000 279 torch/_dynamo/variables/builtin.py:1091] [79/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f99e777bd80>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:47:47 TP4] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1120 13:47:47.161000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1120 13:47:47.278000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:47.302000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[rank0]:W1120 13:47:47.601000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:47.741000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:47.766000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:47.955000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:48.037000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:48.079000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:48.190000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:48.214000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_51 0.0303 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_61 0.0308 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_60 0.0309 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_50 0.0334 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_58 0.0464 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_59 0.0465 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_68 0.0509 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0510 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0541 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 7.0379 seconds and 0.0001 seconds precompiling for 25 choices
[rank6]:W1120 13:47:48.356000 281 torch/_dynamo/variables/builtin.py:1091] [78/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f400d427c90>
[rank6]:W1120 13:47:48.390000 281 torch/_dynamo/variables/builtin.py:1091] [79/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f400d427f30>
[rank2]:W1120 13:47:48.461000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:48.481000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:47:48 TP6] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1120 13:47:48.566000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:48.627000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:48.661000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1120 13:47:48.895000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:48.918000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[rank4]:W1120 13:47:49.002000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:49.109000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:49.119000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:49.336000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:49.354000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:49.438000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:49.443000 282 torch/_dynamo/variables/builtin.py:1091] [78/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f04a5e83810>
[rank6]:W1120 13:47:49.462000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:49.477000 282 torch/_dynamo/variables/builtin.py:1091] [79/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f04a810d8f0>
[rank3]:W1120 13:47:49.545000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:49.554000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:47:49 TP7] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1120 13:47:49.771000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:49.790000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1120 13:47:49.938000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:49.971000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:49.995000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:50.012000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[rank0]:W1120 13:47:50.238000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:50.263000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:50.374000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:50.406000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:50.435000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:50.447000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:50.554000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:50.669000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:50.694000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:50.815000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:50.843000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:50.870000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:50.886000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:51.055000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:51.121000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:51.161000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:51.250000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:51.282000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:51.306000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:51.328000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:51.495000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:51.555000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:51.602000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:51.687000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:51.711000 280 torch/_dynamo/variables/builtin.py:1091] [78/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1400d33480>
[rank1]:W1120 13:47:51.771000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:51.780000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:51.792000 280 torch/_dynamo/variables/builtin.py:1091] [79/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1402f60ae0>
[rank3]:W1120 13:47:51.798000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:47:51 TP5] [fused_moe] using default for (16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1120 13:47:51.939000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:51.993000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:52.037000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1120 13:47:52.127000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:52.226000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:52.231000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:52.250000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[rank7]:W1120 13:47:52.445000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:52.450000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:52.474000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:52.562000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:52.666000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:52.675000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:52.686000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:52.807000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:52.882000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:52.895000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:52.913000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:52.999000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:53.123000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:53.130000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:53.169000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:53.319000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:53.341000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:53.347000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:53.352000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:53.435000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:53.568000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:53.573000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:53.607000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:53.754000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:53.787000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:53.795000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:53.799000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:53.935000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:54.014000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:54.020000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:54.050000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:54.222000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:54.239000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:54.243000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:54.253000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:54.375000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:54.462000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:54.468000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:54.489000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:54.659000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:54.683000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:54.688000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:54.694000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:54.811000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:54.900000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:54.912000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:54.941000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:55.095000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:55.127000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:55.130000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:55.137000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:55.247000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:55.334000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:55.355000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:55.378000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:55.534000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:55.576000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:55.579000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:55.633000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:55.683000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:55.769000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:55.818000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:55.860000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:55.975000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:56.019000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:56.023000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:56.085000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:56.123000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:56.266000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:56.271000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:56.303000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:56.414000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:56.461000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:56.526000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:56.533000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:56.559000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:56.702000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:56.707000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:56.750000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:56.851000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:56.897000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:56.961000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:56.980000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:56.995000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:57.138000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:57.143000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:57.196000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:57.335000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:57.356000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:57.402000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:57.423000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:57.431000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:57.587000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:57.644000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:57.651000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:57.770000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:57.799000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:57.855000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:57.872000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:57.879000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:58.022000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:58.088000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:58.102000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:58.210000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:58.243000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:58.299000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:58.332000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:58.374000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:58.463000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:58.531000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:58.547000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:58.678000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:58.711000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:58.750000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:58.775000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:58.810000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:58.899000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:58.976000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:58.991000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:59.118000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:59.147000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:59.188000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:59.219000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:59.246000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:59.339000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:59.420000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:59.432000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:59.555000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:47:59.582000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:47:59.621000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:47:59.663000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:47:59.683000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:47:59.773000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:47:59.863000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:47:59.875000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:47:59.999000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:00.017000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:00.057000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:00.111000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:00.119000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:00.210000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:00.329000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:00.375000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:00.439000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:00.461000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:00.555000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:00.560000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:00.587000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:00.658000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:00.786000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:00.824000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:00.875000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:00.898000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:01.003000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:01.045000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:01.071000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:01.170000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:01.229000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:01.267000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:01.311000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:01.333000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:01.447000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:01.485000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:01.519000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:01.610000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:01.669000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:01.711000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:01.769000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:01.820000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:01.891000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:01.925000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:01.967000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:02.050000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:02.109000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:02.155000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:02.206000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:02.267000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:02.335000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:02.382000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:02.420000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:02.494000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:02.603000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:02.658000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:02.666000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:02.707000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:02.779000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:02.825000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:02.863000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:02.946000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:03.051000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:03.105000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:03.110000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:03.151000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:03.270000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:03.295000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:03.307000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:03.396000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:03.499000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:03.558000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:03.591000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:03.625000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:03.713000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:03.738000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:03.751000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:03.838000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:03.943000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:04.002000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:04.027000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:04.062000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:04.157000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:04.183000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:04.199000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:04.286000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:04.396000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:04.446000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:04.463000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:04.510000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:04.598000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:04.631000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:04.643000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:04.734000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:04.839000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:04.899000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:04.910000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:04.957000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:05.041000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:05.083000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:05.091000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:05.170000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:05.339000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:05.355000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:05.363000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:05.394000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:05.485000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:05.527000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:05.535000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:05.606000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:05.779000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:05.802000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:05.811000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:05.834000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:05.974000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:06.023000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:06.051000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:06.064000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:06.252000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:06.259000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:06.280000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:06.294000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:06.418000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:06.478000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:06.515000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:06.590000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:06.698000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:06.711000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:06.728000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:06.739000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:06.863000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:06.932000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:06.964000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:07.042000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:07.158000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:07.164000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:07.176000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:07.183000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:07.307000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:07.387000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:07.407000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:07.482000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:07.602000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:07.620000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:07.624000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:07.632000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:07.751000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:07.840000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:07.853000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:07.922000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:08.071000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:08.075000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:08.083000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:08.138000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:08.203000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:08.287000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:08.304000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:08.366000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:08.519000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:08.525000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:08.531000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:08.582000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:08.740000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:08.750000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:08.757000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:08.811000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:08.979000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:08.985000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:09.034000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:09.046000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:09.195000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:09.208000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:09.211000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:09.255000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:09.428000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:09.439000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:09.479000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:09.486000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:09.647000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:09.659000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:09.669000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:09.710000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:09.879000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:09.891000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:09.934000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:09.939000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:10.127000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:10.155000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:10.378000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:10.383000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:10.497000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:10.499000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:10.573000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:10.598000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:10.719000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:10.731000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:10.824000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:10.829000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:10.947000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:10.955000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:11.047000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:11.165000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:11.275000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:11.281000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:11.286000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:11.395000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:11.416000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:11.616000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:11.735000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:11.747000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:11.753000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:11.847000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:12.004000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:12.155000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:12.195000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:12.207000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:12.219000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:12.295000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:12.476000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:12.602000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:12.646000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:12.651000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:12.738000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:12.931000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:13.051000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:13.100000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:13.105000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:13.187000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:13.389000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:13.514000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:13.563000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:13.634000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:13.843000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:13.967000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:14.019000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:14.303000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:14.419000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:14.479000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:14.759000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:14.866000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:14.935000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:15.215000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:15.314000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:15.391000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:15.671000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:15.767000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:16.130000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:16.223000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:16.599000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:16.671000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_75 0.1036 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_74 0.1037 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_84 0.1046 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_85 0.1046 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_92 0.1092 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_93 0.1092 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_82 0.1128 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_83 0.1128 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_79 0.1478 ms 41.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0340 seconds and 0.1707 seconds precompiling for 25 choices
[rank5]:W1120 13:48:17.120000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_74 0.1037 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_75 0.1037 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_85 0.1047 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_84 0.1047 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_92 0.1094 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_93 0.1094 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_82 0.1132 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_83 0.1132 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_78 0.1484 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1179 seconds and 0.1782 seconds precompiling for 25 choices
[rank5]:W1120 13:48:17.679000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:18.131000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_74 0.1036 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_75 0.1037 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_84 0.1044 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_85 0.1045 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1098 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_92 0.1099 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_83 0.1132 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_82 0.1134 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_78 0.1483 ms 41.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0802 seconds and 0.1780 seconds precompiling for 25 choices
[rank5]:W1120 13:48:18.583000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_74 0.1035 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_75 0.1035 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_84 0.1046 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_85 0.1047 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_92 0.1091 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_93 0.1092 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_82 0.1122 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_83 0.1125 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_79 0.1474 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9971 seconds and 0.1812 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_74 0.1035 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_75 0.1036 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_84 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_85 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_92 0.1097 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_93 0.1097 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_82 0.1131 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_83 0.1132 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_79 0.1478 ms 41.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1104 seconds and 0.1772 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0616 ms 100.0% 
  triton_mm_74 0.1034 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_75 0.1035 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_84 0.1043 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_85 0.1043 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_92 0.1101 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_93 0.1102 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_83 0.1137 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_82 0.1138 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_78 0.1486 ms 41.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0276 seconds and 0.1766 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_74 0.1005 ms 60.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_75 0.1006 ms 60.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_84 0.1015 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_85 0.1016 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1064 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_92 0.1064 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_83 0.1098 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_82 0.1098 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_95 0.1391 ms 44.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 6.9905 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0617 ms 100.0% 
  triton_mm_75 0.1036 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_74 0.1036 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_84 0.1042 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_85 0.1043 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1100 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_92 0.1100 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_83 0.1143 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_82 0.1145 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_94 0.1487 ms 41.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 7.0338 seconds and 0.0001 seconds precompiling for 25 choices
Capturing batches (bs=16 avail_mem=59.47 GB):  17%|        | 1/6 [01:41<08:25, 101.17s/it]Capturing batches (bs=12 avail_mem=58.42 GB):  17%|        | 1/6 [01:41<08:25, 101.17s/it][rank6]:W1120 13:48:32.670000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:32.748000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:32.760000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:32.766000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:32.837000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:32.843000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:32.844000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:32.851000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:32.921000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:32.938000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:32.944000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:33.021000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:33.040000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:33.052000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:33.116000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:33.127000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:33.215000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:33.227000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:33.530000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:33.564000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:33.607000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:33.639000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:33.709000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:33.738000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:35.353000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:35.405000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:35.425000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:35.518000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:35.698000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:35.708000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:36.183000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:36.220000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:37.418000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:37.495000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:37.525000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:37.594000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:37.597000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:37.601000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:37.669000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:37.699000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:37.769000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:37.782000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:37.803000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:37.856000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:37.877000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:37.954000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:37.975000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:38.261000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:38.335000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:38.433000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:38.447000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:38.521000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:38.619000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:39.131000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:39.207000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:39.257000 281 torch/_inductor/utils.py:1349] [38/1] Please pip install Composable Kernel package
[rank5]:W1120 13:48:39.306000 280 torch/_inductor/utils.py:1349] [38/1] Please pip install Composable Kernel package
[rank7]:W1120 13:48:39.306000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:39.343000 279 torch/_inductor/utils.py:1349] [38/1] Please pip install Composable Kernel package
[rank3]:W1120 13:48:39.399000 278 torch/_inductor/utils.py:1349] [38/1] Please pip install Composable Kernel package
[rank2]:W1120 13:48:39.411000 277 torch/_inductor/utils.py:1349] [38/1] Please pip install Composable Kernel package
[rank1]:W1120 13:48:39.879000 276 torch/_inductor/utils.py:1349] [38/1] Please pip install Composable Kernel package
[rank0]:W1120 13:48:40.056000 275 torch/_inductor/utils.py:1349] [38/1] Please pip install Composable Kernel package
[rank7]:W1120 13:48:40.845000 282 torch/_inductor/utils.py:1349] [38/1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_99 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_110 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_111 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_100 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_101 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_105 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_106 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_107 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_108 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7861 seconds and 0.1544 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_115 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_101 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_112 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_116 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_117 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_118 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_96 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_99 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9577 seconds and 0.1349 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  bmm 0.0067 ms 100.0% 
  triton_bmm_114 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_98 0.0068 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_99 0.0068 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_100 0.0068 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_107 0.0068 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_110 0.0068 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_112 0.0068 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_113 0.0068 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_115 0.0068 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9704 seconds and 0.1324 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_103 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_110 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_112 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_100 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_109 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_113 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_115 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_116 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9185 seconds and 0.1316 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_110 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_111 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_116 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_117 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_119 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_100 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_109 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_114 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9180 seconds and 0.1299 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_107 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_110 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_119 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_97 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_100 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_105 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_106 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_108 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_111 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9919 seconds and 0.1291 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_100 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_96 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_107 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_97 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_99 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_103 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_104 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_106 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_102 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8739 seconds and 0.1460 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_101 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_107 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_108 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_113 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_116 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0066 ms 99.4% 
  triton_bmm_96 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_97 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_98 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_99 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9493 seconds and 0.1302 seconds precompiling for 25 choices
[rank5]:W1120 13:48:49.302000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:49.335000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:49.344000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:49.747000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:49.800000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:49.829000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:49.840000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:49.878000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:50.259000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:50.343000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:50.378000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:50.392000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:48:50.836000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:48:50.888000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:50.950000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:51.472000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:52.244000 278 torch/_inductor/utils.py:1349] [50/1_1] Please pip install Composable Kernel package
[rank6]:W1120 13:48:52.277000 281 torch/_inductor/utils.py:1349] [50/1_1] Please pip install Composable Kernel package
[rank5]:W1120 13:48:52.284000 280 torch/_inductor/utils.py:1349] [50/1_1] Please pip install Composable Kernel package
[rank1]:W1120 13:48:52.298000 276 torch/_inductor/utils.py:1349] [50/1_1] Please pip install Composable Kernel package
[rank2]:W1120 13:48:52.519000 277 torch/_inductor/utils.py:1349] [50/1_1] Please pip install Composable Kernel package
[rank7]:W1120 13:48:53.456000 282 torch/_inductor/utils.py:1349] [50/1_1] Please pip install Composable Kernel package
[rank0]:W1120 13:48:54.738000 275 torch/_inductor/utils.py:1349] [50/1_1] Please pip install Composable Kernel package
[rank4]:W1120 13:48:55.064000 279 torch/_inductor/utils.py:1349] [50/1_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_123 0.0067 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_122 0.0067 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_133 0.0069 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0069 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_131 0.0072 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_130 0.0072 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_126 0.0080 ms 82.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_127 0.0081 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_143 0.0083 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8164 seconds and 0.4329 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_123 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_122 0.0068 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_133 0.0069 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0069 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_130 0.0071 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_131 0.0071 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_127 0.0081 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_126 0.0081 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_143 0.0083 ms 79.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8575 seconds and 0.4073 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_123 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_122 0.0068 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_132 0.0070 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_133 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_131 0.0077 ms 83.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_130 0.0079 ms 82.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_126 0.0082 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_127 0.0082 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_142 0.0087 ms 75.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9072 seconds and 0.4207 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_123 0.0067 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_122 0.0069 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_132 0.0071 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_133 0.0071 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_131 0.0073 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_130 0.0074 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_126 0.0081 ms 81.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_127 0.0081 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_142 0.0084 ms 78.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8277 seconds and 0.4469 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_122 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_123 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_133 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_130 0.0071 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_131 0.0071 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_126 0.0079 ms 81.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_127 0.0079 ms 81.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_124 0.0083 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7893 seconds and 0.4275 seconds precompiling for 25 choices
[rank5]:W1120 13:48:58.510000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:58.516000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:58.542000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:58.556000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:58.585000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:58.592000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:58.617000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:58.630000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:48:58.634000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:48:58.641000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:48:58.665000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:48:58.678000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:58.707000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_122 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_123 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_133 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_130 0.0071 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_131 0.0072 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_126 0.0080 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_127 0.0082 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_143 0.0083 ms 77.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8054 seconds and 0.3952 seconds precompiling for 25 choices
[rank2]:W1120 13:48:58.781000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:48:58.828000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:59.650000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:59.726000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:48:59.774000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_122 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_123 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_132 0.0071 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_133 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_131 0.0077 ms 82.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_130 0.0078 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_127 0.0081 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_126 0.0081 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_142 0.0084 ms 76.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7446 seconds and 0.4130 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_122 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_123 0.0068 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_132 0.0072 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_133 0.0072 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_131 0.0079 ms 82.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_130 0.0079 ms 81.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_126 0.0081 ms 79.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_127 0.0081 ms 79.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_143 0.0087 ms 74.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8506 seconds and 0.3942 seconds precompiling for 25 choices
[rank0]:W1120 13:49:00.871000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:00.946000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:00.993000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:01.265000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:01.340000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:01.389000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:05.798000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:05.823000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:05.876000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:05.885000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:05.901000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:05.961000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:05.979000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:06.005000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:06.064000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:06.477000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:06.554000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:06.655000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:06.754000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:06.831000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:06.931000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:07.710000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:07.788000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:07.892000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:08.354000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:08.432000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:08.533000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:09.311000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:09.387000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:09.489000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:10.400000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:10.448000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:10.476000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:10.534000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:10.686000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:10.870000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:10.919000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:10.943000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:11.006000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:11.152000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:11.339000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:11.387000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:11.410000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:11.487000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:11.627000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:11.657000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:12.137000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:12.631000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:13.055000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:13.547000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:13.690000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:14.007000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:14.158000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:14.627000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_147 0.0292 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_156 0.0306 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_157 0.0306 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_146 0.0329 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_155 0.0461 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_154 0.0463 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_164 0.0513 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_165 0.0514 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_151 0.0541 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0237 seconds and 0.2389 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_147 0.0325 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_146 0.0333 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_156 0.0334 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_157 0.0338 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_164 0.0506 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_165 0.0518 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_154 0.0526 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_155 0.0527 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_151 0.0608 ms 15.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1177 seconds and 0.2209 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_147 0.0292 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_156 0.0305 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_157 0.0305 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_146 0.0306 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_154 0.0460 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_155 0.0462 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_165 0.0513 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_164 0.0514 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_150 0.0540 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0185 seconds and 0.2398 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_146 0.0331 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_147 0.0331 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_156 0.0334 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_157 0.0337 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_154 0.0499 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_164 0.0503 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_165 0.0506 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_155 0.0507 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_151 0.0622 ms 15.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0290 seconds and 0.2464 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_147 0.0299 ms 31.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_157 0.0307 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_156 0.0307 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_146 0.0320 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_155 0.0459 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_154 0.0459 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_164 0.0512 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_165 0.0512 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_150 0.0538 ms 17.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0303 seconds and 0.2430 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_147 0.0294 ms 31.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_156 0.0303 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_157 0.0304 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_146 0.0314 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_155 0.0459 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_154 0.0459 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_165 0.0514 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_164 0.0514 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_150 0.0542 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0197 seconds and 0.2129 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_147 0.0323 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_146 0.0329 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_157 0.0333 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_156 0.0335 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_164 0.0497 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_154 0.0502 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_155 0.0504 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_165 0.0516 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_151 0.0599 ms 15.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0607 seconds and 0.2112 seconds precompiling for 25 choices
[rank3]:W1120 13:49:20.059000 278 torch/_dynamo/variables/builtin.py:1091] [77/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f109c633c60>
[rank5]:W1120 13:49:20.073000 280 torch/_dynamo/variables/builtin.py:1091] [77/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1400d33480>
[rank3]:W1120 13:49:20.081000 278 torch/_dynamo/variables/builtin.py:1091] [79/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f109c633360>
[rank2]:W1120 13:49:20.086000 277 torch/_dynamo/variables/builtin.py:1091] [77/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef3ce43f300>
[rank5]:W1120 13:49:20.095000 280 torch/_dynamo/variables/builtin.py:1091] [79/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1402f60ae0>
[rank2]:W1120 13:49:20.109000 277 torch/_dynamo/variables/builtin.py:1091] [79/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef3d0088ae0>
[aiter] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:49:20 TP3] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:49:20 TP5] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1120 13:49:20.167000 276 torch/_dynamo/variables/builtin.py:1091] [77/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ee62bcd01b0>
[aiter] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:49:20 TP2] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1120 13:49:20.189000 276 torch/_dynamo/variables/builtin.py:1091] [79/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ee62bcd0b40>
[rank6]:W1120 13:49:20.232000 281 torch/_dynamo/variables/builtin.py:1091] [77/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f400d427c90>
[rank6]:W1120 13:49:20.254000 281 torch/_dynamo/variables/builtin.py:1091] [79/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f400d427f30>
[aiter] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:49:20 TP1] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:49:20 TP6] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_146 0.0333 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_147 0.0335 ms 27.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_156 0.0337 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_157 0.0339 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_165 0.0492 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_164 0.0494 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_155 0.0501 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_154 0.0503 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_150 0.0608 ms 15.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0571 seconds and 0.2137 seconds precompiling for 25 choices
[rank5]:W1120 13:49:20.764000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:20.874000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:20.908000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:20.922000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:20.932000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:21.230000 282 torch/_dynamo/variables/builtin.py:1091] [77/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f04a5e83810>
[rank5]:W1120 13:49:21.232000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:21.253000 282 torch/_dynamo/variables/builtin.py:1091] [79/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f04a810d8f0>
[aiter] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:49:21 TP7] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1120 13:49:21.383000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:21.395000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:21.407000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:21.491000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:21.855000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:21.863000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:21.869000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:21.963000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:22.055000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:22.091000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:22.323000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:22.340000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:22.345000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:22.442000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:22.472000 275 torch/_dynamo/variables/builtin.py:1091] [77/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7745647fc0>
[rank0]:W1120 13:49:22.495000 275 torch/_dynamo/variables/builtin.py:1091] [79/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7745647cc0>
[rank6]:W1120 13:49:22.540000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:49:22 TP0] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1120 13:49:22.578000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:22.803000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:22.816000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:22.823000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:22.911000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:23.015000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:23.060000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:23.067000 279 torch/_dynamo/variables/builtin.py:1091] [77/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f99e78db390>
[rank4]:W1120 13:49:23.089000 279 torch/_dynamo/variables/builtin.py:1091] [79/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f99e777bd80>
[aiter] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:49:23 TP4] [fused_moe] using default for (12, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1120 13:49:23.159000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:23.280000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:23.288000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:23.304000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:23.395000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:23.504000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:23.547000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:23.752000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:23.763000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:23.775000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:23.781000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:23.876000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:23.911000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:23.990000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:24.032000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:24.223000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:24.235000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:24.243000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:24.256000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:24.347000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:24.388000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:24.475000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:24.508000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:24.690000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:24.714000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:24.718000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:24.726000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:24.820000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:24.863000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:24.959000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:24.984000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:25.163000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:25.187000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:25.201000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:25.220000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:25.299000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:25.339000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:25.441000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:25.460000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:25.634000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:25.663000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:25.668000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:25.687000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:25.766000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:25.815000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:25.922000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:25.944000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:26.116000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:26.139000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:26.148000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:26.172000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:26.240000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:26.291000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:26.399000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:26.428000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:26.588000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:26.615000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:26.620000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:26.643000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:26.708000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:26.767000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:26.879000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:26.912000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:27.056000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:27.091000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:27.096000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:27.116000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:27.175000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:27.243000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:27.355000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:27.396000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:27.524000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:27.563000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:27.576000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:27.583000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:27.643000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:27.719000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:27.836000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:27.884000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:27.996000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:28.027000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:28.051000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:28.057000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:28.125000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:28.195000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:28.315000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:28.368000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:28.468000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:28.498000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:28.518000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:28.536000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:28.609000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:28.672000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:28.796000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:28.857000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:28.944000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:28.999000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:29.011000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:29.102000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:29.124000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:29.147000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:29.275000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:29.343000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:29.478000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:29.492000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:29.562000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:29.580000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:29.598000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:29.616000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:29.756000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:29.820000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:29.959000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:29.973000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:30.040000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:30.076000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:30.087000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:30.223000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:30.235000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:30.300000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:30.448000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:30.527000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:30.551000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:30.564000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:30.620000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:30.707000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:30.900000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:30.939000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:30.963000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:31.012000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:31.019000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:31.039000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:31.095000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:31.191000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:31.383000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:31.411000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:31.447000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:31.492000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:31.500000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:31.519000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:31.571000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:31.667000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:31.867000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:31.883000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:31.931000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:31.960000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:31.968000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:32.051000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:32.152000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:32.163000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:32.359000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:32.415000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:32.443000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:32.452000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:32.515000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:32.527000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:32.631000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:32.643000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:32.847000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:32.903000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:32.923000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:32.931000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:33.004000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:33.011000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:33.107000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:33.119000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:33.327000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:33.387000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:33.392000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:33.400000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:33.480000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:33.487000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:33.583000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:33.595000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:33.803000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:33.867000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:33.874000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:33.880000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:33.956000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:33.963000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:34.064000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:34.071000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:34.279000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:34.335000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:34.355000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:34.362000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:34.432000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:34.439000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:34.540000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:34.547000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:34.755000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:34.811000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:34.827000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:34.843000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:34.907000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:34.915000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:35.024000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:35.030000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:35.232000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:35.282000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:35.306000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:35.328000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:35.378000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:35.396000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:35.499000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:35.519000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:35.716000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:35.754000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:35.778000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:35.812000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:35.853000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:35.872000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:35.975000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:36.003000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:36.191000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:36.227000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:36.251000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:36.295000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:36.331000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:36.347000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:36.451000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:36.491000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:36.672000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:36.706000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:36.739000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:36.780000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:36.798000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:36.824000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:36.927000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:36.974000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:37.165000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:37.182000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:37.222000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:37.268000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:37.285000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:37.303000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:37.406000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:37.450000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:37.648000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:37.662000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:37.698000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:37.757000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:37.774000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:37.781000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:37.884000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:37.936000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:38.135000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:38.178000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:38.239000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:38.260000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:38.266000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:38.312000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:38.363000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:38.419000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:38.624000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:38.725000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:38.739000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:38.745000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:38.803000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:38.835000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:38.841000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:38.919000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:39.108000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:39.208000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:39.218000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:39.227000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:39.279000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:39.315000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:39.323000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:39.570000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:39.592000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:39.696000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:39.702000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:39.760000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:39.800000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:39.806000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:39.895000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:40.060000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:40.191000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:40.236000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:40.271000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:40.284000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:40.290000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:40.375000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:40.380000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:40.543000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:40.679000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:40.719000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:40.759000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:40.767000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:40.773000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:40.864000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:40.872000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:41.024000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:41.158000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:41.194000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:41.246000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:41.256000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:41.352000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:41.364000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:41.447000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:41.507000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:41.675000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:41.726000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:41.744000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:41.815000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:41.835000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:41.855000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:41.931000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:41.995000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:42.164000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:42.215000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:42.232000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:42.295000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:42.324000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:42.340000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:42.420000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:42.490000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:42.638000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:42.690000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:42.716000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:42.786000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:42.808000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:42.824000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:42.903000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:42.971000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:43.114000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:43.170000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:43.208000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:43.270000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:43.299000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:43.312000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:43.387000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:43.462000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:43.594000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:43.658000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:43.693000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:43.754000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:43.783000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:43.796000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:43.871000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:43.947000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:44.076000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:44.136000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:44.179000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:44.246000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:44.268000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:44.280000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:44.352000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:44.442000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:44.554000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:44.614000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:44.664000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:44.722000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:44.751000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:44.764000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:44.831000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:44.926000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:45.042000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:45.102000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:45.148000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:45.198000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:45.239000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:45.256000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:45.311000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:45.415000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:45.524000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:45.587000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:45.635000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:45.679000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:45.731000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:45.739000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:45.791000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:45.894000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:45.999000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:46.062000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:46.124000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:46.154000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:46.215000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:46.224000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:46.275000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:46.378000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:46.474000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:46.538000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:46.608000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:46.634000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:46.699000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:46.713000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:46.759000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:46.862000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:46.964000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:47.023000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:47.091000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:47.115000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:47.187000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:47.204000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:47.242000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:47.343000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:47.439000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:47.499000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:47.587000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:47.598000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:47.675000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:47.696000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:47.723000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:47.830000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:49:47.926000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:49:47.990000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:48.075000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:48.086000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:49:48.164000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:48.189000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:48.208000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:49:48.327000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:49:48.564000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:48.573000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:48.676000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:48.692000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:49.067000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:49.177000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:49.188000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:49.594000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:49:49.684000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:49.690000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:50.086000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:50.176000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:49:50.573000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:50.660000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:49:51.147000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_171 0.1041 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_180 0.1041 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.1041 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_181 0.1042 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_188 0.1087 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_189 0.1088 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_178 0.1119 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.1121 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_175 0.1443 ms 42.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0996 seconds and 0.1950 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_171 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_181 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_170 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_180 0.1047 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.1091 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_189 0.1091 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_179 0.1130 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_178 0.1130 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_174 0.1451 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0945 seconds and 0.2020 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_180 0.1042 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_181 0.1043 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_170 0.1043 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_171 0.1044 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_188 0.1090 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_189 0.1092 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_178 0.1126 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.1128 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_174 0.1450 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2703 seconds and 0.1936 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0617 ms 100.0% 
  triton_mm_170 0.1043 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_171 0.1044 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_181 0.1044 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_180 0.1045 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.1093 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_189 0.1094 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_179 0.1131 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_178 0.1132 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_174 0.1452 ms 42.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2293 seconds and 0.1872 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0616 ms 100.0% 
  triton_mm_171 0.1042 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_181 0.1042 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_180 0.1042 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.1043 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_189 0.1093 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_188 0.1094 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.1128 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_178 0.1128 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_174 0.1450 ms 42.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0704 seconds and 0.2115 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_170 0.1011 ms 60.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_171 0.1011 ms 60.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_181 0.1015 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_180 0.1015 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_189 0.1059 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_188 0.1061 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.1088 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.1088 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_174 0.1374 ms 44.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0182 seconds and 0.1834 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_170 0.1044 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_171 0.1045 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_181 0.1046 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_180 0.1047 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.1095 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_189 0.1098 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_178 0.1126 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.1126 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_175 0.1448 ms 42.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2157 seconds and 0.1838 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_170 0.1043 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_171 0.1044 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_180 0.1045 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_181 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.1100 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_188 0.1101 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.1127 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.1132 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_174 0.1448 ms 42.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0637 seconds and 0.1922 seconds precompiling for 25 choices
Capturing batches (bs=12 avail_mem=58.42 GB):  33%|      | 2/6 [03:12<06:20, 95.25s/it] Capturing batches (bs=8 avail_mem=57.71 GB):  33%|      | 2/6 [03:12<06:20, 95.25s/it] [rank2]:W1120 13:50:03.873000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:03.893000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:03.951000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:03.974000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:03.985000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:04.051000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:04.051000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:04.068000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:04.080000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:04.134000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:04.137000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:04.159000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:04.179000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:04.216000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:04.238000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:04.242000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:04.319000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:04.340000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:04.558000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:04.635000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:04.738000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:05.210000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:05.289000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:05.390000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:05.798000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:05.825000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:05.926000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:05.989000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:06.075000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:06.094000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:06.476000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:07.125000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:07.356000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:07.433000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:07.500000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:07.535000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:07.576000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:07.597000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:07.611000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:07.672000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:07.675000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:07.686000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:07.770000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:07.786000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:07.864000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:07.939000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:08.037000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:08.218000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:08.232000 282 torch/_inductor/utils.py:1349] [38/2] Please pip install Composable Kernel package
[rank0]:W1120 13:50:08.295000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:08.365000 280 torch/_inductor/utils.py:1349] [38/2] Please pip install Composable Kernel package
[rank0]:W1120 13:50:08.404000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:08.455000 277 torch/_inductor/utils.py:1349] [38/2] Please pip install Composable Kernel package
[rank1]:W1120 13:50:08.477000 276 torch/_inductor/utils.py:1349] [38/2] Please pip install Composable Kernel package
[rank3]:W1120 13:50:08.739000 278 torch/_inductor/utils.py:1349] [38/2] Please pip install Composable Kernel package
[rank6]:W1120 13:50:08.816000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:08.898000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:08.944000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:09.002000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:09.021000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:09.120000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:09.148000 275 torch/_inductor/utils.py:1349] [38/2] Please pip install Composable Kernel package
[rank6]:W1120 13:50:09.699000 281 torch/_inductor/utils.py:1349] [38/2] Please pip install Composable Kernel package
[rank4]:W1120 13:50:09.811000 279 torch/_inductor/utils.py:1349] [38/2] Please pip install Composable Kernel package
AUTOTUNE bmm(16x8x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_212 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_214 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_194 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_195 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_196 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_199 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_200 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_201 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_202 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8374 seconds and 0.1553 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_196 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_207 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_211 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_214 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_195 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_199 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_203 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_205 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_210 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7927 seconds and 0.1387 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_209 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_205 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_200 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_201 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_208 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_192 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_196 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_206 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_207 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8101 seconds and 0.1541 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_209 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_205 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_208 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_200 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_201 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_206 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_207 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_211 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_212 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8436 seconds and 0.1513 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_205 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_207 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_194 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_199 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_204 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_196 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_197 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_206 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_208 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8823 seconds and 0.1549 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_202 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_206 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_198 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_204 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_205 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_208 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_192 0.0067 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_197 0.0067 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_199 0.0067 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8701 seconds and 0.1440 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  bmm 0.0065 ms 100.0% 
  triton_bmm_194 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_196 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_197 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_203 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_205 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_206 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_192 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_193 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_195 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.7991 seconds and 0.1523 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  bmm 0.0065 ms 100.0% 
  triton_bmm_192 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_193 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_194 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_197 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_198 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_199 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_204 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_206 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_207 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8734 seconds and 0.1317 seconds precompiling for 25 choices
[rank7]:W1120 13:50:18.147000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:18.199000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:18.650000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:18.698000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:18.799000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:18.811000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:18.854000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:19.102000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:19.300000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:19.312000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:19.351000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:19.563000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:19.605000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:19.895000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:20.064000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:20.396000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:20.940000 282 torch/_inductor/utils.py:1349] [50/2_1] Please pip install Composable Kernel package
[rank5]:W1120 13:50:20.945000 280 torch/_inductor/utils.py:1349] [50/2_1] Please pip install Composable Kernel package
[rank1]:W1120 13:50:21.233000 276 torch/_inductor/utils.py:1349] [50/2_1] Please pip install Composable Kernel package
[rank0]:W1120 13:50:21.541000 275 torch/_inductor/utils.py:1349] [50/2_1] Please pip install Composable Kernel package
[rank3]:W1120 13:50:21.551000 278 torch/_inductor/utils.py:1349] [50/2_1] Please pip install Composable Kernel package
[rank4]:W1120 13:50:22.270000 279 torch/_inductor/utils.py:1349] [50/2_1] Please pip install Composable Kernel package
[rank2]:W1120 13:50:22.869000 277 torch/_inductor/utils.py:1349] [50/2_1] Please pip install Composable Kernel package
[rank6]:W1120 13:50:24.486000 281 torch/_inductor/utils.py:1349] [50/2_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_219 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_218 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_228 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_229 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_226 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_227 0.0072 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_222 0.0082 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_223 0.0082 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0083 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0852 seconds and 0.3968 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_219 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_218 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_228 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_229 0.0070 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_226 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_227 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_223 0.0080 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_222 0.0080 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_238 0.0083 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1088 seconds and 0.4010 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_218 0.0069 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_219 0.0069 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_228 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_229 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_227 0.0073 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_226 0.0074 ms 87.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_223 0.0080 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_222 0.0081 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_238 0.0084 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0051 seconds and 0.3985 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_218 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_219 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_229 0.0070 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_228 0.0070 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_226 0.0078 ms 83.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_227 0.0078 ms 83.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_222 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_223 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_239 0.0085 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1322 seconds and 0.4070 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_218 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_219 0.0068 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_228 0.0070 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_229 0.0070 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_226 0.0077 ms 83.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_227 0.0078 ms 83.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_222 0.0079 ms 81.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_223 0.0079 ms 81.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0085 ms 76.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1399 seconds and 0.4072 seconds precompiling for 25 choices
[rank7]:W1120 13:50:27.416000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:27.496000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:27.599000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:27.607000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:27.683000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:27.782000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_219 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_218 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_229 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_228 0.0070 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_227 0.0077 ms 82.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_226 0.0078 ms 82.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_223 0.0081 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_222 0.0083 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0084 ms 76.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0558 seconds and 0.3900 seconds precompiling for 25 choices
[rank5]:W1120 13:50:27.879000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:27.956000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:28.056000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:28.065000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:28.080000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:28.142000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:28.157000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:28.244000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:28.258000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_219 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_218 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_228 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_229 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_227 0.0071 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_226 0.0073 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_222 0.0077 ms 82.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_223 0.0078 ms 82.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_239 0.0082 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9613 seconds and 0.3997 seconds precompiling for 25 choices
[rank4]:W1120 13:50:28.697000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:28.774000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:28.874000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:29.192000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:29.268000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:29.368000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_218 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_219 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_228 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_229 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_227 0.0071 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_226 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_223 0.0080 ms 81.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_222 0.0080 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0082 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0710 seconds and 0.4235 seconds precompiling for 25 choices
[rank6]:W1120 13:50:30.974000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:31.054000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:31.157000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:33.520000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:33.597000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:33.646000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:33.852000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:33.904000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:33.911000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:33.929000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:33.971000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:33.979000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:33.982000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:33.988000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:34.033000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:34.037000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:34.048000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:34.098000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:34.939000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:35.014000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:35.063000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:35.164000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:35.240000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:35.290000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:36.994000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:37.063000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:37.371000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:37.436000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:37.449000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:37.474000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:37.518000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:37.529000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:37.546000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:37.582000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:37.860000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:37.932000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:37.966000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:38.003000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:38.042000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:38.344000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:38.432000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:38.495000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:38.732000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:38.775000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:39.229000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:39.288000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:39.720000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:39.784000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:41.048000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:41.531000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:42.023000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_243 0.0320 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_242 0.0321 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_252 0.0325 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_253 0.0327 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_251 0.0473 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_260 0.0490 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_261 0.0494 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_250 0.0502 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_246 0.0595 ms 15.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0499 seconds and 0.2126 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_243 0.0319 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_242 0.0321 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_252 0.0327 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_253 0.0330 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_251 0.0503 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_261 0.0503 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_260 0.0507 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_250 0.0508 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_246 0.0612 ms 15.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1546 seconds and 0.2181 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_243 0.0293 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_252 0.0301 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_253 0.0301 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_242 0.0317 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_251 0.0458 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_250 0.0460 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_260 0.0514 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_261 0.0514 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_246 0.0538 ms 17.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9889 seconds and 0.2347 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_243 0.0290 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_253 0.0298 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_252 0.0299 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_242 0.0320 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_250 0.0457 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_251 0.0458 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_261 0.0507 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_260 0.0508 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_246 0.0539 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0414 seconds and 0.2156 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_253 0.0299 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_252 0.0299 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_243 0.0324 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_242 0.0329 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_251 0.0457 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_250 0.0457 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_260 0.0509 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_261 0.0509 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_247 0.0540 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0881 seconds and 0.2076 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_243 0.0281 ms 33.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_252 0.0297 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_253 0.0297 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_242 0.0306 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_250 0.0455 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_251 0.0455 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_260 0.0508 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_261 0.0509 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_246 0.0539 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9964 seconds and 0.2220 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_242 0.0324 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_243 0.0326 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_252 0.0330 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_253 0.0331 ms 27.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_251 0.0488 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_261 0.0493 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_260 0.0503 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_250 0.0504 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_246 0.0579 ms 15.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0853 seconds and 0.2129 seconds precompiling for 25 choices
[rank3]:W1120 13:50:46.418000 278 torch/_dynamo/variables/builtin.py:1091] [77/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f109c633c60>
[rank3]:W1120 13:50:46.441000 278 torch/_dynamo/variables/builtin.py:1091] [79/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f109c633360>
[aiter] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:50:46 TP3] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1120 13:50:46.605000 275 torch/_dynamo/variables/builtin.py:1091] [77/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7745647fc0>
[rank0]:W1120 13:50:46.628000 275 torch/_dynamo/variables/builtin.py:1091] [79/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7745647cc0>
[aiter] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:50:46 TP0] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1120 13:50:46.786000 280 torch/_dynamo/variables/builtin.py:1091] [77/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1400d33480>
[rank5]:W1120 13:50:46.809000 280 torch/_dynamo/variables/builtin.py:1091] [79/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1402f60ae0>
[aiter] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:50:46 TP5] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1120 13:50:47.044000 282 torch/_dynamo/variables/builtin.py:1091] [77/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f04a5e83810>
[rank7]:W1120 13:50:47.067000 282 torch/_dynamo/variables/builtin.py:1091] [79/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f04a810d8f0>
[rank3]:W1120 13:50:47.120000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:50:47 TP7] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1120 13:50:47.169000 276 torch/_dynamo/variables/builtin.py:1091] [77/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ee62bcd01b0>
[rank1]:W1120 13:50:47.192000 276 torch/_dynamo/variables/builtin.py:1091] [79/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ee62bcd0b40>
[aiter] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:50:47 TP1] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1120 13:50:47.307000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:47.492000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:47.602000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:47.755000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:47.791000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:47.876000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_243 0.0321 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_242 0.0321 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_253 0.0329 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_252 0.0330 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_251 0.0491 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_250 0.0497 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_260 0.0505 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_261 0.0507 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_246 0.0575 ms 16.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0692 seconds and 0.2248 seconds precompiling for 25 choices
[rank5]:W1120 13:50:47.979000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:48.084000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:48.162000 277 torch/_dynamo/variables/builtin.py:1091] [77/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef3ce43f300>
[rank2]:W1120 13:50:48.184000 277 torch/_dynamo/variables/builtin.py:1091] [79/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef3d0088ae0>
[rank4]:W1120 13:50:48.185000 279 torch/_dynamo/variables/builtin.py:1091] [77/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f99e78db390>
[rank4]:W1120 13:50:48.208000 279 torch/_dynamo/variables/builtin.py:1091] [79/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f99e777bd80>
[rank7]:W1120 13:50:48.248000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:50:48 TP2] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:50:48 TP4] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1120 13:50:48.275000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:48.360000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:48.467000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:48.572000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:48.742000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:48.772000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:48.861000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:48.873000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:48.883000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:48.951000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:49.236000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:49.312000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:49.369000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:49.377000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:49.383000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:49.515000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:49.753000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:49.803000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:49.861000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:50.017000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:50.022000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:50.114000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:50.126000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:50.244000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:50.294000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:50.345000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:50.515000 281 torch/_dynamo/variables/builtin.py:1091] [77/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f400d427c90>
[rank7]:W1120 13:50:50.516000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:50.520000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:50.538000 281 torch/_dynamo/variables/builtin.py:1091] [79/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f400d427f30>
[rank2]:W1120 13:50:50.598000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:50:50 TP6] [fused_moe] using default for (8, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1120 13:50:50.614000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:50.744000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:50.782000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:51.021000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:51.030000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:51.082000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:51.098000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:51.111000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:51.228000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:51.235000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:51.294000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:51.523000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:51.538000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:51.576000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:51.584000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:51.604000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:51.719000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:51.727000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:51.783000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:52.019000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:52.032000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:52.060000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:52.071000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:52.092000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:52.211000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:52.222000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:52.278000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:52.513000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:52.523000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:52.535000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:52.556000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:52.575000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:52.712000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:52.768000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:52.980000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:53.012000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:53.022000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:53.030000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:53.048000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:53.070000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:53.208000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:53.268000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:53.471000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:53.503000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:53.516000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:53.524000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:53.532000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:53.556000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:53.699000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:53.769000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:53.964000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:53.996000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:54.003000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:54.009000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:54.024000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:54.039000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:54.192000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:54.267000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:54.460000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:54.487000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:54.495000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:54.505000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:54.512000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:54.536000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:54.683000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:54.769000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:54.952000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:54.980000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:54.994000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:55.001000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:55.008000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:55.035000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:55.180000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:55.262000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:55.444000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:55.476000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:55.482000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:55.491000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:55.498000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:55.518000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:55.671000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:55.750000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:55.939000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:55.971000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:55.977000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:55.986000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:55.992000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:56.008000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:56.163000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:56.255000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:56.431000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:56.471000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:56.477000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:56.485000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:56.492000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:56.509000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:56.655000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:56.759000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:56.923000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:56.967000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:56.976000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:56.982000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:56.991000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:57.008000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:57.147000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:57.268000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:57.415000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:57.463000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:57.469000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:57.476000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:57.487000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:57.512000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:57.643000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:57.759000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:57.907000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:57.956000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:57.961000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:57.968000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:57.975000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:58.004000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:58.135000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:58.249000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:58.399000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:58.447000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:58.452000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:58.461000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:58.467000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:58.495000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:58.628000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:58.738000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:58.896000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:58.944000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:58.950000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:58.957000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:58.962000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:58.982000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:59.120000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:59.237000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:59.388000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:59.439000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:59.447000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:59.452000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:59.467000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:59.482000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:50:59.611000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:50:59.731000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:50:59.879000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:50:59.931000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:50:59.943000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:50:59.952000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:50:59.967000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:50:59.991000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:00.104000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:00.234000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:00.372000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:00.420000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:00.438000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:00.446000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:00.454000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:00.479000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:00.600000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:00.719000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:00.864000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:00.912000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:00.922000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:00.944000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:00.949000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:00.974000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:01.095000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:01.208000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:01.355000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:01.407000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:01.413000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:01.443000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:01.450000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:01.467000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:01.591000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:01.721000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:01.852000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:01.900000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:01.906000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:01.948000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:01.953000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:01.967000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:02.088000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:02.214000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:02.349000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:02.396000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:02.402000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:02.449000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:02.455000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:02.463000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:02.589000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:02.856000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:02.895000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:02.902000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:02.956000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:02.968000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:03.016000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:03.247000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:03.361000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:03.397000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:03.412000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:03.515000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:03.708000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:03.766000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:03.774000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:03.781000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:03.860000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:03.892000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:03.911000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:04.016000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:04.200000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:04.267000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:04.284000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:04.290000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:04.352000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:04.411000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:04.516000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:04.692000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:04.711000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:04.763000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:04.792000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:04.798000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:04.851000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:04.911000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:05.018000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:05.183000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:05.204000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:05.265000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:05.303000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:05.309000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:05.345000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:05.408000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:05.523000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:05.683000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:05.704000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:05.783000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:05.808000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:05.824000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:05.840000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:05.907000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:06.032000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:06.176000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:06.203000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:06.276000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:06.312000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:06.324000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:06.404000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:06.536000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:06.665000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:06.672000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:06.703000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:06.775000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:06.821000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:06.826000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:06.904000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:07.035000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:07.168000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:07.182000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:07.200000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:07.270000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:07.328000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:07.333000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:07.408000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:07.527000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:07.672000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:07.677000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:07.696000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:07.778000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:07.836000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:07.846000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:07.904000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:08.039000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:08.172000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:08.177000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:08.188000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:08.275000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:08.339000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:08.352000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:08.399000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:08.536000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:08.671000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:08.677000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:08.685000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:08.768000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:08.845000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:08.855000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:08.900000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:09.027000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:09.167000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:09.180000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:09.187000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:09.264000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:09.360000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:09.366000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:09.403000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:09.524000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:09.660000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:09.679000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:09.685000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:09.757000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:09.863000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:09.871000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:09.906000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:10.015000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:10.153000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:10.172000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:10.188000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:10.250000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:10.380000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:10.385000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:10.404000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:10.507000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:10.662000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:10.672000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:10.688000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:10.752000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:10.883000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:10.890000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:10.899000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:11.007000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:11.161000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:11.167000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:11.183000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:11.256000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:11.393000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:11.398000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:11.404000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:11.501000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:11.663000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:11.672000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:11.680000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:11.750000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:11.899000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:11.905000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:11.918000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:11.999000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:12.169000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:12.179000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:12.189000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:12.248000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:12.418000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:12.425000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:12.431000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:12.498000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:12.673000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:12.695000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:12.703000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:12.761000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:12.932000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:12.940000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:12.952000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:13.009000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:13.168000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:13.203000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:13.216000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:13.256000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:13.441000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:13.447000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:13.462000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:13.505000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:13.670000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:13.715000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:13.729000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:13.754000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:13.952000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:13.958000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:13.972000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:14.001000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:14.176000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:14.224000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:14.231000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:14.263000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:14.461000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:14.477000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:14.489000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:14.507000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:14.678000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:14.726000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:14.741000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:14.770000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:14.968000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:14.978000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:14.992000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:15.004000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:15.172000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:15.229000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:15.245000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:15.271000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:15.479000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:15.486000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:15.500000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:15.507000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:15.667000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:15.734000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:15.755000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:15.774000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:15.994000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:16.005000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:16.017000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:16.158000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:16.240000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:16.280000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:16.521000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:16.536000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:16.659000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:16.745000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:16.796000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:17.158000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:17.248000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:17.306000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:17.828000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:18.341000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:18.845000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:19.343000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:19.843000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_267 0.1042 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_266 0.1043 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_276 0.1045 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.1046 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_284 0.1087 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_285 0.1088 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_275 0.1124 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_274 0.1126 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_271 0.1407 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0775 seconds and 0.1936 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_266 0.1044 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.1044 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_277 0.1047 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.1048 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_284 0.1086 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_285 0.1086 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_274 0.1122 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_275 0.1122 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_271 0.1402 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.3351 seconds and 0.2017 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_266 0.1042 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.1042 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_277 0.1043 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.1043 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_284 0.1088 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_285 0.1088 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_274 0.1122 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_275 0.1126 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_271 0.1406 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.4681 seconds and 0.1992 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_266 0.1009 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.1010 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_277 0.1014 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.1014 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_285 0.1050 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_284 0.1050 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_275 0.1086 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_274 0.1086 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_271 0.1333 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.3403 seconds and 0.2007 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_276 0.1042 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.1042 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.1043 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.1044 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_285 0.1090 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_284 0.1090 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_275 0.1127 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_274 0.1129 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_271 0.1405 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.3775 seconds and 0.1942 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_266 0.1043 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.1043 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_277 0.1045 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.1046 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_284 0.1088 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_285 0.1089 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_275 0.1123 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_274 0.1125 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_270 0.1402 ms 43.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0806 seconds and 0.1922 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0608 ms 100.0% 
  triton_mm_267 0.1041 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_266 0.1041 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_276 0.1043 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.1043 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_285 0.1086 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_284 0.1087 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_275 0.1117 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_274 0.1117 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_271 0.1397 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0232 seconds and 0.1853 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_276 0.1039 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.1039 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_267 0.1041 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_266 0.1042 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_284 0.1098 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_285 0.1099 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_274 0.1137 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_275 0.1139 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_270 0.1411 ms 43.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.4044 seconds and 0.1909 seconds precompiling for 25 choices
Capturing batches (bs=8 avail_mem=57.71 GB):  50%|     | 3/6 [04:41<04:37, 92.45s/it]Capturing batches (bs=4 avail_mem=57.02 GB):  50%|     | 3/6 [04:41<04:37, 92.45s/it][rank5]:W1120 13:51:32.977000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:32.984000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:33.062000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:33.065000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:33.166000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:33.169000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:33.193000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:33.237000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:33.271000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:33.327000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:33.374000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:33.430000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:33.643000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:33.710000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:33.721000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:33.788000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:33.799000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:33.807000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:33.825000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:33.881000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:33.889000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:33.891000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:33.988000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:33.996000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:34.929000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:34.934000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:35.113000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:35.189000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:35.557000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:35.634000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:35.743000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:35.749000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:36.477000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:36.555000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:36.635000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:36.658000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:36.724000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:36.827000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:36.847000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:36.891000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:36.924000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:36.968000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:37.023000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:37.069000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:37.297000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:37.343000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:37.375000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:37.409000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:37.420000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:37.475000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:37.485000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:37.520000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:37.583000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:37.588000 280 torch/_inductor/utils.py:1349] [38/3] Please pip install Composable Kernel package
[rank0]:W1120 13:51:37.629000 275 torch/_inductor/utils.py:1349] [38/3] Please pip install Composable Kernel package
[rank1]:W1120 13:51:37.719000 276 torch/_inductor/utils.py:1349] [38/3] Please pip install Composable Kernel package
[rank4]:W1120 13:51:37.762000 279 torch/_inductor/utils.py:1349] [38/3] Please pip install Composable Kernel package
[rank6]:W1120 13:51:37.864000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:37.949000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:38.055000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:38.197000 282 torch/_inductor/utils.py:1349] [38/3] Please pip install Composable Kernel package
[rank3]:W1120 13:51:38.238000 278 torch/_inductor/utils.py:1349] [38/3] Please pip install Composable Kernel package
[rank2]:W1120 13:51:38.296000 277 torch/_inductor/utils.py:1349] [38/3] Please pip install Composable Kernel package
[rank6]:W1120 13:51:38.769000 281 torch/_inductor/utils.py:1349] [38/3] Please pip install Composable Kernel package
AUTOTUNE bmm(16x4x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_300 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_305 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_288 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_293 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_294 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_295 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_298 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_303 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7479 seconds and 0.1487 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_309 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_310 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_311 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_295 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_301 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_302 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_303 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_305 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_306 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8368 seconds and 0.1554 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_305 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_304 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_298 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_300 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_303 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_306 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_307 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7903 seconds and 0.1494 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_295 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_301 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_309 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_292 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_300 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_305 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_306 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8628 seconds and 0.1366 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_299 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_300 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_301 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_302 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_303 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_305 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_306 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_307 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_308 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_311 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8289 seconds and 0.1529 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  bmm 0.0065 ms 100.0% 
  triton_bmm_301 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_295 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_300 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_288 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_291 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_292 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8334 seconds and 0.1415 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_302 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_303 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_307 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_304 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_295 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_296 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_297 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_300 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_301 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8093 seconds and 0.1613 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  bmm 0.0065 ms 100.0% 
  triton_bmm_300 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_296 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_298 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_302 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_304 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_310 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_289 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_290 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8313 seconds and 0.1327 seconds precompiling for 25 choices
[rank0]:W1120 13:51:47.354000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:47.359000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:47.857000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:47.872000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:48.042000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:48.053000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:48.053000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:48.239000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:48.315000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:48.542000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:48.555000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:48.565000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:48.688000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:48.754000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:48.823000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:49.218000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:51:50.212000 275 torch/_inductor/utils.py:1349] [50/3_1] Please pip install Composable Kernel package
[rank5]:W1120 13:51:50.218000 280 torch/_inductor/utils.py:1349] [50/3_1] Please pip install Composable Kernel package
[rank2]:W1120 13:51:50.492000 277 torch/_inductor/utils.py:1349] [50/3_1] Please pip install Composable Kernel package
[rank4]:W1120 13:51:51.134000 279 torch/_inductor/utils.py:1349] [50/3_1] Please pip install Composable Kernel package
[rank1]:W1120 13:51:51.295000 276 torch/_inductor/utils.py:1349] [50/3_1] Please pip install Composable Kernel package
[rank6]:W1120 13:51:51.529000 281 torch/_inductor/utils.py:1349] [50/3_1] Please pip install Composable Kernel package
[rank3]:W1120 13:51:52.064000 278 torch/_inductor/utils.py:1349] [50/3_1] Please pip install Composable Kernel package
[rank7]:W1120 13:51:52.467000 282 torch/_inductor/utils.py:1349] [50/3_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_314 0.0068 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_315 0.0068 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_324 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_325 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_323 0.0077 ms 84.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_322 0.0077 ms 83.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_319 0.0079 ms 82.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_318 0.0080 ms 81.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_334 0.0085 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7509 seconds and 0.4438 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_314 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_315 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_324 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_325 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_322 0.0071 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_323 0.0072 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_318 0.0079 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_319 0.0079 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_316 0.0083 ms 77.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8381 seconds and 0.4491 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0063 ms 100.0% 
  triton_bmm_315 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_314 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_324 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_325 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_323 0.0070 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_322 0.0071 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_318 0.0079 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_319 0.0079 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_317 0.0082 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7426 seconds and 0.4195 seconds precompiling for 25 choices
[rank0]:W1120 13:51:56.399000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_314 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_315 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_325 0.0070 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_324 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_323 0.0079 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_322 0.0079 ms 81.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_318 0.0080 ms 80.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_319 0.0080 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_334 0.0085 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8494 seconds and 0.4035 seconds precompiling for 25 choices
[rank0]:W1120 13:51:56.475000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:56.480000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:56.557000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_314 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_315 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_324 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_325 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_322 0.0072 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_323 0.0072 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_318 0.0081 ms 79.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_319 0.0081 ms 79.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_334 0.0083 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8045 seconds and 0.4030 seconds precompiling for 25 choices
[rank0]:W1120 13:51:56.589000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:56.642000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:51:56.657000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:51:56.718000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_315 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_325 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_314 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_324 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_322 0.0070 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_323 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_318 0.0079 ms 81.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_319 0.0079 ms 81.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_335 0.0082 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8107 seconds and 0.3983 seconds precompiling for 25 choices
[rank2]:W1120 13:51:56.817000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:57.365000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_315 0.0068 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_314 0.0068 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_324 0.0071 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_325 0.0071 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_323 0.0078 ms 84.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_322 0.0079 ms 83.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_319 0.0082 ms 80.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_318 0.0082 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_334 0.0086 ms 76.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8469 seconds and 0.4060 seconds precompiling for 25 choices
[rank4]:W1120 13:51:57.442000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:57.487000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:51:57.542000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:57.565000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:51:57.668000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:57.698000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:51:57.775000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0063 ms 100.0% 
  triton_bmm_314 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_325 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_315 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_324 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_322 0.0069 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_323 0.0069 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_318 0.0077 ms 82.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_319 0.0077 ms 81.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_316 0.0080 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8405 seconds and 0.4163 seconds precompiling for 25 choices
[rank6]:W1120 13:51:57.876000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:58.292000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:58.368000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:51:58.475000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:58.687000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:58.766000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:51:58.868000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:02.254000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:02.332000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:02.436000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:02.742000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:02.833000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:02.851000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:02.929000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:02.939000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:03.032000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:03.920000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:03.999000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:04.057000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:04.102000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:04.136000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:04.174000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:04.239000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:04.251000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:04.659000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:04.764000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:05.095000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:05.211000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:05.473000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:05.553000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:05.656000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:06.515000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:06.612000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:06.798000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:07.032000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:07.123000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:07.310000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:07.532000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:07.621000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:07.814000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:07.860000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:07.975000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:08.164000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:08.369000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:08.494000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:08.688000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:08.696000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:08.884000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:09.007000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:09.160000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:09.195000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:09.204000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:09.680000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:09.706000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:10.196000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_339 0.0286 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_349 0.0295 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_348 0.0296 ms 31.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_338 0.0313 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_347 0.0450 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_346 0.0451 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_356 0.0481 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_357 0.0481 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_343 0.0538 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0708 seconds and 0.2247 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_339 0.0317 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_338 0.0320 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_349 0.0326 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_348 0.0327 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_347 0.0485 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_356 0.0490 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_357 0.0495 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_346 0.0501 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_342 0.0621 ms 15.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0973 seconds and 0.2260 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_349 0.0292 ms 31.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_348 0.0293 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_339 0.0308 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_338 0.0311 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_347 0.0448 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_346 0.0451 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_357 0.0478 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_356 0.0480 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_343 0.0538 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1227 seconds and 0.2176 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_348 0.0293 ms 32.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_349 0.0293 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_339 0.0305 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_338 0.0313 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_347 0.0450 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_346 0.0451 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_357 0.0482 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_356 0.0482 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_342 0.0538 ms 17.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9901 seconds and 0.2252 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_348 0.0320 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_339 0.0321 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_338 0.0322 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_349 0.0326 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_346 0.0490 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_347 0.0494 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.0496 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_356 0.0508 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_343 0.0602 ms 15.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1366 seconds and 0.2573 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_339 0.0313 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_348 0.0318 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_349 0.0322 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_338 0.0325 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_356 0.0485 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_357 0.0485 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_347 0.0486 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_346 0.0494 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_342 0.0600 ms 15.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0910 seconds and 0.2261 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_338 0.0323 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_339 0.0323 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_349 0.0327 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_348 0.0328 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_357 0.0483 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_356 0.0485 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_347 0.0491 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_346 0.0509 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_342 0.0608 ms 15.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1219 seconds and 0.2211 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_339 0.0282 ms 32.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_349 0.0297 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_348 0.0297 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_338 0.0301 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_346 0.0450 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_347 0.0452 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_356 0.0480 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_357 0.0481 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_342 0.0536 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0349 seconds and 0.2079 seconds precompiling for 25 choices
[rank0]:W1120 13:52:16.611000 275 torch/_dynamo/variables/builtin.py:1091] [77/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7745647fc0>
[rank0]:W1120 13:52:16.634000 275 torch/_dynamo/variables/builtin.py:1091] [79/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7745647cc0>
[rank5]:W1120 13:52:16.691000 280 torch/_dynamo/variables/builtin.py:1091] [77/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1400d33480>
[aiter] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:52:16 TP0] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1120 13:52:16.714000 280 torch/_dynamo/variables/builtin.py:1091] [79/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1402f60ae0>
[aiter] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:52:16 TP5] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1120 13:52:17.077000 277 torch/_dynamo/variables/builtin.py:1091] [77/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef3ce43f300>
[rank2]:W1120 13:52:17.099000 277 torch/_dynamo/variables/builtin.py:1091] [79/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef3d0088ae0>
[aiter] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:52:17 TP2] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1120 13:52:17.336000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:17.410000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:17.463000 276 torch/_dynamo/variables/builtin.py:1091] [77/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ee62bcd01b0>
[rank4]:W1120 13:52:17.469000 279 torch/_dynamo/variables/builtin.py:1091] [77/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f99e78db390>
[rank1]:W1120 13:52:17.486000 276 torch/_dynamo/variables/builtin.py:1091] [79/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ee62bcd0b40>
[rank4]:W1120 13:52:17.492000 279 torch/_dynamo/variables/builtin.py:1091] [79/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f99e777bd80>
[aiter] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:52:17 TP1] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:52:17 TP4] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1120 13:52:17.729000 281 torch/_dynamo/variables/builtin.py:1091] [77/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f400d427c90>
[rank6]:W1120 13:52:17.752000 281 torch/_dynamo/variables/builtin.py:1091] [79/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f400d427f30>
[rank2]:W1120 13:52:17.800000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:52:17 TP6] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1120 13:52:17.864000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:17.920000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:18.187000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:18.208000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:18.306000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:18.376000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:18.423000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:18.466000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:18.565000 278 torch/_dynamo/variables/builtin.py:1091] [77/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f109c633c60>
[rank3]:W1120 13:52:18.588000 278 torch/_dynamo/variables/builtin.py:1091] [79/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f109c633360>
[aiter] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:52:18 TP3] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1120 13:52:18.703000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:18.713000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:18.784000 282 torch/_dynamo/variables/builtin.py:1091] [77/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f04a5e83810>
[rank2]:W1120 13:52:18.800000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:18.807000 282 torch/_dynamo/variables/builtin.py:1091] [79/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f04a810d8f0>
[aiter] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:52:18 TP7] [fused_moe] using default for (4, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1120 13:52:18.879000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:18.933000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:18.981000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:19.209000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:19.215000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:19.275000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:19.295000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:19.382000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:19.440000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:19.488000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:19.504000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:19.711000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:19.724000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:19.780000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:19.792000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:19.891000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:19.948000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:19.997000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:20.017000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:20.216000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:20.235000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:20.283000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:20.289000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:20.406000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:20.452000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:20.505000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:20.525000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:20.722000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:20.744000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:20.787000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:20.795000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:20.932000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:20.960000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:21.012000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:21.041000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:21.227000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:21.253000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:21.295000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:21.304000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:21.440000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:21.464000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:21.520000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:21.554000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:21.731000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:21.756000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:21.797000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:21.812000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:21.951000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:21.973000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:22.029000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:22.070000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:22.240000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:22.263000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:22.295000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:22.307000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:22.466000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:22.480000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:22.537000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:22.577000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:22.748000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:22.771000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:22.800000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:22.807000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:22.987000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:22.994000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:23.044000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:23.085000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:23.259000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:23.288000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:23.313000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:23.321000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:23.495000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:23.501000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:23.565000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:23.592000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:23.767000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:23.801000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:23.827000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:23.837000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:24.003000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:24.024000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:24.090000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:24.109000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:24.278000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:24.309000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:24.329000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:24.338000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:24.511000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:24.532000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:24.608000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:24.617000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:24.786000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:24.812000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:24.829000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:24.837000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:25.023000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:25.040000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:25.120000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:25.128000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:25.294000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:25.316000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:25.329000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:25.337000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:25.531000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:25.560000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:25.628000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:25.636000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:25.799000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:25.840000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:25.853000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:25.861000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:26.039000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:26.072000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:26.136000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:26.144000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:26.307000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:26.349000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:26.357000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:26.365000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:26.547000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:26.580000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:26.648000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:26.657000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:26.815000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:26.853000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:26.862000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:26.869000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:27.055000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:27.084000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:27.162000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:27.168000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:27.323000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:27.365000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:27.373000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:27.381000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:27.563000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:27.616000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:27.668000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:27.677000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:27.831000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:27.880000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:27.890000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:27.897000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:28.075000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:28.124000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:28.176000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:28.189000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:28.343000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:28.397000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:28.405000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:28.412000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:28.583000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:28.688000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:28.701000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:28.913000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:28.922000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:29.029000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:29.204000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:29.221000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:29.332000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:29.365000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:29.461000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:29.556000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:29.573000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:29.717000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:29.736000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:29.845000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:29.852000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:29.877000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:30.081000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:30.092000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:30.236000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:30.364000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:30.377000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:30.384000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:30.394000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:30.604000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:30.611000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:30.713000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:30.883000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:30.890000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:30.899000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:30.909000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:31.127000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:31.142000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:31.212000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:31.229000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:31.399000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:31.412000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:31.420000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:31.429000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:31.643000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:31.658000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:31.728000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:31.745000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:31.915000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:31.936000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:31.948000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:31.956000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:32.159000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:32.191000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:32.244000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:32.261000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:32.431000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:32.448000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:32.460000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:32.469000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:32.675000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:32.704000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:32.761000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:32.781000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:32.947000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:32.962000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:32.976000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:32.988000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:33.195000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:33.231000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:33.276000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:33.297000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:33.463000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:33.472000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:33.488000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:33.500000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:33.711000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:33.753000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:33.792000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:33.813000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:33.983000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:33.993000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:34.001000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:34.020000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:34.223000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:34.268000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:34.313000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:34.326000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:34.499000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:34.516000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:34.524000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:34.548000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:34.735000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:34.796000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:34.828000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:34.841000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:35.011000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:35.032000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:35.039000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:35.064000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:35.247000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:35.307000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:35.344000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:35.361000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:35.523000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:35.541000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:35.548000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:35.576000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:35.759000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:35.817000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:35.860000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:35.876000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:36.035000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:36.048000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:36.056000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:36.084000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:36.271000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:36.336000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:36.380000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:36.397000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:36.547000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:36.568000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:36.576000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:36.604000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:36.783000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:36.847000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:36.896000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:36.913000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:37.059000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:37.080000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:37.087000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:37.120000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:37.295000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:37.360000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:37.408000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:37.433000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:37.571000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:37.588000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:37.596000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:37.628000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:37.807000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:37.869000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:37.920000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:37.949000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:38.083000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:38.097000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:38.104000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:38.148000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:38.324000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:38.380000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:38.436000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:38.469000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:38.595000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:38.604000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:38.613000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:38.656000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:38.839000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:38.896000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:38.952000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:38.989000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:39.107000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:39.117000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:39.124000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:39.168000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:39.362000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:39.412000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:39.469000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:39.509000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:39.623000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:39.636000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:39.644000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:39.692000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:39.885000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:39.944000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:39.984000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:40.029000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:40.145000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:40.152000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:40.161000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:40.212000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:40.414000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:40.456000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:40.500000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:40.549000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:40.669000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:40.676000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:40.684000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:40.724000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:40.934000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:40.968000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:41.016000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:41.069000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:41.189000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:41.197000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:41.205000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:41.240000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:41.453000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:41.484000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:41.532000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:41.589000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:41.709000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:41.716000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:41.724000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:41.752000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:41.977000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:41.996000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:42.048000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:42.109000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:42.234000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:42.241000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:42.250000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:42.266000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:42.491000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:42.524000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:42.569000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:42.629000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:42.750000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:42.764000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:42.772000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:42.786000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:43.007000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:43.045000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:43.084000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:43.149000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:43.268000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:43.276000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:43.287000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:43.305000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:43.531000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:43.561000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:43.604000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:43.669000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:43.781000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:43.794000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:43.803000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:43.819000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:44.052000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:44.077000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:44.120000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:44.189000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:44.292000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:44.310000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:44.332000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:44.339000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:44.575000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:44.605000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:44.638000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:44.710000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:44.815000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:44.830000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:44.857000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:44.869000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:45.089000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:45.127000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:45.153000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:45.234000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:45.331000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:45.346000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:45.373000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:45.386000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:45.605000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:45.646000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:45.669000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:45.753000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:45.847000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:45.862000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:45.897000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:45.909000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:46.120000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:46.169000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:46.189000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:46.272000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:46.365000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:46.378000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:46.412000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:46.428000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:52:46.635000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:52:46.685000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:46.704000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:46.792000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:52:46.884000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:46.892000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:46.927000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:46.948000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:47.228000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:47.312000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:47.405000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:52:47.448000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:52:47.465000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:52:47.753000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:47.853000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:47.938000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:48.385000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:52:48.486000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:52:48.920000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0605 ms 100.0% 
  triton_mm_362 0.1040 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_372 0.1041 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.1041 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_373 0.1042 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_380 0.1083 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_381 0.1083 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_371 0.1118 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_370 0.1120 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1334 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1436 seconds and 0.1870 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0603 ms 100.0% 
  triton_mm_363 0.1039 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_362 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_372 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_380 0.1078 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_381 0.1078 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_370 0.1110 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_371 0.1111 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_366 0.1322 ms 45.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.5612 seconds and 0.1847 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_362 0.1043 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_372 0.1043 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.1044 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_363 0.1044 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_380 0.1080 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_381 0.1082 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_370 0.1115 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_371 0.1116 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_366 0.1330 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.5602 seconds and 0.1981 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0608 ms 100.0% 
  triton_mm_373 0.1038 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.1039 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_362 0.1039 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.1040 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_380 0.1081 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_381 0.1084 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_371 0.1118 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_370 0.1120 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1335 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0725 seconds and 0.1993 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_372 0.1041 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.1041 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_363 0.1043 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_362 0.1044 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_380 0.1086 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_381 0.1086 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_370 0.1121 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_371 0.1122 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_366 0.1337 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.5505 seconds and 0.2053 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_373 0.1044 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.1044 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_362 0.1044 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.1044 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_380 0.1084 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_381 0.1086 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_370 0.1118 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_371 0.1119 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_367 0.1333 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0507 seconds and 0.1895 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_362 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_373 0.1044 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.1044 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.1045 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_380 0.1090 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_381 0.1094 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_370 0.1121 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_371 0.1121 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_367 0.1334 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1142 seconds and 0.2029 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0602 ms 100.0% 
  triton_mm_362 0.1007 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.1009 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_372 0.1011 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.1011 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_381 0.1054 ms 57.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_380 0.1055 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_371 0.1078 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_370 0.1079 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1268 ms 47.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0498 seconds and 0.2003 seconds precompiling for 25 choices
Capturing batches (bs=4 avail_mem=57.02 GB):  67%|   | 4/6 [06:10<03:02, 91.33s/it]Capturing batches (bs=2 avail_mem=56.31 GB):  67%|   | 4/6 [06:10<03:02, 91.33s/it][rank4]:W1120 13:53:02.682000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:02.751000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:02.761000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:02.762000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:02.765000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:02.790000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:02.823000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:02.830000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:02.839000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:02.848000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:02.868000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:02.871000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:02.893000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:02.901000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:02.934000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:02.942000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:02.957000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:02.971000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:02.977000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:03.004000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:03.074000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:03.105000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:03.187000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:03.292000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:04.644000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:04.727000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:04.733000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:04.739000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:04.744000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:04.759000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:04.815000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:05.070000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:06.143000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:06.150000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:06.226000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:06.229000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:06.270000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:06.281000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:06.332000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:06.334000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:06.349000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:06.363000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:06.440000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:06.452000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:06.461000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:06.470000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:06.499000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:06.520000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:06.541000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:06.577000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:06.599000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:06.623000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:06.644000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:06.678000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:06.679000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:06.782000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:07.258000 279 torch/_inductor/utils.py:1349] [38/4] Please pip install Composable Kernel package
[rank5]:W1120 13:53:07.269000 280 torch/_inductor/utils.py:1349] [38/4] Please pip install Composable Kernel package
[rank0]:W1120 13:53:07.270000 275 torch/_inductor/utils.py:1349] [38/4] Please pip install Composable Kernel package
[rank7]:W1120 13:53:07.291000 282 torch/_inductor/utils.py:1349] [38/4] Please pip install Composable Kernel package
[rank3]:W1120 13:53:07.325000 278 torch/_inductor/utils.py:1349] [38/4] Please pip install Composable Kernel package
[rank1]:W1120 13:53:07.347000 276 torch/_inductor/utils.py:1349] [38/4] Please pip install Composable Kernel package
[rank2]:W1120 13:53:07.378000 277 torch/_inductor/utils.py:1349] [38/4] Please pip install Composable Kernel package
[rank6]:W1120 13:53:07.587000 281 torch/_inductor/utils.py:1349] [38/4] Please pip install Composable Kernel package
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_407 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_406 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_386 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_396 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_402 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_404 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_405 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_390 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_391 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.7724 seconds and 0.1830 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0067 ms 100.0% 
  triton_bmm_389 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_390 0.0068 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_396 0.0068 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_397 0.0068 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_399 0.0068 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0068 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8201 seconds and 0.1649 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_397 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_389 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_393 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_398 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_399 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_401 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8568 seconds and 0.2051 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_394 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_384 0.0066 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_386 0.0066 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_387 0.0066 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_388 0.0066 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_389 0.0066 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_390 0.0066 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_391 0.0066 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_392 0.0066 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8294 seconds and 0.1569 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_404 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_405 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_397 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_401 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_402 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_406 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_386 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8581 seconds and 0.1131 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_404 0.0066 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_406 0.0066 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_405 0.0066 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_391 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_394 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_396 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_398 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_399 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8545 seconds and 0.1540 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_400 0.0067 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_404 0.0067 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_405 0.0067 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_406 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_387 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_395 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_398 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8881 seconds and 0.1781 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_399 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_388 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_389 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_395 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_403 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_387 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_390 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_392 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8270 seconds and 0.1510 seconds precompiling for 25 choices
[rank0]:W1120 13:53:16.860000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:16.882000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:17.104000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:17.113000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:17.118000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:17.137000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:17.361000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:17.380000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:17.391000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:17.541000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:17.610000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:17.617000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:17.625000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:17.648000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:17.901000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:18.084000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:20.528000 282 torch/_inductor/utils.py:1349] [50/4_1] Please pip install Composable Kernel package
[rank0]:W1120 13:53:20.570000 275 torch/_inductor/utils.py:1349] [50/4_1] Please pip install Composable Kernel package
[rank5]:W1120 13:53:20.612000 280 torch/_inductor/utils.py:1349] [50/4_1] Please pip install Composable Kernel package
[rank3]:W1120 13:53:21.025000 278 torch/_inductor/utils.py:1349] [50/4_1] Please pip install Composable Kernel package
[rank4]:W1120 13:53:21.729000 279 torch/_inductor/utils.py:1349] [50/4_1] Please pip install Composable Kernel package
[rank6]:W1120 13:53:21.795000 281 torch/_inductor/utils.py:1349] [50/4_1] Please pip install Composable Kernel package
[rank1]:W1120 13:53:24.527000 276 torch/_inductor/utils.py:1349] [50/4_1] Please pip install Composable Kernel package
[rank2]:W1120 13:53:26.911000 277 torch/_inductor/utils.py:1349] [50/4_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_411 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_410 0.0068 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_421 0.0068 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_420 0.0068 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0069 ms 97.1% 
  triton_bmm_419 0.0070 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_418 0.0071 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0081 ms 82.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_414 0.0081 ms 82.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0081 ms 82.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 6.2475 seconds and 0.4042 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_411 0.0068 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_410 0.0068 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_420 0.0068 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_421 0.0068 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0071 ms 95.5% 
  triton_bmm_419 0.0071 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_418 0.0073 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_414 0.0079 ms 85.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_415 0.0080 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0083 ms 82.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 6.5946 seconds and 0.3985 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_411 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_410 0.0070 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_420 0.0070 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_421 0.0070 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0072 ms 93.9% 
  triton_bmm_418 0.0078 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_419 0.0078 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0082 ms 82.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_415 0.0083 ms 81.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0084 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.9786 seconds and 0.4122 seconds precompiling for 25 choices
[rank7]:W1120 13:53:28.290000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:28.369000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_411 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_410 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_420 0.0068 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_421 0.0068 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_418 0.0071 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_419 0.0071 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0072 ms 91.7% 
  triton_bmm_414 0.0080 ms 82.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_415 0.0080 ms 82.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_412 0.0082 ms 80.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.1336 seconds and 0.4214 seconds precompiling for 25 choices
[rank7]:W1120 13:53:28.471000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:28.709000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:28.786000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_411 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_421 0.0069 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_410 0.0069 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_420 0.0069 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_418 0.0076 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_419 0.0076 ms 84.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_415 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_431 0.0083 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 7.7604 seconds and 0.4197 seconds precompiling for 25 choices
[rank5]:W1120 13:53:28.887000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:29.242000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_410 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_420 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_421 0.0069 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_411 0.0070 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0072 ms 95.6% 
  triton_bmm_418 0.0076 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_419 0.0076 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0079 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_415 0.0079 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0083 ms 82.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 7.7886 seconds and 0.4028 seconds precompiling for 25 choices
[rank4]:W1120 13:53:29.321000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:29.423000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:29.456000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:29.537000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:29.643000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:29.909000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:29.986000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:30.096000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_410 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_411 0.0067 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_420 0.0068 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_421 0.0069 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0071 ms 93.3% 
  triton_bmm_418 0.0071 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_419 0.0071 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0078 ms 85.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_415 0.0079 ms 84.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_431 0.0083 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1463 seconds and 0.4122 seconds precompiling for 25 choices
[rank3]:W1120 13:53:30.376000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:30.452000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:30.551000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:31.209000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:31.299000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:31.399000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_410 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_411 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_421 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_420 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_418 0.0069 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_419 0.0069 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0077 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_415 0.0077 ms 82.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_412 0.0081 ms 78.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8591 seconds and 0.4068 seconds precompiling for 25 choices
[rank2]:W1120 13:53:33.623000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:33.701000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:33.801000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:34.537000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:34.620000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:34.729000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:35.114000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:35.194000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:35.300000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:35.456000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:35.535000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:35.639000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:35.833000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:35.913000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:36.018000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:36.073000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:36.150000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:36.210000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:36.252000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:36.290000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:36.398000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:36.910000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:36.989000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:37.093000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:38.200000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:38.721000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:38.778000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:39.113000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:39.245000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:39.271000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:39.293000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:39.513000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:39.629000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:39.791000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:39.806000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:40.042000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:40.144000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:40.148000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:40.191000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:40.225000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:40.325000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:40.330000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:40.582000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:40.743000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:40.882000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:41.284000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:41.416000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:41.944000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:43.806000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:44.315000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:44.827000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_435 0.0281 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_444 0.0294 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0294 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_434 0.0304 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_443 0.0441 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_442 0.0441 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_452 0.0443 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_453 0.0444 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_439 0.0535 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1137 seconds and 0.2312 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_445 0.0290 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_444 0.0291 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_435 0.0295 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_434 0.0309 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_443 0.0438 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_442 0.0440 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_452 0.0442 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_453 0.0442 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_438 0.0536 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0361 seconds and 0.2279 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_435 0.0320 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_444 0.0323 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_434 0.0324 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_445 0.0324 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_453 0.0467 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_452 0.0481 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_442 0.0485 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_443 0.0496 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_455 0.0610 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1400 seconds and 0.2183 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_434 0.0314 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_444 0.0324 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0325 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_435 0.0325 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_452 0.0470 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_453 0.0470 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0494 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_442 0.0505 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_439 0.0608 ms 15.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0228 seconds and 0.2297 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_445 0.0321 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_444 0.0323 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_434 0.0324 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_435 0.0325 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_452 0.0466 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_453 0.0468 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0491 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_442 0.0512 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_438 0.0587 ms 15.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.6418 seconds and 0.2214 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_444 0.0320 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_435 0.0321 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_445 0.0321 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_434 0.0325 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_452 0.0468 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_453 0.0479 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_442 0.0499 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_443 0.0503 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_438 0.0601 ms 15.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0968 seconds and 0.2226 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_435 0.0282 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_444 0.0289 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0289 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_434 0.0317 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_443 0.0440 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_442 0.0441 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_452 0.0445 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_453 0.0446 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_439 0.0536 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0439 seconds and 0.2253 seconds precompiling for 25 choices
[rank7]:W1120 13:53:49.181000 282 torch/_dynamo/variables/builtin.py:1091] [77/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f04a5e83810>
[rank7]:W1120 13:53:49.204000 282 torch/_dynamo/variables/builtin.py:1091] [79/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f04a810d8f0>
[aiter] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:53:49 TP7] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1120 13:53:49.288000 280 torch/_dynamo/variables/builtin.py:1091] [77/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1400d33480>
[rank5]:W1120 13:53:49.311000 280 torch/_dynamo/variables/builtin.py:1091] [79/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1402f60ae0>
[aiter] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:53:49 TP5] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1120 13:53:49.428000 279 torch/_dynamo/variables/builtin.py:1091] [77/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f99e78db390>
[rank4]:W1120 13:53:49.451000 279 torch/_dynamo/variables/builtin.py:1091] [79/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f99e777bd80>
[aiter] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:53:49 TP4] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1120 13:53:49.552000 278 torch/_dynamo/variables/builtin.py:1091] [77/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f109c633c60>
[rank3]:W1120 13:53:49.575000 278 torch/_dynamo/variables/builtin.py:1091] [79/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f109c633360>
[rank6]:W1120 13:53:49.637000 281 torch/_dynamo/variables/builtin.py:1091] [77/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f400d427c90>
[aiter] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:53:49 TP3] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1120 13:53:49.661000 281 torch/_dynamo/variables/builtin.py:1091] [79/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f400d427f30>
[aiter] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:53:49 TP6] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1120 13:53:49.929000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:50.030000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:50.169000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:50.283000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:50.381000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:50.457000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:50.561000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:50.693000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_435 0.0285 ms 31.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_444 0.0288 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0289 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_434 0.0303 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_442 0.0438 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_443 0.0438 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_452 0.0443 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_453 0.0443 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_439 0.0536 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0612 seconds and 0.2286 seconds precompiling for 25 choices
[rank3]:W1120 13:53:50.824000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:50.843000 276 torch/_dynamo/variables/builtin.py:1091] [77/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ee62bcd01b0>
[rank1]:W1120 13:53:50.866000 276 torch/_dynamo/variables/builtin.py:1091] [79/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ee62bcd0b40>
[rank6]:W1120 13:53:50.930000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:53:50 TP1] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1120 13:53:50.994000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:51.089000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:51.102000 275 torch/_dynamo/variables/builtin.py:1091] [77/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7745647fc0>
[rank0]:W1120 13:53:51.126000 275 torch/_dynamo/variables/builtin.py:1091] [79/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7745647cc0>
[aiter] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:53:51 TP0] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1120 13:53:51.221000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:51.351000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:51.470000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:51.529000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:51.591000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:51.617000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:51.749000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:51.887000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:51.896000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:52.015000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:52.066000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:52.128000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:52.156000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:52.282000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:52.423000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:52.432000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:52.554000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:52.597000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:52.663000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:52.681000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:52.820000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:52.959000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:52.966000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:53.087000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:53.125000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:53.191000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:53.215000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:53.349000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:53.495000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:53.501000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:53.629000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:53.653000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:53.695000 277 torch/_dynamo/variables/builtin.py:1091] [77/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef3ce43f300>
[rank2]:W1120 13:53:53.719000 277 torch/_dynamo/variables/builtin.py:1091] [79/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef3d0088ae0>
[rank1]:W1120 13:53:53.731000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:53.739000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:53:53 TP2] [fused_moe] using default for (2, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1120 13:53:53.881000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:54.027000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:54.051000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:54.161000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:54.181000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:54.259000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:54.268000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:54.409000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:54.435000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:54.551000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:54.591000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:54.689000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:54.717000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:54.787000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:54.796000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:54.938000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:54.979000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:55.071000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:55.119000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:55.217000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:55.246000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:55.315000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:55.324000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:55.465000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:55.506000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:55.595000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:55.647000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:55.745000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:55.773000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:55.843000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:55.851000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:55.993000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:56.035000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:56.123000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:56.175000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:56.274000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:56.301000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:56.377000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:56.385000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:56.521000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:56.576000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:56.663000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:56.707000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:56.802000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:56.839000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:56.897000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:56.911000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:57.049000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:57.103000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:57.183000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:57.231000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:57.329000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:57.369000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:57.417000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:57.442000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:57.577000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:57.631000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:57.703000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:57.755000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:57.856000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:57.896000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:57.936000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:57.973000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:58.104000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:58.165000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:58.258000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:58.307000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:58.390000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:58.430000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:58.463000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:58.519000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:58.633000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:58.695000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:58.782000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:58.839000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:58.921000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:58.961000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:58.985000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:59.047000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:59.157000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:59.215000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:59.303000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:59.367000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:59.453000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:53:59.502000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:53:59.510000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:53:59.579000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:53:59.685000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:53:59.735000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:53:59.823000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:53:59.895000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:53:59.985000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:00.034000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:00.042000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:00.107000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:00.213000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:00.272000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:00.343000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:00.423000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:00.517000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:00.569000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:00.577000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:00.635000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:00.741000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:00.803000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:00.875000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:00.947000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:01.045000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:01.097000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:01.113000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:01.163000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:01.273000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:01.323000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:01.395000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:01.479000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:01.577000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:01.621000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:01.641000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:01.691000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:01.801000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:01.851000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:01.915000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:02.015000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:02.112000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:02.145000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:02.176000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:02.219000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:02.329000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:02.379000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:02.447000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:02.547000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:02.645000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:02.669000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:02.712000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:02.755000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:02.857000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:02.908000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:02.976000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:03.076000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:03.177000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:03.192000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:03.249000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:03.297000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:03.384000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:03.437000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:03.509000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:03.600000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:03.716000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:03.722000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:03.793000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:03.825000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:03.912000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:03.957000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:04.029000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:04.120000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:04.248000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:04.256000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:04.330000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:04.351000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:04.441000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:04.476000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:04.548000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:04.643000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:04.777000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:04.789000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:04.865000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:04.883000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:04.968000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:05.005000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:05.081000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:05.180000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:05.304000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:05.324000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:05.402000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:05.423000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:05.501000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:05.528000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:05.603000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:05.707000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:05.837000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:05.862000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:05.946000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:05.952000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:06.033000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:06.063000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:06.139000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:06.235000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:06.365000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:06.397000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:06.481000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:06.488000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:06.565000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:06.583000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:06.671000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:06.767000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:06.893000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:06.936000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:07.018000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:07.031000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:07.097000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:07.104000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:07.200000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:07.295000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:07.421000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:07.470000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:07.559000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:07.566000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:07.625000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:07.633000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:07.727000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:07.827000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:07.949000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:08.014000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:08.094000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:08.107000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:08.153000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:08.164000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:08.256000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:08.368000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:08.477000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:08.550000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:08.630000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:08.647000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:08.681000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:08.692000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:08.784000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:08.907000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:09.005000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:09.086000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:09.165000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:09.187000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:09.213000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:09.221000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:09.312000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:09.443000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:09.533000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:09.622000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:09.711000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:09.723000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:09.741000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:09.748000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:09.840000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:09.979000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:10.061000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:10.158000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:10.249000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:10.260000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:10.269000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:10.276000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:10.368000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:10.517000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:10.596000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:10.697000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:10.784000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:10.801000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:10.809000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:10.817000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:10.893000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:11.045000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:11.132000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:11.236000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:11.328000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:11.344000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:11.355000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:11.363000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:11.599000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:11.670000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:11.778000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:11.875000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:11.882000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:11.907000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:11.914000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:12.004000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:12.132000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:12.321000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:12.416000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:12.445000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:12.454000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:12.540000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:12.659000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:12.834000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:12.979000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:12.986000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:13.068000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:13.082000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:13.193000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:13.368000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:13.509000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:13.524000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:13.531000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:13.594000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:13.604000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:13.626000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:13.732000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:13.910000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:14.040000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:14.070000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:14.130000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:14.140000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:14.169000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:14.452000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:14.576000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:14.614000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:14.665000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:14.688000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:14.696000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:14.723000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:14.843000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:14.993000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:15.110000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:15.157000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:15.200000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:15.225000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:15.237000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:15.265000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:15.381000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:15.528000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:15.665000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:15.700000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:15.736000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:15.761000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:15.777000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:15.817000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:15.932000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:16.064000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:16.201000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:16.244000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:16.272000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:16.309000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:16.325000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:16.361000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:16.473000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:16.608000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:16.788000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:16.808000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:16.854000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:16.876000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:16.905000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:17.025000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:17.146000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:17.308000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:17.334000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:17.345000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:17.385000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:17.413000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:17.458000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:17.571000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:17.684000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:17.852000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:17.882000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:17.890000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:17.920000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:17.953000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:18.006000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:18.117000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:18.222000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:18.389000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:18.428000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:18.435000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:18.453000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:18.489000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:18.549000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:18.660000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:18.758000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:18.921000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:18.972000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:18.981000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:18.988000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:19.025000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:19.093000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:19.205000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:19.294000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:19.456000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:19.509000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:19.516000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:19.534000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:19.561000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:19.638000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:19.747000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:19.832000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:19.995000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:20.045000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:20.052000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:20.077000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:20.104000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:20.182000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:20.287000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:20.361000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:20.531000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:20.581000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:20.588000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:20.621000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:20.645000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:20.726000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:20.823000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:21.072000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:21.162000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:21.179000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:21.368000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:21.630000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:21.750000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:21.924000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:22.174000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:22.292000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:22.472000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:22.713000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:23.246000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:23.781000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:24.309000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:24.835000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0603 ms 100.0% 
  triton_mm_469 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_459 0.1042 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_458 0.1042 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_468 0.1043 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_476 0.1067 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_477 0.1068 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_466 0.1117 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_467 0.1119 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_463 0.1290 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0473 seconds and 0.2048 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0602 ms 100.0% 
  triton_mm_459 0.1043 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_458 0.1043 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_468 0.1044 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.1044 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_477 0.1067 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_476 0.1068 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_466 0.1114 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_467 0.1116 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_463 0.1288 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0742 seconds and 0.1973 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0603 ms 100.0% 
  triton_mm_468 0.1039 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.1039 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_476 0.1067 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_477 0.1067 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_467 0.1116 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_466 0.1116 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_463 0.1291 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1289 seconds and 0.2066 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0601 ms 100.0% 
  triton_mm_458 0.1009 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.1010 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_469 0.1010 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.1010 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_477 0.1032 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_476 0.1034 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_466 0.1075 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_467 0.1077 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_463 0.1231 ms 48.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0363 seconds and 0.1853 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_469 0.1039 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_459 0.1039 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_468 0.1041 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_458 0.1041 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_476 0.1067 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_477 0.1069 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_467 0.1116 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_466 0.1117 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_462 0.1292 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0633 seconds and 0.2019 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_458 0.1040 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_469 0.1041 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_459 0.1042 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_468 0.1042 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_477 0.1074 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_476 0.1076 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_466 0.1119 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_467 0.1120 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_462 0.1296 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0842 seconds and 0.1867 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0603 ms 100.0% 
  triton_mm_469 0.1040 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.1043 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_468 0.1043 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.1044 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_476 0.1070 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_477 0.1070 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_467 0.1112 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_466 0.1113 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_463 0.1286 ms 46.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1196 seconds and 0.1924 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0600 ms 100.0% 
  triton_mm_469 0.1039 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.1039 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_468 0.1040 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.1041 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_477 0.1066 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_476 0.1068 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_466 0.1118 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_467 0.1119 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_462 0.1290 ms 46.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0768 seconds and 0.1939 seconds precompiling for 25 choices
Capturing batches (bs=2 avail_mem=56.31 GB):  83%| | 5/6 [07:46<01:32, 92.74s/it]Capturing batches (bs=1 avail_mem=55.61 GB):  83%| | 5/6 [07:46<01:32, 92.74s/it][rank5]:W1120 13:54:37.312000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:37.324000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:37.394000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:37.407000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:37.411000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:37.492000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:37.499000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:37.516000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:37.556000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:37.565000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:37.598000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:37.635000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:37.647000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:37.739000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:37.746000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:37.754000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:37.827000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:37.828000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:37.908000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:37.934000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:38.013000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:38.438000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:38.518000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:38.621000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:39.939000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:39.951000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:40.023000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:40.188000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:40.204000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:40.411000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:40.465000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:41.058000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:41.266000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:41.337000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:41.342000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:41.345000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:41.417000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:41.420000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:41.448000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:41.522000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:41.523000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:41.569000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:41.647000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:41.743000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:41.748000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:41.822000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:41.925000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:41.953000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:41.998000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:42.033000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:42.075000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:42.135000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:42.177000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:42.871000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:42.949000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:43.050000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:47.835000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:47.846000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:48.043000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:48.069000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:48.337000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:48.358000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:48.516000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:48.546000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:48.569000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:48.717000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:49.021000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:49.228000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:49.236000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:49.734000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:50.242000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [43/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:50.757000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:51.330000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:51.390000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:51.409000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:51.470000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:51.511000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:51.574000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:51.644000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:51.654000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:51.724000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:51.733000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:51.828000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:51.837000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:52.005000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:52.084000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:52.186000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:52.614000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:52.692000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:52.790000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:52.793000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:52.869000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:52.972000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:53.985000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:54.064000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:54:54.167000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:57.554000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:57.644000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:54:57.748000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:57.956000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:58.037000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:58.115000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:54:58.144000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:58.155000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:58.196000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:58.233000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:54:58.304000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:54:58.337000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:58.388000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:58.468000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:54:58.574000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:59.196000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:59.274000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:59.302000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:54:59.375000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:59.381000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:54:59.487000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:01.051000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:01.130000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [16/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:01.237000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [28/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:01.584000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:01.697000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:01.797000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:01.980000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:02.088000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:02.113000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:02.244000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:02.337000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:02.530000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:02.620000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:02.634000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:02.650000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:02.801000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:02.871000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:03.074000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:03.165000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:03.173000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:03.570000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:03.708000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:04.113000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:04.661000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:04.668000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:05.239000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:05.789000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:07.980000 277 torch/_dynamo/variables/builtin.py:1091] [77/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef3ce43f300>
[rank2]:W1120 13:55:08.003000 277 torch/_dynamo/variables/builtin.py:1091] [79/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef3d0088ae0>
[aiter] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:55:08 TP2] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1120 13:55:08.522000 279 torch/_dynamo/variables/builtin.py:1091] [77/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f99e78db390>
[rank3]:W1120 13:55:08.534000 278 torch/_dynamo/variables/builtin.py:1091] [77/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f109c633c60>
[rank4]:W1120 13:55:08.545000 279 torch/_dynamo/variables/builtin.py:1091] [79/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f99e777bd80>
[rank3]:W1120 13:55:08.557000 278 torch/_dynamo/variables/builtin.py:1091] [79/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f109c633360>
[aiter] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:55:08 TP4] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:55:08 TP3] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1120 13:55:08.728000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:09.122000 276 torch/_dynamo/variables/builtin.py:1091] [77/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ee62bcd01b0>
[rank1]:W1120 13:55:09.145000 276 torch/_dynamo/variables/builtin.py:1091] [79/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ee62bcd0b40>
[aiter] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:55:09 TP1] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1120 13:55:09.216000 282 torch/_dynamo/variables/builtin.py:1091] [77/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f04a5e83810>
[rank7]:W1120 13:55:09.239000 282 torch/_dynamo/variables/builtin.py:1091] [79/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f04a810d8f0>
[rank2]:W1120 13:55:09.264000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:09.278000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:09.284000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:55:09 TP7] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1120 13:55:09.330000 281 torch/_dynamo/variables/builtin.py:1091] [77/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f400d427c90>
[rank6]:W1120 13:55:09.353000 281 torch/_dynamo/variables/builtin.py:1091] [79/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f400d427f30>
[aiter] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:55:09 TP6] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1120 13:55:09.799000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:09.823000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:09.831000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:09.872000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:09.884000 280 torch/_dynamo/variables/builtin.py:1091] [77/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1400d33480>
[rank5]:W1120 13:55:09.906000 280 torch/_dynamo/variables/builtin.py:1091] [79/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f1402f60ae0>
[aiter] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:55:09 TP5] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1120 13:55:09.987000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:10.104000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:10.351000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:10.369000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:10.385000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:10.424000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:10.538000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:10.625000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:10.658000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:10.892000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:10.914000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:10.929000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:10.968000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:11.086000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:11.161000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:11.202000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:11.428000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:11.453000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:11.464000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:11.508000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:11.642000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:11.697000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:11.754000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:11.964000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:11.998000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:12.006000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:12.048000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:12.153000 275 torch/_dynamo/variables/builtin.py:1091] [77/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7745647fc0>
[rank0]:W1120 13:55:12.176000 275 torch/_dynamo/variables/builtin.py:1091] [79/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f7745647cc0>
[rank7]:W1120 13:55:12.189000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:12.233000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:55:12 TP0] [fused_moe] using default for (1, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1120 13:55:12.298000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:12.500000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:12.537000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:12.545000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:12.591000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:12.742000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:12.769000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:12.842000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:12.891000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:13.051000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:13.077000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:13.092000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:13.136000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:13.289000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:13.305000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:13.390000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:13.427000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:13.599000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:13.617000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:13.632000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:13.681000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:13.841000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:13.849000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:13.938000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:13.963000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:14.135000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:14.157000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:14.172000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:14.230000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:14.390000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:14.397000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:14.487000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:14.501000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:14.679000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:14.697000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:14.719000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:14.784000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:14.937000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:14.954000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:15.030000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:15.062000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:15.224000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:15.241000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:15.256000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:15.335000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:15.477000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:15.510000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:15.574000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:15.608000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:15.768000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:15.785000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:15.795000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:15.880000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:16.019000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:16.063000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:16.122000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:16.157000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:16.326000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:16.432000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:16.562000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:16.618000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:16.666000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:16.703000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:16.976000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:17.036000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:17.048000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:17.114000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:17.219000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:17.252000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:17.584000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:17.596000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:17.607000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:17.804000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:17.919000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:18.128000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:18.140000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:18.162000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:18.228000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:18.347000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:18.386000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:18.470000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:18.507000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:18.672000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:18.696000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:18.710000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:18.801000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:18.895000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:18.941000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:19.026000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:19.058000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:19.228000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:19.252000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:19.266000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:19.352000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:19.443000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:19.500000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:19.582000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:19.610000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:19.773000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:19.796000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:19.814000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:19.905000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:20.050000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:20.142000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:20.158000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:20.328000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:20.348000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:20.366000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:20.464000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:20.602000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:20.683000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:20.698000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:20.708000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:20.880000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:20.892000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:20.914000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:21.016000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:21.154000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:21.227000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:21.258000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:21.268000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:21.436000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:21.452000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:21.461000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:21.568000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:21.708000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:21.771000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:21.814000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:21.824000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:21.980000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:21.996000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:22.014000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:22.124000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:22.258000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:22.319000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:22.371000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:22.380000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:22.532000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:22.544000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:22.562000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:22.676000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:22.813000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:22.867000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:22.926000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:22.936000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:23.089000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:23.104000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:23.113000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:23.228000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:23.376000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:23.423000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:23.482000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:23.493000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:23.655000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:23.669000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:23.677000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:23.788000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:23.946000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:23.971000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:24.040000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:24.048000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:24.204000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:24.222000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:24.232000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:24.344000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:24.498000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:24.523000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:24.590000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:24.600000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:24.748000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:24.773000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:24.794000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:24.892000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:25.047000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:25.075000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:25.147000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:25.156000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:25.294000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:25.320000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:25.337000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:25.449000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:25.595000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:25.625000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:25.702000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:25.710000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:25.840000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:25.871000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:25.884000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:26.000000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:26.148000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:26.171000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:26.258000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:26.267000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:26.386000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:26.420000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:26.437000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:26.561000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:26.693000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:26.728000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:26.813000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:26.820000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:26.928000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:26.967000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:26.980000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:27.124000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:27.244000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:27.267000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:27.374000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:27.383000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:27.468000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:27.519000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:27.539000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:27.676000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:27.795000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:27.812000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:27.931000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:27.939000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:28.008000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:28.071000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:28.087000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:28.232000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:28.347000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:28.363000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:28.490000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:28.499000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:28.548000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:28.623000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:28.639000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:28.792000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:28.899000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:28.915000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:29.050000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:29.058000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:29.092000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:29.178000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:29.195000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:29.352000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:29.456000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:29.470000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:29.610000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:29.619000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:29.640000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:29.731000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:29.748000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:29.912000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:30.011000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:30.025000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:30.171000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:30.179000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:30.193000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:30.286000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:30.302000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:30.469000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:30.560000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:30.579000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:30.732000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:30.739000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:30.755000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:30.832000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:30.855000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:31.032000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:31.107000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:31.124000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:31.290000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:31.298000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:31.310000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:31.383000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:31.399000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:31.587000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:31.655000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:31.672000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:31.849000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:31.866000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:31.872000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:31.928000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:31.948000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:32.158000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:32.201000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:32.223000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:32.410000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:32.422000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:32.435000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:32.475000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:32.493000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:32.712000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:32.752000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:32.768000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:32.970000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:32.978000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:32.991000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:33.027000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:33.047000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:33.269000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:33.300000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:33.323000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:33.530000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:33.538000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:33.544000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:33.578000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:33.594000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:33.836000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:33.854000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:33.870000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:34.084000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:34.097000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:34.104000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:34.129000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:34.148000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:34.401000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:34.409000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:34.428000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:34.640000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:34.657000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:34.664000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:34.688000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:34.708000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:34.958000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:34.972000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:34.991000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:35.200000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:35.212000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:35.220000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:35.248000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:35.270000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:35.513000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:35.529000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:35.547000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:35.764000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:35.773000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:35.780000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:35.808000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:35.826000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:36.073000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:36.088000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:36.107000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:36.328000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:36.336000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:36.343000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:36.369000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:36.387000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:36.645000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:36.654000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:36.670000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:36.898000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:36.907000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:36.916000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:36.933000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:36.957000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:37.208000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:37.235000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:37.244000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:37.456000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:37.464000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:37.479000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:37.486000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:37.513000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:37.763000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:37.800000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:37.813000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:38.016000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:38.025000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:38.041000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:38.047000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:38.076000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:38.325000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:38.352000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:38.365000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:38.578000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:38.584000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:38.603000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:38.618000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:38.625000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:38.882000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:38.926000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:38.949000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:39.136000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:39.150000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:39.163000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:39.180000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:39.195000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:39.440000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:39.493000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:39.506000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:39.696000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:39.720000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:39.728000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:39.735000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:39.757000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:40.000000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:40.065000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:40.075000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:40.258000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1120 13:55:40.280000 277 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:40.297000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:40.307000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:40.314000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:40.564000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:40.616000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:40.622000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:40.818000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1120 13:55:40.858000 279 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:40.870000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1120 13:55:40.876000 278 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:41.113000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:41.171000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:41.180000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:41.378000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:41.430000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:41.674000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:41.730000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1120 13:55:41.740000 276 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1120 13:55:41.937000 281 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1120 13:55:41.993000 282 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1120 13:55:42.236000 280 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:42.283000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:42.849000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:43.414000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:43.989000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1120 13:55:44.531000 275 torch/_functorch/_aot_autograd/autograd_cache.py:852] [45/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
Capturing batches (bs=1 avail_mem=55.61 GB): 100%|| 6/6 [09:02<00:00, 87.19s/it]Capturing batches (bs=1 avail_mem=55.61 GB): 100%|| 6/6 [09:02<00:00, 90.44s/it]
[2025-11-20 13:55:50 TP0] Registering 738 cuda graph addresses
[2025-11-20 13:55:51 TP3] Capture cuda graph end. Time elapsed: 544.29 s. mem usage=4.42 GB. avail mem=54.67 GB.
[2025-11-20 13:55:51 TP2] Capture cuda graph end. Time elapsed: 544.30 s. mem usage=4.45 GB. avail mem=54.62 GB.
[2025-11-20 13:55:51 TP5] Capture cuda graph end. Time elapsed: 544.26 s. mem usage=4.42 GB. avail mem=54.80 GB.
[2025-11-20 13:55:51 TP4] Capture cuda graph end. Time elapsed: 544.28 s. mem usage=4.46 GB. avail mem=54.68 GB.
[2025-11-20 13:55:51 TP0] Capture cuda graph end. Time elapsed: 544.44 s. mem usage=4.45 GB. avail mem=55.04 GB.
[2025-11-20 13:55:51 TP6] Capture cuda graph end. Time elapsed: 544.45 s. mem usage=4.45 GB. avail mem=54.75 GB.
[2025-11-20 13:55:51 TP1] Capture cuda graph end. Time elapsed: 544.42 s. mem usage=4.43 GB. avail mem=54.65 GB.
[2025-11-20 13:55:51 TP7] Capture cuda graph end. Time elapsed: 544.48 s. mem usage=4.43 GB. avail mem=54.78 GB.
[2025-11-20 13:55:51 TP0] max_total_num_tokens=727202, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=55.04 GB
[2025-11-20 13:55:52] INFO:     Started server process [25]
[2025-11-20 13:55:52] INFO:     Waiting for application startup.
[2025-11-20 13:55:52] INFO:     Application startup complete.
[2025-11-20 13:55:52] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-11-20 13:55:53] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-11-20 13:55:53] INFO:     127.0.0.1:36324 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-20 13:55:53] Start of co-locate warmup ...
[2025-11-20 13:55:53 TP0] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-20 13:55:53] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-11-20 13:55:53] INFO:     127.0.0.1:36340 - "GET /get_model_info HTTP/1.1" 200 OK
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-20 13:55:57 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-20 13:55:57 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-20 13:55:57 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-20 13:55:58 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-20 13:55:58 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-20 13:55:58 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-20 13:55:58 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-11-20 13:55:58 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-11-20 13:55:58 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-11-20 13:55:58 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-11-20 13:55:58 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-11-20 13:55:58 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-11-20 13:55:58 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-20 13:55:58 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] start build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-11-20 13:55:58 TP6] start build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-11-20 13:56:01] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-11-20 13:56:01] INFO:     127.0.0.1:55524 - "GET /get_model_info HTTP/1.1" 200 OK
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-11-20 13:56:02 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for gfx942.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for host.
[92mSuccessfully preprocessed all matching files.[0m
fpath=/sgl-workspace/aiter/aiter/ops/triton/configs/gemm/MI300X-GEMM_BLOCKSCALE-A8W8.json
[aiter] finish build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip], cost 55.83536868s
[2025-11-20 13:56:54 TP6] finish build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip], cost 55.83536868s
[aiter] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:56:56 TP6] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:56:56 TP0] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:56:56 TP4] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:56:56 TP1] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:56:56 TP7] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:56:56 TP2] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:56:56 TP5] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:56:56 TP3] [fused_moe] using default for (7, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:09 TP0] Prefill batch, #new-seq: 1, #new-token: 667, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:14 TP6] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:14 TP4] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:14 TP5] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:14 TP7] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:14 TP0] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:14 TP1] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:15 TP2] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:15 TP3] [fused_moe] using default for (667, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:22] INFO:     127.0.0.1:55530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:57:23 TP0] Prefill batch, #new-seq: 132, #new-token: 8276, #cached-token: 88044, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:28 TP5] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:28 TP4] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:28 TP6] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:28 TP7] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:29 TP0] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:29 TP3] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:29 TP1] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:29 TP2] [fused_moe] using default for (1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:29 TP0] Prefill batch, #new-seq: 267, #new-token: 16356, #cached-token: 178089, token usage: 0.01, #running-req: 133, #queue-req: 920, 
[2025-11-20 13:57:36 TP0] Prefill batch, #new-seq: 277, #new-token: 16364, #cached-token: 185444, token usage: 0.03, #running-req: 400, #queue-req: 643, 
[2025-11-20 13:57:38 TP0] Prefill batch, #new-seq: 273, #new-token: 16344, #cached-token: 182841, token usage: 0.06, #running-req: 677, #queue-req: 370, 
[2025-11-20 13:57:40 TP0] Prefill batch, #new-seq: 74, #new-token: 4569, #cached-token: 49567, token usage: 0.08, #running-req: 950, #queue-req: 296, 
[2025-11-20 13:57:52] INFO:     127.0.0.1:36336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:57:52] The server is fired up and ready to roll!
[aiter] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:55 TP6] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:55 TP7] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:55 TP4] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:55 TP2] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:55 TP3] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:55 TP1] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:55 TP5] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:56 TP0] [fused_moe] using default for (1023, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:56 TP0] Prefill batch, #new-seq: 1, #new-token: 54, #cached-token: 670, token usage: 0.09, #running-req: 1023, #queue-req: 295, 
[aiter] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:56 TP2] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:56 TP0] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:56 TP1] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:56 TP3] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:56 TP5] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:56 TP7] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:56 TP6] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:56 TP4] [fused_moe] using default for (54, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:58] INFO:     127.0.0.1:44554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:57:58 TP0] Prefill batch, #new-seq: 1, #new-token: 33, #cached-token: 669, token usage: 0.12, #running-req: 1023, #queue-req: 294, 
[aiter] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:58 TP2] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:58 TP6] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:58 TP3] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:58 TP1] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:58 TP5] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:58 TP7] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:58 TP0] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:58 TP4] [fused_moe] using default for (33, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:59] INFO:     127.0.0.1:50210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:57:59 TP0] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 670, token usage: 0.13, #running-req: 1023, #queue-req: 293, 
[aiter] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:59 TP2] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:59 TP3] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:59 TP1] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:59 TP5] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:59 TP7] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:59 TP6] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:59 TP0] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:57:59 TP4] [fused_moe] using default for (45, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:00 TP0] Decode batch, #running-req: 1024, #token: 101158, token usage: 0.14, cuda graph: False, gen throughput (token/s): 302.53, #queue-req: 293, 
[2025-11-20 13:58:00 TP0] Prefill batch, #new-seq: 1, #new-token: 76, #cached-token: 670, token usage: 0.14, #running-req: 1023, #queue-req: 292, 
[aiter] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP2] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP1] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP3] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP5] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP6] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP7] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP0] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP4] [fused_moe] using default for (76, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00] INFO:     127.0.0.1:41672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:00] INFO:     127.0.0.1:42042 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP3] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP2] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP6] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP7] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP1] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP5] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP0] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP4] [fused_moe] using default for (1022, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:00 TP0] Prefill batch, #new-seq: 2, #new-token: 114, #cached-token: 1341, token usage: 0.14, #running-req: 1022, #queue-req: 290, 
[aiter] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:01 TP6] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:01 TP4] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:01 TP5] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:01 TP7] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:01 TP3] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:01 TP2] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:01 TP0] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:01 TP1] [fused_moe] using default for (114, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:01] INFO:     127.0.0.1:42922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:01] INFO:     127.0.0.1:46060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:01 TP0] Prefill batch, #new-seq: 2, #new-token: 147, #cached-token: 1338, token usage: 0.14, #running-req: 1022, #queue-req: 288, 
[aiter] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP5] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP1] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP6] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP4] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP2] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP7] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP3] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP0] [fused_moe] using default for (147, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06] INFO:     127.0.0.1:42946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:06] INFO:     127.0.0.1:43022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:06] INFO:     127.0.0.1:43892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:06] INFO:     127.0.0.1:44428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:06] INFO:     127.0.0.1:45278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:06] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP0] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP3] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP2] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP1] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP7] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP4] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP5] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP6] [fused_moe] using default for (1018, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:06 TP0] Prefill batch, #new-seq: 6, #new-token: 381, #cached-token: 4021, token usage: 0.14, #running-req: 1018, #queue-req: 282, 
[aiter] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP3] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP2] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP4] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP0] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP7] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP1] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP6] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP5] [fused_moe] using default for (381, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11] INFO:     127.0.0.1:41908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:11] INFO:     127.0.0.1:44574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:11] INFO:     127.0.0.1:47450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:11] INFO:     127.0.0.1:50190 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP2] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP3] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP0] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP4] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP7] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP6] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP5] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP1] [fused_moe] using default for (1020, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:11 TP0] Prefill batch, #new-seq: 4, #new-token: 344, #cached-token: 2680, token usage: 0.14, #running-req: 1020, #queue-req: 278, 
[aiter] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP4] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP6] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP7] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP5] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP0] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP1] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP3] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP2] [fused_moe] using default for (344, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12] INFO:     127.0.0.1:42100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:12] INFO:     127.0.0.1:42164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:12] INFO:     127.0.0.1:42322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:12] INFO:     127.0.0.1:42788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:12] INFO:     127.0.0.1:45998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:12] INFO:     127.0.0.1:46018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:12] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:12] INFO:     127.0.0.1:46602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:12] INFO:     127.0.0.1:46904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:12] INFO:     127.0.0.1:48992 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP5] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP4] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP3] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP1] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP7] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP2] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP0] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP6] [fused_moe] using default for (1014, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:12 TP0] Prefill batch, #new-seq: 10, #new-token: 851, #cached-token: 6702, token usage: 0.14, #running-req: 1014, #queue-req: 268, 
[aiter] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP5] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP1] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP2] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP6] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP0] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP7] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP3] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP4] [fused_moe] using default for (851, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17] INFO:     127.0.0.1:43614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:17] INFO:     127.0.0.1:44534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:17] INFO:     127.0.0.1:44956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:17] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:17] INFO:     127.0.0.1:48490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:17] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:17] INFO:     127.0.0.1:50416 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP2] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP6] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP5] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP3] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP1] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP7] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP4] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP0] [fused_moe] using default for (1017, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:17 TP0] Prefill batch, #new-seq: 7, #new-token: 409, #cached-token: 4687, token usage: 0.15, #running-req: 1017, #queue-req: 261, 
[aiter] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:20 TP3] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:20 TP2] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:20 TP1] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:20 TP0] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:20 TP4] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:20 TP6] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:20 TP7] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP5] [fused_moe] using default for (409, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21] INFO:     127.0.0.1:41924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21] INFO:     127.0.0.1:43226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21] INFO:     127.0.0.1:43832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21] INFO:     127.0.0.1:44830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21] INFO:     127.0.0.1:45458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21] INFO:     127.0.0.1:49678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21] INFO:     127.0.0.1:50308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21 TP0] Prefill batch, #new-seq: 7, #new-token: 366, #cached-token: 4689, token usage: 0.15, #running-req: 1017, #queue-req: 254, 
[aiter] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP2] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP3] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP4] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP0] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP6] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP7] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP1] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP5] [fused_moe] using default for (366, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21] INFO:     127.0.0.1:47698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21] INFO:     127.0.0.1:50274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21 TP0] Prefill batch, #new-seq: 2, #new-token: 75, #cached-token: 1340, token usage: 0.15, #running-req: 1022, #queue-req: 252, 
[aiter] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP3] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP2] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP1] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP0] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP6] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP4] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP5] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP7] [fused_moe] using default for (75, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21] INFO:     127.0.0.1:43426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21] INFO:     127.0.0.1:45066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21] INFO:     127.0.0.1:46002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21] INFO:     127.0.0.1:46280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21] INFO:     127.0.0.1:48258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21] INFO:     127.0.0.1:49284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21] INFO:     127.0.0.1:49508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21] INFO:     127.0.0.1:49792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:21] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP3] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP7] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP2] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP6] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP1] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP0] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP5] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP4] [fused_moe] using default for (1015, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:21 TP0] Prefill batch, #new-seq: 9, #new-token: 576, #cached-token: 6028, token usage: 0.15, #running-req: 1015, #queue-req: 243, 
[aiter] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:27 TP5] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:27 TP6] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:27 TP7] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:27 TP4] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:27 TP0] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:27 TP1] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:27 TP3] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:27 TP2] [fused_moe] using default for (576, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:27] INFO:     127.0.0.1:41654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:27] INFO:     127.0.0.1:43696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:27] INFO:     127.0.0.1:43756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:27] INFO:     127.0.0.1:44284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:27] INFO:     127.0.0.1:47968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:27] INFO:     127.0.0.1:49054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:27] INFO:     127.0.0.1:49368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:27] INFO:     127.0.0.1:49644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:27] INFO:     127.0.0.1:50174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:27] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:27 TP0] Prefill batch, #new-seq: 10, #new-token: 650, #cached-token: 6702, token usage: 0.15, #running-req: 1014, #queue-req: 233, 
[aiter] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP3] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP1] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP2] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP7] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP4] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP0] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP6] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP5] [fused_moe] using default for (650, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP3] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP0] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP4] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP7] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP2] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP6] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP5] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28 TP1] [fused_moe] using default for (1019, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:28] INFO:     127.0.0.1:42058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:28] INFO:     127.0.0.1:43268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:28] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:28] INFO:     127.0.0.1:48836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:28] INFO:     127.0.0.1:49362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:28 TP0] Prefill batch, #new-seq: 5, #new-token: 392, #cached-token: 3352, token usage: 0.15, #running-req: 1019, #queue-req: 228, 
[aiter] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:29 TP7] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:29 TP4] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:29 TP3] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:29 TP2] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:29 TP6] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:29 TP0] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:29 TP5] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:29 TP1] [fused_moe] using default for (392, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:29] INFO:     127.0.0.1:42280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:29] INFO:     127.0.0.1:42854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:29] INFO:     127.0.0.1:44944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:29] INFO:     127.0.0.1:45622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:29] INFO:     127.0.0.1:45982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:29] INFO:     127.0.0.1:46130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:29] INFO:     127.0.0.1:46630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:29] INFO:     127.0.0.1:47352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:29] INFO:     127.0.0.1:49874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:29 TP0] Prefill batch, #new-seq: 9, #new-token: 533, #cached-token: 6029, token usage: 0.15, #running-req: 1015, #queue-req: 219, 
[aiter] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP5] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP6] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP4] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP1] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP3] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP7] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP0] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP2] [fused_moe] using default for (533, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34] INFO:     127.0.0.1:44004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:34] INFO:     127.0.0.1:45794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:34] INFO:     127.0.0.1:45882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:34] INFO:     127.0.0.1:46150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:34] INFO:     127.0.0.1:47192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:34] INFO:     127.0.0.1:47514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:34] INFO:     127.0.0.1:47934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:34] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP5] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP7] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP4] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP3] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP1] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP2] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP0] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP6] [fused_moe] using default for (1016, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:34 TP0] Prefill batch, #new-seq: 8, #new-token: 454, #cached-token: 5356, token usage: 0.15, #running-req: 1016, #queue-req: 211, 
[aiter] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:35 TP3] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:35 TP7] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:35 TP2] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:35 TP1] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:35 TP4] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:35 TP0] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:35 TP6] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:35 TP5] [fused_moe] using default for (454, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:35] INFO:     127.0.0.1:42534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:35] INFO:     127.0.0.1:43204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:35] INFO:     127.0.0.1:43572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:35] INFO:     127.0.0.1:44742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:35] INFO:     127.0.0.1:46916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:35] INFO:     127.0.0.1:48088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:35] INFO:     127.0.0.1:49630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:35] INFO:     127.0.0.1:49682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:36 TP0] Prefill batch, #new-seq: 8, #new-token: 371, #cached-token: 5358, token usage: 0.16, #running-req: 1016, #queue-req: 203, 
[aiter] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:36 TP3] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:36 TP1] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:36 TP2] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:36 TP4] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:36 TP7] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:36 TP6] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:36 TP0] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:36 TP5] [fused_moe] using default for (371, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:37] INFO:     127.0.0.1:42148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:37] INFO:     127.0.0.1:42752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:37] INFO:     127.0.0.1:42822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:37] INFO:     127.0.0.1:42906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:37] INFO:     127.0.0.1:45260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:37] INFO:     127.0.0.1:47466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:37] INFO:     127.0.0.1:47588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:37] INFO:     127.0.0.1:50634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:37 TP0] Prefill batch, #new-seq: 8, #new-token: 499, #cached-token: 5360, token usage: 0.16, #running-req: 1016, #queue-req: 195, 
[aiter] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:37 TP3] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:37 TP2] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:37 TP0] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:37 TP6] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:37 TP4] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:37 TP7] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:37 TP1] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:37 TP5] [fused_moe] using default for (499, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:37] INFO:     127.0.0.1:42578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:37] INFO:     127.0.0.1:46204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:37] INFO:     127.0.0.1:46248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:37] INFO:     127.0.0.1:47270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:37] INFO:     127.0.0.1:47840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:37] INFO:     127.0.0.1:48444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:37 TP0] Prefill batch, #new-seq: 6, #new-token: 331, #cached-token: 4019, token usage: 0.16, #running-req: 1018, #queue-req: 189, 
[aiter] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP2] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP1] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP3] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP7] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP4] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP0] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP6] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP5] [fused_moe] using default for (331, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38] INFO:     127.0.0.1:41640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:43122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:43372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:43804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:47700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:48154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:48586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:49962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:50474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38 TP0] Prefill batch, #new-seq: 10, #new-token: 618, #cached-token: 6698, token usage: 0.16, #running-req: 1014, #queue-req: 179, 
[aiter] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP3] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP2] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP0] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP7] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP4] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP6] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP1] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38 TP5] [fused_moe] using default for (618, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:38] INFO:     127.0.0.1:42726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:42840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:43440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:44112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:44300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:44388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:44404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:45024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:48142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38] INFO:     127.0.0.1:49524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:38 TP0] Prefill batch, #new-seq: 10, #new-token: 528, #cached-token: 6700, token usage: 0.16, #running-req: 1014, #queue-req: 169, 
[aiter] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP3] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP2] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP1] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP7] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP6] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP0] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP4] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP5] [fused_moe] using default for (528, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39] INFO:     127.0.0.1:43370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:39] INFO:     127.0.0.1:43384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:39] INFO:     127.0.0.1:43550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:39] INFO:     127.0.0.1:43884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:39] INFO:     127.0.0.1:43932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:39] INFO:     127.0.0.1:44628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:39] INFO:     127.0.0.1:44968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:39] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:39] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:39] INFO:     127.0.0.1:48382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:39 TP0] Prefill batch, #new-seq: 10, #new-token: 511, #cached-token: 6700, token usage: 0.16, #running-req: 1014, #queue-req: 159, 
[aiter] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP2] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP6] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP4] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP3] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP0] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP7] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP1] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:39 TP5] [fused_moe] using default for (511, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:40] INFO:     127.0.0.1:42110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:40] INFO:     127.0.0.1:48970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:40] INFO:     127.0.0.1:50122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:40] INFO:     127.0.0.1:50302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:40 TP0] Prefill batch, #new-seq: 4, #new-token: 209, #cached-token: 2678, token usage: 0.16, #running-req: 1020, #queue-req: 155, 
[aiter] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:40 TP3] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:40 TP4] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:40 TP6] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:40 TP2] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:40 TP7] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:40 TP0] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:40 TP1] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:40 TP5] [fused_moe] using default for (209, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41] INFO:     127.0.0.1:42446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:45164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:46348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:46470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:46722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:48272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:48316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:48824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:49200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:50574 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP6] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP4] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP2] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP0] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP3] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP7] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP1] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP5] [fused_moe] using default for (1013, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP0] Prefill batch, #new-seq: 11, #new-token: 695, #cached-token: 7368, token usage: 0.16, #running-req: 1013, #queue-req: 144, 
[aiter] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP3] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP2] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP0] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP7] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP6] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP4] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP1] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP5] [fused_moe] using default for (695, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41] INFO:     127.0.0.1:42542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:42664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:43716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:46318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:47764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:49460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:49924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:50654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41 TP0] Prefill batch, #new-seq: 8, #new-token: 484, #cached-token: 5365, token usage: 0.16, #running-req: 1016, #queue-req: 136, 
[aiter] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP2] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP3] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP1] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP7] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP5] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP6] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP0] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP4] [fused_moe] using default for (484, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41] INFO:     127.0.0.1:41814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:42434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:44050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:44080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:45006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:46022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:46562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:46698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:47274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:47366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:49068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:41] INFO:     127.0.0.1:49812 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP3] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP2] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP7] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP1] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP6] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP5] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP0] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP4] [fused_moe] using default for (1010, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:41 TP0] Prefill batch, #new-seq: 14, #new-token: 891, #cached-token: 9377, token usage: 0.16, #running-req: 1010, #queue-req: 122, 
[aiter] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:42 TP3] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:42 TP2] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:42 TP0] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:42 TP5] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:42 TP7] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:42 TP4] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:42 TP6] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:42 TP1] [fused_moe] using default for (891, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:42] INFO:     127.0.0.1:42628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:42] INFO:     127.0.0.1:42728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:42] INFO:     127.0.0.1:44208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:42] INFO:     127.0.0.1:47018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:42] INFO:     127.0.0.1:48046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:42] INFO:     127.0.0.1:48656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:42] INFO:     127.0.0.1:49314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:42] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:42 TP0] Prefill batch, #new-seq: 8, #new-token: 552, #cached-token: 5362, token usage: 0.16, #running-req: 1016, #queue-req: 114, 
[aiter] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP7] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP0] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP4] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP6] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP5] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP1] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP3] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP2] [fused_moe] using default for (552, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43] INFO:     127.0.0.1:42236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:43] INFO:     127.0.0.1:42524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:43] INFO:     127.0.0.1:43162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:43] INFO:     127.0.0.1:44400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:43] INFO:     127.0.0.1:45090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:43] INFO:     127.0.0.1:46424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:43] INFO:     127.0.0.1:46478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:43] INFO:     127.0.0.1:47722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:43] INFO:     127.0.0.1:48572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:43] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:43] INFO:     127.0.0.1:49096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:43] INFO:     127.0.0.1:49742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:43] INFO:     127.0.0.1:50016 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP5] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP4] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP3] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP7] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP1] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP2] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP0] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP6] [fused_moe] using default for (1011, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:43 TP0] Prefill batch, #new-seq: 13, #new-token: 927, #cached-token: 8713, token usage: 0.16, #running-req: 1011, #queue-req: 101, 
[aiter] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP0] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP6] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP5] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP1] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP7] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP4] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP2] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP3] [fused_moe] using default for (927, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45] INFO:     127.0.0.1:43504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:44020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:44386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:44688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:44700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:45750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:47010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:47108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:49594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:49752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45 TP0] Prefill batch, #new-seq: 10, #new-token: 542, #cached-token: 6702, token usage: 0.16, #running-req: 1014, #queue-req: 91, 
[aiter] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP0] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP2] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP1] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP6] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP5] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP4] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP3] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP7] [fused_moe] using default for (542, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45] INFO:     127.0.0.1:42268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:43216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:43794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:44070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:44550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:44662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:44676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:44844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:45422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:46464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:46786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:49656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:45] INFO:     127.0.0.1:50684 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP2] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP6] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP1] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP5] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP3] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP7] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP0] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:45 TP4] [fused_moe] using default for (1009, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:46 TP0] Prefill batch, #new-seq: 15, #new-token: 829, #cached-token: 10048, token usage: 0.16, #running-req: 1009, #queue-req: 76, 
[aiter] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:46 TP5] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:46 TP1] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:46 TP3] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:46 TP2] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:46 TP4] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:46 TP7] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:46 TP0] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:46 TP6] [fused_moe] using default for (829, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:47] INFO:     127.0.0.1:43186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47] INFO:     127.0.0.1:46818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47] INFO:     127.0.0.1:47092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47] INFO:     127.0.0.1:49766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47 TP0] Prefill batch, #new-seq: 6, #new-token: 441, #cached-token: 4017, token usage: 0.17, #running-req: 1018, #queue-req: 70, 
[aiter] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:47 TP0] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:47 TP3] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:47 TP1] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:47 TP5] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:47 TP7] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:47 TP4] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:47 TP2] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:47 TP6] [fused_moe] using default for (441, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:47] INFO:     127.0.0.1:41868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47] INFO:     127.0.0.1:42610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47] INFO:     127.0.0.1:43054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47] INFO:     127.0.0.1:44162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47] INFO:     127.0.0.1:44364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47] INFO:     127.0.0.1:44866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47] INFO:     127.0.0.1:45166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47] INFO:     127.0.0.1:46264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47] INFO:     127.0.0.1:47512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47] INFO:     127.0.0.1:49708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:47 TP0] Prefill batch, #new-seq: 11, #new-token: 714, #cached-token: 7367, token usage: 0.17, #running-req: 1013, #queue-req: 59, 
[aiter] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP1] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP5] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP3] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP2] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP0] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP7] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP4] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP6] [fused_moe] using default for (714, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48] INFO:     127.0.0.1:41978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:42066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:43466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:43556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:45028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:46332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:47302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:47448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:47786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:48498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:48524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:48718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:49514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:49560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:49566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:48] INFO:     127.0.0.1:50666 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP3] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP1] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP5] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP7] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP4] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP0] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP6] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP2] [fused_moe] using default for (1006, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:48 TP0] Prefill batch, #new-seq: 18, #new-token: 1215, #cached-token: 12059, token usage: 0.17, #running-req: 1006, #queue-req: 41, 
[2025-11-20 13:58:54] INFO:     127.0.0.1:44106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:54] INFO:     127.0.0.1:45466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:54] INFO:     127.0.0.1:45610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:54] INFO:     127.0.0.1:46200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:54] INFO:     127.0.0.1:46388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:54] INFO:     127.0.0.1:47338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:54] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:54] INFO:     127.0.0.1:47678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:54] INFO:     127.0.0.1:48020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:54] INFO:     127.0.0.1:48846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:54] INFO:     127.0.0.1:49106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:54] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:54] INFO:     127.0.0.1:50774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:54 TP0] Prefill batch, #new-seq: 13, #new-token: 748, #cached-token: 8710, token usage: 0.17, #running-req: 1011, #queue-req: 28, 
[aiter] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP5] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP0] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP4] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP6] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP7] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP1] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP2] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP3] [fused_moe] using default for (748, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55] INFO:     127.0.0.1:42520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:43648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:44492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:44984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:45418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:45970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:46768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:48032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:49040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:50780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55 TP0] Prefill batch, #new-seq: 11, #new-token: 670, #cached-token: 7370, token usage: 0.17, #running-req: 1013, #queue-req: 17, 
[aiter] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP2] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP6] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP4] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP1] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP3] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP0] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP7] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP5] [fused_moe] using default for (670, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55] INFO:     127.0.0.1:42018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:42356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:42586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:43580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:43590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:44758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:45652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:45886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:46034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:48174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:48558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:55] INFO:     127.0.0.1:48948 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP2] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP6] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP3] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP7] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP0] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP4] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP1] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP5] [fused_moe] using default for (1012, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:55 TP0] Prefill batch, #new-seq: 12, #new-token: 715, #cached-token: 8038, token usage: 0.17, #running-req: 1012, #queue-req: 5, 
[aiter] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:56 TP7] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:56 TP4] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:56 TP5] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:56 TP0] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:56 TP6] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:56 TP2] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:56 TP3] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:56 TP1] [fused_moe] using default for (715, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:56] INFO:     127.0.0.1:42460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:56] INFO:     127.0.0.1:43292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:56] INFO:     127.0.0.1:43540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:56] INFO:     127.0.0.1:43954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:56] INFO:     127.0.0.1:46418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:56] INFO:     127.0.0.1:47600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:56] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:56] INFO:     127.0.0.1:48378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:56] INFO:     127.0.0.1:49612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:56] INFO:     127.0.0.1:49976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:56] INFO:     127.0.0.1:50558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:56] INFO:     127.0.0.1:50850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:56 TP0] Prefill batch, #new-seq: 5, #new-token: 269, #cached-token: 3354, token usage: 0.17, #running-req: 1012, #queue-req: 0, 
[aiter] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:57 TP6] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:57 TP5] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:57 TP0] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:57 TP1] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:57 TP3] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:57 TP2] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:57 TP7] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:57 TP4] [fused_moe] using default for (269, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:57] INFO:     127.0.0.1:42984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:57] INFO:     127.0.0.1:44328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:57] INFO:     127.0.0.1:45158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:57] INFO:     127.0.0.1:45180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:57] INFO:     127.0.0.1:47174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:57] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:41884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:42504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:42750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:47520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:47830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:48282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP3] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP2] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP1] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP5] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP7] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP6] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP0] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP4] [fused_moe] using default for (1003, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58] INFO:     127.0.0.1:42944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:43014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:43604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:43888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:45572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:47854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:49832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:49908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:49952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:50404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:50448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP3] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP2] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP1] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP6] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP5] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP7] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP0] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP4] [fused_moe] using default for (989, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58] INFO:     127.0.0.1:42126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:43968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:45942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:47746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:48616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:48930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:49112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:49280 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP2] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP3] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP1] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP6] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP7] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP5] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP0] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP4] [fused_moe] using default for (978, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58] INFO:     127.0.0.1:42662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:42708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:43138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:43174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:44092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:44420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:47532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:48340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:48578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:50160 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP1] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP2] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP3] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP5] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP6] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP7] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP0] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP4] [fused_moe] using default for (967, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP0] Decode batch, #running-req: 978, #token: 119776, token usage: 0.16, cuda graph: False, gen throughput (token/s): 695.55, #queue-req: 0, 
[2025-11-20 13:58:58] INFO:     127.0.0.1:42672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:42764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:44780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:49772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:50882 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP3] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP0] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP2] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP1] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP5] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP6] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP4] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP7] [fused_moe] using default for (955, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58] INFO:     127.0.0.1:42008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:42176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:42576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:44312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:45768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:48556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:48602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:48792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:49454 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP2] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP1] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP6] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP3] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP5] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP7] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP0] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP4] [fused_moe] using default for (945, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58] INFO:     127.0.0.1:41830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:42302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:42970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:43878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:44774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:45170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:49334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:49806 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP2] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP6] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP1] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP5] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP3] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP7] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP0] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58 TP4] [fused_moe] using default for (935, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:58] INFO:     127.0.0.1:43106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:43320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:44578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:44878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:46996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:48110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:48342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:58] INFO:     127.0.0.1:50722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:42802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:43862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:44510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:45772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:47316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:47672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:47814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:49120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:49420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:49532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:49930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:50756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:50866 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP2] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP3] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP1] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP6] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP5] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP7] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP0] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP4] [fused_moe] using default for (914, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59] INFO:     127.0.0.1:42314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:42684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:44128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:45618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:47898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:48058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:48070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:48698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:49082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:49244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:50406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:50542 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP5] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP6] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP7] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP1] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP2] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP3] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP4] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP0] [fused_moe] using default for (902, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59] INFO:     127.0.0.1:42406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:42408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:43392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:45286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:48814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:49234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:49574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:50386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:50944 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP0] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP4] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP2] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP3] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP5] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP1] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP7] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP6] [fused_moe] using default for (893, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59] INFO:     127.0.0.1:41704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:42182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:44260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:44692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:45036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:45526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:45788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:45956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:48210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:48872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:49210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:49402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:49592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:50100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:50632 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP3] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP5] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP1] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP2] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP7] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP6] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP0] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP4] [fused_moe] using default for (875, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59] INFO:     127.0.0.1:42960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:43248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:44176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:44570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:45062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:46328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:46526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:46732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:46900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:47026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:47142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:48894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP2] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP3] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP1] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP5] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP6] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP7] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP0] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP4] [fused_moe] using default for (862, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59] INFO:     127.0.0.1:41754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:42456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:42638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:43068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:44232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:58:59] INFO:     127.0.0.1:48012 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP2] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP3] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP1] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP5] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP6] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP7] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP0] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:58:59 TP4] [fused_moe] using default for (856, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00] INFO:     127.0.0.1:41992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:42226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:44116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:44622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:45126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:45274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:46584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:47086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:47288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:48300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:48522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:49658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:50072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP2] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP3] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP1] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP5] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP6] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP7] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP0] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP4] [fused_moe] using default for (842, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00] INFO:     127.0.0.1:42220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:43512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:43906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:45142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:45908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:49784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:50746 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP3] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP2] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP1] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP6] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP5] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP7] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP0] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP4] [fused_moe] using default for (834, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00] INFO:     127.0.0.1:43088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:43404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:44892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:45058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:46736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:47656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:49500 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP3] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP2] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP0] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP1] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP5] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP7] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP4] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00 TP6] [fused_moe] using default for (827, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:00] INFO:     127.0.0.1:41940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:42770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:43658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:43796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:45662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:46810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:47208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:48328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:00] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP1] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP2] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP3] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP6] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP7] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP4] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP5] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP0] [fused_moe] using default for (816, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04] INFO:     127.0.0.1:41680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:43082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:43472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:44472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:46182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:46422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:47042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:48648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:50052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:50222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:51350 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP0] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP3] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP2] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP1] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP5] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP7] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP6] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP4] [fused_moe] using default for (805, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04] INFO:     127.0.0.1:42242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:42418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:44226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:45686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:45716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:47012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:47486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:49488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:49750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:49996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:04] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP2] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP6] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP3] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP7] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP1] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP0] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP5] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:04 TP4] [fused_moe] using default for (793, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05] INFO:     127.0.0.1:45476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:46514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:46922 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP3] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP2] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP6] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP7] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP0] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP1] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP5] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP4] [fused_moe] using default for (790, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05] INFO:     127.0.0.1:41694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:42068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:42332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:44818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:44904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:45202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:45510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:45586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:45670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:46502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:46844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:50502 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP2] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP3] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP6] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP7] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP1] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP0] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP5] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP4] [fused_moe] using default for (778, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05] INFO:     127.0.0.1:41952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:42374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:45300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:45818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:46136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:47686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:48136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:49144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP3] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP2] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP6] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP7] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP1] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP5] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP0] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP4] [fused_moe] using default for (765, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05] INFO:     127.0.0.1:42890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:42902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:43632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:46112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:46476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:47188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:47476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:48100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:48462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:48784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:50456 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP2] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP3] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP6] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP7] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP1] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP0] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP5] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP4] [fused_moe] using default for (754, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05] INFO:     127.0.0.1:41896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:45954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:46586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:46850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:48120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:49490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:49588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:52044 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP3] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP2] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP6] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP7] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP1] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP0] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP5] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP4] [fused_moe] using default for (744, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP3] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP2] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP6] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP7] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP0] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP1] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP4] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP5] [fused_moe] using default for (733, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05] INFO:     127.0.0.1:42786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:43538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:46430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:46642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:47744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:49260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:49290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:49448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:50370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:50378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:52060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:42566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:43188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:43662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:44644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:46398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:46438 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP3] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP2] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP6] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP7] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP1] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP0] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP5] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP4] [fused_moe] using default for (727, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05] INFO:     127.0.0.1:42290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:43766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:44604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:44936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:45492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:49670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP3] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP2] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP6] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP7] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP0] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP1] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP4] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP5] [fused_moe] using default for (718, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05] INFO:     127.0.0.1:41770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:41926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:42392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:43740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:43982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:44864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:47414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:48270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:48432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:50704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:51506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:05] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP6] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP5] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP7] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP4] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP0] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP2] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP1] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:05 TP3] [fused_moe] using default for (702, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06] INFO:     127.0.0.1:45244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:45546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:47912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:48712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:49236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:49840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:50252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:51614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:51756 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP3] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP2] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP7] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP6] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP1] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP0] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP5] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP4] [fused_moe] using default for (692, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06] INFO:     127.0.0.1:41802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:42488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:45008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:45190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:46170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:46576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:46896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:48744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:49934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:51670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP0] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP2] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP3] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP1] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP5] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP7] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP6] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP4] [fused_moe] using default for (680, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06] INFO:     127.0.0.1:44330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:45334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:46224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:46360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:47998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:53194 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP2] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP6] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP3] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP7] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP1] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP5] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP0] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP4] [fused_moe] using default for (674, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06] INFO:     127.0.0.1:42088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:45554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:46776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:49224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:49384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:49910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:50326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:51718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:52016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:52222 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP2] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP3] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP6] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP7] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP1] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP5] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP0] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP4] [fused_moe] using default for (661, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06] INFO:     127.0.0.1:41852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:43488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:44438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:44452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:44670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:45388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:45990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:46308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:46450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:49002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:49398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:50692 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP2] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP3] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP6] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP7] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP1] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP5] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP0] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP4] [fused_moe] using default for (647, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06] INFO:     127.0.0.1:41808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:42674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:42734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:44422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:44462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:45536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:47990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:48362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:50526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:50568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:51754 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP2] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP3] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP6] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP7] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP1] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP5] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP0] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP4] [fused_moe] using default for (635, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06] INFO:     127.0.0.1:41956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:41972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:43916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:44094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:46166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:47706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:47826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:50144 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP2] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP3] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP6] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP7] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP1] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP5] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP0] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP4] [fused_moe] using default for (627, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06] INFO:     127.0.0.1:41828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:42054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:43000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:44270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:44794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:45596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:45860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:47072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:47682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:48358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:50042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:06] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP2] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP3] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP6] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP7] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP1] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP5] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP0] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:06 TP4] [fused_moe] using default for (615, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07] INFO:     127.0.0.1:42932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:43686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:44346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:46986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:50510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP2] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP3] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP6] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP7] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP1] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP5] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP0] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP4] [fused_moe] using default for (607, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07] INFO:     127.0.0.1:43148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:44190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:47632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:51174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:52590 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP2] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP3] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP6] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP7] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP1] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP5] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP0] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP4] [fused_moe] using default for (600, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07] INFO:     127.0.0.1:43728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:44034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:44852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:45212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:45446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:46426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:48228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:49896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:51632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:52956 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP2] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP3] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP6] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP7] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP1] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP5] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP0] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP4] [fused_moe] using default for (589, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07] INFO:     127.0.0.1:42346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:43848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:44508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:44556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:45562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:45808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:45916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:48416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:48672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:50466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:52606 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP3] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP2] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP6] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP7] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP1] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP5] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP0] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP4] [fused_moe] using default for (575, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07] INFO:     127.0.0.1:42026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:43270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:43354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:43840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:44158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:44730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:46316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:47150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:47398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:47548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:48268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:48632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:49268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:51044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:51572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:51822 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP2] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP3] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP6] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP7] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP1] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP5] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP0] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP4] [fused_moe] using default for (558, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07] INFO:     127.0.0.1:41794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:42830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:44216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:44870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:49432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:50826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:51196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:51420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:52314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP3] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP2] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP6] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP7] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP1] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP5] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP0] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP4] [fused_moe] using default for (545, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07] INFO:     127.0.0.1:42754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:42814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:42878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:43596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:46096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:47252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:48352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:49372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:50284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:51784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:52824 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP6] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP5] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP7] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP4] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP2] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP0] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP3] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP1] [fused_moe] using default for (534, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP0] Decode batch, #running-req: 545, #token: 83881, token usage: 0.12, cuda graph: False, gen throughput (token/s): 3264.16, #queue-req: 0, 
[2025-11-20 13:59:07] INFO:     127.0.0.1:42974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:44792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:44918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:47292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:48298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:52108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:07] INFO:     127.0.0.1:52720 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP2] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP6] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP3] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP7] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP1] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP0] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP5] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:07 TP4] [fused_moe] using default for (525, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:42650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:43702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:44712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:45870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:46218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:47552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:51252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:51812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:52172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:52490 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP2] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP6] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP3] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP7] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP1] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP0] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP5] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP4] [fused_moe] using default for (514, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08] INFO:     127.0.0.1:43254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:45064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:48828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:48860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:49424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:50354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:51788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:51950 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP6] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP0] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP2] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP1] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP3] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP7] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP4] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP5] [fused_moe] using default for (505, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08] INFO:     127.0.0.1:44164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:44354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:44522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:44752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:45814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:46684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:48082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:52034 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP2] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP3] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP7] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP0] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP6] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP4] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP5] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP1] [fused_moe] using default for (495, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08] INFO:     127.0.0.1:46050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:49132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:49322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:49692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:49942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:52856 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP2] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP6] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP4] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP3] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP1] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP0] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP5] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP7] [fused_moe] using default for (488, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08] INFO:     127.0.0.1:45436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:45896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:47640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:47748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:51908 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP2] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP6] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP4] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP0] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP3] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP7] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP1] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP5] [fused_moe] using default for (483, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08] INFO:     127.0.0.1:41738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:42324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:43286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:43674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:44928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:45220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:45728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:47976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:49034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:49472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:51132 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP2] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP6] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP3] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP7] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP0] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP4] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP1] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP5] [fused_moe] using default for (470, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08] INFO:     127.0.0.1:44974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:45298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:47406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:47526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:48408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:49044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:49350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:49604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:50310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:50906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:52580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:08] INFO:     127.0.0.1:52752 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP2] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP6] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP3] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP7] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP1] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP0] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP5] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:08 TP4] [fused_moe] using default for (458, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09] INFO:     127.0.0.1:45816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:49304 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP2] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP6] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP3] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP7] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP1] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP5] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP0] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP4] [fused_moe] using default for (455, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09] INFO:     127.0.0.1:42142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:43866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:44410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:49406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:50622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:51834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:52088 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP2] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP3] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP6] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP7] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP1] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP5] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP0] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP4] [fused_moe] using default for (447, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09] INFO:     127.0.0.1:42828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:46190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:46334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:47780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:47816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:48190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:49158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:51792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:52416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:53202 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP2] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP3] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP7] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP6] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP1] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP5] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP0] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP4] [fused_moe] using default for (437, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09] INFO:     127.0.0.1:41656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:42626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:43934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:45044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:46668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:47572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:52164 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP3] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP2] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP6] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP7] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP1] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP5] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP0] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP4] [fused_moe] using default for (429, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09] INFO:     127.0.0.1:41712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:42256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:47496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:49416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:50392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:53054 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP2] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP3] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP7] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP6] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP1] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP5] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP0] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP4] [fused_moe] using default for (421, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09] INFO:     127.0.0.1:45746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:46530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:47158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:48242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:52618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:53110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP2] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP3] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP7] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP6] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP1] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP5] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP0] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP4] [fused_moe] using default for (414, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09] INFO:     127.0.0.1:44802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:45330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:46682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:49182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:50580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:53538 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP2] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP3] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP6] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP7] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP1] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP5] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP0] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP4] [fused_moe] using default for (408, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09] INFO:     127.0.0.1:43070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:44990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:45638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:47040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:48598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:52442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:52714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP2] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP3] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP7] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP6] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP1] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP5] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP0] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP4] [fused_moe] using default for (399, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09] INFO:     127.0.0.1:45266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:46232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:47266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:51216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:52080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:52402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:52992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:09] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP2] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP3] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP7] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP6] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP1] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP5] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP0] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:09 TP4] [fused_moe] using default for (389, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:10] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:10] INFO:     127.0.0.1:51606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:10] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:10] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:10] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP5] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP4] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP7] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP6] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP1] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP0] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP3] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP2] [fused_moe] using default for (384, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:48976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:49822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:52342 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP3] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP5] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP1] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP7] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP4] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP2] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP0] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP6] [fused_moe] using default for (379, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13] INFO:     127.0.0.1:42676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:43568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:48700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:49538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:51916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:52828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP0] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP3] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP1] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP4] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP5] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP7] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP2] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP6] [fused_moe] using default for (372, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13] INFO:     127.0.0.1:43522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:44474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:47876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:48474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:50240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:51444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP1] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP5] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP2] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP6] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP3] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP7] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP0] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP4] [fused_moe] using default for (363, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13] INFO:     127.0.0.1:44726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:47884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:51594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:52566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:52634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:52780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:13] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP2] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP1] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP5] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP6] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP3] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP7] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP0] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:13 TP4] [fused_moe] using default for (355, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14] INFO:     127.0.0.1:46976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:48808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:49162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:51770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:52886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP2] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP3] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP1] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP5] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP6] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP7] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP0] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP4] [fused_moe] using default for (347, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14] INFO:     127.0.0.1:42530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:43420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:44236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:45830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:50954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:51988 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP3] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP2] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP1] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP5] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP7] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP6] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP0] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP4] [fused_moe] using default for (339, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14] INFO:     127.0.0.1:45922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:47390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:49726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:51700 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP2] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP3] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP1] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP5] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP6] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP7] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP0] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP4] [fused_moe] using default for (334, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14] INFO:     127.0.0.1:43520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:50196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:50488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:51116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:51482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:52554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP3] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP2] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP1] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP7] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP5] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP6] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP0] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP4] [fused_moe] using default for (324, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14] INFO:     127.0.0.1:48878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:49552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:50930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:51162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:51544 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP3] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP2] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP1] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP6] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP5] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP7] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP0] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP4] [fused_moe] using default for (318, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:51860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP3] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP2] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP1] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP5] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP6] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP7] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP0] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP4] [fused_moe] using default for (315, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14] INFO:     127.0.0.1:43308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:43340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:44198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:50046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:51398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:52934 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP3] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP2] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP1] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP7] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP5] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP6] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP0] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP4] [fused_moe] using default for (308, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14] INFO:     127.0.0.1:42252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:45372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:51798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:52068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:52712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:53050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:53218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:53558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:53566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:14] INFO:     127.0.0.1:53584 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP3] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP2] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP1] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP5] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP6] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP7] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP0] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:14 TP4] [fused_moe] using default for (298, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:52054 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP0] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP3] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP2] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP1] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP5] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP6] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP7] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP4] [fused_moe] using default for (296, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15] INFO:     127.0.0.1:42604 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP5] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP1] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15] INFO:     127.0.0.1:45076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:45846 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP3] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP7] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP2] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP6] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP4] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP0] [fused_moe] using default for (293, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15] INFO:     127.0.0.1:42572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:43090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:43454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:45426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:51422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:53560 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP0] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP2] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP3] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP1] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP7] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP6] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP5] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP4] [fused_moe] using default for (286, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15] INFO:     127.0.0.1:41842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:43332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:43818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:51194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:53614 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP1] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP5] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP3] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP7] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP2] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP6] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP0] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP4] [fused_moe] using default for (280, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15] INFO:     127.0.0.1:48062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:51236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:51296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:53268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:53320 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP3] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP2] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP1] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP6] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP5] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP7] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP0] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP4] [fused_moe] using default for (274, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15] INFO:     127.0.0.1:42310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:42866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:46554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:47910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:47936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:51384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:52344 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP2] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP3] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP1] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP7] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP5] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP6] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP0] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP4] [fused_moe] using default for (267, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15] INFO:     127.0.0.1:43772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:45752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:50060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:51200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:51334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:51674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:52608 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP3] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP2] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP1] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP6] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP7] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP5] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP0] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP4] [fused_moe] using default for (260, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15] INFO:     127.0.0.1:43508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:51712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:53160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:53446 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP3] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP1] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP2] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP5] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP6] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP7] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP0] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP4] [fused_moe] using default for (255, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15] INFO:     127.0.0.1:42562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:44250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:48990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:15] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP2] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP3] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP1] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP6] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP7] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP5] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP0] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:15 TP4] [fused_moe] using default for (251, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16] INFO:     127.0.0.1:43902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:44736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:47654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:50432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP2] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP1] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP3] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP5] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP6] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP7] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP0] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP4] [fused_moe] using default for (245, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP0] Decode batch, #running-req: 251, #token: 48543, token usage: 0.07, cuda graph: False, gen throughput (token/s): 1834.95, #queue-req: 0, 
[2025-11-20 13:59:16] INFO:     127.0.0.1:42474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:48198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:51684 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP2] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP3] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP1] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP6] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP7] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP5] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP0] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP4] [fused_moe] using default for (242, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16] INFO:     127.0.0.1:48964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:51084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:52348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:53392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:53554 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP2] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP3] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP1] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP5] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP7] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP6] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP0] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP4] [fused_moe] using default for (237, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16] INFO:     127.0.0.1:44594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:44890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:51040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP3] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP2] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP1] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP7] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP6] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP5] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP0] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP4] [fused_moe] using default for (232, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16] INFO:     127.0.0.1:45344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:46884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:52536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:53146 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP6] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP5] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP2] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP7] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP3] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP1] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP0] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP4] [fused_moe] using default for (228, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16] INFO:     127.0.0.1:43898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:52252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:52702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:52880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:53346 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP1] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP3] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP2] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP5] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP7] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP6] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP4] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP0] [fused_moe] using default for (222, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:52460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:16] INFO:     127.0.0.1:53134 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP3] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP2] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP1] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP5] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP6] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP7] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP0] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:16 TP4] [fused_moe] using default for (219, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:17] INFO:     127.0.0.1:45680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:17] INFO:     127.0.0.1:48406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:17] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:17] INFO:     127.0.0.1:52100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:17] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:17] INFO:     127.0.0.1:52798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:17] INFO:     127.0.0.1:53150 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:17 TP0] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:17 TP1] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:17 TP2] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:17 TP3] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:17 TP5] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:17 TP6] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:17 TP7] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:17 TP4] [fused_moe] using default for (212, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:17] INFO:     127.0.0.1:43124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:17] INFO:     127.0.0.1:47382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:17] INFO:     127.0.0.1:48912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:17] INFO:     127.0.0.1:53254 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP3] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP1] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP7] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP2] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP4] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP6] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP0] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP5] [fused_moe] using default for (208, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20] INFO:     127.0.0.1:45112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:20] INFO:     127.0.0.1:45288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:20] INFO:     127.0.0.1:45620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:20] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP2] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP0] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP3] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP7] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP6] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP4] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP5] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP1] [fused_moe] using default for (204, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20] INFO:     127.0.0.1:43628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:20] INFO:     127.0.0.1:44142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:20] INFO:     127.0.0.1:51978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:20] INFO:     127.0.0.1:52326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:20] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:20] INFO:     127.0.0.1:52976 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP2] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP6] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP0] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP3] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP4] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP7] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP1] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:20 TP5] [fused_moe] using default for (198, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21] INFO:     127.0.0.1:43656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:48734 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP2] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP3] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP6] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP7] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP4] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP5] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP0] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP1] [fused_moe] using default for (195, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21] INFO:     127.0.0.1:43784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:43992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:47114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:49172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:50920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:52684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP2] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP6] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP4] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP0] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP1] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP3] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP7] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP5] [fused_moe] using default for (188, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21] INFO:     127.0.0.1:48226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:51226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:51948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:53338 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP2] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP3] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP6] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP7] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP0] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP5] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP1] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP4] [fused_moe] using default for (183, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21] INFO:     127.0.0.1:41784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:44148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP2] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP3] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP7] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP6] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP1] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP5] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP0] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP4] [fused_moe] using default for (180, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21] INFO:     127.0.0.1:41724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:50154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:51030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:52892 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP2] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP3] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP1] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP5] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP6] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP7] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP0] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP4] [fused_moe] using default for (175, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21] INFO:     127.0.0.1:42198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:50898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:51876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:52026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP2] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP3] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP1] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP7] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP6] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP5] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP0] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP4] [fused_moe] using default for (169, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21] INFO:     127.0.0.1:42072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:50976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:51922 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP3] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP1] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP2] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP7] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP5] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP6] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP0] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP4] [fused_moe] using default for (164, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21] INFO:     127.0.0.1:43474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:49018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:49584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:51872 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP2] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP3] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP1] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP6] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP7] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP5] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP0] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP4] [fused_moe] using default for (160, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21] INFO:     127.0.0.1:42570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:45496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:46288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:51428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:21] INFO:     127.0.0.1:52494 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP3] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP2] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP1] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP7] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP6] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP5] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP0] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:21 TP4] [fused_moe] using default for (155, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22] INFO:     127.0.0.1:42032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:43240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:51276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:51408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP2] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP1] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP3] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP7] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP6] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP5] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP0] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP4] [fused_moe] using default for (150, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22] INFO:     127.0.0.1:53280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:53476 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP3] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP1] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP2] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP7] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP6] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP5] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP0] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP4] [fused_moe] using default for (148, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22] INFO:     127.0.0.1:52124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:52294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:52654 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP2] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP1] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP3] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP6] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP7] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP5] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP0] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP4] [fused_moe] using default for (145, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22] INFO:     127.0.0.1:46816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:51934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:52610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP3] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP2] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP1] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP7] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP6] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP5] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP0] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP4] [fused_moe] using default for (141, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22] INFO:     127.0.0.1:46998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:51254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:51360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:52002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:52182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:52242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:52284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:52646 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP2] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP1] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP3] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP7] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP5] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP6] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP0] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP4] [fused_moe] using default for (133, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22] INFO:     127.0.0.1:42720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:44616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:47546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:52522 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP3] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP2] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP1] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP7] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP6] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP5] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP0] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP4] [fused_moe] using default for (129, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22] INFO:     127.0.0.1:43038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:51006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:53104 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP2] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP3] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP1] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP7] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP5] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP6] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP0] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP4] [fused_moe] using default for (126, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22] INFO:     127.0.0.1:42370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:45930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:50934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:52150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:22] INFO:     127.0.0.1:53402 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:22 TP7] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP1] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP5] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP6] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP3] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP2] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP0] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP4] [fused_moe] using default for (121, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23] INFO:     127.0.0.1:51518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:23] INFO:     127.0.0.1:52970 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP2] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP3] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP1] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP7] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP6] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP5] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP4] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP0] [fused_moe] using default for (119, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23] INFO:     127.0.0.1:47322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:23] INFO:     127.0.0.1:48376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:23] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP2] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP3] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP1] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP6] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP7] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP5] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP0] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP4] [fused_moe] using default for (116, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23] INFO:     127.0.0.1:53098 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP2] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP3] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP1] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP7] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP6] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP5] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP0] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23 TP4] [fused_moe] using default for (115, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:23] INFO:     127.0.0.1:42992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:23] INFO:     127.0.0.1:51322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:23] INFO:     127.0.0.1:52552 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP5] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP6] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP7] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP4] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP2] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP0] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP1] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP3] [fused_moe] using default for (112, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:27] INFO:     127.0.0.1:44026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:27] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP2] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP0] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP5] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP6] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP4] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP3] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP1] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:27 TP7] [fused_moe] using default for (109, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28] INFO:     127.0.0.1:42452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:47470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:49616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:53544 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP2] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP1] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP5] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP6] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP3] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP7] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP0] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP4] [fused_moe] using default for (105, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28] INFO:     127.0.0.1:46808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:46860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:51484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:51956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:52276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:52462 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP0] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP3] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP2] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP1] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP5] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP7] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP6] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP4] [fused_moe] using default for (98, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28] INFO:     127.0.0.1:47878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:52478 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP2] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP1] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP3] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP5] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP7] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP6] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP0] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP4] [fused_moe] using default for (96, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28] INFO:     127.0.0.1:42884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:48508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP1] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP2] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP3] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP5] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP6] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP7] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP0] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP4] [fused_moe] using default for (93, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28] INFO:     127.0.0.1:51120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:52738 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP2] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP1] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP3] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP5] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP6] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP7] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP0] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP4] [fused_moe] using default for (91, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28] INFO:     127.0.0.1:42598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:50026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:52838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:53432 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP3] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP2] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP1] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP5] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP6] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP7] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP0] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP4] [fused_moe] using default for (87, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP0] Decode batch, #running-req: 91, #token: 21592, token usage: 0.03, cuda graph: False, gen throughput (token/s): 516.21, #queue-req: 0, 
[2025-11-20 13:59:28] INFO:     127.0.0.1:51688 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP3] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP2] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP1] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP5] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP7] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP6] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP0] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP4] [fused_moe] using default for (86, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28] INFO:     127.0.0.1:44066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:28] INFO:     127.0.0.1:46654 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP2] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP1] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP3] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP5] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP6] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP7] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP0] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:28 TP4] [fused_moe] using default for (84, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29] INFO:     127.0.0.1:44374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:29] INFO:     127.0.0.1:52514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:29] INFO:     127.0.0.1:53234 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP2] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP3] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP1] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP5] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP7] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP6] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP0] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP4] [fused_moe] using default for (81, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29] INFO:     127.0.0.1:44390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:29] INFO:     127.0.0.1:48446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:29] INFO:     127.0.0.1:52794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:29] INFO:     127.0.0.1:53290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP3] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP2] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP1] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP5] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP7] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP6] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP0] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP4] [fused_moe] using default for (77, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29] INFO:     127.0.0.1:50586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:29] INFO:     127.0.0.1:52206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:29] INFO:     127.0.0.1:53578 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP2] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP1] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP3] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP5] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP6] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP7] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP0] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP4] [fused_moe] using default for (74, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29] INFO:     127.0.0.1:45316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:29] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP1] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP3] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP2] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP5] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP6] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP7] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP0] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP4] [fused_moe] using default for (72, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29] INFO:     127.0.0.1:46374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:29] INFO:     127.0.0.1:51648 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP2] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP1] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP3] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP5] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP7] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP6] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP0] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP4] [fused_moe] using default for (70, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29] INFO:     127.0.0.1:45738 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP0] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP2] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP1] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP3] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP5] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP7] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP6] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP4] [fused_moe] using default for (69, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP5] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP7] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP6] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP4] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP2] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP1] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP3] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:29 TP0] [fused_moe] using default for (68, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30] INFO:     127.0.0.1:46024 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP0] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP2] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP1] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP3] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP5] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP7] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP6] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP4] [fused_moe] using default for (67, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30] INFO:     127.0.0.1:53024 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP2] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP1] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP3] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP5] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP7] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP6] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP0] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP4] [fused_moe] using default for (66, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30] INFO:     127.0.0.1:42696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:30] INFO:     127.0.0.1:43072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:30] INFO:     127.0.0.1:44524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:30] INFO:     127.0.0.1:52656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:30] INFO:     127.0.0.1:52924 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP2] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP1] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP3] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP5] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP6] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP7] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP0] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP4] [fused_moe] using default for (61, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30] INFO:     127.0.0.1:51940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:30] INFO:     127.0.0.1:52300 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP2] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP1] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP3] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP5] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP7] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP6] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP0] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP4] [fused_moe] using default for (59, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30] INFO:     127.0.0.1:51100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:30] INFO:     127.0.0.1:52868 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP3] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP1] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP2] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP5] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP6] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP7] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP0] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP4] [fused_moe] using default for (57, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:30] INFO:     127.0.0.1:53508 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP3] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP2] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP1] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP5] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP7] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP6] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP0] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP4] [fused_moe] using default for (55, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30] INFO:     127.0.0.1:42886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:30] INFO:     127.0.0.1:50984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:30] INFO:     127.0.0.1:53598 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP1] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP2] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP3] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP5] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP7] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP6] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP0] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP4] [fused_moe] using default for (52, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30] INFO:     127.0.0.1:48072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP2] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP3] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP1] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP7] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP5] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP6] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP0] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30 TP4] [fused_moe] using default for (51, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:30] INFO:     127.0.0.1:51534 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP1] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP7] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP6] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP0] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP3] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP2] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP5] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP4] [fused_moe] using default for (50, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31] INFO:     127.0.0.1:51414 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP2] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP1] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP3] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP5] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP7] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP6] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP0] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP4] [fused_moe] using default for (49, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31] INFO:     127.0.0.1:43190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:31] INFO:     127.0.0.1:47082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:31] INFO:     127.0.0.1:52458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:31] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:31] INFO:     127.0.0.1:50844 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP0] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP3] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP1] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP2] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP5] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP7] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP6] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP4] [fused_moe] using default for (44, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31] INFO:     127.0.0.1:51778 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP6] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP2] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP0] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP3] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP7] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP5] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP1] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:31 TP4] [fused_moe] using default for (43, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32] INFO:     127.0.0.1:52604 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP2] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP1] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP3] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP7] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP5] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP6] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP4] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP0] [fused_moe] using default for (42, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP0] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP2] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP1] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP3] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP7] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP5] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP6] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP4] [fused_moe] using default for (41, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32] INFO:     127.0.0.1:46540 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP2] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP1] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP3] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP5] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP6] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP7] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP0] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP4] [fused_moe] using default for (40, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32] INFO:     127.0.0.1:52306 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP1] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP2] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP3] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP7] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP6] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP5] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP0] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32 TP4] [fused_moe] using default for (39, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:32] INFO:     127.0.0.1:48158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:32] INFO:     127.0.0.1:51374 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP6] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP7] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP4] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP5] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP1] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP3] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP2] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP0] [fused_moe] using default for (37, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33] INFO:     127.0.0.1:51524 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP3] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP2] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP0] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP1] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP7] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP5] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP6] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP4] [fused_moe] using default for (36, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:33] INFO:     127.0.0.1:50508 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP2] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP3] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP1] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP5] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP7] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP6] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP0] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP4] [fused_moe] using default for (34, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:33] INFO:     127.0.0.1:53440 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP4] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP6] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP7] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP5] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP2] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP1] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP0] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP3] [fused_moe] using default for (32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33] INFO:     127.0.0.1:42312 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP4] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP7] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP1] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP5] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP6] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP3] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP0] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:33 TP2] [fused_moe] using default for (31, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP0] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP1] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP3] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP2] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP6] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP4] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP5] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP7] [fused_moe] using default for (30, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP0] Decode batch, #running-req: 31, #token: 9194, token usage: 0.01, cuda graph: False, gen throughput (token/s): 398.62, #queue-req: 0, 
[2025-11-20 13:59:34] INFO:     127.0.0.1:53452 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP2] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP6] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP4] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP3] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP0] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP1] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP7] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP5] [fused_moe] using default for (29, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:34] INFO:     127.0.0.1:52912 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP1] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP0] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP4] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP2] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP3] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP7] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:34 TP6] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP5] [fused_moe] using default for (27, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35] INFO:     127.0.0.1:45358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:35] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:35] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP4] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP6] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP7] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP5] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP3] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP2] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP0] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP1] [fused_moe] using default for (24, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35] INFO:     127.0.0.1:51556 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP4] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP6] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP2] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP3] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP7] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP5] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP0] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35 TP1] [fused_moe] using default for (23, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:35] INFO:     127.0.0.1:43940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:35] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP3] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP7] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP5] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP6] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP2] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP1] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP0] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP4] [fused_moe] using default for (21, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36] INFO:     127.0.0.1:52134 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP1] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP5] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP4] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP0] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP2] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP6] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP7] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36 TP3] [fused_moe] using default for (20, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:36] INFO:     127.0.0.1:42702 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP5] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP6] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP0] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP4] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP7] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP2] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP3] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP1] [fused_moe] using default for (19, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37] INFO:     127.0.0.1:52230 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP2] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP6] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP4] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP1] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP5] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP3] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP0] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP7] [fused_moe] using default for (18, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37] INFO:     127.0.0.1:51786 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP2] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP5] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP3] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP1] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP7] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP6] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP0] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37 TP4] [fused_moe] using default for (17, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 13:59:37] INFO:     127.0.0.1:45702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:37] INFO:     127.0.0.1:53326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:37] INFO:     127.0.0.1:44488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:37] INFO:     127.0.0.1:51380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:37] INFO:     127.0.0.1:51616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:37] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:38 TP0] Decode batch, #running-req: 11, #token: 4264, token usage: 0.01, cuda graph: True, gen throughput (token/s): 188.39, #queue-req: 0, 
[2025-11-20 13:59:38] INFO:     127.0.0.1:48684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:38] INFO:     127.0.0.1:46962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:38] INFO:     127.0.0.1:51470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:38] INFO:     127.0.0.1:52212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:38] INFO:     127.0.0.1:52512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:38] INFO:     127.0.0.1:51070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:39] INFO:     127.0.0.1:44656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:39] INFO:     127.0.0.1:51848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:39 TP0] Decode batch, #running-req: 3, #token: 1449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 220.51, #queue-req: 0, 
[2025-11-20 13:59:39] INFO:     127.0.0.1:52362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:39] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:39] INFO:     127.0.0.1:50990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 13:59:45] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-11-20 13:59:45] INFO:     127.0.0.1:58248 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-20 13:59:53] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-11-20 13:59:53] INFO:     127.0.0.1:58254 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-20 13:59:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-20 13:59:56 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-20 13:59:56 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-20 13:59:56 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-20 13:59:56 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-20 13:59:56 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-20 13:59:56 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-20 13:59:56 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-20 13:59:57 TP4] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for gfx942.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for host.
[92mSuccessfully preprocessed all matching files.[0m
fpath=/sgl-workspace/aiter/aiter/ops/triton/configs/gemm/MI300X-GEMM_BLOCKSCALE-A8W8.json
[aiter] finish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip], cost 55.04753288s
[2025-11-20 14:00:51 TP4] finish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip], cost 55.04753288s
[2025-11-20 14:00:54] INFO:     127.0.0.1:58264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:00:54 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-20 14:00:54 TP0] Prefill batch, #new-seq: 50, #new-token: 50, #cached-token: 36418, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-20 14:00:55 TP0] Prefill batch, #new-seq: 365, #new-token: 365, #cached-token: 265716, token usage: 0.03, #running-req: 51, #queue-req: 0, 
[aiter] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:56 TP4] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:56 TP3] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:56 TP0] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:56 TP1] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:56 TP2] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:56 TP7] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:56 TP5] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:56 TP6] [fused_moe] using default for (365, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:56 TP0] Prefill batch, #new-seq: 580, #new-token: 580, #cached-token: 422189, token usage: 0.08, #running-req: 416, #queue-req: 0, 
[aiter] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:57 TP5] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:57 TP1] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:57 TP0] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:57 TP3] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:57 TP7] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:57 TP2] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:57 TP6] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:57 TP4] [fused_moe] using default for (580, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:58 TP0] Prefill batch, #new-seq: 28, #new-token: 28, #cached-token: 20392, token usage: 0.08, #running-req: 996, #queue-req: 295, 
[aiter] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:59 TP0] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:59 TP3] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:59 TP1] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:59 TP7] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:59 TP5] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:59 TP2] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:59 TP6] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:00:59 TP4] [fused_moe] using default for (28, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:02 TP0] Decode batch, #running-req: 1024, #token: 86891, token usage: 0.12, cuda graph: False, gen throughput (token/s): 294.82, #queue-req: 295, 
[2025-11-20 14:01:03] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:03 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 746, token usage: 0.12, #running-req: 1023, #queue-req: 294, 
[2025-11-20 14:01:03] INFO:     127.0.0.1:57108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:03] INFO:     127.0.0.1:37412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:03 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1476, token usage: 0.13, #running-req: 1022, #queue-req: 292, 
[2025-11-20 14:01:04] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:04 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 730, token usage: 0.14, #running-req: 1023, #queue-req: 291, 
[2025-11-20 14:01:05] INFO:     127.0.0.1:56976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:05] INFO:     127.0.0.1:57694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:05] INFO:     127.0.0.1:60562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:05] INFO:     127.0.0.1:33300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:05] INFO:     127.0.0.1:57248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:05] INFO:     127.0.0.1:33414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:05] INFO:     127.0.0.1:33426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:05 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2961, token usage: 0.14, #running-req: 1020, #queue-req: 287, 
[2025-11-20 14:01:06] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:59188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:59698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:34542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:37488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5847, token usage: 0.14, #running-req: 1016, #queue-req: 279, 
[2025-11-20 14:01:06] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:59862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:34596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:37820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3660, token usage: 0.14, #running-req: 1019, #queue-req: 274, 
[aiter] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:06 TP3] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:06 TP2] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:06 TP1] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:06 TP7] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:06 TP5] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:06 TP6] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:06 TP0] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:06 TP4] [fused_moe] using default for (5, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:06] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:59450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:33384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:33610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:33690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06] INFO:     127.0.0.1:36670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:06 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6486, token usage: 0.15, #running-req: 1015, #queue-req: 265, 
[aiter] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP0] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP1] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP4] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP7] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP2] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP3] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP6] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP5] [fused_moe] using default for (9, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07] INFO:     127.0.0.1:58864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:07] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:07] INFO:     127.0.0.1:33816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:07] INFO:     127.0.0.1:35728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:07] INFO:     127.0.0.1:37512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:07] INFO:     127.0.0.1:37692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:07 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4460, token usage: 0.15, #running-req: 1018, #queue-req: 259, 
[aiter] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP2] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP3] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP0] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP6] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP7] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP4] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP1] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07 TP5] [fused_moe] using default for (6, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:07] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:07] INFO:     127.0.0.1:60706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:07] INFO:     127.0.0.1:36174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:07] INFO:     127.0.0.1:36854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:07] INFO:     127.0.0.1:37532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:08 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3573, token usage: 0.15, #running-req: 1019, #queue-req: 254, 
[2025-11-20 14:01:09] INFO:     127.0.0.1:59026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:09] INFO:     127.0.0.1:34860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:09] INFO:     127.0.0.1:37350 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:09 TP3] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:09 TP0] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:09 TP2] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:09 TP1] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:09 TP5] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:09 TP6] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:09 TP7] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:09 TP4] [fused_moe] using default for (1021, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:09 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2203, token usage: 0.15, #running-req: 1021, #queue-req: 251, 
[aiter] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:09 TP5] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10 TP7] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10 TP2] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10 TP3] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10 TP1] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10 TP4] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10 TP6] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10 TP0] [fused_moe] using default for (3, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:57568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:58628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:59418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:60358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:33124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:33592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:37008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:37604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7333, token usage: 0.15, #running-req: 1014, #queue-req: 241, 
[aiter] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10 TP1] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10 TP3] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10 TP2] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10 TP0] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10 TP7] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10 TP5] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10 TP6] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10 TP4] [fused_moe] using default for (10, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:10] INFO:     127.0.0.1:56952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:58992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:59178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:36550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:36802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:37070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:37954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:38074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7242, token usage: 0.15, #running-req: 1014, #queue-req: 231, 
[2025-11-20 14:01:10] INFO:     127.0.0.1:57478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:57764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:32830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:33294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:33940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:36036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10] INFO:     127.0.0.1:36528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:10 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5395, token usage: 0.15, #running-req: 1017, #queue-req: 224, 
[2025-11-20 14:01:11] INFO:     127.0.0.1:57450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:11] INFO:     127.0.0.1:59654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:11] INFO:     127.0.0.1:60868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:11] INFO:     127.0.0.1:33264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:11] INFO:     127.0.0.1:33310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:11] INFO:     127.0.0.1:33732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:11] INFO:     127.0.0.1:34422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:11] INFO:     127.0.0.1:35806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:11] INFO:     127.0.0.1:36246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:11 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6661, token usage: 0.15, #running-req: 1015, #queue-req: 215, 
[2025-11-20 14:01:12] INFO:     127.0.0.1:59496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:12] INFO:     127.0.0.1:60226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:12] INFO:     127.0.0.1:33154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:12] INFO:     127.0.0.1:34328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:12] INFO:     127.0.0.1:34642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:12] INFO:     127.0.0.1:35108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:12] INFO:     127.0.0.1:36882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:12] INFO:     127.0.0.1:37796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:12] INFO:     127.0.0.1:37928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6566, token usage: 0.15, #running-req: 1015, #queue-req: 206, 
[2025-11-20 14:01:13] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:59386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:60034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:60278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:35154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:35272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:36806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:37788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7298, token usage: 0.16, #running-req: 1014, #queue-req: 196, 
[2025-11-20 14:01:13] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:57870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:60230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:33284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:33602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:34052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:34748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:36636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8007, token usage: 0.16, #running-req: 1013, #queue-req: 185, 
[aiter] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:13 TP5] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:13 TP7] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:13 TP6] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:13 TP4] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:13 TP3] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:13 TP2] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:13 TP1] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:13 TP0] [fused_moe] using default for (11, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:13] INFO:     127.0.0.1:57444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:58494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:58934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:33162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:34368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:35008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4343, token usage: 0.16, #running-req: 1018, #queue-req: 179, 
[2025-11-20 14:01:13] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:34870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:35348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:35814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:37758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:13] INFO:     127.0.0.1:38006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:14 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5831, token usage: 0.16, #running-req: 1016, #queue-req: 171, 
[2025-11-20 14:01:14] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:14] INFO:     127.0.0.1:58874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:14] INFO:     127.0.0.1:59354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:14] INFO:     127.0.0.1:60300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:14] INFO:     127.0.0.1:35338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:14] INFO:     127.0.0.1:36678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:14] INFO:     127.0.0.1:37854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:14 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 4966, token usage: 0.16, #running-req: 1017, #queue-req: 164, 
[2025-11-20 14:01:15] INFO:     127.0.0.1:58022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:59128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:60234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:34740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:34824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3639, token usage: 0.16, #running-req: 1019, #queue-req: 159, 
[2025-11-20 14:01:15] INFO:     127.0.0.1:58222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:33204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:34970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:35712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:36198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:37500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5109, token usage: 0.16, #running-req: 1017, #queue-req: 152, 
[2025-11-20 14:01:15] INFO:     127.0.0.1:60418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:33556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:34766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:35208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:35540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:36020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:36364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15] INFO:     127.0.0.1:37152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:15 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5826, token usage: 0.16, #running-req: 1016, #queue-req: 144, 
[2025-11-20 14:01:16] INFO:     127.0.0.1:57244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:33806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:35476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:37094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:37302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4428, token usage: 0.16, #running-req: 1018, #queue-req: 138, 
[2025-11-20 14:01:16] INFO:     127.0.0.1:57262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:57836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:59508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:33800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:34378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:34462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:34958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:36254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:37142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:37830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:38018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8693, token usage: 0.16, #running-req: 1012, #queue-req: 126, 
[2025-11-20 14:01:16] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:58534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:59366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:35870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:35928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:36474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:36900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:37020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7413, token usage: 0.16, #running-req: 1014, #queue-req: 116, 
[2025-11-20 14:01:16] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:57734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:58972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:59172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:33274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:34254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:35900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:36274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:36930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16] INFO:     127.0.0.1:37380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:16 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8816, token usage: 0.16, #running-req: 1012, #queue-req: 104, 
[2025-11-20 14:01:17 TP0] Decode batch, #running-req: 1012, #token: 118774, token usage: 0.16, cuda graph: False, gen throughput (token/s): 2859.18, #queue-req: 104, 
[2025-11-20 14:01:17] INFO:     127.0.0.1:57278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:17] INFO:     127.0.0.1:58104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:17] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:17] INFO:     127.0.0.1:58884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:17] INFO:     127.0.0.1:59946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:17] INFO:     127.0.0.1:59992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:17] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:17] INFO:     127.0.0.1:36754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:17] INFO:     127.0.0.1:36940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:17 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6647, token usage: 0.17, #running-req: 1015, #queue-req: 95, 
[2025-11-20 14:01:18] INFO:     127.0.0.1:57318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:58808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:59848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:59926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:60378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:60676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:33026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:33200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:33910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:34582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:36156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:36822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:37192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10829, token usage: 0.17, #running-req: 1009, #queue-req: 80, 
[aiter] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP2] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP6] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP0] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP4] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP1] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP5] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP3] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP7] [fused_moe] using default for (15, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18] INFO:     127.0.0.1:58050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:58726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:58948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:34340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:35128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:35636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:36966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6571, token usage: 0.17, #running-req: 1015, #queue-req: 71, 
[2025-11-20 14:01:18] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:57788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:58212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:58316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:58566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:59410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:59606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:33688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:34536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:34540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:34626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:36276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:36676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:36910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:18] INFO:     127.0.0.1:38112 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP3] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP7] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP1] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP5] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP2] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP6] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP0] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP4] [fused_moe] using default for (1008, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:18 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11923, token usage: 0.17, #running-req: 1008, #queue-req: 55, 
[2025-11-20 14:01:20] INFO:     127.0.0.1:58284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:20] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:20] INFO:     127.0.0.1:59304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:20] INFO:     127.0.0.1:60306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:20] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:20] INFO:     127.0.0.1:33158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:20] INFO:     127.0.0.1:34436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:20] INFO:     127.0.0.1:34552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:20] INFO:     127.0.0.1:34992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:20] INFO:     127.0.0.1:35742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:20] INFO:     127.0.0.1:35758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:20] INFO:     127.0.0.1:36074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:20] INFO:     127.0.0.1:36710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:20] INFO:     127.0.0.1:36734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:20] INFO:     127.0.0.1:37058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:20 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10887, token usage: 0.17, #running-req: 1009, #queue-req: 40, 
[2025-11-20 14:01:21] INFO:     127.0.0.1:59102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:60834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:33276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:33632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:34446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:34688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:34828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:34952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:35192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6588, token usage: 0.17, #running-req: 1015, #queue-req: 31, 
[2025-11-20 14:01:21] INFO:     127.0.0.1:59140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:60062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:60130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:60266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:60672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:33852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:35214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:35968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:36252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:37476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:21] INFO:     127.0.0.1:37728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:22 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7994, token usage: 0.17, #running-req: 1013, #queue-req: 20, 
[2025-11-20 14:01:22] INFO:     127.0.0.1:57184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:22] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:22] INFO:     127.0.0.1:57674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:22] INFO:     127.0.0.1:59572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:22] INFO:     127.0.0.1:60058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:22] INFO:     127.0.0.1:60882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:22] INFO:     127.0.0.1:32932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:22] INFO:     127.0.0.1:33468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:22] INFO:     127.0.0.1:35012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:22] INFO:     127.0.0.1:35374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:22] INFO:     127.0.0.1:35790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:22] INFO:     127.0.0.1:36138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:22] INFO:     127.0.0.1:38090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:22 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9508, token usage: 0.17, #running-req: 1011, #queue-req: 7, 
[aiter] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP2] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP1] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP3] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP5] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP7] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP4] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP6] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP0] [fused_moe] using default for (13, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:59098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:60716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:33478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:35026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:35590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:35632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:36792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:37312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5038, token usage: 0.17, #running-req: 1015, #queue-req: 0, 
[2025-11-20 14:01:23] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:58838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:58898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:60446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:36042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:37034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:57168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:57854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:58224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:58830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:33184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:34658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:35486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:37648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:37744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:58848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:59264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:59520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:33130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:35450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:37122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:37130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:23] INFO:     127.0.0.1:37720 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP0] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP1] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP5] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP7] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP2] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP3] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP6] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:23 TP4] [fused_moe] using default for (997, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24] INFO:     127.0.0.1:59190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:33178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:34914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:35498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:35842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:35988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:36282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:37208 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP3] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP1] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP7] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP5] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP2] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP6] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP0] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP4] [fused_moe] using default for (986, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:58438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:58508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:60816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:32994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:33246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:34138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:34678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:37368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:37822 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP1] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP3] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP5] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP7] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP2] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP6] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP0] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP4] [fused_moe] using default for (975, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:33012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:33746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:33780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:33838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:35572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:36768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:37000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:37336 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP2] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP3] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP1] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP5] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP7] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP6] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP0] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP4] [fused_moe] using default for (966, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24] INFO:     127.0.0.1:57174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:59254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:60214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:33894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:34130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:35836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:36136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:36652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:37076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:37660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:38060 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP1] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP3] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP2] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP5] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP7] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP6] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP0] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP4] [fused_moe] using default for (951, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24] INFO:     127.0.0.1:57784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:58190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:58416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:58680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:59312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:59668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:60842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:33620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:33866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:33996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:36724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:37032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:37864 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP3] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP2] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP1] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP5] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP7] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP6] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP0] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP4] [fused_moe] using default for (937, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:57658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:34068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:34310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:35300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:37294 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP2] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP3] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP1] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP5] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP7] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP6] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP0] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP4] [fused_moe] using default for (930, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24] INFO:     127.0.0.1:58254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:58746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:60992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:33230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:34408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:34820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:34974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:35634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:36292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:36592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:36686 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP2] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP1] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP3] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP6] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP5] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP7] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP0] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP4] [fused_moe] using default for (918, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24] INFO:     127.0.0.1:57374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:58548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:59336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:60846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:33532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:35072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:35240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:36410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:37106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:24] INFO:     127.0.0.1:38044 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP2] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP1] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP3] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP5] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP6] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP7] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP0] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:24 TP4] [fused_moe] using default for (908, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25] INFO:     127.0.0.1:57370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:60458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:60568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:33448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:35178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:36008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:36386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:36498 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP1] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP3] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP2] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP5] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP6] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP7] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP0] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP4] [fused_moe] using default for (898, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25] INFO:     127.0.0.1:56996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:58654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:59292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:59970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:60784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:32796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:32856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:33492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:34406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:34746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:35776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:35826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:36086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:36258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:36860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:37298 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP2] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP3] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP1] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP5] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP7] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP6] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP0] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP4] [fused_moe] using default for (880, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25] INFO:     127.0.0.1:56990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:60404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:60756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:33822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:33934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:34162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:34292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:35284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:35400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:36100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:36556 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP2] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP1] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP3] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP5] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP7] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP6] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP0] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP4] [fused_moe] using default for (867, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:57276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:60732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:34022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:37630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:37698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:38346 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP2] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP1] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP3] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP7] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP5] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP6] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP0] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP4] [fused_moe] using default for (857, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25] INFO:     127.0.0.1:57500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:57512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:57804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:59484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:60168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:60560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:33674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:33842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:34234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:34814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:35514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:35750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:35920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:36222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:36830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:37006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:37052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:37290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:37806 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP3] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP2] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP1] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP7] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP5] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP6] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP0] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP4] [fused_moe] using default for (836, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25] INFO:     127.0.0.1:57254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:58278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:32976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:37734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:38096 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP3] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP1] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP2] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP5] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP6] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP7] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP0] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP4] [fused_moe] using default for (828, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25] INFO:     127.0.0.1:58308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:59248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:36666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:37158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:37178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:37774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:38234 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP1] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP5] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP3] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP2] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP7] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP6] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP0] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP4] [fused_moe] using default for (821, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25] INFO:     127.0.0.1:57474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:57710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:58364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:58862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:59214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:60320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:60912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:34332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:35556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:36892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:25] INFO:     127.0.0.1:38282 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP3] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP1] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP2] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP5] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP7] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP6] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP0] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:25 TP4] [fused_moe] using default for (809, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26] INFO:     127.0.0.1:56988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:59472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:32998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:33398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:35398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:35868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:37420 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP2] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP1] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP3] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP5] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP6] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP7] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP0] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP4] [fused_moe] using default for (799, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26] INFO:     127.0.0.1:57292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:60918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:60958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:33470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:34544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:34638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:36656 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP3] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP2] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP1] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP5] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP6] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP7] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP0] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP4] [fused_moe] using default for (792, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26] INFO:     127.0.0.1:57304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:60384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:34056 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP2] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP3] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP1] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP5] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP6] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP7] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP0] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP4] [fused_moe] using default for (787, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:60478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:60822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:60892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:34180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:36054 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP2] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP3] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP1] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP5] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP7] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP6] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP0] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP4] [fused_moe] using default for (779, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26] INFO:     127.0.0.1:58696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:59242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:32862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:33048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:33338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:33832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:33970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:34512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:34850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:36336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:36448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:37450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:38100 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP3] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP2] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP1] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP5] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP7] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP6] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP0] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP4] [fused_moe] using default for (766, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:33074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:33212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:34602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:35706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:35972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:37712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:39314 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP2] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP3] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP1] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP5] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP7] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP6] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP0] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP4] [fused_moe] using default for (757, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26] INFO:     127.0.0.1:57158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:32782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:33034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:34170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:34520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:34904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:35310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:35332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:36648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:39116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:39338 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP3] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP2] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP1] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP5] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP7] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP6] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP0] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP4] [fused_moe] using default for (745, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP2] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP3] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP1] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP5] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP7] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP6] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP0] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26 TP4] [fused_moe] using default for (732, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:26] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:58758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:59880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:33096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:33720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:36406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:36606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:37460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:37590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:37620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:37882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:38462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:26] INFO:     127.0.0.1:38708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:58196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:58606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:59914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:60334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:33076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:33516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:36846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:37044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:37944 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP3] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP2] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP1] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP5] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP6] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP7] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP0] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP4] [fused_moe] using default for (723, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27] INFO:     127.0.0.1:57414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:58466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:59116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:59900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:60098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP2] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP3] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP1] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP5] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP7] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP6] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP0] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP4] [fused_moe] using default for (717, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27] INFO:     127.0.0.1:57050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:57520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:58390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:58806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:58902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:59272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:60126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:60284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:34500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:35468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:35588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:35680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:37772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:38032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:59562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:60520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:60800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:33360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:35098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:35926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:39256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27 TP0] Decode batch, #running-req: 702, #token: 101025, token usage: 0.14, cuda graph: False, gen throughput (token/s): 3417.72, #queue-req: 0, 
[2025-11-20 14:01:27] INFO:     127.0.0.1:57816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:58172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:59162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:60472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:33354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:34036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:35160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:35230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:35316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:35958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:37134 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP2] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP3] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP1] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP5] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP6] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP7] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP0] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP4] [fused_moe] using default for (684, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27] INFO:     127.0.0.1:60646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:33370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:33458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:33638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:35200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:35626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:37396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:40418 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP2] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP1] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP3] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP5] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP6] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP7] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP0] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27 TP4] [fused_moe] using default for (676, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:27] INFO:     127.0.0.1:57092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:58290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:59536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:60666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:60814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:60942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:33882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:34216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:34318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:35434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:35992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:37272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:37568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:39302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:39462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:58642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:59956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:33328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:33396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:33582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:36564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:37256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:39064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:27] INFO:     127.0.0.1:39612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:57742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:60776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:33144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:34782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:35658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:38336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:39872 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP2] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP3] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP1] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP7] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP6] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP5] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP0] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP4] [fused_moe] using default for (642, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28] INFO:     127.0.0.1:58664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:59290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:34148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:34240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:35614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:36742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:37324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:37988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:39052 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP3] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP2] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP1] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP5] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP6] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP7] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP0] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP4] [fused_moe] using default for (633, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28] INFO:     127.0.0.1:57134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:57198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:59728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:60080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:60506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:60662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:34352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:34836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:38394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:38414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:39424 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP3] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP1] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP2] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP5] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP7] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP6] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP0] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP4] [fused_moe] using default for (622, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:57428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:57592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:58512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:59154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:33030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:33776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:34756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:37844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:40570 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP3] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP1] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP2] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP7] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP5] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP6] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP0] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP4] [fused_moe] using default for (612, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28] INFO:     127.0.0.1:59682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:34200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:35144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:37548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:38366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:38378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:38496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:38596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:39848 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP2] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP3] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP1] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP5] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP7] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP6] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP0] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP4] [fused_moe] using default for (603, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28] INFO:     127.0.0.1:59056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:60042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:60696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:32814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:35934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:35946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:36376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:36482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:37124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:38678 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP3] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP1] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP2] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP5] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP7] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP6] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP0] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP4] [fused_moe] using default for (593, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:32964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:33252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:34212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:34876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:35884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:37526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:38266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:38342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:38488 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP2] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP7] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP5] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP1] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP6] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP3] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP0] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP4] [fused_moe] using default for (582, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:33006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:34724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:35462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:36416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:37216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:38148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:38316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:39416 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP3] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP2] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP1] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP5] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP7] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP6] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP0] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28 TP4] [fused_moe] using default for (571, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:28] INFO:     127.0.0.1:57066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:58338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:59578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:60022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:60068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:60142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:60210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:60578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:34300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:34486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:34976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:35760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:36238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:36366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:36602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:36870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:37288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:37490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:39554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:28] INFO:     127.0.0.1:40462 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP2] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP1] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP3] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP5] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP7] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP6] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP0] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP4] [fused_moe] using default for (550, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:32900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:33326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:33704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:34358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:36022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:36534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:37064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:38434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:40064 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP2] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP3] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP1] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP5] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP7] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP6] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP0] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP4] [fused_moe] using default for (537, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:60198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:37434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:39364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:58092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:60006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:34710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:37090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:38352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:39244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:40006 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP1] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP2] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP3] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP5] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP6] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP7] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP0] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP4] [fused_moe] using default for (526, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29] INFO:     127.0.0.1:58710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:60354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:60820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:32886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:34944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:35094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:35854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:36566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:37574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:37912 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP3] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP2] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP1] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP5] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP7] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP6] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP0] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP4] [fused_moe] using default for (516, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:32838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:33214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:33794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:35262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:36396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:38190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:38860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:39306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:39348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:60574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:34392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:34892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:36302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:36314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:37562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:38344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:38554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:38812 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP3] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP1] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP2] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP5] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP6] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP7] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP0] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP4] [fused_moe] using default for (493, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:59576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:59714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:33962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:34808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:34932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:35172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:39106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:59040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:32804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:35602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:36598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:36620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:37186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:38906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:39080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:39138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:39188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:29] INFO:     127.0.0.1:40112 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP5] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP7] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP4] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP6] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP0] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP1] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP3] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:29 TP2] [fused_moe] using default for (473, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:30] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:30] INFO:     127.0.0.1:60490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:30] INFO:     127.0.0.1:60586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:30] INFO:     127.0.0.1:33546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:30] INFO:     127.0.0.1:34664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:30] INFO:     127.0.0.1:34986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:30] INFO:     127.0.0.1:36250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:30] INFO:     127.0.0.1:36914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:30] INFO:     127.0.0.1:39004 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP7] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP3] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP6] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP4] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP1] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP5] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP2] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP0] [fused_moe] using default for (464, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34] INFO:     127.0.0.1:59852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:32848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:34930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:36458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:37862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:40036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:57726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:59774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:33570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:36578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:39398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:39574 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP3] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP7] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP2] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP6] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP1] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP5] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP0] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP4] [fused_moe] using default for (452, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:33920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:34270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:39668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:40446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:33748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:37842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:38326 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP3] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP1] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP2] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP6] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP5] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP7] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP0] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP4] [fused_moe] using default for (442, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34] INFO:     127.0.0.1:57664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:59464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:34620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:36978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:38768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:40226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:40286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:40728 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP3] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP1] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP2] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP6] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP5] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP7] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP0] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP4] [fused_moe] using default for (434, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34] INFO:     127.0.0.1:58614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:60974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:33598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:36514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:39670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:40350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:40718 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP3] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP1] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP2] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP6] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP7] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP5] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP0] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP4] [fused_moe] using default for (427, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34] INFO:     127.0.0.1:57406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:37632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:37996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:34] INFO:     127.0.0.1:38992 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP3] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP1] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP2] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP7] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP6] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP5] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP0] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:34 TP4] [fused_moe] using default for (423, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35] INFO:     127.0.0.1:58006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:59396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:60274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:34194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:34282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:36986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:38400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:39980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:40198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:40392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:40576 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP3] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP1] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP2] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP5] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP7] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP6] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP0] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP4] [fused_moe] using default for (411, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35] INFO:     127.0.0.1:60550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:60906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:33644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:34362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:35054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:37914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:39354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:39640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:39796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:40108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:40182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:40224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:59206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:59758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:60086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:34468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:36350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:36552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:37138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:39740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:39836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:39958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:40514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:40704 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP3] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP1] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP2] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP5] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP7] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP6] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP0] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP4] [fused_moe] using default for (386, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35] INFO:     127.0.0.1:38406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:38798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:39180 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP3] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP1] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP2] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP7] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP6] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP5] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP0] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP4] [fused_moe] using default for (383, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35] INFO:     127.0.0.1:57190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:58786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:35922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:36166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:36700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:37426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:38450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:39722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:40078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:40096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:40688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:59374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:59616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:35440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:35716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:38958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:39164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:58754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:59072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:35042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:36112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:38878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:40020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:40314 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP3] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP1] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP2] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP7] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP5] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP6] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP0] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP4] [fused_moe] using default for (358, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:34090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:36346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:36706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:38514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:38894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:39292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:40628 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP3] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP1] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP2] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP5] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP7] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP6] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP0] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35 TP4] [fused_moe] using default for (350, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:35] INFO:     127.0.0.1:57908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:58264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:35528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:35670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:37986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:35] INFO:     127.0.0.1:39144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:57584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:32974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:37022 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP1] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP5] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP2] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP6] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP3] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP7] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP0] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP4] [fused_moe] using default for (341, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36] INFO:     127.0.0.1:57838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:37502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:38788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:39830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:39888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:40166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:40412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36 TP0] Decode batch, #running-req: 341, #token: 61685, token usage: 0.08, cuda graph: False, gen throughput (token/s): 2289.54, #queue-req: 0, 
[2025-11-20 14:01:36] INFO:     127.0.0.1:60656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:38558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:38628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:57322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:60624 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP3] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP1] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP2] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP7] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP5] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP6] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP0] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP4] [fused_moe] using default for (329, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:36782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:37228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:39328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:40434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:40764 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP3] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP1] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP2] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP5] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP7] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP6] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP0] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP4] [fused_moe] using default for (323, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36] INFO:     127.0.0.1:57976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:58210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:39316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:39824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:40722 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP0] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP2] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP3] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP1] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP4] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP7] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP5] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP6] [fused_moe] using default for (317, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36] INFO:     127.0.0.1:58936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:33650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:37866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:37918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:40754 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP3] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP7] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP2] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP1] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP6] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP5] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP0] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP4] [fused_moe] using default for (312, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP3] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP7] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP1] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP2] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP5] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP6] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:58916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:59658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:60362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:37362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:36] INFO:     127.0.0.1:38948 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP0] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:36 TP4] [fused_moe] using default for (306, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:35644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:35850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:40278 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP3] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP1] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP2] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP6] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP5] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP7] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP0] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP4] [fused_moe] using default for (301, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:60122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:60396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:33504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:39492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:39696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:40530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:40782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:40802 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP3] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP1] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP2] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP5] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP7] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP6] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP0] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP4] [fused_moe] using default for (292, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37] INFO:     127.0.0.1:57010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:57550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:35258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:35388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:38206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:39132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:40046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:40486 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP3] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP1] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP2] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP5] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP7] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP6] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP0] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP4] [fused_moe] using default for (284, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37] INFO:     127.0.0.1:58932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:59388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:33664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:35078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:35118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:38298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:38640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:38682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:39096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:39576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:38670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:39900 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP3] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP2] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP1] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP5] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP6] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP7] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP0] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP4] [fused_moe] using default for (272, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:60934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:38564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:38566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:38646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:38762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:39942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:40404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:40752 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP2] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP3] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP1] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP5] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP6] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP7] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP0] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP4] [fused_moe] using default for (263, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37] INFO:     127.0.0.1:59186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:36188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:37676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:39776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:40056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:40644 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP1] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP3] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP2] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP7] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP6] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP5] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP0] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP4] [fused_moe] using default for (257, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37] INFO:     127.0.0.1:57132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:59328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:60184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:60688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:34796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:37710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:38162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:38340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:40362 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP1] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP2] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP3] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP5] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP7] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP6] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP0] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP4] [fused_moe] using default for (246, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:39368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:40672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:37] INFO:     127.0.0.1:40738 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP3] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP1] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP2] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP7] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP5] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP6] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP0] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:37 TP4] [fused_moe] using default for (241, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38] INFO:     127.0.0.1:38796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:39654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:40140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:40598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:59898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:32874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:39808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:40218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:34010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:36140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:38852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:39048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:38466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:39594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:40176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:40538 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP3] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP1] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP2] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP7] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP5] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP6] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP0] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP4] [fused_moe] using default for (224, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38] INFO:     127.0.0.1:39598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:40372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:37896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:38180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:39268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:40038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:40388 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP1] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP3] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP2] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP7] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP5] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP6] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP0] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP4] [fused_moe] using default for (216, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:39152 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP3] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP1] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP2] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP7] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP6] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP5] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP0] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP4] [fused_moe] using default for (214, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38] INFO:     127.0.0.1:34474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:38446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:39506 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP1] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP3] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP2] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP7] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP6] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP5] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP0] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38 TP4] [fused_moe] using default for (211, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:38] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:59494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:35406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:36120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:38830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:39200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:38] INFO:     127.0.0.1:39912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:39500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:39562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:40206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:40330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:40472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:34566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:39982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:58976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:34278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:35740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:39028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:39978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:40262 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP3] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP1] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP2] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP7] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP5] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP6] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP0] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP4] [fused_moe] using default for (189, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39] INFO:     127.0.0.1:35422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:34336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:38874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:38976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:39222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:39780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:40556 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP0] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP2] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP1] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP3] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP7] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP5] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP6] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP4] [fused_moe] using default for (182, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:60608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:34700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:38194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:38416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:39298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:40076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:40610 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP3] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP7] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP2] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP6] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP5] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP1] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP0] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP4] [fused_moe] using default for (174, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:58378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:33574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:33954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:34118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:39160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:39628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:39970 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP3] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP2] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP7] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP6] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP1] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP5] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP0] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP4] [fused_moe] using default for (166, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39] INFO:     127.0.0.1:58242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:60860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:39] INFO:     127.0.0.1:39902 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP1] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP3] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP2] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP7] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP5] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP6] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP0] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP4] [fused_moe] using default for (163, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39] INFO:     127.0.0.1:39734 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP3] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP1] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP2] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP6] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP7] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP5] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP0] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:39 TP4] [fused_moe] using default for (162, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:33108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:36354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:38310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:38624 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP3] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP1] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP2] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP7] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP5] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP6] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP0] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP4] [fused_moe] using default for (157, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40] INFO:     127.0.0.1:58596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:38538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:38612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:39756 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP3] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP1] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP2] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP5] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP7] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP6] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP0] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP4] [fused_moe] using default for (153, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40] INFO:     127.0.0.1:36756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:38138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:38692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:40502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:40660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:36130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:38922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:39042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:39378 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP3] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP1] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP2] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP5] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP7] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP6] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP0] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP4] [fused_moe] using default for (144, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:58790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:38580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:39284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:40258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:40490 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP2] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP3] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP1] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP7] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP5] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP6] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP0] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP4] [fused_moe] using default for (137, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40] INFO:     127.0.0.1:32990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:34104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:40448 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP3] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP2] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP1] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP6] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP7] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP5] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP0] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP4] [fused_moe] using default for (134, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40] INFO:     127.0.0.1:57022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:38222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:39998 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP3] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP1] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP2] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP5] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP6] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP7] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP0] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP4] [fused_moe] using default for (131, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP0] Decode batch, #running-req: 134, #token: 30371, token usage: 0.04, cuda graph: False, gen throughput (token/s): 2009.50, #queue-req: 0, 
[2025-11-20 14:01:40] INFO:     127.0.0.1:59480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:38748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:40124 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP3] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP1] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP2] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP7] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP6] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP5] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP0] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP4] [fused_moe] using default for (128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40] INFO:     127.0.0.1:38128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:39448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:39914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:40] INFO:     127.0.0.1:40582 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP1] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP2] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP3] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP7] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP6] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:40 TP5] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP0] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP4] [fused_moe] using default for (124, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41] INFO:     127.0.0.1:59904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:38476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:38784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:39478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:40132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:40328 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP3] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP1] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP2] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP7] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP6] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP5] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP0] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP4] [fused_moe] using default for (118, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41] INFO:     127.0.0.1:35630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:38318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:39240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:40144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:40334 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP0] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP3] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP1] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP2] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP7] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP5] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP6] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP4] [fused_moe] using default for (113, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41] INFO:     127.0.0.1:59798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:34410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:35074 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP3] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP2] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP7] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP6] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP1] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP5] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP0] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP4] [fused_moe] using default for (110, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41] INFO:     127.0.0.1:39920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:59548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:35070 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP3] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP2] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP1] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP7] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP5] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP6] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP0] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP4] [fused_moe] using default for (107, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41] INFO:     127.0.0.1:36796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:39232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:39412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:40732 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP3] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP1] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP2] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP6] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP5] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP7] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP0] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP4] [fused_moe] using default for (103, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41] INFO:     127.0.0.1:59590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:33980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:39520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:39712 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP3] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP2] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP1] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP7] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP5] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP6] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP0] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41 TP4] [fused_moe] using default for (99, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:41] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:60594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:41] INFO:     127.0.0.1:60972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:42] INFO:     127.0.0.1:60990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:42] INFO:     127.0.0.1:40776 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP3] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP1] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP2] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP6] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP5] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP7] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP0] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP4] [fused_moe] using default for (94, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42] INFO:     127.0.0.1:38974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:42] INFO:     127.0.0.1:40616 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP0] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP2] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP3] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP1] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP5] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP7] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP6] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP4] [fused_moe] using default for (92, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42] INFO:     127.0.0.1:38250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:42] INFO:     127.0.0.1:40000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:42] INFO:     127.0.0.1:40090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:42] INFO:     127.0.0.1:40558 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP3] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP2] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP1] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP7] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP6] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP5] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP0] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP4] [fused_moe] using default for (88, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42] INFO:     127.0.0.1:37266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:42] INFO:     127.0.0.1:37970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:42] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:42] INFO:     127.0.0.1:58586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:42] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:42] INFO:     127.0.0.1:33762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:42] INFO:     127.0.0.1:35692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:42] INFO:     127.0.0.1:38494 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP3] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP1] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP2] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP5] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP7] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP6] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP0] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP4] [fused_moe] using default for (80, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42] INFO:     127.0.0.1:59636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:42] INFO:     127.0.0.1:37340 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP3] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP2] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP1] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP7] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP5] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP6] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP0] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:42 TP4] [fused_moe] using default for (78, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43] INFO:     127.0.0.1:34610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:43] INFO:     127.0.0.1:39438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:43] INFO:     127.0.0.1:40146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:43] INFO:     127.0.0.1:35938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:43] INFO:     127.0.0.1:39930 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP3] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP2] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP1] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP6] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP5] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP7] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP0] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP4] [fused_moe] using default for (73, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43] INFO:     127.0.0.1:60016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:43] INFO:     127.0.0.1:60762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:43] INFO:     127.0.0.1:33434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:43] INFO:     127.0.0.1:36006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:43] INFO:     127.0.0.1:57350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:43] INFO:     127.0.0.1:36214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:43] INFO:     127.0.0.1:40244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:43] INFO:     127.0.0.1:39208 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP2] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP3] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP1] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP6] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP7] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP5] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP0] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP4] [fused_moe] using default for (65, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43] INFO:     127.0.0.1:58572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:43] INFO:     127.0.0.1:40162 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP1] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP3] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP2] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP7] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP5] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP6] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP0] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43 TP4] [fused_moe] using default for (63, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:43] INFO:     127.0.0.1:38688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:43] INFO:     127.0.0.1:38744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:43] INFO:     127.0.0.1:39534 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP3] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP2] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP1] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP7] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP6] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP5] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP0] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP4] [fused_moe] using default for (60, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44] INFO:     127.0.0.1:37814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:44] INFO:     127.0.0.1:40236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:44] INFO:     127.0.0.1:40706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:44] INFO:     127.0.0.1:40796 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP1] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP3] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP2] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP5] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP6] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP7] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP0] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP4] [fused_moe] using default for (56, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:44] INFO:     127.0.0.1:40118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:44] INFO:     127.0.0.1:38838 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP3] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP1] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP2] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP7] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP6] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP5] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP0] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44 TP4] [fused_moe] using default for (53, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:44] INFO:     127.0.0.1:57610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:44] INFO:     127.0.0.1:40306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:44] INFO:     127.0.0.1:34218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:44] INFO:     127.0.0.1:36068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:44] INFO:     127.0.0.1:40652 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP6] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP7] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP5] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP2] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP1] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP3] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP0] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP4] [fused_moe] using default for (48, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45] INFO:     127.0.0.1:34076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:45] INFO:     127.0.0.1:37162 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP2] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP3] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP1] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP5] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP7] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP6] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP0] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45 TP4] [fused_moe] using default for (46, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:45] INFO:     127.0.0.1:38506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:45 TP0] Decode batch, #running-req: 45, #token: 12392, token usage: 0.02, cuda graph: False, gen throughput (token/s): 700.79, #queue-req: 0, 
[2025-11-20 14:01:45] INFO:     127.0.0.1:60638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:45] INFO:     127.0.0.1:39860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:45] INFO:     127.0.0.1:33082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:46] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:46] INFO:     127.0.0.1:39394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:46] INFO:     127.0.0.1:40300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:46] INFO:     127.0.0.1:37242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:46] INFO:     127.0.0.1:38820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:46] INFO:     127.0.0.1:39766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:46] INFO:     127.0.0.1:39720 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:46 TP2] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:46 TP3] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:46 TP1] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:46 TP5] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:46 TP6] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:46 TP7] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:46 TP0] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:46 TP4] [fused_moe] using default for (35, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:46] INFO:     127.0.0.1:36328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:46] INFO:     127.0.0.1:40634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:46] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:46] INFO:     127.0.0.1:38912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:46] INFO:     127.0.0.1:39786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:47] INFO:     127.0.0.1:40196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:47] INFO:     127.0.0.1:38742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:48] INFO:     127.0.0.1:40680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:48] INFO:     127.0.0.1:39680 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP3] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP2] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP1] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP5] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP7] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP6] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP0] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP4] [fused_moe] using default for (26, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP3] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP2] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP1] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP5] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP7] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP6] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP0] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP4] [fused_moe] using default for (25, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48] INFO:     127.0.0.1:35362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:48] INFO:     127.0.0.1:59742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:48] INFO:     127.0.0.1:37738 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP3] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP1] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP2] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP5] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP7] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP6] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP0] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48 TP4] [fused_moe] using default for (22, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:01:48] INFO:     127.0.0.1:58524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:48] INFO:     127.0.0.1:38658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:48] INFO:     127.0.0.1:39474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:49] INFO:     127.0.0.1:39018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:49] INFO:     127.0.0.1:39546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:49] INFO:     127.0.0.1:36226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:49 TP0] Decode batch, #running-req: 16, #token: 5735, token usage: 0.01, cuda graph: True, gen throughput (token/s): 282.99, #queue-req: 0, 
[2025-11-20 14:01:49] INFO:     127.0.0.1:40544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:49] INFO:     127.0.0.1:40308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:49] INFO:     127.0.0.1:60944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:49] INFO:     127.0.0.1:40358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:49] INFO:     127.0.0.1:38720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:49] INFO:     127.0.0.1:39092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:50] INFO:     127.0.0.1:38176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:50] INFO:     127.0.0.1:35910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:50] INFO:     127.0.0.1:39770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:50] INFO:     127.0.0.1:39452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:50] INFO:     127.0.0.1:39510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:50 TP0] Decode batch, #running-req: 5, #token: 2485, token usage: 0.00, cuda graph: True, gen throughput (token/s): 299.80, #queue-req: 0, 
[2025-11-20 14:01:51] INFO:     127.0.0.1:59942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:51] INFO:     127.0.0.1:58772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:51] INFO:     127.0.0.1:39588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:51] INFO:     127.0.0.1:38422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:51] INFO:     127.0.0.1:38736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:01:57] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-11-20 14:01:57] INFO:     127.0.0.1:46278 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-20 14:02:05] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-11-20 14:02:05] INFO:     127.0.0.1:33280 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-20 14:02:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-20 14:02:05] INFO:     127.0.0.1:33288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-20 14:02:05 TP0] Prefill batch, #new-seq: 23, #new-token: 23, #cached-token: 16706, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-11-20 14:02:06 TP0] Prefill batch, #new-seq: 401, #new-token: 401, #cached-token: 292089, token usage: 0.04, #running-req: 24, #queue-req: 0, 
[aiter] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:07 TP4] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:07 TP5] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:07 TP2] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:07 TP0] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:07 TP6] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:07 TP7] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:07 TP3] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:07 TP1] [fused_moe] using default for (401, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:07 TP0] Prefill batch, #new-seq: 566, #new-token: 566, #cached-token: 412020, token usage: 0.08, #running-req: 425, #queue-req: 0, 
[aiter] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:08 TP0] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:08 TP1] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:08 TP3] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:08 TP2] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:08 TP5] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:08 TP4] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:08 TP7] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:08 TP6] [fused_moe] using default for (566, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:09 TP0] Prefill batch, #new-seq: 33, #new-token: 33, #cached-token: 24094, token usage: 0.08, #running-req: 991, #queue-req: 295, 
[2025-11-20 14:02:11 TP0] Decode batch, #running-req: 1024, #token: 68647, token usage: 0.09, cuda graph: False, gen throughput (token/s): 302.92, #queue-req: 295, 
[2025-11-20 14:02:13] INFO:     127.0.0.1:38474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 749, token usage: 0.12, #running-req: 1023, #queue-req: 294, 
[2025-11-20 14:02:14] INFO:     127.0.0.1:42000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 734, token usage: 0.13, #running-req: 1023, #queue-req: 293, 
[2025-11-20 14:02:15] INFO:     127.0.0.1:35762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 718, token usage: 0.14, #running-req: 1023, #queue-req: 292, 
[2025-11-20 14:02:15] INFO:     127.0.0.1:33336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:15] INFO:     127.0.0.1:37640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:15] INFO:     127.0.0.1:38862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:15] INFO:     127.0.0.1:39096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:15] INFO:     127.0.0.1:39484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:15 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3726, token usage: 0.14, #running-req: 1019, #queue-req: 287, 
[2025-11-20 14:02:16] INFO:     127.0.0.1:33586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:16] INFO:     127.0.0.1:33922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:16] INFO:     127.0.0.1:35678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:16 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2277, token usage: 0.14, #running-req: 1021, #queue-req: 284, 
[2025-11-20 14:02:17] INFO:     127.0.0.1:34308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:35572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:37364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:37986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2936, token usage: 0.14, #running-req: 1020, #queue-req: 280, 
[2025-11-20 14:02:17] INFO:     127.0.0.1:33536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:37374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:38374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2218, token usage: 0.14, #running-req: 1021, #queue-req: 277, 
[2025-11-20 14:02:17] INFO:     127.0.0.1:33516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:33716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:37214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:37442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:39022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:40258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:40556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:40894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5810, token usage: 0.15, #running-req: 1016, #queue-req: 269, 
[2025-11-20 14:02:17] INFO:     127.0.0.1:37292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:40184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:42092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17] INFO:     127.0.0.1:42212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:17 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3632, token usage: 0.15, #running-req: 1019, #queue-req: 264, 
[2025-11-20 14:02:18] INFO:     127.0.0.1:34786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:37066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:37812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:42106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2939, token usage: 0.15, #running-req: 1020, #queue-req: 260, 
[2025-11-20 14:02:18 TP0] Decode batch, #running-req: 1020, #token: 107934, token usage: 0.15, cuda graph: False, gen throughput (token/s): 5953.35, #queue-req: 260, 
[2025-11-20 14:02:18] INFO:     127.0.0.1:34550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 701, token usage: 0.15, #running-req: 1023, #queue-req: 259, 
[2025-11-20 14:02:18] INFO:     127.0.0.1:33520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:33576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:33658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:34178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:35096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:37580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:37962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:38188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:38282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:38624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:41594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:42140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8633, token usage: 0.15, #running-req: 1012, #queue-req: 247, 
[2025-11-20 14:02:18] INFO:     127.0.0.1:33314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:34346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:35210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:36122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:38352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:38638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:39020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:39402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18] INFO:     127.0.0.1:41408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:18 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6640, token usage: 0.15, #running-req: 1015, #queue-req: 238, 
[2025-11-20 14:02:19] INFO:     127.0.0.1:35360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:39082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:39398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:41300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:40992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3593, token usage: 0.15, #running-req: 1019, #queue-req: 233, 
[2025-11-20 14:02:19] INFO:     127.0.0.1:33630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:33786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:36012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:37030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:38354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:39202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:40352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:40846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:41632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6540, token usage: 0.15, #running-req: 1015, #queue-req: 224, 
[2025-11-20 14:02:19] INFO:     127.0.0.1:36968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:37074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:37682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:38008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:38946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:39350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:41494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5060, token usage: 0.15, #running-req: 1017, #queue-req: 217, 
[2025-11-20 14:02:19] INFO:     127.0.0.1:36702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:36826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:37570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:37972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:40780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:41286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:41396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19] INFO:     127.0.0.1:42342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:19 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5893, token usage: 0.16, #running-req: 1016, #queue-req: 209, 
[2025-11-20 14:02:20] INFO:     127.0.0.1:34946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:35818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:36586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:37828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:38406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:38718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:39928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:40804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5805, token usage: 0.16, #running-req: 1016, #queue-req: 201, 
[2025-11-20 14:02:20] INFO:     127.0.0.1:34290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:34950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:35310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:35506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:37646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:37880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:38928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:40464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5985, token usage: 0.16, #running-req: 1016, #queue-req: 193, 
[2025-11-20 14:02:20] INFO:     127.0.0.1:33302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:34872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:35628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:36924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:38250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:38680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:39838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:41728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:42258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6515, token usage: 0.16, #running-req: 1015, #queue-req: 184, 
[2025-11-20 14:02:20] INFO:     127.0.0.1:35522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:38542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:39376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:39418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:40784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:40842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20] INFO:     127.0.0.1:41216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:20 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5134, token usage: 0.16, #running-req: 1017, #queue-req: 177, 
[2025-11-20 14:02:21] INFO:     127.0.0.1:34546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:21] INFO:     127.0.0.1:35758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:21] INFO:     127.0.0.1:36714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:21] INFO:     127.0.0.1:37540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:21] INFO:     127.0.0.1:38724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:21] INFO:     127.0.0.1:41432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4386, token usage: 0.16, #running-req: 1018, #queue-req: 171, 
[2025-11-20 14:02:22] INFO:     127.0.0.1:37864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:38910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:39186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:39518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:41328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:42082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4260, token usage: 0.16, #running-req: 1018, #queue-req: 165, 
[2025-11-20 14:02:22] INFO:     127.0.0.1:33544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:33600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:34422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:34708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:35160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:36250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:36734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:37010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:37358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:37670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7201, token usage: 0.16, #running-req: 1014, #queue-req: 155, 
[2025-11-20 14:02:22] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:34258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:35140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:35632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:36466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:40138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22] INFO:     127.0.0.1:41644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:22 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5123, token usage: 0.16, #running-req: 1017, #queue-req: 148, 
[2025-11-20 14:02:23] INFO:     127.0.0.1:33434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:23] INFO:     127.0.0.1:33848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:23] INFO:     127.0.0.1:34588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:23] INFO:     127.0.0.1:35044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:23] INFO:     127.0.0.1:35198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:23] INFO:     127.0.0.1:35426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:23] INFO:     127.0.0.1:36622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:23] INFO:     127.0.0.1:36928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:23] INFO:     127.0.0.1:38152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:23] INFO:     127.0.0.1:39946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:23] INFO:     127.0.0.1:40874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:23] INFO:     127.0.0.1:41616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:23] INFO:     127.0.0.1:41958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:23] INFO:     127.0.0.1:42408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:23 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10290, token usage: 0.16, #running-req: 1010, #queue-req: 134, 
[aiter] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:23 TP0] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:23 TP4] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:23 TP6] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:23 TP7] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:24 TP3] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:24 TP5] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:24 TP1] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:24 TP2] [fused_moe] using default for (14, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:24] INFO:     127.0.0.1:34852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:35164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:35378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:36284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:36808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:37020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:37582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:37786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:38930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:39902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:40072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7975, token usage: 0.16, #running-req: 1013, #queue-req: 123, 
[2025-11-20 14:02:24] INFO:     127.0.0.1:34602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:35824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:36440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:36458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:36944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:36996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:37696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:38692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:38778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:40194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:40530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:41256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:41798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24] INFO:     127.0.0.1:41994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:24 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10424, token usage: 0.16, #running-req: 1010, #queue-req: 109, 
[2025-11-20 14:02:25] INFO:     127.0.0.1:35300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:35816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:35870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:36980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:37018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:38428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:39876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:40278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:40482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:40686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7379, token usage: 0.17, #running-req: 1014, #queue-req: 99, 
[2025-11-20 14:02:25] INFO:     127.0.0.1:34014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:35348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:35836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:36028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:37512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:38384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:38396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:38452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:38752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:38764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:38792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:38886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:39924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25] INFO:     127.0.0.1:41784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:25 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10100, token usage: 0.17, #running-req: 1010, #queue-req: 85, 
[2025-11-20 14:02:26] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:26] INFO:     127.0.0.1:40226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:26] INFO:     127.0.0.1:40242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:26] INFO:     127.0.0.1:40416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:26] INFO:     127.0.0.1:41218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:26] INFO:     127.0.0.1:41436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:26] INFO:     127.0.0.1:42438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:26 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5086, token usage: 0.17, #running-req: 1017, #queue-req: 78, 
[2025-11-20 14:02:27] INFO:     127.0.0.1:33464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:33660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:33844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:33896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:34310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:36242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:37142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:37252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:37662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:37760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:38908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:38964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:39638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:39892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:40358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:40420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:41626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:41876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27 TP0] Prefill batch, #new-seq: 18, #new-token: 18, #cached-token: 13238, token usage: 0.17, #running-req: 1006, #queue-req: 60, 
[2025-11-20 14:02:27] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:36708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:37908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:38108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:38350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:39080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:39274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:40460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:41484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:42264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:42312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8152, token usage: 0.17, #running-req: 1013, #queue-req: 49, 
[2025-11-20 14:02:27] INFO:     127.0.0.1:34026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:34460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:36536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:36914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:37304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:38556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:38656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:39622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:39656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:40302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:40316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:40814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:41200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:41890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:42608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10898, token usage: 0.17, #running-req: 1009, #queue-req: 34, 
[2025-11-20 14:02:27] INFO:     127.0.0.1:34088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:34354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:34858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:35356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:35804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:36446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:36720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:38002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:39918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:40402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:40766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:42042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:42454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:27] INFO:     127.0.0.1:42550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:28 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10203, token usage: 0.17, #running-req: 1010, #queue-req: 20, 
[2025-11-20 14:02:28] INFO:     127.0.0.1:34728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:28] INFO:     127.0.0.1:35476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:28] INFO:     127.0.0.1:35698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:28] INFO:     127.0.0.1:35730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:28] INFO:     127.0.0.1:36010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:28] INFO:     127.0.0.1:36796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:28] INFO:     127.0.0.1:37486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:28] INFO:     127.0.0.1:38154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:28] INFO:     127.0.0.1:38510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:28] INFO:     127.0.0.1:39672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:28] INFO:     127.0.0.1:41014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:28] INFO:     127.0.0.1:41076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:28] INFO:     127.0.0.1:41402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:28] INFO:     127.0.0.1:41428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:28 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10226, token usage: 0.17, #running-req: 1010, #queue-req: 6, 
[2025-11-20 14:02:29] INFO:     127.0.0.1:34082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:35136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:36354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:37286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:37498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:39942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:41424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:41738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4320, token usage: 0.17, #running-req: 1016, #queue-req: 0, 
[2025-11-20 14:02:29] INFO:     127.0.0.1:34982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:37134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:39864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:40172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:33478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:34056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:34316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:35052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:37102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:38222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:39206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:39324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:40122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:40862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:42282 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP3] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP1] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP7] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP5] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP2] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP6] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP0] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP4] [fused_moe] using default for (1007, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29] INFO:     127.0.0.1:35672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:39104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:40086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:40698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:40936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:41596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:41704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:42192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:29] INFO:     127.0.0.1:42232 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP1] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP3] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP7] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP5] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP2] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP6] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP0] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:29 TP4] [fused_moe] using default for (998, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30] INFO:     127.0.0.1:33496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:33810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:34564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:35250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:36326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:36880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:37024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:37268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:37858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:40202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:41478 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP1] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP3] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP5] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP7] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP2] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP6] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP0] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP4] [fused_moe] using default for (987, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30] INFO:     127.0.0.1:33482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:33578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:37528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:39294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:40594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:40806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:40836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:41338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:41944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:33732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:34772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:36042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:37848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:38094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:39360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:39608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:40612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:41536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:41904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:42490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:42658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:33636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:34128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:36386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:36636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:36688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:37112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:38120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:38146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:38834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:39388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:40520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:40916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:41624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:41630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:42498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:33698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:35384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:35546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:36292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:36528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:36876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:39068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:39582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:41094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:41572 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP3] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP7] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP1] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP5] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP2] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP6] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP0] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP4] [fused_moe] using default for (941, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30] INFO:     127.0.0.1:33454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:33930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:34320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:35986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:36080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:37810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:38242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:38704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:39566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:39992 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP3] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP7] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP1] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP5] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP2] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP6] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP0] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP4] [fused_moe] using default for (931, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30] INFO:     127.0.0.1:34584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:34570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:34692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:35062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:35180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:37048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:37494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:38074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:39710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:39824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:41684 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP3] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP7] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP1] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP5] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP2] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP6] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP0] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30 TP4] [fused_moe] using default for (920, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:30] INFO:     127.0.0.1:33570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:34334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:34754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:35458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:35624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:36322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:37228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:37758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:39240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:40758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:42178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:30] INFO:     127.0.0.1:42708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:33942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:35538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:35588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:35970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:36404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:37660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:38044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:38342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:39144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:40728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:41230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:41618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:42152 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP3] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP1] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP7] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP5] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP2] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP6] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP0] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP4] [fused_moe] using default for (895, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31] INFO:     127.0.0.1:33352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:34274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:34538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:35002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:35078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:35274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:36334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:37188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:37924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:38360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:38920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:39764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:40022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:40114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:41274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:41902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:42266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:42286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:42422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:42502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31 TP0] Decode batch, #running-req: 895, #token: 114180, token usage: 0.16, cuda graph: False, gen throughput (token/s): 3085.90, #queue-req: 0, 
[2025-11-20 14:02:31] INFO:     127.0.0.1:33640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:33746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:35392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:36844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:37398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:37892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:39866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:40506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:41260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:41874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:42222 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP1] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP3] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP5] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP7] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP2] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP6] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP0] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP4] [fused_moe] using default for (864, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:34268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:35320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:35444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:36802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:37592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:39306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:40628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:34164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:34192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:35226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:36388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:37314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:38326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:38576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:38684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:39400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:39648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:39974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:39978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:40268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:40972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:41006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:41558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:41848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:42296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:42360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:42734 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP3] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP1] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP5] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP7] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP2] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP6] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP0] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP4] [fused_moe] using default for (835, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31] INFO:     127.0.0.1:35944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:36236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:38822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:39044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:42358 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP3] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP1] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP7] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP5] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP2] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP6] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP0] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP4] [fused_moe] using default for (830, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31] INFO:     127.0.0.1:35332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:37730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:37740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:39158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:43084 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP3] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP1] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP2] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP5] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP7] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP6] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP0] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP4] [fused_moe] using default for (825, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31] INFO:     127.0.0.1:34168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:34818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:35950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:35994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:36198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:36592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:38560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:39220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:39822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:40098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:31] INFO:     127.0.0.1:41384 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP2] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP3] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP1] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP5] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP7] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP6] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP0] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:31 TP4] [fused_moe] using default for (814, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32] INFO:     127.0.0.1:33340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:34218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:36110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:37330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:38604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:41122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:41820 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP1] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP2] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP3] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP5] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP7] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP6] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP0] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP4] [fused_moe] using default for (807, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32] INFO:     127.0.0.1:34612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:35468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:35954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:36482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:36684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:36716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:37630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:37776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:38402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:39740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:40330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:40496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:41370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:41526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:41744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:34568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:35774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:38874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:38896 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP3] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP2] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP1] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP7] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP5] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP6] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP0] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP4] [fused_moe] using default for (788, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32] INFO:     127.0.0.1:33348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:36066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:36860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:38292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:38588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:39358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:40336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:41088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:42532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:42674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:42768 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP2] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP1] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP3] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP5] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP7] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP6] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP0] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP4] [fused_moe] using default for (777, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32] INFO:     127.0.0.1:33722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:33834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:34300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:35716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:36176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:36668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:36778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:37106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:38226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:39686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:39810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:40386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:40852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:40880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:42016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:42018 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP3] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP1] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP2] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP5] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP7] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP6] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP0] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP4] [fused_moe] using default for (761, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32] INFO:     127.0.0.1:33574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:33642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:34208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:34410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:35120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:35948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:36308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:38986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:39004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:42054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:42254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:43002 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP2] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP3] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP1] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP7] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP5] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP6] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP0] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP4] [fused_moe] using default for (749, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32] INFO:     127.0.0.1:33466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:34394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:35750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:36494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:37172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:37478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:42252 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP3] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP2] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP1] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP5] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP7] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP6] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP0] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP4] [fused_moe] using default for (742, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP5] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP4] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP6] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP3] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP7] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP2] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP0] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32 TP1] [fused_moe] using default for (726, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:32] INFO:     127.0.0.1:33684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:33998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:34496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:35364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:35878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:36542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:38798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:39232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:39530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:40640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:41816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:42048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:42124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:42148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:43314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:32] INFO:     127.0.0.1:43586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:33562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:38208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:39598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:43890 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP2] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP3] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP1] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP5] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP6] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP7] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP0] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP4] [fused_moe] using default for (721, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33] INFO:     127.0.0.1:35036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:36884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:39038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:40562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:41606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:43366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:33404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:33882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:34380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:34624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:34654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:36424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:37554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:38132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:38196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:38304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:38528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:38810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:39100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:39796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:40290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:40474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:41028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:42294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:34072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:39468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:39780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:40296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:40298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:40670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:43014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:43374 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP1] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP3] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP5] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP2] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP7] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP6] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP0] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP4] [fused_moe] using default for (681, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33] INFO:     127.0.0.1:33870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:34908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:36374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:37116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:38976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:40032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:40638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:41110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:41334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:41698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:42638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:43726 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP3] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP7] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP1] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP5] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP2] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP6] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP0] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP4] [fused_moe] using default for (666, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33] INFO:     127.0.0.1:33612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:34834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:36816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:40446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:43814 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP1] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP3] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP7] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP5] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP2] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP6] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP0] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP4] [fused_moe] using default for (658, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33] INFO:     127.0.0.1:33440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:37462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:38438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:38466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:38486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:38940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:39888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:40378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:41842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:42122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:43436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:44996 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP3] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP2] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP1] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP6] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP7] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP5] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP0] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP4] [fused_moe] using default for (645, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33] INFO:     127.0.0.1:33460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:33558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:36340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:37964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:40004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:40824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:41980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:42716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:43872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:44042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:44186 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP2] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP3] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP1] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP5] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP6] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP7] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP0] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP4] [fused_moe] using default for (634, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33] INFO:     127.0.0.1:34132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:34644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:34752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:34804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:35862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:37432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:38180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:39444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:40220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:41196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:42508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:42878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:33] INFO:     127.0.0.1:43856 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP2] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP3] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP1] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP6] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP5] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:33 TP7] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP0] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP4] [fused_moe] using default for (621, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34] INFO:     127.0.0.1:34298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:35634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:38974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:41048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:41062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:41916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:42848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:42966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:43030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:44452 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP2] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP1] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP3] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP7] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP5] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP6] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP0] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP4] [fused_moe] using default for (611, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:34912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:36094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:38686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:40368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:41510 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP2] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP1] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP3] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP5] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP7] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP6] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP0] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP4] [fused_moe] using default for (604, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34] INFO:     127.0.0.1:33668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:34528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:34712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:36070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:37154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:37744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:38116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:39954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:40962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:45138 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP1] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP3] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP2] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP5] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP7] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP6] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP0] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP4] [fused_moe] using default for (594, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34] INFO:     127.0.0.1:33954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:35286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:35876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:39820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:43244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:44410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:44758 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP3] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP2] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP1] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP5] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP7] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP6] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP0] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP4] [fused_moe] using default for (587, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34] INFO:     127.0.0.1:33774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:39136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:42068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:42766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:44008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:34172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:34714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:36752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:37346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:40016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:40546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:41012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:41770 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP5] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP2] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP1] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP3] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP7] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP6] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP0] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP4] [fused_moe] using default for (574, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34] INFO:     127.0.0.1:34510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:37042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:37608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:37714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:38078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:38168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:40056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:41248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:43124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:44140 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP2] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP0] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP3] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP1] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP4] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP5] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP6] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP7] [fused_moe] using default for (564, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34] INFO:     127.0.0.1:33412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:35318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:39124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:39322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:39428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:39932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:41358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:42328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:43056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:43998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:34] INFO:     127.0.0.1:45046 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP3] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP1] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP7] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP5] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP2] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP6] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP0] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:34 TP4] [fused_moe] using default for (553, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35] INFO:     127.0.0.1:33854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:34386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:36406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:36464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:36576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:36758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:38058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:39266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:40704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:40790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:41628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:42940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:43108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:44626 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP5] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP7] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP6] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP4] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP1] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP3] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP2] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP0] [fused_moe] using default for (539, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35] INFO:     127.0.0.1:34900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:34940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:35116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:35746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:35990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:37256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:37926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:41466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:42040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:43304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:43382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:37280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:38184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:41656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:42062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:44526 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP3] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP7] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP1] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP5] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP2] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP6] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP0] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP4] [fused_moe] using default for (523, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35] INFO:     127.0.0.1:33798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:33842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:34418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:36132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:36652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:41426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:43158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:43934 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP2] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP3] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP1] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP5] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP6] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP7] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP0] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP4] [fused_moe] using default for (515, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35] INFO:     127.0.0.1:33686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:34846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:35026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:36050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:36256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:36788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:37058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:37308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:40982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:41180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:42556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:42714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:43216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:43794 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP1] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP2] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP5] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP3] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP6] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP7] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP0] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP4] [fused_moe] using default for (500, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35] INFO:     127.0.0.1:33814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:33948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:34352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:34480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:34974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:35936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:37238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:41670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:42134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:43164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:43206 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP2] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP3] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP1] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP5] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP6] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP7] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP0] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP4] [fused_moe] using default for (489, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35] INFO:     127.0.0.1:34234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:34486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:34918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:35618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:36608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:40908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:41864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:42836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:43430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:43840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:44674 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP2] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP3] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP1] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP5] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP6] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP7] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP0] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP4] [fused_moe] using default for (478, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35] INFO:     127.0.0.1:36574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:37840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:39300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:40390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:40430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:40646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:41754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:43240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:43532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:43576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:43906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:35] INFO:     127.0.0.1:44308 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP2] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP3] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP1] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP5] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP6] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP7] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP0] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:35 TP4] [fused_moe] using default for (466, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:34958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:36224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:36572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:42102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:42110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:43488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:44398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36 TP0] Decode batch, #running-req: 466, #token: 75537, token usage: 0.10, cuda graph: False, gen throughput (token/s): 5490.74, #queue-req: 0, 
[2025-11-20 14:02:36] INFO:     127.0.0.1:33602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:35406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:37162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:41718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:43410 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP2] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP3] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP1] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP5] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP6] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP7] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP0] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP4] [fused_moe] using default for (453, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36] INFO:     127.0.0.1:33760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:35908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:36412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:37854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:38060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:40208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP2] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP1] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP3] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP6] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP5] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP7] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP0] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP4] [fused_moe] using default for (446, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36] INFO:     127.0.0.1:35710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:43092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:43730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:44242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:33330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:33910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:37946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:39698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:41164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:42384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:42530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:45028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:38262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:42168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:42524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:43960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:44812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:45274 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP2] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP3] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP1] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP5] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP7] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP6] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP0] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP4] [fused_moe] using default for (428, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36] INFO:     127.0.0.1:33966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:34666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:36764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:44866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:44934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:45272 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP1] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP5] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP3] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP2] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP7] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP6] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP0] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP4] [fused_moe] using default for (422, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36] INFO:     127.0.0.1:33744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:37676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:38472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:44240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:45090 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP3] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP2] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP1] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP7] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP6] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP5] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP0] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36 TP4] [fused_moe] using default for (417, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:36] INFO:     127.0.0.1:36352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:37748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:38880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:41242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:42624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:42864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:43356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:36] INFO:     127.0.0.1:45140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:33502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:34998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:35014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:35112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:35442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:35472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:37936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:42568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44806 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP5] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP7] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP6] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP4] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP2] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP1] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP0] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP3] [fused_moe] using default for (398, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37] INFO:     127.0.0.1:36954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:38022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:39538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:40128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:40326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:40976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:41284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:42374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:42598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:45258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:36552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:36556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:41578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:42112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:42898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:43116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:43334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:43910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44520 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP3] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP7] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP1] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP5] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP2] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP0] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP6] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP4] [fused_moe] using default for (374, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37] INFO:     127.0.0.1:37198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:38244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:39336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:39630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:42030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:43476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:43676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44986 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP3] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP7] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP1] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP5] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP2] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP6] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP0] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP4] [fused_moe] using default for (364, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37] INFO:     127.0.0.1:35156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:38502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:40600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:42926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44638 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP3] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP7] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP1] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP5] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP2] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP6] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP0] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP4] [fused_moe] using default for (359, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37] INFO:     127.0.0.1:34462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:35416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:42804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:43040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:43392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44872 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP3] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP1] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP7] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP5] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP2] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP6] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP0] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP4] [fused_moe] using default for (351, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37] INFO:     127.0.0.1:36164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:36896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:37204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:38548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:42396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:45178 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP1] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP3] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP5] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP7] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP2] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP6] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP0] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP4] [fused_moe] using default for (345, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37] INFO:     127.0.0.1:33982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:35018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:38278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:38412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:38646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:38750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:37] INFO:     127.0.0.1:44686 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP4] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP6] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP5] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP7] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP0] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP1] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP2] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:37 TP3] [fused_moe] using default for (336, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38] INFO:     127.0.0.1:36268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:42754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:43150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:43704 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP3] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP1] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP7] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP5] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP2] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP6] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP0] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP4] [fused_moe] using default for (332, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38] INFO:     127.0.0.1:37388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:37444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:41966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:42582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:44420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:45244 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP3] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP7] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP1] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP5] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP2] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP6] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP0] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP4] [fused_moe] using default for (326, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38] INFO:     127.0.0.1:42684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:42728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:43602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:44740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:45000 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP3] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP1] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP7] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP5] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP2] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP6] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP0] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP4] [fused_moe] using default for (321, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38] INFO:     127.0.0.1:41132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:43202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:43448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:33628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:34038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:39286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:39456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:39894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:41808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:42776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:43650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:44396 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP3] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP2] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP1] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP7] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP5] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP6] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP0] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP4] [fused_moe] using default for (309, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38] INFO:     127.0.0.1:33446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:34148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:38664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:40156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:44524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:45014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:45306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:45338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:34434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:38738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:41142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:44864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:45060 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP2] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP3] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP1] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP6] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP5] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP7] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP0] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP4] [fused_moe] using default for (295, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP3] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP2] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP1] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP6] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP5] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP7] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38] INFO:     127.0.0.1:36518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:36618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:36934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:39724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:40718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:43886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:45326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP0] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38 TP4] [fused_moe] using default for (287, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:38] INFO:     127.0.0.1:40494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:42240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:38] INFO:     127.0.0.1:43290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:34466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:35652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:37708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:44280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:45336 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP2] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP3] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP1] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP5] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP7] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP6] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP0] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP4] [fused_moe] using default for (279, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:37452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:42912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:43572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:43834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:43902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:44486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:44608 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP5] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP2] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP3] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP6] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP1] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP7] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP0] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP4] [fused_moe] using default for (271, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39] INFO:     127.0.0.1:33350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:34366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:40944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:42814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:43394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:44076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:45320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:35926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:41832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:42994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:44146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:44202 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP1] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP3] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP2] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP6] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP5] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP7] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP0] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP4] [fused_moe] using default for (258, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39] INFO:     127.0.0.1:35166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:44916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:35230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:37200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:43232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:43764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:44994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:45202 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP2] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP3] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP1] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP5] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP7] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP6] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP0] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP4] [fused_moe] using default for (249, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39] INFO:     127.0.0.1:39002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:39958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:40044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:40178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:41040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:42202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:42480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:42974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:43608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:44352 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP2] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP3] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP1] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP5] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP7] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP6] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP0] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP4] [fused_moe] using default for (239, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39] INFO:     127.0.0.1:34100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:37118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:37618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:42846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:42894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:43278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:44970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:45316 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP3] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP2] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP1] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP5] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP6] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP7] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP0] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP4] [fused_moe] using default for (231, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39] INFO:     127.0.0.1:42788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:43736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:45152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:39] INFO:     127.0.0.1:45212 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP2] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP1] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP3] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP5] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP7] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP6] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP0] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:39 TP4] [fused_moe] using default for (227, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40] INFO:     127.0.0.1:38612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:43922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:44178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:44394 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP2] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP3] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP1] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP5] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP7] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP6] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP0] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP4] [fused_moe] using default for (223, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40] INFO:     127.0.0.1:34756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:40960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:44748 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP3] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP2] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP1] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP5] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP7] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP6] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP0] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP4] [fused_moe] using default for (220, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40] INFO:     127.0.0.1:34450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:37294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:44068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:44940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:45126 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP2] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP3] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP1] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP5] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP7] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP6] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP0] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP4] [fused_moe] using default for (215, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40] INFO:     127.0.0.1:37796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:43696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:44232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:44714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:33424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:36208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:44978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40 TP0] Decode batch, #running-req: 211, #token: 43036, token usage: 0.06, cuda graph: False, gen throughput (token/s): 2826.19, #queue-req: 0, 
[2025-11-20 14:02:40] INFO:     127.0.0.1:36218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:38786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:43360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:43808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:45058 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP2] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP3] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP1] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP7] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP5] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP6] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP0] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP4] [fused_moe] using default for (203, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40] INFO:     127.0.0.1:34738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:38950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:43618 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP2] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP3] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP1] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP5] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP6] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP7] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP0] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP4] [fused_moe] using default for (200, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40] INFO:     127.0.0.1:41032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:43072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:44584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:40] INFO:     127.0.0.1:44790 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP3] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP2] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP1] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP5] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP7] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP6] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP0] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:40 TP4] [fused_moe] using default for (196, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41] INFO:     127.0.0.1:34526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:36792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:39554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:44130 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP3] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP2] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP1] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP5] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP7] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP6] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP0] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP4] [fused_moe] using default for (192, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41] INFO:     127.0.0.1:34634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:44458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:44544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:38568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:42802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:44724 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP3] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP1] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP2] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP5] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP7] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP6] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP0] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP4] [fused_moe] using default for (186, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41] INFO:     127.0.0.1:33528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:34924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:42542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:45098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:45160 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP2] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP1] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP3] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP6] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP5] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP7] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP0] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP4] [fused_moe] using default for (181, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41] INFO:     127.0.0.1:40406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:42826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:43718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:43752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:43838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:44512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:44620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:43226 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP3] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP2] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP1] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP7] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP6] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP5] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP0] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP4] [fused_moe] using default for (173, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41] INFO:     127.0.0.1:33988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:35922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:37392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:42466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:43178 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP3] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP1] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP2] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP5] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP6] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP7] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP0] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41 TP4] [fused_moe] using default for (168, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:41] INFO:     127.0.0.1:44066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:44468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:35168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:39174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:41] INFO:     127.0.0.1:40744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:38018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:42978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:43330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:44344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:44728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:44848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:37536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:43948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:45072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP2] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP1] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP3] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP5] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP7] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP6] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP0] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP4] [fused_moe] using default for (154, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42] INFO:     127.0.0.1:37156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:44470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP3] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP2] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP1] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP5] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP7] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP6] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP0] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP4] [fused_moe] using default for (151, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42] INFO:     127.0.0.1:35562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:38848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:43132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:43346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:43818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:44122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:44834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:37538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:40580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:43516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:33430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:35844 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP3] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP2] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP1] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP5] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP7] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP6] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP0] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP4] [fused_moe] using default for (139, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42] INFO:     127.0.0.1:35614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:36248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:44472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:44908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:45156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:36502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:42] INFO:     127.0.0.1:45074 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP2] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP3] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP1] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP6] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP5] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP7] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP0] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:42 TP4] [fused_moe] using default for (132, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43] INFO:     127.0.0.1:36740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:34628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:41316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:44930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:33398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:34788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:36148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:39496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:39502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:43550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:44368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:44726 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP3] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP2] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP1] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP5] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP7] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP6] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP0] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP4] [fused_moe] using default for (120, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43] INFO:     127.0.0.1:35064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:38030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:38310 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP2] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP3] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP1] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP5] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP7] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP6] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP0] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP4] [fused_moe] using default for (117, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43] INFO:     127.0.0.1:38494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:40654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:40928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:44328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:45290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:39284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:39464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:43140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:43780 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP3] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP2] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP1] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP5] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP6] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP7] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP0] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP4] [fused_moe] using default for (108, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43] INFO:     127.0.0.1:34154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:41548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:44266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:44780 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP2] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP3] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP1] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP5] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP7] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP6] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP0] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP4] [fused_moe] using default for (104, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43] INFO:     127.0.0.1:36736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:43198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:43] INFO:     127.0.0.1:44092 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP3] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP2] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP1] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP5] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP7] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP6] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP0] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:43 TP4] [fused_moe] using default for (101, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44] INFO:     127.0.0.1:43194 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP1] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP3] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP2] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP5] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP7] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP6] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP0] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP4] [fused_moe] using default for (100, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44] INFO:     127.0.0.1:35376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:38258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:39250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:44572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:45168 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP2] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP3] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP1] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP5] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP7] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP6] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP0] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP4] [fused_moe] using default for (95, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44] INFO:     127.0.0.1:40668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:41932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:43636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:43966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:44028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:44642 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP3] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP1] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP2] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP5] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP7] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP6] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP0] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP4] [fused_moe] using default for (89, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44] INFO:     127.0.0.1:38206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:42700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:45130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:33876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:38050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:39982 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP3] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP2] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP1] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP5] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP7] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP6] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP0] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP4] [fused_moe] using default for (83, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44] INFO:     127.0.0.1:36188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:36834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:45036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:44] INFO:     127.0.0.1:41800 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP2] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP1] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP3] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP5] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP6] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP7] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP0] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44 TP4] [fused_moe] using default for (79, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:44] INFO:     127.0.0.1:45330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45 TP0] Decode batch, #running-req: 78, #token: 18873, token usage: 0.03, cuda graph: False, gen throughput (token/s): 1251.47, #queue-req: 0, 
[2025-11-20 14:02:45] INFO:     127.0.0.1:33828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:35848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:36992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:40774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:44722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:44502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:35246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:43634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:44814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:37418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:39108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:43534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:44082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:43566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:44020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:44106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:44292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:41756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:44732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:44824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:45] INFO:     127.0.0.1:45262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:46] INFO:     127.0.0.1:37406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:46] INFO:     127.0.0.1:43422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:46] INFO:     127.0.0.1:45224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:46] INFO:     127.0.0.1:44700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:46] INFO:     127.0.0.1:35642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:46] INFO:     127.0.0.1:44890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:46] INFO:     127.0.0.1:44250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:46] INFO:     127.0.0.1:43458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:46] INFO:     127.0.0.1:40566 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:46 TP0] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:46 TP1] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:46 TP2] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:46 TP3] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:46 TP5] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:46 TP7] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:46 TP6] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:46 TP4] [fused_moe] using default for (47, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:47] INFO:     127.0.0.1:44880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:47] INFO:     127.0.0.1:39750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:47] INFO:     127.0.0.1:34112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:47] INFO:     127.0.0.1:43254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:47] INFO:     127.0.0.1:34778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:47] INFO:     127.0.0.1:42756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:47] INFO:     127.0.0.1:43074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:47] INFO:     127.0.0.1:43770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:47] INFO:     127.0.0.1:44360 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:47 TP2] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:47 TP3] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:47 TP1] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:47 TP5] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:47 TP7] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:47 TP6] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:47 TP0] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:47 TP4] [fused_moe] using default for (38, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:02:48] INFO:     127.0.0.1:44128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:48] INFO:     127.0.0.1:42548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:48] INFO:     127.0.0.1:45190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:48] INFO:     127.0.0.1:44762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:48] INFO:     127.0.0.1:45348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:49] INFO:     127.0.0.1:36988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:49] INFO:     127.0.0.1:39052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:49] INFO:     127.0.0.1:41158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:49] INFO:     127.0.0.1:44596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:49] INFO:     127.0.0.1:45112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:49 TP0] Decode batch, #running-req: 31, #token: 8753, token usage: 0.01, cuda graph: False, gen throughput (token/s): 450.65, #queue-req: 0, 
[2025-11-20 14:02:49] INFO:     127.0.0.1:41344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:49] INFO:     127.0.0.1:45240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:49] INFO:     127.0.0.1:40800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:49] INFO:     127.0.0.1:44558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:50] INFO:     127.0.0.1:40146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:50] INFO:     127.0.0.1:44054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:50] INFO:     127.0.0.1:43464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:50] INFO:     127.0.0.1:37090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:50] INFO:     127.0.0.1:42560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:50] INFO:     127.0.0.1:43666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:50] INFO:     127.0.0.1:43982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:51] INFO:     127.0.0.1:43080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:51] INFO:     127.0.0.1:39848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:51] INFO:     127.0.0.1:44432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:51] INFO:     127.0.0.1:44954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:51] INFO:     127.0.0.1:34574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:51] INFO:     127.0.0.1:43486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:51] INFO:     127.0.0.1:43570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:51] INFO:     127.0.0.1:44894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:51] INFO:     127.0.0.1:44036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:51] INFO:     127.0.0.1:44348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:51 TP0] Decode batch, #running-req: 7, #token: 2872, token usage: 0.00, cuda graph: True, gen throughput (token/s): 279.49, #queue-req: 0, 
[2025-11-20 14:02:52] INFO:     127.0.0.1:42646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:52] INFO:     127.0.0.1:43504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:52] INFO:     127.0.0.1:42954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:52] INFO:     127.0.0.1:43684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:52] INFO:     127.0.0.1:44154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:52 TP0] Decode batch, #running-req: 2, #token: 1459, token usage: 0.00, cuda graph: True, gen throughput (token/s): 141.76, #queue-req: 0, 
[2025-11-20 14:02:53] INFO:     127.0.0.1:42746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:53] INFO:     127.0.0.1:43266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:02:58] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-11-20 14:02:58] INFO:     127.0.0.1:42428 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-20 14:03:06] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-11-20 14:03:06] INFO:     127.0.0.1:33644 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-20 14:03:06 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-20 14:03:07] INFO:     127.0.0.1:33656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-20 14:03:07 TP0] Prefill batch, #new-seq: 58, #new-token: 58, #cached-token: 42071, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:07 TP4] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:08 TP7] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:08 TP5] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:08 TP6] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:08 TP2] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:08 TP3] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:08 TP1] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:08 TP0] [fused_moe] using default for (58, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:08 TP0] Prefill batch, #new-seq: 490, #new-token: 490, #cached-token: 357186, token usage: 0.05, #running-req: 59, #queue-req: 0, 
[aiter] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:09 TP0] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:09 TP7] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:09 TP3] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:09 TP4] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:09 TP6] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:09 TP5] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:09 TP1] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:09 TP2] [fused_moe] using default for (490, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:09 TP0] Prefill batch, #new-seq: 433, #new-token: 433, #cached-token: 315007, token usage: 0.08, #running-req: 549, #queue-req: 0, 
[aiter] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:10 TP0] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:10 TP1] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:10 TP4] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:10 TP7] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:10 TP5] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:10 TP6] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:10 TP3] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:10 TP2] [fused_moe] using default for (433, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:11 TP0] Prefill batch, #new-seq: 42, #new-token: 42, #cached-token: 30716, token usage: 0.08, #running-req: 982, #queue-req: 295, 
[2025-11-20 14:03:15 TP0] Decode batch, #running-req: 1024, #token: 85107, token usage: 0.12, cuda graph: False, gen throughput (token/s): 1012.54, #queue-req: 295, 
[2025-11-20 14:03:15] INFO:     127.0.0.1:35954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 736, token usage: 0.12, #running-req: 1023, #queue-req: 294, 
[2025-11-20 14:03:16] INFO:     127.0.0.1:42120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:16 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 731, token usage: 0.13, #running-req: 1023, #queue-req: 293, 
[2025-11-20 14:03:16] INFO:     127.0.0.1:36600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:17 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 715, token usage: 0.14, #running-req: 1023, #queue-req: 292, 
[2025-11-20 14:03:17] INFO:     127.0.0.1:34006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:17] INFO:     127.0.0.1:36640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:17] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:17] INFO:     127.0.0.1:35074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:17] INFO:     127.0.0.1:38612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:17 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1419, token usage: 0.14, #running-req: 1022, #queue-req: 290, 
[2025-11-20 14:03:17] INFO:     127.0.0.1:34798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:17] INFO:     127.0.0.1:35304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:17] INFO:     127.0.0.1:36086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:17] INFO:     127.0.0.1:36264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:17] INFO:     127.0.0.1:38532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5917, token usage: 0.14, #running-req: 1016, #queue-req: 282, 
[2025-11-20 14:03:18] INFO:     127.0.0.1:34320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:37188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:38660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:42100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2946, token usage: 0.14, #running-req: 1020, #queue-req: 278, 
[2025-11-20 14:03:18] INFO:     127.0.0.1:33942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:34156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:34746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:35046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:37266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:38652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:38948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:39042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:39088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:39228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:42608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8152, token usage: 0.15, #running-req: 1013, #queue-req: 267, 
[2025-11-20 14:03:18] INFO:     127.0.0.1:35284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:35920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:39390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:39596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:42222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18] INFO:     127.0.0.1:42352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:18 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4362, token usage: 0.15, #running-req: 1018, #queue-req: 261, 
[2025-11-20 14:03:19] INFO:     127.0.0.1:34620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:19] INFO:     127.0.0.1:37224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:19] INFO:     127.0.0.1:37512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:19] INFO:     127.0.0.1:40384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:19] INFO:     127.0.0.1:41542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:19] INFO:     127.0.0.1:42226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:19 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4386, token usage: 0.15, #running-req: 1018, #queue-req: 255, 
[2025-11-20 14:03:20] INFO:     127.0.0.1:38234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 709, token usage: 0.15, #running-req: 1023, #queue-req: 254, 
[2025-11-20 14:03:20] INFO:     127.0.0.1:34134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:34278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:34438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:36452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:36708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:37456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:37966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:38510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:40152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:41136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:41346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:41500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:41714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:42286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10338, token usage: 0.15, #running-req: 1010, #queue-req: 240, 
[2025-11-20 14:03:20] INFO:     127.0.0.1:33680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:35394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:35856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:36152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:36336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:36426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:39864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:40934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:41190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:42086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:42528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8013, token usage: 0.15, #running-req: 1013, #queue-req: 229, 
[2025-11-20 14:03:20] INFO:     127.0.0.1:35208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:42538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20] INFO:     127.0.0.1:41184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:20 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2192, token usage: 0.15, #running-req: 1021, #queue-req: 226, 
[2025-11-20 14:03:21] INFO:     127.0.0.1:34368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:34588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:37620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:38958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:39110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:39274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:39374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:40484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:41798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6597, token usage: 0.15, #running-req: 1015, #queue-req: 217, 
[2025-11-20 14:03:21] INFO:     127.0.0.1:37094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:37320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:37784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:37852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:38096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:38470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:39242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:39414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:39834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6444, token usage: 0.15, #running-req: 1015, #queue-req: 208, 
[2025-11-20 14:03:21] INFO:     127.0.0.1:34206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:34998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:35258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:36214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:36576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:38884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:39992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:40730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:41512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:41592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:41678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:42460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8669, token usage: 0.16, #running-req: 1012, #queue-req: 196, 
[2025-11-20 14:03:21] INFO:     127.0.0.1:34112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:34980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:35348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:39502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21] INFO:     127.0.0.1:42480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:21 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3681, token usage: 0.16, #running-req: 1019, #queue-req: 191, 
[2025-11-20 14:03:22] INFO:     127.0.0.1:34604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:38068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:38626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:40366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2907, token usage: 0.16, #running-req: 1020, #queue-req: 187, 
[2025-11-20 14:03:22] INFO:     127.0.0.1:33664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:35456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:35840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:36020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:39594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:39724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:40054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:42404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5825, token usage: 0.16, #running-req: 1016, #queue-req: 179, 
[2025-11-20 14:03:22] INFO:     127.0.0.1:35286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:35546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:35604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:36168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:37336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:37500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:38912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:39472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:40052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:41364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22] INFO:     127.0.0.1:41528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:22 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7921, token usage: 0.16, #running-req: 1013, #queue-req: 168, 
[2025-11-20 14:03:23] INFO:     127.0.0.1:34336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:23] INFO:     127.0.0.1:35124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:23] INFO:     127.0.0.1:35992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:23] INFO:     127.0.0.1:36482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:23] INFO:     127.0.0.1:37536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:23] INFO:     127.0.0.1:39546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:23] INFO:     127.0.0.1:40312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:23 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5037, token usage: 0.16, #running-req: 1017, #queue-req: 161, 
[2025-11-20 14:03:23] INFO:     127.0.0.1:34276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:23] INFO:     127.0.0.1:40860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:23] INFO:     127.0.0.1:41316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:23] INFO:     127.0.0.1:42002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:23] INFO:     127.0.0.1:42214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3612, token usage: 0.16, #running-req: 1019, #queue-req: 156, 
[2025-11-20 14:03:24] INFO:     127.0.0.1:35750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24] INFO:     127.0.0.1:37108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24] INFO:     127.0.0.1:38494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24] INFO:     127.0.0.1:38924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24] INFO:     127.0.0.1:39934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24] INFO:     127.0.0.1:40182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24] INFO:     127.0.0.1:40234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24] INFO:     127.0.0.1:40710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24] INFO:     127.0.0.1:41080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6563, token usage: 0.16, #running-req: 1015, #queue-req: 147, 
[2025-11-20 14:03:24] INFO:     127.0.0.1:33892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24] INFO:     127.0.0.1:35430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24] INFO:     127.0.0.1:36130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24] INFO:     127.0.0.1:37288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24] INFO:     127.0.0.1:38512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24] INFO:     127.0.0.1:38562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24] INFO:     127.0.0.1:39670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24] INFO:     127.0.0.1:41832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:24 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5896, token usage: 0.16, #running-req: 1016, #queue-req: 139, 
[2025-11-20 14:03:25] INFO:     127.0.0.1:33834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:34694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:35098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:36374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:38316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:38344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:38722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:38972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:39684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:39848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:40890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:40936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:41864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9446, token usage: 0.16, #running-req: 1011, #queue-req: 126, 
[2025-11-20 14:03:25 TP0] Decode batch, #running-req: 1011, #token: 117871, token usage: 0.16, cuda graph: False, gen throughput (token/s): 3867.28, #queue-req: 126, 
[2025-11-20 14:03:25] INFO:     127.0.0.1:35410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:39948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:40488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:40530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:41154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:41622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25] INFO:     127.0.0.1:41748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:25 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5147, token usage: 0.16, #running-req: 1017, #queue-req: 119, 
[2025-11-20 14:03:26] INFO:     127.0.0.1:33918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:35754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:36962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:37992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:38116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:39062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:39612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:40968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:41642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:41794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:41926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8191, token usage: 0.16, #running-req: 1013, #queue-req: 108, 
[2025-11-20 14:03:26] INFO:     127.0.0.1:33924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:33922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:34904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:35790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:36522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:36944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:37246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:37314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:37734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:38696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:40550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:41456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:41650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9538, token usage: 0.17, #running-req: 1011, #queue-req: 95, 
[2025-11-20 14:03:26] INFO:     127.0.0.1:34282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:34364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:34672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:35366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:35416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:35998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:36114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:36424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:36678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:38310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:38374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:40852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:41916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9387, token usage: 0.17, #running-req: 1011, #queue-req: 82, 
[2025-11-20 14:03:26] INFO:     127.0.0.1:34478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:34770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:36242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:37098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:37240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:38338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:38798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:38852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26] INFO:     127.0.0.1:39566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:26 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6544, token usage: 0.17, #running-req: 1015, #queue-req: 73, 
[2025-11-20 14:03:27] INFO:     127.0.0.1:33876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:27] INFO:     127.0.0.1:34104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:27] INFO:     127.0.0.1:34468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:27] INFO:     127.0.0.1:34972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:27] INFO:     127.0.0.1:35172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:27] INFO:     127.0.0.1:35994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:27] INFO:     127.0.0.1:39076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:27] INFO:     127.0.0.1:39406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:27] INFO:     127.0.0.1:39506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:27] INFO:     127.0.0.1:39920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:27] INFO:     127.0.0.1:41354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:27] INFO:     127.0.0.1:41640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:27 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8899, token usage: 0.17, #running-req: 1012, #queue-req: 61, 
[2025-11-20 14:03:28] INFO:     127.0.0.1:35340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:35534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:36230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:36720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:37476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:38162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:38386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:38546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:39312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:39676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:40428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:40442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:40604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:40774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:41400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:41422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28] INFO:     127.0.0.1:42408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:28 TP0] Prefill batch, #new-seq: 17, #new-token: 17, #cached-token: 12489, token usage: 0.17, #running-req: 1007, #queue-req: 44, 
[2025-11-20 14:03:29] INFO:     127.0.0.1:33896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:34948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:36530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:37580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:38054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:38322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:38600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:40316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:40980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:42024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:42582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8009, token usage: 0.17, #running-req: 1013, #queue-req: 33, 
[2025-11-20 14:03:29] INFO:     127.0.0.1:35038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:35814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:36398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:36462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:37074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:37090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:38380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:38752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:39452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:39940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:40650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:40918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:42698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9464, token usage: 0.17, #running-req: 1011, #queue-req: 20, 
[2025-11-20 14:03:29] INFO:     127.0.0.1:34192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:34764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:36032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:37042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:37628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:38746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:39284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:40470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:40836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29] INFO:     127.0.0.1:42574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:29 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7316, token usage: 0.17, #running-req: 1014, #queue-req: 10, 
[2025-11-20 14:03:30] INFO:     127.0.0.1:33998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:38514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:39740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:39746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:40088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:40300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:41474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:41886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:42670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6514, token usage: 0.17, #running-req: 1015, #queue-req: 1, 
[2025-11-20 14:03:30] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:35164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:35704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:37862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:40746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:42184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:42368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 716, token usage: 0.17, #running-req: 1017, #queue-req: 0, 
[2025-11-20 14:03:30] INFO:     127.0.0.1:34060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:34872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:35476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:35570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:35632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:38276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:39124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:39332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:39428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:39712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:40184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:42332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:42444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:42736 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP2] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP3] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP6] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP7] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP1] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP5] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP0] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP4] [fused_moe] using default for (1004, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30] INFO:     127.0.0.1:34880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:36544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:41762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:41856 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP2] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP3] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP6] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP1] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP7] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP5] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP0] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP4] [fused_moe] using default for (1000, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30] INFO:     127.0.0.1:35908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:36850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:37012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:37556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:39102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:39450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:39646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:40514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:40828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:40986 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP3] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP1] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP2] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP5] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP7] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP6] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP0] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30 TP4] [fused_moe] using default for (990, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:30] INFO:     127.0.0.1:34394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:34718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:34990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:35154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:35328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:36176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:36276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:37924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:37982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:38506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:41284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:30] INFO:     127.0.0.1:42062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:33964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:35818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:38306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:38400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:38450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:38570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:39380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:40236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:40674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:41784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:42030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:34350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:34440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:34808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:36296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:36774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:37758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:38808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:39140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:40504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:41180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:41300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:42342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:33856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:34230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:34520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:34712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:35088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:36202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:37254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:37600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:38200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:38328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:38790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:40250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:41732 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP3] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP2] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP1] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP5] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP7] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP6] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP0] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP4] [fused_moe] using default for (942, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:34508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:34640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:36736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:38774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:39046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:39774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:40020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:41098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:41374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:41996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:35030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:35578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:36922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:37738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:38718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:39680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:40996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:41134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:41260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:41812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:41836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:42684 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP3] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP2] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP1] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP5] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP7] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP6] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP0] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP4] [fused_moe] using default for (919, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31] INFO:     127.0.0.1:34244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:36584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:36668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:37592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:38662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:39960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:40546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:40574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:42668 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP3] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP1] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP2] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP5] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP6] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP7] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP0] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP4] [fused_moe] using default for (910, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31] INFO:     127.0.0.1:34352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:35696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:35726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:36252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:38872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:40966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:42304 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP3] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP1] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP2] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP5] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP6] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP7] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP0] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31 TP4] [fused_moe] using default for (903, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:31] INFO:     127.0.0.1:33738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:34666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:34782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:36382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:36742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:37376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:37746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:38060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:38520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:39486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:40490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:40792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:41082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:41204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:41458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:41548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:31] INFO:     127.0.0.1:42478 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP3] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP2] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP1] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP5] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP6] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP7] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP0] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP4] [fused_moe] using default for (885, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32] INFO:     127.0.0.1:35658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:35762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:36778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:38104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:38244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:38814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:40706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:40798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:42012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:42348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:42568 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP3] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP2] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP1] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP5] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP6] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP7] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP0] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP4] [fused_moe] using default for (874, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32] INFO:     127.0.0.1:33776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:34080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:35612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:36122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:36964 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP3] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP2] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP1] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP5] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP6] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP7] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP0] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP4] [fused_moe] using default for (869, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32] INFO:     127.0.0.1:34022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:34410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:34838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:36046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:36460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:38530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:38828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:38920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:40004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:40098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:40216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:40440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:40494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:40912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:41536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:41768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:41974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:42436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:34530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:34822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:35228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:37244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:37908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:40226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:41406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:41662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:42592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:34738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:35262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:35770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:41334 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP1] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP2] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP5] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP6] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP3] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP7] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP0] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP4] [fused_moe] using default for (838, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32] INFO:     127.0.0.1:34146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:34296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:34406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:34444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:37006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:37278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:37670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:38650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:38666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:41606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:41730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:33724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:34552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:34888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:36120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:36364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:38318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:39346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:40524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:42132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:43220 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP2] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP1] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP5] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP6] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP3] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP7] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP0] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32 TP4] [fused_moe] using default for (817, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:32] INFO:     127.0.0.1:33960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:36992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:37684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:37710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:38228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:39398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:39662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:40468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:41320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:41894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:42818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:32] INFO:     127.0.0.1:43432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:36434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:38146 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP3] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP2] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP1] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP5] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP6] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP7] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP0] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP4] [fused_moe] using default for (803, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33] INFO:     127.0.0.1:33732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:33958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:34262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:34260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:35002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:35744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:35894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:35968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:37152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:37252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:37570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:37644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:38188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:38878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:38898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:40380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:42178 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP3] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP0] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP1] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP2] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP6] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP7] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP4] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP5] [fused_moe] using default for (786, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33] INFO:     127.0.0.1:34222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:36636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:37820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:37956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:39030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:39580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:40002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:40072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:41024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:42146 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP2] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP3] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP6] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP7] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP1] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP5] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP0] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP4] [fused_moe] using default for (776, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33] INFO:     127.0.0.1:35522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:35964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:38124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:38664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:39262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:39280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:39532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:40008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:41700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:42382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:33890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:38350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:39636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:40036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:40666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:42578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:42648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:43352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:43886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:36054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:36608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:37480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:38370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:38424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:40268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:41116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:42292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:42318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:42488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:43468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:43908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:34682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:35052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:38232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:38584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:38668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:39308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:39328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:41766 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP4] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP5] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP1] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP0] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP2] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP3] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP6] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP7] [fused_moe] using default for (737, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33] INFO:     127.0.0.1:34308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:35504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:36562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:38478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:41138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:41562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:33] INFO:     127.0.0.1:43876 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP1] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP5] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP3] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP4] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP7] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP0] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP2] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:33 TP6] [fused_moe] using default for (730, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP0] Decode batch, #running-req: 737, #token: 104238, token usage: 0.14, cuda graph: False, gen throughput (token/s): 4405.32, #queue-req: 0, 
[2025-11-20 14:03:34] INFO:     127.0.0.1:33794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:33820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:34464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:35184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:35984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:36106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:36448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:36874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:36948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:37100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:37828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:38004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:38960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:40168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:40218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:40348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:41342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:42772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:43594 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP3] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP7] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP2] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP1] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP5] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP6] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP0] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP4] [fused_moe] using default for (711, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34] INFO:     127.0.0.1:34354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:35192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:36190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:37554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:39806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:40594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:41088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:41090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:42170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:34090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:35932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:37212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:37450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:37522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:38266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:38918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:39522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:40196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:40932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:41844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:42690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:43498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:43574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:43850 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP2] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP3] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP7] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP6] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP1] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP5] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP0] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP4] [fused_moe] using default for (686, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34] INFO:     127.0.0.1:35244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:35388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:38502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:39898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:40278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:41694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:42108 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP2] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP3] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP6] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP7] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP1] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP5] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP0] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP4] [fused_moe] using default for (679, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34] INFO:     127.0.0.1:33822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:38138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:38436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:39156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:40124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:41196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:41968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:42260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:43562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:44048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:45010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:33870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:33912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:34164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:34786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:37150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:37168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:37186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:38466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:39080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:39250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:41218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:41936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:41948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:42388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:43324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:43598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:44206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:34396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:36070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:41914 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP3] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP2] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP5] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP1] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP7] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP6] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP0] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34 TP4] [fused_moe] using default for (646, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:34] INFO:     127.0.0.1:34046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:35742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:35872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:36286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:38760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:39050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:39598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:39910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:41112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:41982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:42042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:34] INFO:     127.0.0.1:42640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:33850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:34988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:35776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:36272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:38020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:39552 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP2] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP3] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP1] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP5] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP7] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP6] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP0] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP4] [fused_moe] using default for (628, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35] INFO:     127.0.0.1:34538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:35176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:37418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:37470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:37958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:39508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:43878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:45134 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP2] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP3] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP1] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP5] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP7] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP6] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP0] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP4] [fused_moe] using default for (620, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35] INFO:     127.0.0.1:36488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:37486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:39520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:40610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:42242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:44768 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP5] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP2] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP3] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP1] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP6] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP7] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP0] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP4] [fused_moe] using default for (614, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35] INFO:     127.0.0.1:34572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:35718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:36304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:36558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:36980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:37428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:38140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:38608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:39846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:40636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:41578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:41824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:41840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:42710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:43456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:43990 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP0] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP3] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP2] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP1] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP4] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP5] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP7] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP6] [fused_moe] using default for (598, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35] INFO:     127.0.0.1:34176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:34610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:35824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:37438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:37778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:37886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:38252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:40342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:42548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:42980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:44380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:33936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:36408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:36860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:39082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:39470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:39700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:40166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:42910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:44392 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP3] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP2] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP7] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP6] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP1] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP5] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP0] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP4] [fused_moe] using default for (577, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35] INFO:     127.0.0.1:33780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:34794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:36656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:36788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:36856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:38734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:40458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:40762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:41066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:41266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:41446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:41826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:42492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:42628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:42804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:43058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:43292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:45028 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP2] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP7] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP3] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP6] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP1] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP5] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP0] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP4] [fused_moe] using default for (559, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35] INFO:     127.0.0.1:34382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:35356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:35442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:35676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:36506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:36810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:37136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:39148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:39184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:39358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:39466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:40264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:40718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:42272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:42652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:43018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:43146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:43400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:43624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:35] INFO:     127.0.0.1:44592 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP2] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP3] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP6] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP7] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP1] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP5] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP0] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:35 TP4] [fused_moe] using default for (538, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36] INFO:     127.0.0.1:34246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:37540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:38070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:39652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:42156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:42194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:42756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:43086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:43674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:34166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:35788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:39330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:40686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:42254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:43626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:43958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:43980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:44528 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP3] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP2] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP5] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP7] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP1] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP6] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP0] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP4] [fused_moe] using default for (518, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36] INFO:     127.0.0.1:34124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:35506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:36754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:36892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:37128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:37848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:39800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:40526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:41244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:43082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:43514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:43822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:44140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:44264 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP2] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP5] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP3] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP6] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP1] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP7] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP0] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP4] [fused_moe] using default for (504, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36] INFO:     127.0.0.1:35610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:37800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:38412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:39200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:39988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:35962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:36004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:37302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:38036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:38896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:38980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:39628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:41000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:42240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:43448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:34706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:35374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:36906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:37196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:37356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:37874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:39884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:43754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:43942 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP2] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP3] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP5] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP1] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP6] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP7] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP0] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP4] [fused_moe] using default for (480, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36] INFO:     127.0.0.1:33760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:34036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:37170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:37762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:41270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:41900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:43130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:43682 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP2] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP5] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP3] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP1] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP7] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP6] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP0] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP4] [fused_moe] using default for (472, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36] INFO:     127.0.0.1:39512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:40952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:42978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:44376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:44658 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP2] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP3] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP1] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP5] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP6] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP7] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP0] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP4] [fused_moe] using default for (467, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36] INFO:     127.0.0.1:36102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:39002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:39438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:40292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:41636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:36] INFO:     127.0.0.1:44570 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP2] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP0] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP1] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP3] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP4] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP5] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP6] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:36 TP7] [fused_moe] using default for (461, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37] INFO:     127.0.0.1:36802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:36822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:37422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:38352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:38996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:41158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:41226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:43186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:43700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:43962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:44226 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP3] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP2] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP7] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP6] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP1] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP5] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP0] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP4] [fused_moe] using default for (450, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37] INFO:     127.0.0.1:38578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:39692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:39874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:40140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:41004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:42472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:43344 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP2] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP3] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP6] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP7] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP1] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP5] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP0] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP4] [fused_moe] using default for (443, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37] INFO:     127.0.0.1:33696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:35482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:35852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:37342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:37392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:39052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:42726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:42860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:43334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:43438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP2] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP3] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP6] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP7] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP1] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP5] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP0] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP4] [fused_moe] using default for (432, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37] INFO:     127.0.0.1:33754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:39388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:39766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:41234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:42296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:44806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:44848 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP3] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP2] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP6] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP7] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP5] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP1] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP0] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP4] [fused_moe] using default for (425, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37] INFO:     127.0.0.1:34002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:35314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:37714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:38082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:38360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:43556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:44210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:44952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:45262 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP2] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP3] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP5] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP1] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP6] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP7] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP0] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP4] [fused_moe] using default for (416, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37] INFO:     127.0.0.1:35800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:41048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:42876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:44434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:45276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:36186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:37016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:37538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:38292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:42746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:44502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP1] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP2] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP3] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP5] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP6] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP7] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP0] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP4] [fused_moe] using default for (404, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37] INFO:     127.0.0.1:35678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:37638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:39018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:39170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:43938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:44194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:44754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:37] INFO:     127.0.0.1:44800 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP3] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP2] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP1] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP5] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP7] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP6] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP0] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:37 TP4] [fused_moe] using default for (396, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38] INFO:     127.0.0.1:36132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:36630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:44394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:44474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:45244 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP2] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP3] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP5] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP1] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP7] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP6] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP0] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP4] [fused_moe] using default for (391, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38] INFO:     127.0.0.1:36458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:38780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:41758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:44156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:44642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:45064 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP2] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP3] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP1] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP5] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP6] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP7] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP0] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP4] [fused_moe] using default for (385, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38] INFO:     127.0.0.1:34012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:35014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:36622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:39300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:39654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:40588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:41380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:42150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:43008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:43746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:44604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:44628 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP2] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP3] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP1] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP5] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP7] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP6] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP0] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP4] [fused_moe] using default for (373, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38] INFO:     127.0.0.1:36144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:36692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:40876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:43296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:43614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:44986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:37026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:39786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:42996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:43430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:43742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:44558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:44874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:35554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:38642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:41040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:41384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:42984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:43694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:43726 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP2] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP6] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP5] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP1] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP3] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP7] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP0] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP4] [fused_moe] using default for (352, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38] INFO:     127.0.0.1:35138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:35436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:37826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:40208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:40334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:43854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:44296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:45160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38 TP0] Decode batch, #running-req: 352, #token: 62180, token usage: 0.09, cuda graph: False, gen throughput (token/s): 4395.91, #queue-req: 0, 
[2025-11-20 14:03:38] INFO:     127.0.0.1:34116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:37806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:37898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:38704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:41168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP1] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP2] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP5] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP3] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP6] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP7] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP0] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP4] [fused_moe] using default for (338, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38] INFO:     127.0.0.1:37404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:42078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:43582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:44366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:38] INFO:     127.0.0.1:45016 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP1] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP5] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP2] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP3] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP6] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP7] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP0] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:38 TP4] [fused_moe] using default for (333, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39] INFO:     127.0.0.1:35142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:39454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:40778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:42206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:42832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:43416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:45002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:33986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:40802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:42914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:43488 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP1] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP2] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP3] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP6] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP5] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP7] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP0] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP4] [fused_moe] using default for (322, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39] INFO:     127.0.0.1:34542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:35888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:41460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:43260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:43668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:44340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:44354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:44748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:45332 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP2] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP3] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP1] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP5] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP7] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP6] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP0] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP4] [fused_moe] using default for (313, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39] INFO:     127.0.0.1:34882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:35560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:36834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:38836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:39756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:41182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:43042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:43212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:43892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:43924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:45004 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP2] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP1] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP3] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP6] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP5] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP7] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP0] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP4] [fused_moe] using default for (302, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39] INFO:     127.0.0.1:37010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:38860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:42778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:45040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:45282 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP3] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP2] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP1] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP5] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP6] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP7] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP0] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP4] [fused_moe] using default for (297, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP1] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP3] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP2] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP7] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP5] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP6] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39] INFO:     127.0.0.1:36470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:36932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:37842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:42844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:42898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:42942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:45358 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP0] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39 TP4] [fused_moe] using default for (290, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:39] INFO:     127.0.0.1:34496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:40326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:42928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:33862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:34732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:34762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:37694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:44068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:44250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:44840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:45340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:39818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:39964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:42884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:43178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:43248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:44468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:39] INFO:     127.0.0.1:44590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:35268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:37122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:39788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:41946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:42498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:42512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:42788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:43526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:45312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:35948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:42982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:44170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:45066 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP3] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP2] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP1] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP5] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP6] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP7] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP0] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP4] [fused_moe] using default for (259, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40] INFO:     127.0.0.1:34522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:36222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:42362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:42812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:43552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:45330 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP2] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP3] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP5] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP1] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP7] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP6] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP0] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP4] [fused_moe] using default for (253, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40] INFO:     127.0.0.1:36882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:40882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:42760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:44312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:45190 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP3] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP1] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP2] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP5] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP6] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP7] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP0] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP4] [fused_moe] using default for (247, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40] INFO:     127.0.0.1:36320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:36610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:39526 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP3] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP2] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP1] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP6] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP5] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP7] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP0] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP4] [fused_moe] using default for (244, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40] INFO:     127.0.0.1:34266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:39322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:43946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:44994 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP2] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP3] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP1] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP5] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP7] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP6] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP0] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP4] [fused_moe] using default for (240, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40] INFO:     127.0.0.1:43122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:44208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:45118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:45194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:45306 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP1] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP3] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP2] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP5] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP6] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP7] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP0] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40 TP4] [fused_moe] using default for (235, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:40] INFO:     127.0.0.1:34452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:35950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:40] INFO:     127.0.0.1:44702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:38354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:40102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:44978 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP3] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP1] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP2] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP6] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP5] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP7] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP0] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP4] [fused_moe] using default for (229, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41] INFO:     127.0.0.1:42716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:44072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:44178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:44942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:45210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:40808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:42722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:44184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:44752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:44958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP2] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP1] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP3] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP5] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP6] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP7] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP0] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP4] [fused_moe] using default for (218, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41] INFO:     127.0.0.1:42346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:44672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:35766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:43808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:43826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:44982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:35464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:42562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:44090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:45036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:33810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:35294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:35690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:37654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:43738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:44402 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP1] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP2] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP3] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP5] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP7] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP6] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP0] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41 TP4] [fused_moe] using default for (202, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:41] INFO:     127.0.0.1:40398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:43770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:44124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:44578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:44790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:45140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:40412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:40812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:43298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:41] INFO:     127.0.0.1:44518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:43028 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP1] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP2] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP3] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP6] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP7] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP5] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP0] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP4] [fused_moe] using default for (191, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42] INFO:     127.0.0.1:34850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:36504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:37058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:40110 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP1] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP3] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP2] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP5] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP6] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP7] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP0] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP4] [fused_moe] using default for (187, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42] INFO:     127.0.0.1:35060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:38074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:38216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:40090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:42600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:43862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:45090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:44490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:44608 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP2] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP1] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP3] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP5] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP6] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP7] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP0] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP4] [fused_moe] using default for (178, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42] INFO:     127.0.0.1:34652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:35114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:37606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:38226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:38414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:39290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:43134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:43736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:44188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:44496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:42052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:43210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:43948 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP3] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP2] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP1] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP5] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP7] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP6] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP0] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP4] [fused_moe] using default for (165, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42] INFO:     127.0.0.1:41434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:43174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:43720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:44274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:44836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:45020 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP3] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP2] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP1] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP5] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP7] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP6] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP0] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42 TP4] [fused_moe] using default for (159, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:42] INFO:     127.0.0.1:34934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:38128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:42948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:43270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:43312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:43390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:34204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:44286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:42] INFO:     127.0.0.1:44412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:43286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:45056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:35592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:38680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:42964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:43368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:44446 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP3] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP2] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP1] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP5] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP6] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP7] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP0] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP4] [fused_moe] using default for (143, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP0] Decode batch, #running-req: 144, #token: 32925, token usage: 0.05, cuda graph: False, gen throughput (token/s): 2078.78, #queue-req: 0, 
[2025-11-20 14:03:43] INFO:     127.0.0.1:44100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:44828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:35886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:36768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:43348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:43842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:44336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:44928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:37940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:43974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:44428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:45150 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP3] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP7] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP1] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP5] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP2] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP6] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP0] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP4] [fused_moe] using default for (130, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43] INFO:     127.0.0.1:33744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:34268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:44252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:44918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:45052 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP3] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP1] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP2] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP5] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP7] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP6] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP0] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP4] [fused_moe] using default for (125, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43] INFO:     127.0.0.1:39216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:43] INFO:     127.0.0.1:44064 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP3] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP2] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP1] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP7] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP5] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP6] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP0] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:43 TP4] [fused_moe] using default for (123, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44] INFO:     127.0.0.1:38042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:39764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:40634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:43796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:43102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:43152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:43342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:44036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:44710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:40360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:42452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:43200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:44344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:34558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:36318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:41490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:41698 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP3] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP2] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP1] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP5] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP7] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP6] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP0] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP4] [fused_moe] using default for (106, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44] INFO:     127.0.0.1:38908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:43806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:45110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:45296 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP3] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP2] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP1] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP5] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP6] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP7] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP4] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44 TP0] [fused_moe] using default for (102, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:44] INFO:     127.0.0.1:44076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:37736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:40840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:44] INFO:     127.0.0.1:34918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:35300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:41924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:43542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:44544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:44612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:45182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:38460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:44260 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP3] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP1] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP2] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP5] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP6] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP7] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP0] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP4] [fused_moe] using default for (90, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45] INFO:     127.0.0.1:34958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:35828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:39026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:35842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:38486 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP3] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP2] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP1] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP5] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP6] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP7] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP0] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP4] [fused_moe] using default for (85, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45] INFO:     127.0.0.1:37368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:41904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:44780 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP3] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP1] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP2] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP5] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP6] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP7] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP0] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45 TP4] [fused_moe] using default for (82, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:45] INFO:     127.0.0.1:41008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:45326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:40696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:44006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:45] INFO:     127.0.0.1:44322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:37718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:38178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:43480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:44458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:34142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:40904 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:46 TP3] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:46 TP2] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:46 TP1] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:46 TP5] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:46 TP7] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:46 TP6] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:46 TP0] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:46 TP4] [fused_moe] using default for (71, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:03:46] INFO:     127.0.0.1:43106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:38914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:44814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:35276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:36038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:36352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:41064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:44724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:40914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:43784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:44116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:36346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:44696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:44818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:46] INFO:     127.0.0.1:45250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:34514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:34532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:37700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:39976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:34422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:44328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:44890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:43234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47 TP0] Decode batch, #running-req: 49, #token: 12748, token usage: 0.02, cuda graph: False, gen throughput (token/s): 811.11, #queue-req: 0, 
[2025-11-20 14:03:47] INFO:     127.0.0.1:39098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:43602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:44758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:35042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:45168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:42434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:43096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:44234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:47] INFO:     127.0.0.1:44390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:48] INFO:     127.0.0.1:44682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:48] INFO:     127.0.0.1:39354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:48] INFO:     127.0.0.1:43376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:48] INFO:     127.0.0.1:37394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:48] INFO:     127.0.0.1:43370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:48] INFO:     127.0.0.1:44534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:48] INFO:     127.0.0.1:41962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:48] INFO:     127.0.0.1:44862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:48] INFO:     127.0.0.1:41870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:49] INFO:     127.0.0.1:44104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:49] INFO:     127.0.0.1:40062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:49] INFO:     127.0.0.1:45222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:49] INFO:     127.0.0.1:44740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:50] INFO:     127.0.0.1:34624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:50] INFO:     127.0.0.1:45348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:50] INFO:     127.0.0.1:34862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:50] INFO:     127.0.0.1:42418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:50] INFO:     127.0.0.1:43954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:50] INFO:     127.0.0.1:45234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:50] INFO:     127.0.0.1:35662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:50] INFO:     127.0.0.1:35644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:51] INFO:     127.0.0.1:43658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:51] INFO:     127.0.0.1:44056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:51 TP0] Decode batch, #running-req: 16, #token: 5663, token usage: 0.01, cuda graph: True, gen throughput (token/s): 286.78, #queue-req: 0, 
[2025-11-20 14:03:51] INFO:     127.0.0.1:43454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:51] INFO:     127.0.0.1:43250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:51] INFO:     127.0.0.1:45082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:51] INFO:     127.0.0.1:43330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:51] INFO:     127.0.0.1:44968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:51] INFO:     127.0.0.1:40566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:51] INFO:     127.0.0.1:44902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:52] INFO:     127.0.0.1:44020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:52] INFO:     127.0.0.1:39984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:52] INFO:     127.0.0.1:44308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:52] INFO:     127.0.0.1:42618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:52 TP0] Decode batch, #running-req: 5, #token: 2330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 288.88, #queue-req: 0, 
[2025-11-20 14:03:53] INFO:     127.0.0.1:43706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:53] INFO:     127.0.0.1:44182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:53] INFO:     127.0.0.1:42792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:53 TP0] Decode batch, #running-req: 2, #token: 1480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 121.74, #queue-req: 0, 
[2025-11-20 14:03:54] INFO:     127.0.0.1:43068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:03:54 TP0] Decode batch, #running-req: 1, #token: 1134, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.52, #queue-req: 0, 
[2025-11-20 14:03:55 TP0] Decode batch, #running-req: 1, #token: 1174, token usage: 0.00, cuda graph: True, gen throughput (token/s): 41.52, #queue-req: 0, 
[2025-11-20 14:03:56 TP0] Decode batch, #running-req: 1, #token: 1214, token usage: 0.00, cuda graph: True, gen throughput (token/s): 41.51, #queue-req: 0, 
[2025-11-20 14:03:56] INFO:     127.0.0.1:35490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:02] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-11-20 14:04:02] INFO:     127.0.0.1:36262 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-20 14:04:10] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-11-20 14:04:10] INFO:     127.0.0.1:50932 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-20 14:04:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-20 14:04:10] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-20 14:04:10 TP0] Prefill batch, #new-seq: 48, #new-token: 48, #cached-token: 35008, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-11-20 14:04:11 TP0] Prefill batch, #new-seq: 487, #new-token: 487, #cached-token: 354361, token usage: 0.04, #running-req: 49, #queue-req: 0, 
[aiter] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:12 TP2] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:12 TP4] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:12 TP5] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:12 TP1] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:12 TP3] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:12 TP7] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:12 TP0] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:12 TP6] [fused_moe] using default for (487, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:12 TP0] Prefill batch, #new-seq: 397, #new-token: 397, #cached-token: 288999, token usage: 0.08, #running-req: 536, #queue-req: 0, 
[aiter] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:13 TP2] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:13 TP0] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:13 TP3] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:13 TP1] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:13 TP4] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:13 TP5] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:13 TP7] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:13 TP6] [fused_moe] using default for (397, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:14 TP0] Prefill batch, #new-seq: 91, #new-token: 91, #cached-token: 66636, token usage: 0.08, #running-req: 933, #queue-req: 295, 
[2025-11-20 14:04:18] INFO:     127.0.0.1:53732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 855, token usage: 0.12, #running-req: 1023, #queue-req: 294, 
[2025-11-20 14:04:19] INFO:     127.0.0.1:59418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 719, token usage: 0.13, #running-req: 1023, #queue-req: 293, 
[2025-11-20 14:04:20 TP0] Decode batch, #running-req: 1024, #token: 98497, token usage: 0.14, cuda graph: False, gen throughput (token/s): 1533.47, #queue-req: 293, 
[2025-11-20 14:04:20] INFO:     127.0.0.1:53786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 736, token usage: 0.14, #running-req: 1023, #queue-req: 292, 
[2025-11-20 14:04:20] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:20] INFO:     127.0.0.1:54434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:20] INFO:     127.0.0.1:55408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:20 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2224, token usage: 0.14, #running-req: 1021, #queue-req: 289, 
[2025-11-20 14:04:20] INFO:     127.0.0.1:50982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:20] INFO:     127.0.0.1:51526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:20] INFO:     127.0.0.1:55180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:20] INFO:     127.0.0.1:55418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2978, token usage: 0.14, #running-req: 1020, #queue-req: 285, 
[2025-11-20 14:04:21] INFO:     127.0.0.1:51704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:52820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:59524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5124, token usage: 0.14, #running-req: 1017, #queue-req: 278, 
[2025-11-20 14:04:21] INFO:     127.0.0.1:52660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:53754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:56584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2922, token usage: 0.14, #running-req: 1020, #queue-req: 274, 
[2025-11-20 14:04:21] INFO:     127.0.0.1:52160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:52506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:52670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:55490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:55780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:58166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6523, token usage: 0.15, #running-req: 1015, #queue-req: 265, 
[2025-11-20 14:04:21] INFO:     127.0.0.1:51842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:55882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:59508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:59652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:21] INFO:     127.0.0.1:59898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:22 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3691, token usage: 0.15, #running-req: 1019, #queue-req: 260, 
[2025-11-20 14:04:22] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:22] INFO:     127.0.0.1:54008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:22] INFO:     127.0.0.1:54612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:22] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:22] INFO:     127.0.0.1:58840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:23 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3708, token usage: 0.15, #running-req: 1019, #queue-req: 255, 
[2025-11-20 14:04:23] INFO:     127.0.0.1:56872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 702, token usage: 0.15, #running-req: 1023, #queue-req: 254, 
[2025-11-20 14:04:23] INFO:     127.0.0.1:51304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:23] INFO:     127.0.0.1:51356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:23] INFO:     127.0.0.1:51606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:23] INFO:     127.0.0.1:55266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:23] INFO:     127.0.0.1:55712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:23] INFO:     127.0.0.1:56612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:23] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:23] INFO:     127.0.0.1:58786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:23] INFO:     127.0.0.1:58988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:23] INFO:     127.0.0.1:59584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:23 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7270, token usage: 0.15, #running-req: 1014, #queue-req: 244, 
[2025-11-20 14:04:24] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:53174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:53240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:53434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:53758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:57144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:58234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:58502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5798, token usage: 0.15, #running-req: 1016, #queue-req: 236, 
[2025-11-20 14:04:24] INFO:     127.0.0.1:51246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:52788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:53254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:54132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:55980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:56674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:58496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:24] INFO:     127.0.0.1:59706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8057, token usage: 0.15, #running-req: 1013, #queue-req: 225, 
[2025-11-20 14:04:25] INFO:     127.0.0.1:51384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:52608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:54774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:55654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:55834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:59122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5095, token usage: 0.15, #running-req: 1017, #queue-req: 218, 
[2025-11-20 14:04:25] INFO:     127.0.0.1:53328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:55028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:56370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2240, token usage: 0.16, #running-req: 1021, #queue-req: 215, 
[2025-11-20 14:04:25] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:51482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:52286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:53932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:54178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:58806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:58856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5715, token usage: 0.16, #running-req: 1016, #queue-req: 207, 
[2025-11-20 14:04:25] INFO:     127.0.0.1:51638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:52348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:54118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:54398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:54944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:56090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:25] INFO:     127.0.0.1:57854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7217, token usage: 0.16, #running-req: 1014, #queue-req: 197, 
[2025-11-20 14:04:26] INFO:     127.0.0.1:51374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:52350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:55382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:56410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:57770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5835, token usage: 0.16, #running-req: 1016, #queue-req: 189, 
[2025-11-20 14:04:26] INFO:     127.0.0.1:50948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:52332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:52338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:53042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:54090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:57356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:57772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:59170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:60054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7306, token usage: 0.16, #running-req: 1014, #queue-req: 179, 
[2025-11-20 14:04:26] INFO:     127.0.0.1:51976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:52128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:52528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:53496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5086, token usage: 0.16, #running-req: 1017, #queue-req: 172, 
[2025-11-20 14:04:26] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:52416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:53646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:53826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:54134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:56762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:26] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5692, token usage: 0.16, #running-req: 1016, #queue-req: 164, 
[2025-11-20 14:04:27] INFO:     127.0.0.1:52404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:56942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:58462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:58600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:59960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7229, token usage: 0.16, #running-req: 1014, #queue-req: 154, 
[2025-11-20 14:04:27] INFO:     127.0.0.1:54290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:57462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:57520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3660, token usage: 0.16, #running-req: 1019, #queue-req: 149, 
[2025-11-20 14:04:27] INFO:     127.0.0.1:52616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:53112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:53456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:53842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:55514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:55556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:56548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:59110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6658, token usage: 0.16, #running-req: 1015, #queue-req: 140, 
[2025-11-20 14:04:27] INFO:     127.0.0.1:51094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:51780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:51886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:52690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:52810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:55548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:55688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:55862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:57116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:58242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:27] INFO:     127.0.0.1:59396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10907, token usage: 0.16, #running-req: 1009, #queue-req: 125, 
[2025-11-20 14:04:28] INFO:     127.0.0.1:51924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:52258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:56008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:56222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:57226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:59020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6656, token usage: 0.16, #running-req: 1015, #queue-req: 116, 
[2025-11-20 14:04:28] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:53404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:53560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:56294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:58254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:58342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:58792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7369, token usage: 0.16, #running-req: 1014, #queue-req: 106, 
[2025-11-20 14:04:28] INFO:     127.0.0.1:51254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:51178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:53880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:54308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:54878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:56188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:58744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:58918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6660, token usage: 0.17, #running-req: 1015, #queue-req: 97, 
[2025-11-20 14:04:28] INFO:     127.0.0.1:51336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:52134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:53712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:55468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:55516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:58962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:28] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:29 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10086, token usage: 0.17, #running-req: 1010, #queue-req: 83, 
[2025-11-20 14:04:29] INFO:     127.0.0.1:51954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:29] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:29] INFO:     127.0.0.1:52858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:29] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:29] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:29] INFO:     127.0.0.1:56388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:29] INFO:     127.0.0.1:58942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5075, token usage: 0.17, #running-req: 1017, #queue-req: 76, 
[2025-11-20 14:04:30] INFO:     127.0.0.1:51142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:51174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:51932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:52548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:53582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:53660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:53876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:55528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:55796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:56658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:57016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:58904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30 TP0] Prefill batch, #new-seq: 17, #new-token: 17, #cached-token: 12522, token usage: 0.17, #running-req: 1007, #queue-req: 59, 
[2025-11-20 14:04:30] INFO:     127.0.0.1:51788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:54194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:56436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:57912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:58654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:58704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:60026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8890, token usage: 0.17, #running-req: 1012, #queue-req: 47, 
[2025-11-20 14:04:30] INFO:     127.0.0.1:51734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:51870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:52922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:54714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:55016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:55208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:55608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:56456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:56714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:56916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30] INFO:     127.0.0.1:59346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:30 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11626, token usage: 0.17, #running-req: 1008, #queue-req: 31, 
[2025-11-20 14:04:31] INFO:     127.0.0.1:51720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:31] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:31] INFO:     127.0.0.1:53626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:31] INFO:     127.0.0.1:54032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:31] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:31] INFO:     127.0.0.1:54558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:31] INFO:     127.0.0.1:55544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:31] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:31] INFO:     127.0.0.1:56250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:31] INFO:     127.0.0.1:56292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:31] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:31] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:31] INFO:     127.0.0.1:59468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:31] INFO:     127.0.0.1:59938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10214, token usage: 0.17, #running-req: 1010, #queue-req: 17, 
[2025-11-20 14:04:32] INFO:     127.0.0.1:51626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:51772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:53934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:54858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:57764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8023, token usage: 0.17, #running-req: 1013, #queue-req: 6, 
[2025-11-20 14:04:32] INFO:     127.0.0.1:52008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:52144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:52646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:54272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:54622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:55582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:57020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:57212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4336, token usage: 0.17, #running-req: 1011, #queue-req: 0, 
[2025-11-20 14:04:32] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:52430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:54294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:55046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:58026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:51170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:51194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:51572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:52776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:56998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:58046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:32] INFO:     127.0.0.1:60056 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:32 TP1] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:32 TP5] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:32 TP3] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:32 TP7] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:32 TP2] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:32 TP6] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:32 TP0] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:32 TP4] [fused_moe] using default for (1002, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:32 TP0] Decode batch, #running-req: 1011, #token: 121543, token usage: 0.17, cuda graph: False, gen throughput (token/s): 3162.27, #queue-req: 0, 
[2025-11-20 14:04:33] INFO:     127.0.0.1:51830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:52362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:52804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:55522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:59026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:59618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:59866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:51760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:55384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:56692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:57806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:58268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:59238 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP1] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP5] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP3] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP7] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP0] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP4] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP2] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP6] [fused_moe] using default for (981, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33] INFO:     127.0.0.1:51566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:51678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:51810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:52158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:52540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:53302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:59394 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP5] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP1] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP3] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP7] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP0] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP4] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP2] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP6] [fused_moe] using default for (972, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33] INFO:     127.0.0.1:51394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:52454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:55824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:55850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:55892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:57556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:59352 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP5] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP1] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP3] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP7] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP0] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP4] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP2] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP6] [fused_moe] using default for (965, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33] INFO:     127.0.0.1:51496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:51898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:52058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:52098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:52638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:55670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:55940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:57786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:58472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:59638 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP5] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP1] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP3] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP7] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP0] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP4] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP2] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP6] [fused_moe] using default for (952, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33] INFO:     127.0.0.1:51108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:52950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:55132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:55728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:58394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:58996 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP5] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP1] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP3] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP7] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP0] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP4] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP2] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP6] [fused_moe] using default for (944, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33] INFO:     127.0.0.1:51410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:51582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:53500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:53978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:55934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:56054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:56200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:57306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:59336 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP1] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP5] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP3] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP7] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP0] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP4] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP2] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP6] [fused_moe] using default for (934, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33] INFO:     127.0.0.1:52070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:52576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:54896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:56432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:56964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:57842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:58282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:58402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:59136 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP5] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP1] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP3] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP7] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP0] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP4] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP2] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33 TP6] [fused_moe] using default for (924, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:33] INFO:     127.0.0.1:50988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:52164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:54208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:54754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:55194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:57050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:57240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:33] INFO:     127.0.0.1:59784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:51896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:53488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:54348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:54446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:54708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:55226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:55888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:59624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:59826 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP5] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP1] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP3] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP7] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP0] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP2] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP4] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP6] [fused_moe] using default for (899, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34] INFO:     127.0.0.1:51004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:53878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:54694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:54902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:55352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:56756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:58072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:58358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:58756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:58874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:59046 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP5] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP1] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP3] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP7] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP0] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP2] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP4] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP6] [fused_moe] using default for (882, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34] INFO:     127.0.0.1:52702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:53762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:55574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:56216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:56320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:58518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:59332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:59666 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP5] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP1] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP3] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP2] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP7] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP0] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP6] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP4] [fused_moe] using default for (870, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34] INFO:     127.0.0.1:51034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:53098 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP5] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP1] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP2] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP3] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP7] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP6] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP0] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP4] [fused_moe] using default for (865, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34] INFO:     127.0.0.1:51294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:53804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:54276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:54424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:56068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:56424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:56910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:57494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:57736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:58932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:59298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:60012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:60378 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP5] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP1] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP3] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP2] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP7] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP6] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP0] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP4] [fused_moe] using default for (847, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34] INFO:     127.0.0.1:51538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:59204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:59776 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP1] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP5] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP2] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP3] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP6] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP7] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP0] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP4] [fused_moe] using default for (840, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34] INFO:     127.0.0.1:51616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:51992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:57962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:58672 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP5] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP1] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP2] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP6] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP3] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP0] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP7] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP4] [fused_moe] using default for (831, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34] INFO:     127.0.0.1:51286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:52708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:53530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:54214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:34] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP5] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP1] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP2] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP6] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP0] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP3] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP4] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:34 TP7] [fused_moe] using default for (823, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:51222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:51426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:51854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:53146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:55084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:56242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:57812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:59420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:51230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:51320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:54830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:56224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:56646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:56956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:57428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:58624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:60182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP5] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP1] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP2] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP6] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP3] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP7] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP0] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP4] [fused_moe] using default for (798, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35] INFO:     127.0.0.1:54624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:55324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:55884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:56112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:51232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:52482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:54006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:54646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:54742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:54798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:55240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP1] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP5] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP2] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP6] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP3] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP7] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP0] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP4] [fused_moe] using default for (782, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35] INFO:     127.0.0.1:52094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:54258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:54974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:56856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP1] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP5] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP2] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP6] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP3] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP7] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP0] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP4] [fused_moe] using default for (773, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35] INFO:     127.0.0.1:53472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:55482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:55616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:56628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:57302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:59700 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP1] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP5] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP2] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP6] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP3] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP7] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP0] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP4] [fused_moe] using default for (767, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:52478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:54906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:55114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:56020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:56534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:57954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:60370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:60696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:32984 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP1] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP5] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP2] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP6] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP3] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP7] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP0] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35 TP4] [fused_moe] using default for (753, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:35] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:56236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:35] INFO:     127.0.0.1:57488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:58384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:58572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:33020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:60100 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP2] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP0] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP1] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP3] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP4] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP5] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP7] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP6] [fused_moe] using default for (740, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36] INFO:     127.0.0.1:51644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:52100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:54638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:58826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:60002 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP1] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP5] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP0] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP2] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP4] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP3] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP6] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP7] [fused_moe] using default for (731, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36] INFO:     127.0.0.1:51058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:51270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:52460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:53320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:54014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:55002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:56504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:57370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:57458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:58214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:58304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:60076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:51290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:52768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:53772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:54548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:54692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:57076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:58378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:60928 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP5] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP1] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP2] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP3] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP6] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP0] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP7] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP4] [fused_moe] using default for (700, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36] INFO:     127.0.0.1:51512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:54184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:54336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:55360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:56820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:57940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:58144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:60036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP1] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP5] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP2] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP6] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP3] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP0] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP7] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP4] [fused_moe] using default for (685, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36] INFO:     127.0.0.1:51750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:52510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:52562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:57170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:60842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:60906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:51084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:52868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:56076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:56272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:56348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:33124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:34102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:51130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:51998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:53860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:54546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:55586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:55686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:57318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:58892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:59410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:60890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:32982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:36] INFO:     127.0.0.1:33332 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP5] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP2] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP1] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP6] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP3] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP0] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP7] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:36 TP4] [fused_moe] using default for (651, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37] INFO:     127.0.0.1:51328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:51668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:52668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:53066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:57586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:51342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:52982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:56266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:57178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:59366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:59728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:60306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:60920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:51112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:53706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:56386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:56854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:57012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:57592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:59764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:33102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:51326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:51450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:51470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:51658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:52130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:52294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:52972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:53264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:56788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:58954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:59486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:60646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:34198 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP2] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP1] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP5] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP6] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP3] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP0] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP7] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP4] [fused_moe] using default for (608, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37] INFO:     127.0.0.1:51564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:51802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:56792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:57654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:57748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:58374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:59540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:60310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:33522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:33896 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP1] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP2] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP5] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP6] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP3] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP0] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP7] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP4] [fused_moe] using default for (596, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37] INFO:     127.0.0.1:53224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:53344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:53388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:54020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:54360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:54930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:55214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:57924 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP1] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP5] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP2] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP6] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP3] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP0] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP7] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP4] [fused_moe] using default for (584, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37] INFO:     127.0.0.1:52032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:53744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:54230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:55252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:57900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:59742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37 TP0] Decode batch, #running-req: 584, #token: 88203, token usage: 0.12, cuda graph: False, gen throughput (token/s): 6536.73, #queue-req: 0, 
[2025-11-20 14:04:37] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:51692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:52738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:52752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:52844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:53124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:60754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:33096 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP2] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP1] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP5] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP6] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP3] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP7] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP0] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37 TP4] [fused_moe] using default for (563, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:37] INFO:     127.0.0.1:52886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:53516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:54054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:57574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:58566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:58692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:59094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:59722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:60198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:60416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:37] INFO:     127.0.0.1:34114 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP2] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP1] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP5] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP6] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP3] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP7] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP0] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP4] [fused_moe] using default for (551, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38] INFO:     127.0.0.1:52502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:52586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:53484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:53896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:55124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:56334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:56398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:56732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:59064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:60344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:60498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:60588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:33762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP1] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP2] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP5] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP6] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP3] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP7] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP0] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP4] [fused_moe] using default for (535, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38] INFO:     127.0.0.1:51048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:53964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:56926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:59444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:60964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:51018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:53798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:57972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:60784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:60992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:33058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:33690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:53446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:57074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:59552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:59936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:60968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:32930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:33246 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP2] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP1] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP5] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP6] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP3] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP7] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP0] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP4] [fused_moe] using default for (508, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38] INFO:     127.0.0.1:52438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:53498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:55858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:57278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:60574 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP1] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP2] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP5] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP6] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP3] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP7] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP0] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP4] [fused_moe] using default for (502, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38] INFO:     127.0.0.1:53162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:53338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:53642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:58290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:59560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:60538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:52912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:53566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:55032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:59314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:32866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:33038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:33792 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP2] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP1] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP5] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP3] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP6] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP7] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP0] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP4] [fused_moe] using default for (485, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38] INFO:     127.0.0.1:52674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:53136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:54372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:55806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:56900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:57150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:58544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:58580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:38] INFO:     127.0.0.1:33556 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP3] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP1] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP2] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP5] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP6] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP7] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP0] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:38 TP4] [fused_moe] using default for (475, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39] INFO:     127.0.0.1:54136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:54454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:56568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:56986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:58236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:60046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:60798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:60826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:60984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:32770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:52732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:54958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:56936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:58446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:33736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:59818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:32784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP2] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP1] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP3] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP5] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP6] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP7] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP0] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP4] [fused_moe] using default for (448, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:55158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:55160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:57268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:57434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:60176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:60560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:33360 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP1] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP2] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP3] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP5] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP6] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP7] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP0] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP4] [fused_moe] using default for (439, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39] INFO:     127.0.0.1:50970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:53698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:59842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:60632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:60668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:34090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:53520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:60670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:33924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:34390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:52268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:52704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:53994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:54872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:55646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:56332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:33966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:34040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:34130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:34376 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP2] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP1] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP5] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP3] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP6] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP7] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP0] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39 TP4] [fused_moe] using default for (413, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:39] INFO:     127.0.0.1:53610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:51974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:52368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:54152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:54782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:33376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:39] INFO:     127.0.0.1:34212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:56406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:59854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:60290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:33034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:33286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:33678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:33890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:33916 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP3] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP2] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP1] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP5] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP6] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP7] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP0] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP4] [fused_moe] using default for (390, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:59872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:60782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:33646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:34350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:54100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:55838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:32842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:33594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:33810 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP1] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP2] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP3] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP5] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP6] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP7] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP0] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP4] [fused_moe] using default for (377, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:57888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:58688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:59432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:60104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:60232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:32856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:33782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:33794 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP1] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP2] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP5] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP6] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP3] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP7] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP0] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP4] [fused_moe] using default for (367, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40] INFO:     127.0.0.1:53026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:53596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:59012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:60394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:32966 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP1] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP2] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP3] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP7] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP5] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP6] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP0] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP4] [fused_moe] using default for (362, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40] INFO:     127.0.0.1:51434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:57036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:60114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:60252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:60940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:33728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP1] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP3] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP5] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP7] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP2] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP6] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP0] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP4] [fused_moe] using default for (354, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40] INFO:     127.0.0.1:52110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:54596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:58314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:58842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP1] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP3] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP5] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP7] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP2] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP6] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP0] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40 TP4] [fused_moe] using default for (348, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:40] INFO:     127.0.0.1:51742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:52188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:53286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:53722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:54984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:57510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:59550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:60766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:33448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:40] INFO:     127.0.0.1:34334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:55062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:55758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:59744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:60266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:53480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:60702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:60878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:32818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:33516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:33550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:58768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:60896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:33642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:34080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:52902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:33688 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP1] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP3] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP5] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP7] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP2] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP6] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP0] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP4] [fused_moe] using default for (316, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41] INFO:     127.0.0.1:52108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:59258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:60218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:33490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:34468 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP1] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP3] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP7] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP5] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP2] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP0] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP6] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41 TP4] [fused_moe] using default for (311, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:41] INFO:     127.0.0.1:52378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:54532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:60982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:33028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:34086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:34250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:34406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:34486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:53534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:53602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:55760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:33006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:33174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:33308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:52514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:52730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:54246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:54988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:33278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:51914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:52570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:41] INFO:     127.0.0.1:33960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:51006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:52354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:54846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:56464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:33384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:34126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:34132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:34440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:34480 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP3] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP1] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP2] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP0] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP5] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP7] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP4] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP6] [fused_moe] using default for (277, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42] INFO:     127.0.0.1:52486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:60408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:33744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:52792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:53012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:57066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:59416 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP1] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP5] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP0] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP3] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP4] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP7] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP2] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP6] [fused_moe] using default for (265, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42] INFO:     127.0.0.1:34446 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP1] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP5] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP0] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP3] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP4] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP7] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP2] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP6] [fused_moe] using default for (264, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP0] Decode batch, #running-req: 265, #token: 50728, token usage: 0.07, cuda graph: False, gen throughput (token/s): 3489.86, #queue-req: 0, 
[2025-11-20 14:04:42] INFO:     127.0.0.1:51880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:52604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:54812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:59686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:60856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:34266 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP1] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP5] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP3] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP0] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP7] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP4] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP2] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP6] [fused_moe] using default for (256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:59682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:33162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:34068 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP1] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP5] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP3] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP7] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP0] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP4] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP2] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP6] [fused_moe] using default for (252, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42] INFO:     127.0.0.1:53138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:53912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:54070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:54378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:54586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:56808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:59494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:59780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:34280 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP1] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP5] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP3] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP7] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP0] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP4] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP2] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42 TP6] [fused_moe] using default for (243, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:42] INFO:     127.0.0.1:52628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:59288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:60524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:60852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:33842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:34426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:42] INFO:     127.0.0.1:34230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:33464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:33498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:33898 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP1] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP5] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP3] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP7] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP0] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP4] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP2] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP6] [fused_moe] using default for (230, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43] INFO:     127.0.0.1:54500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP1] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP5] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP3] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP7] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP0] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP4] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP2] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP6] [fused_moe] using default for (226, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43] INFO:     127.0.0.1:34028 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP1] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP5] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP3] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP7] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP0] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP4] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP2] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP6] [fused_moe] using default for (225, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43] INFO:     127.0.0.1:33182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:33356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:34036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:34176 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP1] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP5] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP3] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP7] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP0] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP4] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP2] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP6] [fused_moe] using default for (221, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43] INFO:     127.0.0.1:52156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:53902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:33740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:33834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:52308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:52476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:54266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:60836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:33050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:33600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:33950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:34050 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP1] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP5] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP3] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP7] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP0] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP4] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP2] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP6] [fused_moe] using default for (206, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43] INFO:     127.0.0.1:53210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:60068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:33210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:34242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:51824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:59922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:60414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:32836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:34110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:33262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:33910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:43] INFO:     127.0.0.1:34122 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP1] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP3] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP5] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP7] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP4] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP0] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP2] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:43 TP6] [fused_moe] using default for (190, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44] INFO:     127.0.0.1:60200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:33570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:53984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:57412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:58090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:60242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:33664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:55136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:32904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:34164 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP1] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP0] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP3] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP5] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP4] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP7] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP2] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP6] [fused_moe] using default for (177, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44] INFO:     127.0.0.1:56302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:56362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:32962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:33660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:33698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:33876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:34410 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP1] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP5] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP0] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP4] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP3] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP7] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP2] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP6] [fused_moe] using default for (170, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44] INFO:     127.0.0.1:57726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:60272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:60336 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP1] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP0] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP3] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP5] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP4] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP7] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP2] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP6] [fused_moe] using default for (167, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:32832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:33336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:33584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:33764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:54758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:56172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP1] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP0] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP3] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP5] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP4] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP7] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP2] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44 TP6] [fused_moe] using default for (158, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:44] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:53086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:56308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:44] INFO:     127.0.0.1:60546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:60486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:33416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:33420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:34148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:52750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:58734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:60144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:34306 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP1] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP3] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP0] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP5] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP4] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP7] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP2] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP6] [fused_moe] using default for (146, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45] INFO:     127.0.0.1:52244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:53176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:60698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:60476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:33220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:32876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:33934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:51032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:60684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:32958 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP1] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP0] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP3] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP5] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP4] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP7] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP2] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45 TP6] [fused_moe] using default for (135, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:45] INFO:     127.0.0.1:55094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:33616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:51082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:60170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:33066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:33840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:34222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:51478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:45] INFO:     127.0.0.1:34018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:54436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:60862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:51592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:56444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:59802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:32898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:34008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:58776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:60600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:33852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:51708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:53686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:33386 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:46 TP1] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:46 TP0] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:46 TP3] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:46 TP5] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:46 TP4] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:46 TP7] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:46 TP2] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:46 TP6] [fused_moe] using default for (111, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:46] INFO:     127.0.0.1:56632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:57042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:60660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:56040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:33114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:33196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:33854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46 TP0] Decode batch, #running-req: 105, #token: 25338, token usage: 0.03, cuda graph: False, gen throughput (token/s): 1569.06, #queue-req: 0, 
[2025-11-20 14:04:46] INFO:     127.0.0.1:54888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:46] INFO:     127.0.0.1:34458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:59910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:33400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:52448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:33774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:51072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:60278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:32920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:51368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:56796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:52878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:55836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:34124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:53674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:34184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:57978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:47] INFO:     127.0.0.1:33632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:53854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:54660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:54866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:60340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:60418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:60814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:34256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:57262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:33928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:51988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:60718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:33874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:48 TP1] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:48 TP3] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:48 TP5] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:48 TP7] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:48 TP4] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:48 TP2] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:48 TP0] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:48 TP6] [fused_moe] using default for (62, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-20 14:04:48] INFO:     127.0.0.1:33824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:33918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:48] INFO:     127.0.0.1:34366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:49] INFO:     127.0.0.1:51718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:49] INFO:     127.0.0.1:33224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:49] INFO:     127.0.0.1:34472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:49] INFO:     127.0.0.1:60428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:49] INFO:     127.0.0.1:55498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:49] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:49] INFO:     127.0.0.1:60616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:49] INFO:     127.0.0.1:52016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:49] INFO:     127.0.0.1:33988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:49] INFO:     127.0.0.1:60948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:49] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:49] INFO:     127.0.0.1:33380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:49] INFO:     127.0.0.1:34260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:50] INFO:     127.0.0.1:59834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:50] INFO:     127.0.0.1:33546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:50] INFO:     127.0.0.1:33972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:50] INFO:     127.0.0.1:54524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:50] INFO:     127.0.0.1:33478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:50] INFO:     127.0.0.1:55992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:50] INFO:     127.0.0.1:56284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:50] INFO:     127.0.0.1:59272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:50] INFO:     127.0.0.1:34294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:50] INFO:     127.0.0.1:33092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:50] INFO:     127.0.0.1:33456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:51] INFO:     127.0.0.1:58056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:51] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:51] INFO:     127.0.0.1:60732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:51] INFO:     127.0.0.1:54476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:51 TP0] Decode batch, #running-req: 32, #token: 9247, token usage: 0.01, cuda graph: False, gen throughput (token/s): 596.81, #queue-req: 0, 
[2025-11-20 14:04:51] INFO:     127.0.0.1:52146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:51] INFO:     127.0.0.1:57362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:51] INFO:     127.0.0.1:60204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:51] INFO:     127.0.0.1:33752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:51] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:52] INFO:     127.0.0.1:33892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:52] INFO:     127.0.0.1:60748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:52] INFO:     127.0.0.1:51552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:52] INFO:     127.0.0.1:54856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:52] INFO:     127.0.0.1:34174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:52] INFO:     127.0.0.1:34318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:52] INFO:     127.0.0.1:52356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:52] INFO:     127.0.0.1:53402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:53] INFO:     127.0.0.1:60978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:53] INFO:     127.0.0.1:33436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:53] INFO:     127.0.0.1:59972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:53] INFO:     127.0.0.1:33140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:53] INFO:     127.0.0.1:33232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:53] INFO:     127.0.0.1:58202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:53] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:53] INFO:     127.0.0.1:34046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:53 TP0] Decode batch, #running-req: 10, #token: 3534, token usage: 0.00, cuda graph: True, gen throughput (token/s): 267.57, #queue-req: 0, 
[2025-11-20 14:04:53] INFO:     127.0.0.1:34004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:54] INFO:     127.0.0.1:32810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:54] INFO:     127.0.0.1:33146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:54] INFO:     127.0.0.1:32794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:54] INFO:     127.0.0.1:60618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:54] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:55 TP0] Decode batch, #running-req: 4, #token: 2081, token usage: 0.00, cuda graph: True, gen throughput (token/s): 181.43, #queue-req: 0, 
[2025-11-20 14:04:55] INFO:     127.0.0.1:33292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:55] INFO:     127.0.0.1:60548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:55] INFO:     127.0.0.1:60422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:04:55] INFO:     127.0.0.1:60322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-20 14:05:01] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-11-20 14:05:03] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
