INFO 11-06 11:00:44 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-06 11:00:44] WARNING model_config.py:715: quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-06 11:00:44] WARNING server_args.py:1165: Attention backend not explicitly specified. Use aiter backend by default.
[2025-11-06 11:00:44] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-06 11:00:44] server_args=ServerArgs(model_path='/mnt/raid/models/deepseek-ai/amd-DeepSeek-R1-MXFP4-Preview', tokenizer_path='/mnt/raid/models/deepseek-ai/amd-DeepSeek-R1-MXFP4-Preview', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.765, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=970329444, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/deepseek-ai/amd-DeepSeek-R1-MXFP4-Preview', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, decrypted_config_file=None, decrypted_draft_config_file=None)
[2025-11-06 11:00:44] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-06 11:00:45] Using default HuggingFace chat template with detected content format: string
INFO 11-06 11:00:53 [__init__.py:241] Automatically detected platform rocm.
INFO 11-06 11:00:53 [__init__.py:241] Automatically detected platform rocm.
INFO 11-06 11:00:53 [__init__.py:241] Automatically detected platform rocm.
INFO 11-06 11:00:53 [__init__.py:241] Automatically detected platform rocm.
INFO 11-06 11:00:53 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-06 11:00:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
INFO 11-06 11:00:53 [__init__.py:241] Automatically detected platform rocm.
INFO 11-06 11:00:53 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-11-06 11:00:53 TP1] Process 209 gpu_id 1 is running on CPUs: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
[2025-11-06 11:00:53 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
INFO 11-06 11:00:53 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-06 11:00:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-11-06 11:00:53 TP5] Process 213 gpu_id 5 is running on CPUs: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
[2025-11-06 11:00:53 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
INFO 11-06 11:00:53 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-06 11:00:53 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-06 11:00:53 TP1] Init torch distributed begin.
[2025-11-06 11:00:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-06 11:00:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-06 11:00:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-06 11:00:53 TP0] Process 208 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
[2025-11-06 11:00:53 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-06 11:00:53 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-06 11:00:53 TP5] Init torch distributed begin.
[2025-11-06 11:00:53 TP3] Process 211 gpu_id 3 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
[2025-11-06 11:00:53 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-06 11:00:54] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-06 11:00:54] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-06 11:00:54 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-06 11:00:54 TP0] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-06 11:00:54 TP7] Process 215 gpu_id 7 is running on CPUs: [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[2025-11-06 11:00:54 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-06 11:00:54 TP4] Process 212 gpu_id 4 is running on CPUs: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
[2025-11-06 11:00:54 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-11-06 11:00:54] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-06 11:00:54 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-06 11:00:54 TP3] Init torch distributed begin.
[2025-11-06 11:00:54 TP2] Process 210 gpu_id 2 is running on CPUs: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
[2025-11-06 11:00:54 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-06 11:00:54] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-06 11:00:54 TP6] Process 214 gpu_id 6 is running on CPUs: [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
[2025-11-06 11:00:54 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-06 11:00:54 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-06 11:00:54 TP7] Init torch distributed begin.
[2025-11-06 11:00:54 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-06 11:00:54 TP4] Init torch distributed begin.
[2025-11-06 11:00:54 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-06 11:00:54 TP2] Init torch distributed begin.
[2025-11-06 11:00:54 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-06 11:00:54 TP6] Init torch distributed begin.
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-11-06 11:00:54 TP0] sglang is using nccl==2.26.6
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-11-06 11:01:01 TP7] Init torch distributed ends. mem usage=3.10 GB
[2025-11-06 11:01:01 TP4] Init torch distributed ends. mem usage=3.10 GB
[2025-11-06 11:01:01 TP5] Init torch distributed ends. mem usage=3.17 GB
[2025-11-06 11:01:01 TP0] Init torch distributed ends. mem usage=3.22 GB
[2025-11-06 11:01:01 TP6] Init torch distributed ends. mem usage=3.11 GB
[2025-11-06 11:01:01 TP3] Init torch distributed ends. mem usage=3.24 GB
[2025-11-06 11:01:01 TP2] Init torch distributed ends. mem usage=3.24 GB
[2025-11-06 11:01:01 TP1] Init torch distributed ends. mem usage=2.82 GB
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
[2025-11-06 11:01:02 TP0] Load weight begin. avail mem=284.28 GB
[2025-11-06 11:01:02 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
[2025-11-06 11:01:02 TP5] Load weight begin. avail mem=284.33 GB
[2025-11-06 11:01:02 TP2] Load weight begin. avail mem=284.26 GB
[2025-11-06 11:01:02 TP4] Load weight begin. avail mem=284.40 GB
[2025-11-06 11:01:02 TP7] Load weight begin. avail mem=284.40 GB
[2025-11-06 11:01:02 TP1] Load weight begin. avail mem=284.68 GB
[2025-11-06 11:01:02 TP3] Load weight begin. avail mem=284.26 GB
[2025-11-06 11:01:02 TP6] Load weight begin. avail mem=284.39 GB
Loading safetensors checkpoint shards:   0% Completed | 0/73 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/73 [00:00<00:11,  6.53it/s]
Loading safetensors checkpoint shards:   3% Completed | 2/73 [00:00<00:15,  4.53it/s]
Loading safetensors checkpoint shards:   4% Completed | 3/73 [00:00<00:18,  3.85it/s]
Loading safetensors checkpoint shards:   5% Completed | 4/73 [00:01<00:21,  3.28it/s]
Loading safetensors checkpoint shards:   7% Completed | 5/73 [00:01<00:21,  3.09it/s]
Loading safetensors checkpoint shards:   8% Completed | 6/73 [00:01<00:19,  3.47it/s]
Loading safetensors checkpoint shards:  10% Completed | 7/73 [00:02<00:20,  3.15it/s]
Loading safetensors checkpoint shards:  11% Completed | 8/73 [00:02<00:19,  3.42it/s]
Loading safetensors checkpoint shards:  12% Completed | 9/73 [00:02<00:15,  4.22it/s]
Loading safetensors checkpoint shards:  15% Completed | 11/73 [00:02<00:09,  6.57it/s]
Loading safetensors checkpoint shards:  18% Completed | 13/73 [00:03<00:15,  3.83it/s]
Loading safetensors checkpoint shards:  19% Completed | 14/73 [00:03<00:14,  4.11it/s]
Loading safetensors checkpoint shards:  22% Completed | 16/73 [00:03<00:10,  5.58it/s]
Loading safetensors checkpoint shards:  25% Completed | 18/73 [00:03<00:07,  7.29it/s]
Loading safetensors checkpoint shards:  27% Completed | 20/73 [00:04<00:06,  8.33it/s]
Loading safetensors checkpoint shards:  30% Completed | 22/73 [00:04<00:05, 10.07it/s]
Loading safetensors checkpoint shards:  33% Completed | 24/73 [00:04<00:04, 11.71it/s]
Loading safetensors checkpoint shards:  36% Completed | 26/73 [00:06<00:19,  2.41it/s]
Loading safetensors checkpoint shards:  38% Completed | 28/73 [00:08<00:27,  1.61it/s]
Loading safetensors checkpoint shards:  40% Completed | 29/73 [00:09<00:25,  1.75it/s]
Loading safetensors checkpoint shards:  41% Completed | 30/73 [00:09<00:22,  1.93it/s]
Loading safetensors checkpoint shards:  42% Completed | 31/73 [00:09<00:19,  2.18it/s]
Loading safetensors checkpoint shards:  45% Completed | 33/73 [00:09<00:11,  3.35it/s]
Loading safetensors checkpoint shards:  47% Completed | 34/73 [00:10<00:12,  3.14it/s]
Loading safetensors checkpoint shards:  48% Completed | 35/73 [00:10<00:11,  3.31it/s]
Loading safetensors checkpoint shards:  49% Completed | 36/73 [00:10<00:10,  3.58it/s]
Loading safetensors checkpoint shards:  51% Completed | 37/73 [00:10<00:10,  3.40it/s]
Loading safetensors checkpoint shards:  52% Completed | 38/73 [00:11<00:10,  3.28it/s]
Loading safetensors checkpoint shards:  53% Completed | 39/73 [00:11<00:09,  3.59it/s]
Loading safetensors checkpoint shards:  56% Completed | 41/73 [00:11<00:07,  4.32it/s]
Loading safetensors checkpoint shards:  58% Completed | 42/73 [00:12<00:08,  3.74it/s]
Loading safetensors checkpoint shards:  59% Completed | 43/73 [00:12<00:10,  2.82it/s]
Loading safetensors checkpoint shards:  60% Completed | 44/73 [00:13<00:09,  3.08it/s]
Loading safetensors checkpoint shards:  62% Completed | 45/73 [00:13<00:08,  3.44it/s]
Loading safetensors checkpoint shards:  63% Completed | 46/73 [00:13<00:08,  3.22it/s]
Loading safetensors checkpoint shards:  64% Completed | 47/73 [00:13<00:08,  3.23it/s]
Loading safetensors checkpoint shards:  66% Completed | 48/73 [00:14<00:07,  3.39it/s]
Loading safetensors checkpoint shards:  67% Completed | 49/73 [00:14<00:10,  2.19it/s]
Loading safetensors checkpoint shards:  68% Completed | 50/73 [00:15<00:08,  2.77it/s]
Loading safetensors checkpoint shards:  71% Completed | 52/73 [00:15<00:04,  4.31it/s]
Loading safetensors checkpoint shards:  75% Completed | 55/73 [00:15<00:02,  6.98it/s]
Loading safetensors checkpoint shards:  78% Completed | 57/73 [00:15<00:01,  8.80it/s]
Loading safetensors checkpoint shards:  81% Completed | 59/73 [00:15<00:01, 10.18it/s]
Loading safetensors checkpoint shards:  84% Completed | 61/73 [00:15<00:01, 11.25it/s]
Loading safetensors checkpoint shards:  86% Completed | 63/73 [00:15<00:00, 11.15it/s]
Loading safetensors checkpoint shards:  89% Completed | 65/73 [00:16<00:00, 11.56it/s]
Loading safetensors checkpoint shards:  92% Completed | 67/73 [00:16<00:00, 12.73it/s]
Loading safetensors checkpoint shards:  95% Completed | 69/73 [00:19<00:02,  1.62it/s]
Loading safetensors checkpoint shards:  97% Completed | 71/73 [00:20<00:01,  1.78it/s]
Loading safetensors checkpoint shards: 100% Completed | 73/73 [00:20<00:00,  2.45it/s]
Loading safetensors checkpoint shards: 100% Completed | 73/73 [00:20<00:00,  3.48it/s]

[2025-11-06 11:01:27 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=241.66 GB, mem usage=42.60 GB.
[2025-11-06 11:01:27 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=241.68 GB, mem usage=42.60 GB.
[2025-11-06 11:01:27 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=242.08 GB, mem usage=42.60 GB.
[2025-11-06 11:01:28 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=241.72 GB, mem usage=42.60 GB.
[2025-11-06 11:01:28 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=241.80 GB, mem usage=42.60 GB.
[2025-11-06 11:01:28 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=241.65 GB, mem usage=42.60 GB.
[2025-11-06 11:01:29 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=241.80 GB, mem usage=42.60 GB.
[2025-11-06 11:01:29 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=241.78 GB, mem usage=42.60 GB.
[2025-11-06 11:01:29 TP0] Using KV cache dtype: torch.bfloat16
[2025-11-06 11:01:29 TP7] KV Cache is allocated. #tokens: 2671567, KV size: 174.84 GB
[2025-11-06 11:01:29 TP0] KV Cache is allocated. #tokens: 2671567, KV size: 174.84 GB
[2025-11-06 11:01:29 TP7] Memory pool end. avail mem=66.25 GB
[2025-11-06 11:01:29 TP5] KV Cache is allocated. #tokens: 2671567, KV size: 174.84 GB
[2025-11-06 11:01:29 TP4] KV Cache is allocated. #tokens: 2671567, KV size: 174.84 GB
[2025-11-06 11:01:29 TP0] Memory pool end. avail mem=66.13 GB
[2025-11-06 11:01:29 TP5] Memory pool end. avail mem=66.18 GB
[2025-11-06 11:01:29 TP4] Memory pool end. avail mem=66.25 GB
[2025-11-06 11:01:29 TP3] KV Cache is allocated. #tokens: 2671567, KV size: 174.84 GB
[2025-11-06 11:01:29 TP3] Memory pool end. avail mem=66.11 GB
[2025-11-06 11:01:29 TP1] KV Cache is allocated. #tokens: 2671567, KV size: 174.84 GB
[2025-11-06 11:01:29 TP1] Memory pool end. avail mem=66.53 GB
[2025-11-06 11:01:29 TP2] KV Cache is allocated. #tokens: 2671567, KV size: 174.84 GB
[2025-11-06 11:01:29 TP2] Memory pool end. avail mem=66.11 GB
[2025-11-06 11:01:29 TP6] KV Cache is allocated. #tokens: 2671567, KV size: 174.84 GB
[2025-11-06 11:01:29 TP6] Memory pool end. avail mem=66.24 GB
[2025-11-06 11:01:29 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=66.12 GB
[2025-11-06 11:01:29 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=66.00 GB
[2025-11-06 11:01:29 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=65.98 GB
[2025-11-06 11:01:29 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-11-06 11:01:29 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=66.05 GB
[2025-11-06 11:01:29 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=66.12 GB
[2025-11-06 11:01:29 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=65.98 GB
[2025-11-06 11:01:29 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=66.11 GB
[2025-11-06 11:01:29 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=66.40 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=65.36 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx950//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:33 TP3] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[2025-11-06 11:01:33 TP3] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:33 TP3] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:33 TP3] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx950//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:33 TP5] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[2025-11-06 11:01:33 TP5] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:33 TP5] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:33 TP5] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx950//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:34 TP6] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[2025-11-06 11:01:34 TP6] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:34 TP6] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:34 TP6] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx950//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:35 TP0] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[2025-11-06 11:01:35 TP0] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:35 TP0] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:35 TP0] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx950//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:36 TP1] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[2025-11-06 11:01:36 TP1] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:36 TP1] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:36 TP1] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx950//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:36 TP2] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[2025-11-06 11:01:36 TP2] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:36 TP2] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:36 TP2] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx950//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:37 TP7] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[2025-11-06 11:01:37 TP7] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:37 TP7] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:37 TP7] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx950//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:38 TP4] [fused_moe] using 2stage default for (256, 512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[2025-11-06 11:01:38 TP4] type hints mismatch, override to --> dynamic_per_group_scaled_quant_fp4(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, group_size: int = 32, shuffle_scale: bool = True, num_rows: Optional[torch.Tensor] = None, num_rows_factor: int = 1) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:38 TP4] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-11-06 11:01:38 TP4] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
Capturing batches (bs=512 avail_mem=65.36 GB):   2%|         | 1/52 [00:08<07:10,  8.44s/it]Capturing batches (bs=496 avail_mem=64.64 GB):   2%|         | 1/52 [00:08<07:10,  8.44s/it]Capturing batches (bs=496 avail_mem=64.64 GB):   4%|         | 2/52 [00:08<03:09,  3.80s/it]Capturing batches (bs=480 avail_mem=64.39 GB):   4%|         | 2/52 [00:08<03:09,  3.80s/it]Capturing batches (bs=480 avail_mem=64.39 GB):   6%|         | 3/52 [00:09<01:48,  2.22s/it]Capturing batches (bs=464 avail_mem=64.38 GB):   6%|         | 3/52 [00:09<01:48,  2.22s/it]Capturing batches (bs=464 avail_mem=64.38 GB):   8%|         | 4/52 [00:09<01:12,  1.51s/it]Capturing batches (bs=448 avail_mem=64.38 GB):   8%|         | 4/52 [00:09<01:12,  1.51s/it]Capturing batches (bs=448 avail_mem=64.38 GB):  10%|         | 5/52 [00:10<00:55,  1.18s/it]Capturing batches (bs=432 avail_mem=64.36 GB):  10%|         | 5/52 [00:10<00:55,  1.18s/it]Capturing batches (bs=432 avail_mem=64.36 GB):  12%|        | 6/52 [00:10<00:43,  1.05it/s]Capturing batches (bs=416 avail_mem=64.36 GB):  12%|        | 6/52 [00:10<00:43,  1.05it/s]Capturing batches (bs=416 avail_mem=64.36 GB):  13%|        | 7/52 [00:11<00:33,  1.33it/s]Capturing batches (bs=400 avail_mem=64.35 GB):  13%|        | 7/52 [00:11<00:33,  1.33it/s]Capturing batches (bs=400 avail_mem=64.35 GB):  15%|        | 8/52 [00:11<00:30,  1.46it/s]Capturing batches (bs=384 avail_mem=64.34 GB):  15%|        | 8/52 [00:11<00:30,  1.46it/s]Capturing batches (bs=384 avail_mem=64.34 GB):  17%|        | 9/52 [00:12<00:27,  1.55it/s]Capturing batches (bs=368 avail_mem=64.32 GB):  17%|        | 9/52 [00:12<00:27,  1.55it/s]Capturing batches (bs=368 avail_mem=64.32 GB):  19%|        | 10/52 [00:12<00:22,  1.86it/s]Capturing batches (bs=352 avail_mem=64.31 GB):  19%|        | 10/52 [00:12<00:22,  1.86it/s]Capturing batches (bs=352 avail_mem=64.31 GB):  21%|        | 11/52 [00:13<00:21,  1.89it/s]Capturing batches (bs=336 avail_mem=64.30 GB):  21%|        | 11/52 [00:13<00:21,  1.89it/s]Capturing batches (bs=336 avail_mem=64.30 GB):  23%|       | 12/52 [00:13<00:20,  1.95it/s]Capturing batches (bs=320 avail_mem=64.29 GB):  23%|       | 12/52 [00:13<00:20,  1.95it/s]Capturing batches (bs=320 avail_mem=64.29 GB):  25%|       | 13/52 [00:14<00:19,  2.05it/s]Capturing batches (bs=304 avail_mem=64.29 GB):  25%|       | 13/52 [00:14<00:19,  2.05it/s]Capturing batches (bs=304 avail_mem=64.29 GB):  27%|       | 14/52 [00:14<00:16,  2.27it/s]Capturing batches (bs=288 avail_mem=64.28 GB):  27%|       | 14/52 [00:14<00:16,  2.27it/s]Capturing batches (bs=288 avail_mem=64.28 GB):  29%|       | 15/52 [00:14<00:15,  2.37it/s]Capturing batches (bs=272 avail_mem=64.27 GB):  29%|       | 15/52 [00:14<00:15,  2.37it/s]Capturing batches (bs=272 avail_mem=64.27 GB):  31%|       | 16/52 [00:15<00:15,  2.27it/s]Capturing batches (bs=256 avail_mem=64.26 GB):  31%|       | 16/52 [00:15<00:15,  2.27it/s][aiter] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:45 TP1] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:45 TP0] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:45 TP3] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:45 TP2] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:45 TP5] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:45 TP7] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:45 TP4] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:45 TP6] [fused_moe] using 2stage default for (256, 256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
Capturing batches (bs=256 avail_mem=64.26 GB):  33%|      | 17/52 [00:15<00:17,  2.01it/s]Capturing batches (bs=248 avail_mem=64.23 GB):  33%|      | 17/52 [00:15<00:17,  2.01it/s]Capturing batches (bs=248 avail_mem=64.23 GB):  35%|      | 18/52 [00:16<00:17,  1.99it/s]Capturing batches (bs=240 avail_mem=64.20 GB):  35%|      | 18/52 [00:16<00:17,  1.99it/s]Capturing batches (bs=240 avail_mem=64.20 GB):  37%|      | 19/52 [00:16<00:15,  2.07it/s]Capturing batches (bs=232 avail_mem=64.18 GB):  37%|      | 19/52 [00:16<00:15,  2.07it/s]Capturing batches (bs=232 avail_mem=64.18 GB):  38%|      | 20/52 [00:17<00:15,  2.09it/s]Capturing batches (bs=224 avail_mem=64.17 GB):  38%|      | 20/52 [00:17<00:15,  2.09it/s]Capturing batches (bs=224 avail_mem=64.17 GB):  40%|      | 21/52 [00:17<00:13,  2.28it/s]Capturing batches (bs=216 avail_mem=64.16 GB):  40%|      | 21/52 [00:17<00:13,  2.28it/s]Capturing batches (bs=216 avail_mem=64.16 GB):  42%|     | 22/52 [00:18<00:18,  1.59it/s]Capturing batches (bs=208 avail_mem=64.14 GB):  42%|     | 22/52 [00:18<00:18,  1.59it/s]Capturing batches (bs=208 avail_mem=64.14 GB):  44%|     | 23/52 [00:19<00:16,  1.79it/s]Capturing batches (bs=200 avail_mem=64.13 GB):  44%|     | 23/52 [00:19<00:16,  1.79it/s]Capturing batches (bs=200 avail_mem=64.13 GB):  46%|     | 24/52 [00:19<00:14,  1.91it/s]Capturing batches (bs=192 avail_mem=64.12 GB):  46%|     | 24/52 [00:19<00:14,  1.91it/s]Capturing batches (bs=192 avail_mem=64.12 GB):  48%|     | 25/52 [00:20<00:14,  1.90it/s]Capturing batches (bs=184 avail_mem=64.10 GB):  48%|     | 25/52 [00:20<00:14,  1.90it/s]Capturing batches (bs=184 avail_mem=64.10 GB):  50%|     | 26/52 [00:20<00:13,  1.89it/s]Capturing batches (bs=176 avail_mem=64.08 GB):  50%|     | 26/52 [00:20<00:13,  1.89it/s]Capturing batches (bs=176 avail_mem=64.08 GB):  52%|    | 27/52 [00:21<00:16,  1.53it/s]Capturing batches (bs=168 avail_mem=64.07 GB):  52%|    | 27/52 [00:21<00:16,  1.53it/s]Capturing batches (bs=168 avail_mem=64.07 GB):  54%|    | 28/52 [00:22<00:16,  1.48it/s]Capturing batches (bs=160 avail_mem=64.05 GB):  54%|    | 28/52 [00:22<00:16,  1.48it/s]Capturing batches (bs=160 avail_mem=64.05 GB):  56%|    | 29/52 [00:22<00:15,  1.46it/s]Capturing batches (bs=152 avail_mem=64.04 GB):  56%|    | 29/52 [00:22<00:15,  1.46it/s]Capturing batches (bs=152 avail_mem=64.04 GB):  58%|    | 30/52 [00:23<00:13,  1.65it/s]Capturing batches (bs=144 avail_mem=64.03 GB):  58%|    | 30/52 [00:23<00:13,  1.65it/s]Capturing batches (bs=144 avail_mem=64.03 GB):  60%|    | 31/52 [00:23<00:12,  1.67it/s]Capturing batches (bs=136 avail_mem=64.01 GB):  60%|    | 31/52 [00:23<00:12,  1.67it/s]Capturing batches (bs=136 avail_mem=64.01 GB):  62%|   | 32/52 [00:24<00:11,  1.69it/s]Capturing batches (bs=128 avail_mem=64.00 GB):  62%|   | 32/52 [00:24<00:11,  1.69it/s][aiter] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:54 TP0] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:54 TP3] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:54 TP4] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:54 TP7] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:54 TP5] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:54 TP2] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:54 TP1] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:54 TP6] [fused_moe] using 2stage default for (256, 128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
Capturing batches (bs=128 avail_mem=64.00 GB):  63%|   | 33/52 [00:24<00:09,  1.91it/s]Capturing batches (bs=120 avail_mem=63.97 GB):  63%|   | 33/52 [00:24<00:09,  1.91it/s]Capturing batches (bs=120 avail_mem=63.97 GB):  65%|   | 34/52 [00:25<00:09,  1.82it/s]Capturing batches (bs=112 avail_mem=63.95 GB):  65%|   | 34/52 [00:25<00:09,  1.82it/s]Capturing batches (bs=112 avail_mem=63.95 GB):  67%|   | 35/52 [00:26<00:09,  1.82it/s]Capturing batches (bs=104 avail_mem=63.94 GB):  67%|   | 35/52 [00:26<00:09,  1.82it/s]Capturing batches (bs=104 avail_mem=63.94 GB):  69%|   | 36/52 [00:26<00:07,  2.14it/s]Capturing batches (bs=96 avail_mem=63.92 GB):  69%|   | 36/52 [00:26<00:07,  2.14it/s] Capturing batches (bs=96 avail_mem=63.92 GB):  71%|   | 37/52 [00:26<00:07,  1.94it/s]Capturing batches (bs=88 avail_mem=63.91 GB):  71%|   | 37/52 [00:26<00:07,  1.94it/s]Capturing batches (bs=88 avail_mem=63.91 GB):  73%|  | 38/52 [00:27<00:07,  1.77it/s]Capturing batches (bs=80 avail_mem=63.89 GB):  73%|  | 38/52 [00:27<00:07,  1.77it/s]Capturing batches (bs=80 avail_mem=63.89 GB):  75%|  | 39/52 [00:28<00:06,  1.95it/s]Capturing batches (bs=72 avail_mem=63.88 GB):  75%|  | 39/52 [00:28<00:06,  1.95it/s]Capturing batches (bs=72 avail_mem=63.88 GB):  77%|  | 40/52 [00:28<00:05,  2.26it/s]Capturing batches (bs=64 avail_mem=63.86 GB):  77%|  | 40/52 [00:28<00:05,  2.26it/s][aiter] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:58 TP2] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:58 TP1] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:58 TP5] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:58 TP4] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:58 TP6] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:58 TP7] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:58 TP3] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:01:58 TP0] [fused_moe] using 2stage default for (256, 64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
Capturing batches (bs=64 avail_mem=63.86 GB):  79%|  | 41/52 [00:28<00:05,  2.07it/s]Capturing batches (bs=56 avail_mem=63.83 GB):  79%|  | 41/52 [00:28<00:05,  2.07it/s]Capturing batches (bs=56 avail_mem=63.83 GB):  81%|  | 42/52 [00:29<00:05,  1.92it/s]Capturing batches (bs=48 avail_mem=63.81 GB):  81%|  | 42/52 [00:29<00:05,  1.92it/s]Capturing batches (bs=48 avail_mem=63.81 GB):  83%| | 43/52 [00:30<00:05,  1.64it/s]Capturing batches (bs=40 avail_mem=63.80 GB):  83%| | 43/52 [00:30<00:05,  1.64it/s]Capturing batches (bs=40 avail_mem=63.80 GB):  85%| | 44/52 [00:30<00:04,  1.75it/s]Capturing batches (bs=32 avail_mem=63.78 GB):  85%| | 44/52 [00:30<00:04,  1.75it/s]/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:02:21 TP6] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:02:23 TP4] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:02:25 TP3] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:02:25 TP2] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:02:28 TP1] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:02:28 TP7] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:02:29 TP5] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
[2025-11-06 11:02:30 TP0] [fused_moe] using 2stage default for (256, 32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float4_e2m1fn_x2', 'torch.float4_e2m1fn_x2', 'QuantType.per_1x32', True, False) 
Capturing batches (bs=32 avail_mem=63.78 GB):  87%| | 45/52 [03:26<06:11, 53.14s/it]Capturing batches (bs=24 avail_mem=63.66 GB):  87%| | 45/52 [03:26<06:11, 53.14s/it]ler_TP4: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
#blocked = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_batched_gemm_afp4_wfp4_pre_quant_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg4: i32, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32) attributes {noinline = false} {
    %cst = arith.constant dense<127> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128xbf16, #blocked1>
    %cst_1 = arith.constant dense<2097152> : tensor<4x4x1xi32, #blocked>
    %cst_2 = arith.constant dense<4> : tensor<4x4x16xi8, #blocked2>
    %c31_i32 = arith.constant 31 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x32xf32, #blocked3>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %true = arith.constant true
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #blocked>
    %cst_5 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_7 = arith.constant dense<-2147483648> : tensor<4x4x32xi32, #blocked>
    %cst_8 = arith.constant dense<23> : tensor<4x4x32xi32, #blocked>
    %cst_9 = arith.constant dense<255> : tensor<4x4x32xi32, #blocked>
    %cst_10 = arith.constant dense<8388607> : tensor<4x4x32xi32, #blocked>
    %cst_11 = arith.constant dense<1> : tensor<4x4x32xi32, #blocked>
    %cst_12 = arith.constant dense<127> : tensor<4x4x32xi32, #blocked>
    %cst_13 = arith.constant dense<4194304> : tensor<4x4x32xi32, #blocked>
    %cst_14 = arith.constant dense<126> : tensor<4x4x32xi32, #blocked>
    %cst_15 = arith.constant dense<2> : tensor<4x4x32xi32, #blocked>
    %cst_16 = arith.constant dense<21> : tensor<4x4x32xi32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, startler_TP6: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
 = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>> -> tensor<64x1xi32, #blocked6>
      %45 = arith.extsi %44 : tensor<64x1xi32, #blocked6> to tensor<64x1xi64, #blocked6>
      %46 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>> -> tensor<1x32xi32, #blocked6>
      %47 = tt.splat %arg10 : i32 -> tensor<1x32xi32, #blocked6>
      %48 = arith.muli %46, %47 : tensor<1x32xi32, #blocked6>
      %49 = arith.extsi %48 : tensor<1x32xi32, #blocked6> to tensor<1x32xi64, #blocked6>
      %50 = tt.broadcast %45 : tensor<64x1xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %51 = tt.broadcast %49 : tensor<1x32xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %52 = arith.addi %50, %51 : tensor<64x32xi64, #blocked6>
      %53 = tt.addptr %arg1, %42 : !tt.ptr<i8>, i64
      %54 = arith.extsi %arg14 : i32 to i64
      %55 = arith.muli %7, %54 : i64
      %56 = tt.addptr %arg3, %55 : !tt.ptr<i8>, i64
      %57 = tt.expand_dims %41 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<32x1xi32, #linear1>
      %58 = tt.splat %arg15 : i32 -> tensor<32x1xi32, #linear1>
      %59 = arith.muli %57, %58 : tensor<32x1xi32, #linear1>
      %60 = arith.extsi %59 : tensor<32x1xi32, #linear1> to tensor<32x1xi64, #linear1>
      %61 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>>
      %62 = tt.broadcast %60 : tensor<32x1xi64, #linear1> -> tensor<32x4xi64, #linear1>
      %63 = tt.expand_dims %61 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>> -> tensor<1x4xi32, #linear1>
      %64 = tt.broadcast %63 : tensor<1x4xi32, #linear1> -> tensor<32x4xi32, #linear1>
      %65 = arith.extsi %64 : tensor<32x4xi32, #linear1> to tensor<32x4xi64, #linear1>
      %66 = arith.addi %65, %62 : tensor<32x4xi64, #linear1>
      %67 = tt.splat %56 : !tt.ptr<i8> -> tensor<32x4x!tt.ptr<i8>#, blocked#linear = 1#>ttg
      %68 = .tt.addptrblocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}> 
%#67, blocked%661 :  = tensor<32x4x#!tt.ptr<i8>ttg, #linear1.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>, 
tensor<32#x4blockedx2i = 64#, #ttglinear1.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>>

      %69 = #blockedtt.load3 =  #%68ttg .blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>:
 #blocked4 = tensor<32#ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>x
#blocked5 = 4x#!ttttg.ptr<i8>, #.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>linear1>

      #blocked6 = %70#ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}> = 
#blockedtt.trans7 =  %69# {ttgorder = .blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>array<
#blocked8 = i#32ttg: .blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>1
#blocked, 9 = 0#>ttg} : .blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>tensor<
#32linearx4 = x#i8, #linear1> ttg-> tensor<4x32x.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>i
#8, linear#ttg.slice<{dim = 2, parent = #blocked4}>1 = >
      #ttg%.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>71 = 
tt.splat#linear2 =  #ttg%29 : .linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>!tt
.ptr<bf16> -> #sharedtensor< = 4x#ttg128x.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>!
#smemtt = .#ttgptr<bf16>, .shared_memory#blocked
1>module
      %72 =  attributestt.addptr  {%"71, %28t t: g.tensor<4xnu128mx-ctas!tt.ptr<bf16>, "#blocked = 1>1,  : tensor<i4x32128x, i64, #"ttg.blocked1>
      %73 = nutt.load m-war%ps72" :  = tensor<44x : 128xi!tt32.ptr<bf16>, , ttg.target#blocked1> = 
      "%74 = htt.splat ip%:gfx9553 : 0!tt.ptr<i8> "-> , tensor<64"ttg.threads-per-warpx" = 3264x : !tt.ptr<i8>, i32#blocked6>}
      %75 =  tt.addptr {%
74,   %tt.func52  public:  tensor<@64_batched_gemm_afp4_wfp4_pre_quant_kernelx32x(!tt.ptr<i8>, #blocked6>, %tensor<arg064x32x: i64, #blocked6>
      !tt%76 = .ptr<bf16>tt.load %75 { tt.divisibility = cacheModifier16  : = icg32 : }tensor<, 64%arg1: x32x!tt!tt.ptr<i8>.ptr<i8>, #blocked6> {
      tt.divisibility = %77 = 16ttg.convert_layout :  i%7632 }:,  %tensor<64x32xarg2i: 8, #blocked!tt.ptr<bf16> {6> tt.divisibility = ->16  : tensor<64ix32x32i}8, , #blocked3>%arg3: 
      %78 = !tttt.reshape.ptr<i8>  {%73tt.divisibility  = :16  : tensor<i432}, x%128xarg4: bf16, i32#blocked1> , ->%arg5:  itensor<324 {xtt.divisibility4 = x3216x : ibf16, #blocked32}, >%arg6: 
      %79i = 32math.absf { tt.divisibility = %7816  : :i 32}, %tensor<arg7: 4i32 {xtt.divisibility = 4x32x16bf16 : , #blockedi32>
      %80 = }, %arg8: arith.extfi 32%79 { tt.divisibility = :16  : tensor<i4x4x32x32bf16, #blocked}>,  %toarg9 : tensor<4i32x4x32 {xtt.divisibility = f3216,  : i32}, #blocked%>
      arg10%81 = : "i32 {ttt.divisibility = t.re16duce : i"32}, (%%80arg11: )i32 < {{tt.divisibility = axis = 162 :  : ii3232}}, > (%{
arg12: i32       {^bb0tt.divisibility( = %arg16: 16f32 : i32}, , %%arg13: arg17: f32i)32: {
tt.divisibility         = %16209 =  : arith.maxnumfi 32}, %%arg14: arg16i,32  {%tt.divisibility = arg1716  : :i 32}, %f32
        arg15: tt.reduce.returni32 )% attributes209 { noinline = :false }f32 
{
          }%)cst :  = (arith.constanttensor<4x 4x32dense<x127f32, #blocked>> : ) -> tensor<tensor<44xx44xxf32, 1#ttgx.slice<{dim = 2, parent = #blocked}>i>8
,       #blocked%82 = >ttg.convert_layout %81 : 
tensor<4x4    x%cst_0 = f32, arith.constant#ttg.slice<{dim = 2, parent = #blocked}> >dense< -> 0x7FC0tensor<4x>4 : xf32, tensor<#ttg4.slice<{dim = 2, parent = #linear}>x>128
x      bf16, %83 = #blockedtt.expand_dims1> 
%82     {%axiscst_1 =  = 2arith.constant :  idense<322097152} : >tensor<4 : tensor<x44xf32, x#ttg.slice<{dim = 2, parent = #linear}>4> -> x1tensor<4x4x1xxf32, i32, #linear>#blocked
      %>84 = 
tt.expand_dims     %81% {cst_2 = axis = arith.constant2  : dense<i4>32 : } : tensor<tensor<4x44xxf32, 4x#ttg.slice<{dim = 2, parent = #blocked}>16>x -> tensor<4x4ix1x8f32, , #blocked>
      %85 = #blockedtt.bitcast2> 
    %%c31_i32 = 83arith.constant  : 31tensor<4x4x1 : xif3232, 
#linear>     ->% cst_3 = tensor<4x4x1arith.constantx idense<32, 0.000000e+00#linear>>
      % : 86 = tensor<tt.bitcast %84 : 4tensor<4x4x1xxf32, #blocked>32 x->f32 , #blockedtensor<43>x
4x1    x%c32_i32i = 32, #blocked>arith.constant
      % 87 = 32arith.addi %85, % : cst_28i : 32
tensor<4x4x1x    i32, #linear>%c4_i32 = 
      arith.constant% 88 = 4arith.addi %86,  : %icst_132
     : %true = tensor<4x4x1arith.constantx i32, true
    %c0_i32#blocked = >
      %89 = arith.constanttt.bitcast %87 :  tensor<4x40x1x : i32, #linear> -> tensor<4x4xi132x
    %cst_4 = iarith.constant32,  #dense<linear-8388608>>
       : %90 = tensor<tt.bitcast %88 : 4xtensor<4x4x14x1xix32i, 32#blocked> -> , #blocked>tensor<4x4x1
    xi32%cst_5, #blocked>
       = %91 = arith.constantarith.andi  dense<%892.000000e+00>, :  tensor<%4cst_27x 4:x 1tensor<4x4x1xxi32f32, #linear>, 
      #blocked%>
    %cst_6 = 92 = arith.constantarith.andi  dense<%90, %0.000000e+00>cst_4 :  : tensor<4x4tensor<x1xi324, #blocked>
x      4%93 = x1tt.bitcast %91 : xtensor<4x4x1f32, xi32, ##blocked>linear> -> tensor<4
    x4%xcst_71 = xarith.constantf32 , #dense<linear-2147483648>> : 
tensor<      4x%94 = 4tt.bitcast x%92 : 32tensor<4xx4xi132, xi32, #blocked>#blocked>
 ->     tensor<4x%4cst_8x = 1arith.constantx f32, #blocked>dense<
      %95 = 23math.log2>  : %93tensor< 4x:4x 32tensor<4x4xx1ix32, f32, ##blocked>linear>
      %96 = 
math.log2     %cst_9%94 =  arith.constant:  dense<tensor<4x4x1x255>f32, # : blockedtensor<>4x4x
32      %97 = xmath.floori 32, #blocked>%95
     %cst_10: =  arith.constanttensor<4x 4dense<x83886071>x : f32, #linear>tensor<
      %98 = 4xmath.floor4 %96x : 32tensor<4x4x1xxif32, #blocked>
      %9932, #blocked> = 
    arith.subf% cst_11 = %97arith.constant,  dense<%1cst_26>  : :tensor< 4x4tensor<4x4x1xx32f32, x#ilinear>32, #
blocked>      %100 = 
arith.subf %98, %    cst_5% : cst_12tensor<4x4x1xf32, #blocked> = 
      arith.constant% 101 = dense<tt.clampf127>  : %99tensor<,4x4 x%32cst_25x,i 32, #blocked>%
    cst_24, %propagateNancst_13  = = arith.constantnone  : dense<tensor<4x4x1x4194304f32, #linear>>
      % : 102 = tensor<tt.clampf %100, %4xcst_18, 4x%32cst_23, propagateNan = xnonei 32: , #blocked>tensor<4
    %xcst_144 = x1xarith.constantf32 , #blocked>
      %dense<103 = 126>arith.fptoui :  tensor<%1014x 4:x 32tensor<4xx4x1ix32, f32, #linear>#blocked>
     %tocst_15 =  arith.constanttensor< 4dense<x2>4x1 : xtensor<i8, #linear>4x
      %104 = 4xarith.fptoui %102 :32 xtensor<4x4x1ixf32, #blocked32>,  #blocked>to 
tensor<4x4x1    x%i8, #blocked>
      %105 = cst_16arith.addi %103,  = %arith.constantcst_29  : dense<tensor<21>4x : 4x1tensor<x4ix84, #linear>
x      %32106 = xarith.addi %104, i32, %#blockedcst> :
    %cst_17 =  arith.constanttensor<4x4x1 xdense<i8, #blocked>
28>      % : 107 = tensor<arith.subf %4x4x32cst_6, x%102i : 32, #blocked>tensor<4x4x1
    xf32, #blocked>
      %%108 = cst_18math.exp2 =  arith.constant% 107dense< -1.270000e+02:>  : tensor<4x4x1tensor<x4f32, #blocked>x
4      %109 = xarith.extf %781 : xtensor<f324, x4x32#blockedx>
    bf16, #blocked>% tocst_19 =  arith.constanttensor<4x4x 32dense<xf32, #blocked7>>
      %110 =  : tt.broadcasttensor< 4%108x : 32tensor<4x4x1xxf32, #blocked> -> itensor<4x4x32xf32, #blocked>16
      %, 111#ttg = .slice<{dim = 2, parent = #blocked4}>arith.mulf> 
    %109%,cst_20 =  arith.constant% 110dense< -1>: :  tensor<tensor<4x44x32xxf32, #blocked>32
      %112 = xtt.bitcast i%1118 : , tensor<4x4x32x#ttgf32, #blocked> ->.slice<{dim = 2, parent = #blocked4}> tensor<4x4x32>x
i32, #blocked>    
%      cst_21 = %113 = arith.constantarith.andi  dense<%112,0x7FC0> % : cst_7 : tensor<tensor<4x4128xx3232xxi32, #blocked>bf16
,       %114 = #arith.shruiblocked 5%112>,
     %cst_22 = %arith.constantcst_8  dense<:7>  : tensor<4x4x32tensor<x4i32, #blocked>x4x
      %115 = 32arith.andi %114, %xcst_9 : itensor<4x432, x32#blocked>x
i32    , #blocked>
      %116 = %arith.andi %112, %cst_23 = cst_10 : tensor<4x4x32xi32, #arith.constantblocked> 
dense<      1.270000e+02>%117 =  : arith.addi tensor<4x%115,4 %xcst_111 : tensor<4x4x32xxf32i, 32, #blocked>
#blocked      %118 = >arith.subi
    % cst_24 = %arith.constantcst_12 ,dense< %1.270000e+02117>  : :tensor< 4xtensor<4x44x32xx1i32, x#f32blocked, >#linear>
    
      %%119 = cst_25arith.cmpi  = ultarith.constant,  dense<%-1.270000e+02>115 : ,tensor< %4x4cst_12x : 1tensor<4x4x32xxf32i32, #, blocked>#
      linear%120 = >arith.shrui %116, %cst_11 : 
    tensor<4x4x32x%i32, #blocked>
      cst_26 = %121 = arith.constantarith.ori  dense<%1202.000000e+00>, :  tensor<%cst_134x 4x:1 xtensor<4x4x32xi32, #blocked>
      %122 = f32arith.shrui , %121, %118 :# tensor<4x4xlinear>32
    xi32, #blocked>
%      cst_27% = 123arith.constant =  arith.selectdense< -8388608>% : 119tensor<, 4x4%122x, 1%116x : itensor<4x4x3232x, i1, #blocked>#, lineartensor<4x4x32>xi32, #blocked>
    
      %%124 = cst_28 = arith.maxuiarith.constant  %115dense<,2097152 >% : cst_14tensor< 4:x 4tensor<4xx14x32xxii32, 32, #blocked>#linear>
      
%125 =     arith.subi %%124cst_29 = , %cst_14arith.constant  :dense< tensor<4127x4x32x>i32, #blocked> : 
      tensor<%126 = 4xarith.shli4x 1%125x, %icst_158, #linear> 
:    % cst_30 = tensor<4xarith.constant4x32 xdense<i7>32, # : blocked>
      tensor<%4x127 = 4arith.shrui x%123, %icst_16 : tensor<4x4x3216, x#i32, #blocked>
      %128ttg = .slice<{dim = 2, parent = #blocked}>arith.ori >
    %126,%cst_31 =  %127 : arith.constanttensor< 4x4x32dense<xi32, #blocked>-1>
      %129 =  : arith.addi %tensor<1284, %cst_11x4 : xtensor<i4x48, x#ttg32.slice<{dim = 2, parent = #blocked}>xi32, #blocked>
      %130 = >arith.shrui 
%129, %    cst_11 : llvm.intr.assumetensor<4x4x32 xi32, #blocked>%true
       %:131  = iarith.minui1 
%130    ,llvm.intr.assume  %%cst_22true  ::  tensor<4iler_TP5: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
x4x321xi32, #blocked>

      %132 =     arith.shruillvm.intr.assume  %113, %%truecst_17  : : itensor<4x41x32
    xllvm.intr.assume i32, #%blocked>true
      %133 =  : arith.ori i%132, 1%131 : 
    tensor<4x4xllvm.intr.assume 32%truexi32, #blocked>
      %134 =  arith.trunci:  %133i 1:
     tensor<4x4x32llvm.intr.assume x%i32, #blocked>true  : toi 1tensor<
    4x4xllvm.intr.assume32 xi8, #blocked%>true
       : %135i = 1tt.reshape
     %134llvm.intr.assume  : %truetensor<4x :4x32 xi8, #iblocked1>
     -> llvm.intr.assumetensor<4x4x16 x%true2 :xi8, #blocked 7i>1

          %outLHSllvm.intr.assume , %true% outRHS: =  tt.spliti 1%135
     :llvm.intr.assume  tensor<%true4 :x4x16x 2ixi8, #blocked17
    > -> llvm.intr.assume tensor<4x4x%true16 : xi8, #blockedi2>1

          %136 = %arith.shli 0% = outRHStt.get_program_id,  %cst_2x :  :tensor<4x4x16 #xiblockedi8, #blocked32 = 2
#>
          ttg%%.137 = 1blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>arith.ori  = 
%outLHS, tt.get_program_id #blocked%136y1   = : : #tensor<4x4x16ittgxi32.8
blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>,     
#blocked2>
      %138 = %#blockedtt.reshape %13722 =  :  = #tensor<4x4xarith.addittg16 .blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>x%
i8arg5#blocked, #blocked2> ->,3   = tensor<4x%#64c31_i32ttgxi8, # .blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>blocked8>:
#blocked
 4 =       i#%139 = 32ttgtt.reshape 
.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>%    
#blocked105%35 =   = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>: arith.divsi
#blocked6 = tensor< #4%2ttgx4x1,.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>xi8, #linear 
#blocked7 = >%#ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}> -> c32_i32
tensor< #4:blockedx 8 = 4i#x32ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>i

8    #blocked9, %4 = # = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>ttgarith.extsi
. #linearslice<{dim = 2, parent = #blocked}>% = >arg7#
 ttg      :.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>%140 =  
tt.reshapei# %10632linear1 =   #:tottg  .linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>tensor<4x4x1xi
i8, #blocked> -> tensor<4x64#linear4
2x     = i%#8, #linear2>5ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
 = 
      arith.extsi#%141 =  sharedttg.convert_layout % = %arg9#ttg140 .swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}> :
#smem:  =  i32#ttgtensor<4x4 to .xi8, #lineari64shared_memory2>

 ->     moduletensor<% attributes4x46 =  {xarith.extsi"i8,  t#%tg.nttgarg11um-ct. asslice<{dim = 2, parent = #blocked}>:">  = 
i1      32 : %142 =  toiarith.extui 32 i, %64"141
t     tg.:%num-war 7 = pstensor<4x4arith.extsi" = x 4i% : 8, 0i#ttg.slice<{dim = 2, parent = #blocked}> : 32>i,  32ttg.targetto to =   "tensor<4x4xihi64i16, 
p#    :ttg.slice<{dim = 2, parent = #blocked}>%gfx950>8"
      % = , 143 = arith.divsi"arith.shli t %t%1g142,., t %h%3rcst_30 ea :d: s-per-warp i32
    "tensor<4x4%9 =  = xarith.remsi64i16,   : #ttg.slice<{dim = 2, parent = #blocked}>>
      %i%132144 = ,}tt.bitcast   %3{% 
143:   :  tt.functensor<4i x432publicxi16, 
 #    @ttgllvm.intr.assume_batched_gemm_afp4_wfp4_pre_quant_kernel. (slice<{dim = 2, parent = #blocked}>%%> ->truearg0  : tensor<:!4 ttxi.41ptr<bf16>x
 {bf16,     tt.divisibility = #ttg.slice<{dim = 2, parent = #blocked}>llvm.intr.assume16>  : 
%i      true32%145 =  :}tt.expand_dims  , %i%1441arg1 {
: axis    ! = llvm.intr.assumett.ptr<i8>2  { : %tt.divisibility = itrue1632  : }:i32 :  }tensor<i, %4x4x1arg2: bf16, 
    !tt.ptr<bf16> {#%tt.divisibilityttg.slice<{dim = 2, parent = #blocked}>> -> 10 = tensor< = 164x4x1arith.cmpi : ixbf16,  32#sgt}blocked,, > %
%arg3      arg6: %146 = ,!tt.tt.broadcast ptr<i8> % {%145c0_i32tt.divisibility :   = tensor<4x4x1:16x  : bf16, ii32}#blocked32
, > ->     %tensor<4x4x32scf.ifarg4: xbf16,  i32#blocked%, >
      %147 = 10%tt.reshape % arg5146{
:  :       i32tensor<4x4x32% {x11 = tt.divisibility = bf16arith.muli16,   : i#blocked%32}, >8% ,arg6-> :  %i32tensor<4xc4_i32 {128 tt.divisibilityxbf16, #blocked: = 1 16>i : 
      %148 = 32
iamdgpu.scaled_upcast_fp4      32 %12 = }, %arg7: %138tt.make_rangei  {32 {scaleend = tt.divisibility =  416%147 :  : i {i32axis = 32}1, , % : startarg8: i = i32} 032: :  { i32tt.divisibility = tensor<4x64}16x  : i32}i8, #blocked8>:, %arg9: , i tensor<32tensor<4 {4xxtt.divisibility = 128i16x32 : bf16, i, #32}#ttg, blocked.slice<{dim = 1, parent = #blocked1}>%1>arg10>
: i32 {       tt.divisibility = ->%16 13 =  : tensor<4x128tt.make_rangeix {32}, bf16, #blocked1>end = %
      %4arg11149 =  : : arith.cmpiii 3232eq,  {,starttt.divisibility  =  = %016139 :  : ,ii 3232%}}cst_31 ,  :%:  arg12tensor<tensor<: 44i32 {x4xtt.divisibilityxi = i32168,  : , #i#ttg32ttg.slice<{dim = 1, parent = #blocked3}>}, .slice<{dim = 2, parent = #blocked}>>%arg13: >
      
i32 {%150 =       tt.divisibility = tt.expand_dims %16%14 : i149 = 32}, %arg14: i32 { {tt.splattt.divisibilityaxis  =  = %16211 :  :  ii:3232 }}i,  32%: arg15: i32 ->)tensor<  attributes4x4xtensor< {i1, 4noinline#x = ttg.slice<{dim = 2, parent = #blocked}>ifalse>32} ->,   #{tensor<4x4x1ttg
x.    islice<{dim = 1, parent = #blocked1}>%cst1, #blocked> = >
arith.constant
             %dense<%15127151 =  = >tt.broadcastarith.addi  :  %tensor<%15014,4  x: tensor<4x4x1%124x x1i1, #blocked:x> -> tensor< i4x4xtensor<8324xi32, , xi1, #blocked>
      %152 = ##blockedtt.reshape ttg>%.
151slice<{dim = 1, parent = #blocked1}>     : >%cst_0 = tensor<4x4x32
arith.constantx       i1, #blocked%dense<>16 = 0x7FC0 -> tt.splat>tensor<  : 4x%tensor<128arg44x :xi 1281ix, #blocked32 bf16, 1>
      -> #%tensor<blocked15341> = x
arith.selecti     32, %%152#cst_1, ttg = %.arith.constantcst_0slice<{dim = 1, parent = #blocked1}> , %148 : >dense<tensor<
2097152>4       : x%17tensor<128 = 4xarith.remsixi 4x1%15,1, #blocked1 x>%i, 1632tensor<4x128 , x:#bf16 blocked, tensor<>#4
blockedx    1i%cst_2>
      32,  = %154 = #arith.constantttg.local_allocttg  .dense<%153slice<{dim = 1, parent = #blocked1}>4> > : tensor<4x4:
x       16(%xtensor<4x12818 = i8, #xarith.muliblockedbf16, #blocked1> 2>)%
     -> 7%!,c31_i32 = ttg arith.constant.% memdesc<4x128xbf16, #shared, #smem>431
      %  : 155 = :i32
ttg.local_load      i64%%154
cst_3 =        arith.constant:%  19dense<! = 0.000000e+00ttgtt.expand_dims>.  : memdesc<4x128xbf16, #shared, #smem>%tensor< 174x-> {32 axisxtensor<4x128 = f32x1, #blocked3>
    %c32_i32 = bf16 : arith.constant, i #ttg3232.} : dot_op<{opIdx = 0, parent = #blocked3}> i32
    >:%
      %156 =  c4_i32 = arith.extui tensor<arith.constant%70 :4x  i32, 4tensor<# : 4x32ttg.ixslice<{dim = 1, parent = #blocked1}>32i>
    %8 true = , ->arith.constant#ttg  .tensor<trueslice<{dim = 2, parent = #blocked4}>4
    > x%to1c0_i32 =  xarith.constanttensor<i 4x32, 032#blocked : x1i32
i16, >    #
%ttg      cst_4.%20 =  = slice<{dim = 2, parent = #blocked4}>tt.splatarith.constant>  
      %157 = %dense<arith.shliarg8-8388608 %156, % >cst_19: :  : tensor<4x32 tensor<4x4x1xixi16, 32i# 32ttg->, . #slice<{dim = 2, parent = #blocked4}>tensor<blocked>
      4>%158 = x
    tt.bitcast %157 : 1%tensor<xcst_54xi = 3232arith.constantx,  i16, #dense<#2.000000e+00blockedttg.slice<{dim = 2, parent = #blocked4}>>1>> ->  : 
tensor<4x32xtensor<4x4x      bf161%21, x = #f32arith.mulittg.slice<{dim = 2, parent = #blocked4}>, #blocked >>%
      %159 = 
    19, tt.expand_dims %cst_6 = %20%arith.constant 158 : {dense< axis = 0.000000e+00>tensor<42 : x : i32} : tensor<1tensor<4x324xxxibf16, 4x132#ttg.slice<{dim = 2, parent = #blocked4}>x, > -> f32, ##tensor<4x32x1blockedblockedx>1bf16, 
    %cst_7 = >#arith.constant
blocked4>       
      dense<%22 = %160 = -2147483648>arith.extsitt.broadcast :   tensor<%%421159x  4:: x tensor<4x32x132tensor<xx4bf16, ix#321xblocked4, #blocked>i32, > 
#->    blocked1> %cst_8 =  to tensor<4x32x32arith.constanttensor<xbf16,  4#dense<xblocked4>231x
      %161 = >itt.trans : 64 tensor<, %1604x4x# {32blockedorder = x1>array<i32, #blocked>
i
      32:     %0, %232cst_9 =  = , arith.constanttt.make_range1  {>} : dense<endtensor<255 = 4x32x>12832 :  : xbf16, #blocked4>tensor<i -> 432tensor<4x32x32x, x4start = bf16x0, #blocked32 : 9xi>
i32, 32      #blocked>}%
 : 162 =     %cst_10 = tensor<tt.reshape arith.constant128%161 x : dense<itensor<8388607324x32>,  : #xtensor<ttg324.xxslice<{dim = 0, parent = #blocked1}>bf164>, x
#blocked9>32       x%->i24 32 = tensor<, tt.expand_dims128# x32xbf16blocked%, >23#
 {blocked5>    axis
% =       cst_110% =  : 163 = arith.constantiamdgpu.scaled_upcast_fp4 32 dense<} : %1tensor<77 >128scale : tensor<x %1624i {x4x32axis32,  = x#0 : i32ttgi, .32#slice<{dim = 0, parent = #blocked1}>}blocked> : > tensor<
->64x32     x%tensor<icst_1218, #blocked3>,  = xtensor<arith.constant128128 xxdense<i32, 32127>#x : tensor<blockedbf1641, #blocked5> -> x>tensor<4
128x      x32xbf16, #blocked5>
      %164 = 32%arith.cmpix25 i = eq32arith.extsi, ,  %70#%24 : ,blockedtensor<1 >x%
    128cst_20%x : cst_13 = itensor<arith.constant32, 4x32 #blockedxdense<1> to i8, 4194304>tensor<# : 1ttg.slice<{dim = 2, parent = #blocked4}>tensor<x>4128
xx      4i%165 = x64, #blockedtt.expand_dims 321>
%164x       {axis = i32, #blocked>%2
26 =  : i    tt.broadcast32% } : cst_14 = %tensor<arith.constant224x32  xdense<: i1126>tensor<,  : tensor<4x4x4#ttg.slice<{dim = 2, parent = #blocked4}>> -> 32xtensor<4x32xx11ixx32ii, 64, 1, #blocked4>#blocked>#
      %166 = 
    blockedtt.broadcast%1 cst_15>% =  165arith.constant->   : dense<tensor<4tensor<4x2>x32x1x : 128i1, #blockedtensor<x44i>x64 4, #blocked-> x1tensor<4x32x32x32>ix
1, #blocked4>i      
32%27      ,  = %#tt.broadcast167 = blocked tt.trans>% 
    %cst_16 = 25%166arith.constant :  { tensor<orderdense<1 = 21>xarray< : tensor<4x4128ixx32: 32i0x64, #blocked, i32, #blocked>12
>,      1%cst_17 = ->>arith.constant } tensor< : dense<4tensor<28>x4x32x : 12832tensor<xx4x4ii1, #blockedx64432, > -> x#tensor<iblocked4x32x3232, 1x#>
i1, #blocked      blocked9>
      >%%
28168 =      = tt.reshape%arith.addi cst_18 =  %arith.constant%167 26, dense< :-1.270000e+02% >27tensor< :  4x32x32tensor<:x4 ixtensor<1, #blocked9> -> tensor<44128xxx321128xxi1, #blocked5>
xi64      f32, %, #blocked>#blocked1>
169
           = %%arith.selectcst_1929  =  = %168, %arith.constanttt.addptrcst_21  , %163 : dense<%tensor<7arg0128x>,32 :  xi1, #blocked5>, tensor<128x32xtensor<%bf16, #418blockedx 5>
32:      x %i!17016tt = , .ttg.local_alloc#ptr<bf16> ttg,%. 169slice<{dim = 2, parent = #blocked4}>i64 >
:
           %%30(cst_20 =  = tensor<arith.constantarith.muli 128x32 %xdense<9bf16, -1>,# :  blockedtensor<%5>)4c32_i32 -> x !ttg.memdesc<128x32xbf16, #shared, #smem>32: 
      %171 = xittg.local_load i32%8
170,        :#% ttg31!.slice<{dim = 2, parent = #blocked4}> = ttg>tt.make_range.memdesc<128x32xbf16, #shared, #smem>
     { %cst_21end->  =  = tensor<arith.constant32128x  : 32dense<ix0x7FC0>32bf16,  : , #ttgtensor<start.128 = dot_op<{opIdx = 1, parent = #blocked3}>x0>
      32 : %172 = xitt.dotbf16, 32}  #:%155blocked ,5tensor< >32%171,
    x %i%cst_22 = 32cst_3arith.constant,  : # dense<ttgtensor<4x1287>.x : tensor<slice<{dim = 0, parent = #blocked6}>bf16, 4>#ttgx4
.x      dot_op<{opIdx = 0, parent = #blocked3}>32%> x32* i = tensor<32tt.make_range128,  {x32#endxblocked = bf16>32, 
     : #ttg%i.dot_op<{opIdx = 1, parent = #blocked3}>cst_23 = 32, >arith.constantstart   = ->dense<0 1.270000e+02 : tensor<>i4x32x : 32f32tensor<}, 4 :#blocked3>x 
      %173 = 4tensor<arith.addfx32 1x%172xi32, ,f32# , ttg%#.slice<{dim = 0, parent = #blocked3}>cst_3blocked> >
:
           %tensor<%334cst_24 = x32x = tt.make_rangef32arith.constant {,  end#dense< = blocked3>1.270000e+02>32
 :  :       tensor<i%432174 = x4x, arith.truncf1start =  x0%173f32, #linear> :  
i:    32 %} tensor<4x32cst_25:x =  f32, arith.constanttensor<#blocked3> 32x dense<ito-1.270000e+0232,  >#tensor<4x32x : ttgbf16, #blocked3tensor<.>4x4x1slice<{dim = 1, parent = #linear1}>
      %175 = x>arith.extsif32, 
 #      %13linear% >34:
 =      tt.splat tensor<4x%cst_26%i = 30 32arith.constant:,   #ttg.slice<{dim = 1, parent = #blocked3}>dense<i> to2.000000e+00>32  :  tensor<tensor<-> 44xtensor<x4x32i64, 1x#ttg.slice<{dim = 1, parent = #blocked3}>xf32, #linear>
i>    32
%,       cst_27#% = ttg176arith.constant. =  slice<{dim = 0, parent = #blocked6}>arith.extsidense<> -8388608>
%11 :        : tensor<%i4x4x13532 to x = i64itt.splat
      %17732  = , #linear>%tt.splat
30      :%176 : % i64 -> cst_28 = itensor<4xarith.constant32i64,   ->#ttg.slice<{dim = 1, parent = #blocked3}>dense< >
      %178 = 2097152tensor<arith.addi>32  : x%177, tensor<i%175432 : x4, tensor<4xx#i1ttg64, x.slice<{dim = 1, parent = #linear1}>>#ttg.slice<{dim = 1, parent = #blocked3}>i
>32      
      , %%179 = #36arith.extsilinear =  >arith.addi%
 32 : tensor<    %32x%cst_29 = 34, i32, arith.constant%# 31ttgdense< : .slice<{dim = 0, parent = #blocked3}>127>tensor<> : 32x to tensor<itensor<43232x, x4x1#i64, xttg#i.ttg8slice<{dim = 0, parent = #blocked6}>.slice<{dim = 0, parent = #blocked3}>>, >
      %180 = #linear>
arith.extsi 
          %30 :%% cst_30 = 37i32 to arith.constant = i64 arith.addi 
      dense<%%181 = 7>35tt.splat %180 :  : ,i64 ->tensor<  4x4%tensor<x3332i x16:i,  64, #ttgtensor<#.32xttg.slice<{dim = 0, parent = #blocked3}>slice<{dim = 2, parent = #blocked}>i>>32, 

#ttg.slice<{dim = 1, parent = #linear1}>          >%182 = %
arith.addi cst_31 =       %181, arith.constant%%179 38 =  : dense<tt.splat tensor<-1%32x>arg5i64,  :  #ttg.slice<{dim = 0, parent = #blocked3}>>
      %183 = tensor<:arith.muli4  x4i%7, xi32%68  , ->: #ttg i.tensor<64slice<{dim = 2, parent = #blocked}>32
      >x%
    i184llvm.intr.assume32,  =  #ttgtt.addptr%.slice<{dim = 0, parent = #blocked6}> true>% 
arg2:      , % i39%1 = 183
tt.splat      :llvm.intr.assume % %truearg5!tt  .ptr<bf16>: :,i  1i32i64
      
 %185 =     ->tt.expand_dims %178llvm.intr.assume   {%true : tensor<axisi32 = 1x1
i :     32illvm.intr.assume, 32} :  #ttgtensor<4xi64, #ttg..slice<{dim = 1, parent = #blocked3}>slice<{dim = 1, parent = #linear1}>>>% 
true->        tensor<%:440 x = i1arith.remsi1x 
    i%36,llvm.intr.assume 64,  %#%trueblocked3>38 
      % :186:  =  i1arith.extsi tensor<
    %32llvm.intr.assume arg13x% : itruei32 to i64
      32,  %187#: = ttg i1tt.expand_dims.
     slice<{dim = 0, parent = #blocked6}>llvm.intr.assume %175>% {
trueaxis       :  = %i1411 :  = 
    i32} : arith.remsi llvm.intr.assume tensor<4x%%truei6437 , , : i#ttg.slice<{dim = 1, parent = #blocked3}>%1>39
      llvm.intr.assume ->:%true   : tensor<4x1xi64, tensor<i#321blockedx
3>i    
32llvm.intr.assume      %188 = ,  %truearith.muli %186, %176#  ttg::.  slice<{dim = 1, parent = #linear1}>ii64
      %189 = >
1tt.splat %186 :       
    i%llvm.intr.assume6442 %true ->  =  : tensor<arith.mulii4 1x%
17    x,llvm.intr.assumei64,   #blocked3>%%true : 
5i1      %190 =  :
    arith.muli %189,  %%187i0 64 = :
tt.get_program_id        tensor<4x1%xx43 i64,  = :#tt.make_range blocked {i3>
      %191 = end32tt.addptr  = 
%184, %188 64    : : % i1 = !tt.32tt.get_program_idptr<bf16>,  , start = yi0 64 : :
i       %192 = 32i32tt.expand_dims}
  :     %182tensor<%2 =  {64xarith.addiaxisi  = 32%0, arg5 : #ttg,i. 32} : slice<{dim = 1, parent = #blocked6}>%tensor<>c31_i3232
 x      :i% 64, 44i# = 32ttg.slice<{dim = 0, parent = #blocked3}>> -> tt.expand_dims
    tensor< %1%3x43 = 32xi64,  {arith.divsi#blocked3>
      axis %193 =  = %2tt.broadcast %1,190 :   : i32%tensor<4x1x}c32_i32i64,   #blocked3> -> : :tensor<tensor< 464ixx3232i
x32    i64, #blocked3>, %4
# =       ttgarith.extsi%194 = .slice<{dim = 1, parent = #blocked6}> tt.expand_dims > %%179-> arg7 {tensor< axis = 64:0x  : 1i32i32} : x tensor<32xi64, ito#32 ttg, i.#64
    %5 = slice<{dim = 0, parent = #blocked3}>blocked6>arith.extsi> -> 
 tensor<1      %x32x%45 = arg9 iarith.extsi:64  , %i32 to#44 blocked3> : i
      tensor<64%64
    %6195x =  = 1xarith.extsi tt.broadcasti% 32, arg11 :%194#blocked  6i32 to :>i  64tensor<to
1     x32xtensor<%i64647, x = #1xarith.extsiblockedi 3> -> 64, %0 : tensor<#i4blocked32x6 32>to xi64, #blocked3>
      
i64
%196 =           tt.addptr %%8 = %191, %180 : 46 = arith.divsi !tt.expand_dims%tt 1.%,ptr<bf16>40 ,  {%iaxis364 =  :
0        : i%i32197 = 32
arith.addi }    % %9 = 195, %193:arith.remsi :   tensor<4tensor<%1x32xi64, #blocked3>
32,      %198 = x arith.extsi i%3%32,  arg4#: ttg : .slice<{dim = 0, parent = #blocked6}>i32i>
32      ->llvm.intr.assumeto   itensor<%641true
x       32x: %199 = iitt.splat321 , 
    %198 :#blockedllvm.intr.assume % i64 -> 6truetensor<> 4
:x       1x%i1i64, #blocked3>
      %200 = 47
arith.cmpi =      tt.splatllvm.intr.assumeslt, %185, %199  % %true:arg10 :   : itensor<4i1x1xi64, #blocked3>
      32 -> 
    %201 = tensor<%arith.extsi 110 = %xarith.cmpiarg5 :32xi  32, sgti32 to i#,64blocked 
      %202 = 6>%tt.splat 
arg6%      ,201%  : 48%i = c0_i3264arith.muli  ->  %46, %:tensor<1x32xi64, #blocked3>
      47 %203 : i32 = tensor<
    arith.cmpi 1scf.ifslt, x %192, 32%10%202x  i{
:32       , %tensor<#111x32xi64, #blocked3>
      blocked = %6arith.muli204 = >
 tt.broadcast      % %8%20049 = , arith.extsi :  %tensor<4%c4_i32x48 1 :x: i i32
      %12 = 1, #blocked3> -> tensor<tt.make_rangetensor<1 {4xend = x32x32x4ii : 1, #blocked3>
      %20532i32 = , , tt.broadcast #start%blocked = 2036> 0 : to  : tensor<1xtensor<1i3232x}x32x ii:164 , , tensor<##4blocked3> -> blockedxtensor<46i32, x>#32x
      ttg.slice<{dim = 1, parent = #blocked1}>i%>150
      , #blocked3>
       = %13%206 = tt.broadcast  = arith.andi%tt.make_range 45 {%204, %205 :  : endtensor<4tensor< = x64432xix : 1, #blocked3>1i32, 
xstart      i = %640207 = ,  : tt.splat #blockedi32} : %6tensor<196>4  x: ->i32, !tt #ttg.slice<{dim = 1, parent = #blocked3}>.ptr<bf16>tensor<> -> 64
tensor<x      432%14 = x32xxtt.splat!tt.ptr<bf16>i , 64%#, 11blocked# 3blocked:>6 
      %208 = >i32tt.addptr 
 %      ->207% , %197 : 51tensor<tensor<4x32x = 4!tttt.broadcastxi32. , ptr<bf16>, #blocked3>, %49#ttgtensor<4 .slice<{dim = 1, parent = #blocked1}>x32:>x 
itensor<      64, #blocked3>1%
x15      32x = tt.storeiarith.addi  64, #%14%blocked, 2086%, > ->12%  174tensor<:, 64 %206xtensor<4x 32i:x32,  i#tensor<4x32x64, ttg!#.tt.ptr<bf16>, #blockedblocked6slice<{dim = 1, parent = #blocked1}>3>>>
      %16 = 

tt.splat           }%%
    52arg4 : tt.return = i32 ->
arith.addi    tensor<}%4
50x},i32, 
 #
%ttg.slice<{dim = 1, parent = #blocked1}>{-#51>
 
      %17 =   :arith.remsi %15, %16 external :_resources: {tensor< 
64tensor<    x4xi32, mlir_reproducer32xi#ttg.slice<{dim = 1, parent = #blocked1}>: {64>
, 
      #blocked6      %18 = pipeline>arith.muli: 
 "      %7,b% u53%i = 4ltt.addptr : ti i64n%
.marg1,      %od 19ul% = e(optim42 : tt.expand_dimsi! ze-amd-lds-usage{lds-litt%17m. {iptr<i8>axist=0 target-arch=, = gfx950}, trit 1oi : n-scf-to-cf, con64iv
32e      }r% t54:-index-t =  o-llvarith.extsi tensor<4m{index-b%xitwarg14ii 32, d: #ttg.tislice<{dim = 1, parent = #blocked1}>h=0},32>   allocate-amdgpu-shto->ared  -memory, itensor<convert-644t
xri      1xton-amdgpu-t%io-llvm{a5532r = , carith.muli#h=gfx blocked9%157>0 ftz=true}, canoni, 
      c%%ali5420 = ze{  max- : tt.splatii ter64%ations=10 
arg8 max-      :n% um-re56i32 -> writes=-1 region-simplif = tensor<4x1xi32ytt.addptr, = #n%blockedoarg31r,>m 
      %21 = a%arith.muli l55%19, % te 20s: t :-converge!tt nce=false top-down.ptr<i8>,tensor<4= xtrue}, cse, i1convert-cf-to-llvm64x{
i32, ind      #e%blockedx571>-bi = 
twidth=0},tt.expand_dims       con %vert-arith-to-ll%22vm41 = { {arith.extsiindex-axis =  b1%i : 21 :twidth=0}, i tensor<4c32xa} :1n xotensor<i32, n32#ixblockedcalize{ i1 max32>-,  i#to terationsttgtensor<=10 max-num-r.slice<{dim = 1, parent = #linear1}>4ew>xrites= 1-1->x  i64, #blocked1>
      %23 = rtensor<tt.make_rangeegion-simplify=32 {normaxend = l test-co1x128nvergence=fai : l32i32se , , t#startolinear = p1>0-down
 : i32}=       t%:r58 u = tensor<e},tt.splat128 cse, symbol- xd%icarg1532e , ,:# enable-l ttgii.slice<{dim = 0, parent = #blocked1}>ne-inf32>o 
, conve->      r %ttensor<24-32 = bxtt.expand_dims u1%ix23li32, #linear {t1axisi> = n
0-       : i32func-to-llv%} : m59tensor<{ = 128xfarith.muliit 32, z%#ttg.slice<{dim = 0, parent = #blocked1}>=true})57> ",-> , tensor<
%1      58xdisable_threading 128xi: :32, false #,tensor<blocked1>
32
      %25 =       xarith.extsiverify_each1 : truex%24 : tensor<1x
    }i32, 128x
  }#i
linear32#-}1>, #blocked1> to 

tensor<      1%60 = xarith.extsi 128x%i64, #blocked59/tmp/torchinductor_root/ms/cms52sse2shmzp2fqvzvsd5vrrsdaqqga5xo6xhkisdsdkrc5lnn.py:18:01> : : 
      tensor<error: %32Failures have been detected while processing an MLIR pass pipeline26x
 = 1/tmp/torchinductor_root/ms/cms52sse2shmzp2fqvzvsd5vrrsdaqqga5xo6xhkisdsdkrc5lnn.py:18:0tt.broadcastx:  inote: %2232Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
 :, #linear1 >tensor<4 tox1x i64, #blocked1>tensor< -> 32tensor<4x1xxi64, 128x#linear1>i
      64, #blocked1>%
      61% = 27 = tt.make_rangett.broadcast  {%end = 25 : 4tensor<1 : ix128xi64, #32, blocked1> -> start = tensor<04 : xi128x32} : itensor<64, 4#xblocked1>i
      32%28 = , arith.addi #ttg.slice<{dim = 0, parent = #linear1}>%26, %>27
 :       tensor<%462x = 128xtt.broadcasti 64, #blocked1>%
      60% : 29tensor<32 = xtt.addptr1x i64%, arg0#linear1> -> ,tensor< 32%18x 4x:i 64, !tt.ptr<bf16>,#linear1>
       i64%63 = 
      tt.expand_dims%30  = %61arith.muli { axis = %09 : , i32%} c32_i32:  tensor<:4 xi32i
32,       %31 = #ttgtt.make_range.slice<{dim = 0, parent = #linear1}> {> -> end = tensor<321 : x4xii32, #linear132, >start = 
0       : %i3264}  = : tt.broadcasttensor< 32x%i3263,  #ttg:.slice<{dim = 0, parent = #blocked6}> >tensor<
      1%x32 = 4xtt.make_rangei32,  {#linearend = 1> ->32  : i32tensor<, 32start = x04x : ii32, 32} : #lineartensor<1>32x
i      [rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     a_ptr,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_ptr,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     c_ptr,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_scales_ptr,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     M,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     N,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     K,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_am,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ak,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bk,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bn,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ck,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cm,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cn,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsb,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsn,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsk,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Meta-parameters
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_M: tl.constexpr,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_N: tl.constexpr,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_K: tl.constexpr,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GROUP_SIZE_M: tl.constexpr,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     NUM_KSPLIT: tl.constexpr,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     EVEN_K: tl.constexpr,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GRID_MN: tl.constexpr,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     cache_modifier: tl.constexpr,
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] ):
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """Kernel for computing the matmul C = A x B.
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A_scales and B_scales are in e8m0 format.
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ab > 0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_am > 0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ak > 0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bb > 0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bk > 0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bn > 0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cb > 0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cm > 0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cn > 0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsb > 0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsk > 0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsn > 0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # -----------------------------------------------------------
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Map program ids `pid` to the block of C it should compute.
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # This is done in a grouped ordering to promote L2 data reuse.
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.program_id(axis=0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_unified = tl.program_id(axis=1)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_k = pid_unified % NUM_KSPLIT
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid = pid_unified // NUM_KSPLIT
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if NUM_KSPLIT == 1:
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         remap_xcd(pid, GRID_MN)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     else:
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m = pid // num_pid_n
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_n = pid % num_pid_n
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_batch >= 0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_m >= 0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_n >= 0)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # We assume 32 elements along K share the same scale.
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for first block of A and B input matrices
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         a_ptrs = a_ptr + (
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_ab
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_am[:, None] * stride_am
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split_bf16[None, :] * stride_ak
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_ptrs = b_ptr + (
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_bb
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split[:, None] * stride_bk
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[None, :] * stride_bn
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for the first block of A and B scales
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # B scales are N x K even though B operand is K x N.
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_scale_ptrs = (
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales_ptr
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_bsb
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[:, None] * stride_bsn
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_ks[None, :] * stride_bsk
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales = tl.load(b_scale_ptrs)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # If it is out of bounds, set it to 0.
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             if EVEN_K:
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(a_ptrs)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             else:
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Advance the ptrs to the next K block.
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c = accumulator.to(c_ptr.type.element_ty)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Write back the block of the output matrix C with masks.
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_ptrs = (
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             c_ptr
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_cb
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cm * offs_cm[:, None]
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cn * offs_cn[None, :]
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_k * stride_ck
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         tl.store(c_ptrs, c, mask=c_mask)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 4, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 96, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Traceback (most recent call last):
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     next_module = compile_ir(module, metadata)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pm.run(mod)
[rank4]:E1106 11:04:58.374000 212 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] RuntimeError: PassManager::run failed
32, %#ttg65.slice<{dim = 0, parent = #blocked3}> = >arith.extsi 
      %%33 = 64tt.make_range  {: end = tensor<3232 : ix32, 4xstart = i032,  : #i32linear} 1> to : tensor<tensor<3232xxi32, 4x#ttg.slice<{dim = 1, parent = #linear1}>i>64, #linear1>

            %%6634 =  = arith.additt.splat  %65%30,  :%62  :i32  -> tensor<32tensor<x32x4ix32, i#ttg64, #linear1>.slice<{dim = 0, parent = #blocked6}>
>      
%      67 = %35tt.splat  = %56tt.splat :  !tt%.30 : ptr<i8> -> itensor<32 ->32 xtensor<4x32xi32, !tt#ttg..slice<{dim = 1, parent = #linear1}>ptr<i8>, #linear1>>
      
%      %6836 =  = arith.addi %tt.addptr34, %31  : %67, tensor<%32x66i 32:,  #tensor<ttg.slice<{dim = 0, parent = #blocked6}>32>
      %37 = x4arith.addix !tt.ptr<i8>, %#linear35, %133> : , tensor<tensor<32x32i32, x4x#ttg.slice<{dim = 1, parent = #linear1}>i>
      %6438, #linear = 1tt.splat >%
arg5 :       i32 -> %tensor<69 = 32xtt.loadi 32, %68#ttg .slice<{dim = 0, parent = #blocked6}>:> 
tensor<      32%39 = xtt.splat4 x%!tt.ptr<i8>, #linear1arg5> : 
i      32 ->%70  = tensor<tt.trans32xi32,  #ttg.slice<{dim = 1, parent = #linear1}>%>69
       {%order = 40 = array<arith.remsi i32%36, %: 381 :,  0tensor<>32x}i32 : , tensor<#32ttg.slice<{dim = 0, parent = #blocked6}>x>4
      %41 = xarith.remsi i%8, #linear1>37 ->,  tensor<%439x :32x itensor<8, 32xi#ttg32, .slice<{dim = 2, parent = #blocked4}>#ttg>.slice<{dim = 1, parent = #linear1}>
>      
%71       = %tt.splat 42 = %29 :arith.muli  !tt.%ptr<bf16>7,  -> %5tensor< 4: xi64128x
!      tt%43 = .ptr<bf16>, #blocked1>tt.make_range
 {      end = %6472 =  : itt.addptr32,  start = %071 : i,32} :  %28tensor< :64x i32, tensor<#ttg.slice<{dim = 1, parent = #blocked6}>4>x
128      x%44 = !tt.ptr<bf16>tt.expand_dims, # blocked%431>, { axis = tensor<14 : xi32128x} : i64tensor<, #blocked64x1>i32, 
#ttg.slice<{dim = 1, parent = #blocked6}>      > -> %73 = tensor<tt.load 64%x721 x:i 32, tensor<4#blocked6>x
128x      !tt%45.ptr<bf16>, #blocked1> = 
arith.extsi       %74 = %44tt.splat :  tensor<64%x531x :i32,  #blocked6> to !tt.ptr<i8> ->tensor<64x 1xi64, tensor<#blocked6>64
      x%46 = 32tt.expand_dims x%!tt.40ptr<i8> {, axis = #blocked06> : i
32      }% :75 =  tt.addptr tensor<%7432xi32, , #%52ttg : .slice<{dim = 0, parent = #blocked6}>tensor<> ->64x 32tensor<x1!xtt32xi32, .ptr<i8>#blocked6>, 
      #%blocked6>,47  = tensor<tt.splat64 x%32xarg10i64,  :#blocked 6i>32 
->       tensor<%1x76 = 32xtt.loadi32,  #%blocked75 cacheModifier 6=> 
cg       %48 = :arith.muli  tensor<%46,64 x%4732x : !tensor<tt1.xptr<i8>32x, i32, ##blocked6>blocked6>

      %      49 = %arith.extsi 77% = 48 : ttg.convert_layouttensor< 1%76x 32xi:32 , #blocked6>tensor< to 64tensor<x321xxi32xi64, #blocked6>8
      , %#50 = blockedtt.broadcast6>  %->45  tensor<: 64tensor<x6432xxi8, 1x#blocked3>i64, #blocked6>
 ->       %tensor<7864 = x32xtt.reshapei64, # blocked%736>
       %:51 =  tt.broadcasttensor< 4%49 : xtensor<1281xxbf16, 32x#blockedi1>64,  #blocked6>-> ->  tensor<tensor<464xx324xx32xibf16, #blocked64>, #blocked6
>      
%79 =       math.absf%52 =  arith.addi%78  %:50, %51  : tensor<tensor<464xx324xxi32x64, #blocked6>bf16
      , %53#blocked = >tt.addptr
       %%80 = arg1, arith.extf%42  %: 79!tt .ptr<i8>:, i64 
      tensor<4%54 = xarith.extsi 4%x32arg14 x: bf16, #blockedi>32 to  ito64 
      %55 = tensor<4arith.muli x%47x, 32%54x : f32i64, #blocked
      >%
56 =       tt.addptr %81 = %"arg3t, t.r%55educ : e!"tt(.ptr<i8>, %80i)64 <
{      axis% = 572 =  : tt.expand_dims i%3241} {>axis ( = {1
 : i32      } : ^bb0tensor<(32x%iarg1632, : #ttg.slice<{dim = 1, parent = #linear1}>f32>,  -> %tensor<arg17: 32f32x)1x:i32, 
#linear1>        
      %%58 = 209 = tt.splatarith.maxnumf  %%arg15arg16 :,  i32 ->% arg17tensor<32x 1xi32:, #linear1>
       %59 = f32
        arith.muli %tt.reduce.return57 , %%58209 :  tensor<:32 xf321x
i      32, #linear1>}
      )% : 60 = (arith.extsi tensor<%459 :x 4xtensor<3232xxf321, x#i32, #linearblocked1>>) ->  to tensor<tensor<432xx41xxi64, #linear1>f32, 
#      ttg%.slice<{dim = 2, parent = #blocked}>61 = >tt.make_range
 {      end = %482 :  = i32, ttg.convert_layoutstart =  0% : 81i : 32}tensor< : 4xtensor<44xi32, x#ttg.slice<{dim = 0, parent = #linear1}>f32, >#
      ttg%.slice<{dim = 2, parent = #blocked}>62> =  tt.broadcast ->% 60tensor< : 4tensor<x324x1xix64, #linear1>f32,  -> #ttgtensor<.slice<{dim = 2, parent = #linear}>32>
x      4xi%83 = 64tt.expand_dims , %82#linear {1>
      axis = %263 =  : tt.expand_dims i%6132 {}axis =  : 0tensor< : 4xi324} x: f32tensor<, 4x#ttgi32, .slice<{dim = 2, parent = #linear}>#ttg.slice<{dim = 0, parent = #linear1}>>>  ->->  tensor<4xtensor<4x11x4xxf32, i32#linear>, 
#linear1>      
%84 =       tt.expand_dims%64 =  tt.broadcast%81  {%axis = 632 :  : tensor<i132x4x} i:32, # linear1> ->tensor<4x 4tensor<x32f32x, 4#xttgi32, #linear1>.slice<{dim = 2, parent = #blocked}>
      >%65 =  arith.extsi-> %64 :  tensor<tensor<432xx4x4x1i32, #linear1> to xtensor<f3232, x4xi#blocked>64, #linear1>
      
%66 =       arith.addi%85  = %65, tt.bitcast% 62% :83  tensor<:32x 4xi64tensor<, 4#linear1>x4
      %67 = x1tt.splat x%56f32,  #linear>:  ->! tttensor<4x4x.ptr<i8> -> 1tensor<x32i32, x4x#linear>!tt
.ptr<i8>, #linear1>      
%86 =       tt.bitcast%68 =  tt.addptr %84% :67,  %66tensor< :4x4x 1tensor<x32f32x, 4x#blocked> !tt-> .ptr<i8>, #tensor<linear4x1>4,x 1tensor<32xxi4x32, i#blocked>64, 
#linear      1>%
      87%69 =  = arith.additt.load  %85%, 68% cst_28: :  tensor<tensor<432xx4x4x1!xtti.ptr<i8>, #linear32, 1#linear>
>      
      %%70 = 88tt.trans =  arith.addi %69% {86, order = %array<cst_1i 32: : tensor<14x4, x10x>i}32 :,  #tensor<blocked>32
x      4x%i8, #linear891> =  ->tt.bitcast  %87tensor< 4: xtensor<32x4x4i8, x1#ttgx.slice<{dim = 2, parent = #blocked4}>i>32
      %71 = , tt.splat#linear>  -> %tensor<294 :x 4!ttx1.xptr<bf16>i32,  -> #lineartensor<>4x
128x      !tt%90.ptr<bf16> = , #blocked1>tt.bitcast 
      %88% :72  = tensor<tt.addptr 4x4x1%x71,i32,  #blocked> -> %28tensor<4x4x1 : xtensor<i432, x#blocked>128x
!tt.ptr<bf16>, #blocked      1%91> = , arith.anditensor< 4%x89128x,i64 , %#blocked1>cst_27
       %73 = :tt.load  %72tensor<4x4 :x 1tensor<x4ix32128x, !tt.#linearptr<bf16>, #blocked1>>

            %%9274 =  = tt.splatarith.andi  %90%, 53 %: cst_4!tt.ptr<i8> ->  tensor<: 64tensor<x4x4x321xx!tt.ptr<i8>i32, , #blocked6>#blocked
>      
%75       = %tt.addptr 93 = %74, %52tt.bitcast  : %91tensor<64 : x32xtensor<!tt.4x4ptr<i8>x, 1#blocked6>x, i32, tensor<#linear>64 -> xtensor<32x4xi64, #blocked46x>1
x      %76f32 = , tt.load#linear> 
%75       %94 = cacheModifiertt.bitcast  %92= :  cgtensor< 4x:4 xtensor<164xxi32, 32x#blocked!tt.ptr<i8>, >#blocked6> ->
       %tensor<774 = xttg.convert_layout4x1 x%76f32,  #blocked:> 
tensor<      64x32x%95 = imath.log28 , #blocked6>% 93->  :tensor< 64tensor<4xx4x132xxf32i, 8#linear>
, #blocked3>      
      %78 = %tt.reshape96  = %73math.log2  %94: :  tensor<tensor<44xx4128xxbf16, 1#blockedx1> f32, -># blocked>tensor<
4      x%974 = x32xmath.floorbf16, #blocked >
%95       %:79 =  math.absftensor< 4x%784 x1:x f32tensor<, 4#linear>x
4      x32%x98bf16,  = #blockedmath.floor >
      %%9680  = :arith.extf  tensor<4x%479x 1:x f32, tensor<#4blockedx>4x32
x      bf16, #blocked%99> =  arith.subfto  %tensor<4x974x,32 x%f32, #blockedcst_26> 
      %:81  = tensor<4x"4xt1t.reducex"f32, #linear>(
%      80%)100 < = {arith.subfaxis =  2%98 : i32, }%>cst_5 ( {
:       ^bb0(tensor<%4xarg164x1: xf32f32, , #blocked>%
arg17:       f32%)101 = :tt.clampf
         %99%,209  = %arith.maxnumfcst_25 ,% arg16%,cst_24 ,% arg17propagateNan  :=  f32none
         tt.reduce.return:  %209tensor<4x 4x1:x f32, f32#linear>

            }%)102 :  = (tt.clampftensor< 4x%4x32100, x%f32, #blocked>cst_18) -> ,tensor< 4x%4cst_23x,f32,  propagateNan = #ttgnone.slice<{dim = 2, parent = #blocked}> >:
       tensor<%482 = x4ttg.convert_layoutx 1%xf32, 81#blocked> 
:       %tensor<103 = 4xarith.fptoui4 x%f32101,  #ttg:.slice<{dim = 2, parent = #blocked}> >tensor<4x -> 4x1tensor<x4x4f32x, f32, #linear#ttg>.slice<{dim = 2, parent = #linear}> >to
      %83  = tensor<tt.expand_dims 4x%824 {x1axis = x2i : i328},  : #tensor<4xlinear4>x
f32      , %104 = #ttgarith.fptoui .slice<{dim = 2, parent = #linear}>%>102 ->  : tensor<tensor<4x44x4x1x1xxf32f32, , #linear>#
blocked> to      %84 =  tt.expand_dimstensor<4 x4%81x {1axisx = i28,  : #blocked>i
32} :       tensor<4x4%x105 = f32arith.addi , %#103,ttg.slice<{dim = 2, parent = #blocked}> >% cst_29-> :  tensor<tensor<4x44xx4x11xxf32, i#blocked8, >
      #linear>
%85 =       tt.bitcast% 106% = 83arith.addi  %104:,  tensor<%4x4cstx1 :x f32, tensor<#4linear> x->4 xtensor<41x4x1xxii328, , #linear>#blocked>
      
%      86 = %107tt.bitcast =  %84arith.subf  : %tensor<cst_6, 4x%4102x1 x:f32,  #tensor<blocked>4x 4x->1 xtensor<f32, 4#x4blockedx1>x
i32, #blocked>      
      %%87 = 108 = arith.addi math.exp2% 85, %107% cst_28:  : tensor<4xtensor<44x4x1x1xxi32f32, , #linear>#blocked>
      
%      88% = 109arith.addi =  arith.extf %%8678,  :% cst_1tensor<4x : 4x32tensor<x4xbf164, #blocked> tox 1tensor<x4x4xi32, #blocked>
      %8932 = xtt.bitcastf32, #blocked> 
%87       :% tensor<1104 = x4xtt.broadcast1 x%i10832,  :#linear> ->  tensor<4x4tensor<4xx14xxf321, x#blocked> ->i 32, tensor<4x#linear>4x
      32%xf32, 90 = #blocked>
tt.bitcast       %%111 = 88arith.mulf  :% 109tensor<,4x4x1 x%i11032 , #:blocked> ->  tensor<tensor<4x44x1x4xxi3232, #xblockedf32>, #blocked>
      
%91 =       arith.andi%112 =  tt.bitcast %%111 89: ,tensor< 4x4%xcst_2732 x:f32 , #blocked> ->tensor< 4x4xtensor<14xx4ix32, #linear>32
      x%i9232 = , #blockedarith.andi >%90,
       %%cst_4113 =  arith.andi:  %tensor<4x1124, x%1cst_7 x: i32tensor<4, #blocked>x4
      %93 = xtt.bitcast32 x%i9132,  :#blocked> 
      tensor<%4x4114 = xarith.shrui1x i%32, 112#linear>,  -> %tensor<cst_84x4x 1:x f32tensor<4, #linearx>4x
32      x%i32, 94#blocked> = 
tt.bitcast       %92 : %tensor<115 = 4x4arith.andi x1%x114, i%32, cst_9 : #blocked>tensor<4x ->4x 32tensor<x4xi4x132, x#blocked>
f32      , %#blocked>116 = 
arith.andi       %95 = %math.log2112 , %%93cst_10  ::  tensor<tensor<4x44xx14xxf32, #linear>32
      %96 = xmath.log2 i32, #blocked%94> 
:       tensor<%1174x4 = x1arith.addi x%115f32, #blocked>
      %97 = ,math.floor  %%cst_1195  ::  tensor<tensor<44x4xx432xx1i32, x#blocked>
f32      , #linear>%
      118% = 98arith.subi =  math.floor% cst_12%,96  %:117  tensor<:4 x4tensor<x4x41xx32f32x, #blockedi>
      %99 = 32arith.subf,  #%blocked>97
,       %%cst_26119  = :arith.cmpi  tensor<ult4x4x1,x f32, #linear>%115,
       %%cst_12100  = :arith.subf  tensor<4x%498, x%32cst_5x i32, : #blocked>tensor<
4      x%4x1120x = f32, #blocked>
      arith.shrui %%116101,  = %tt.clampfcst_11  %: 99,tensor< 4x%4xcst_2532,xi32, #blocked> 
%      cst_24%,121 =  arith.oripropagateNan  %=120 ,none  %:cst_13  tensor<:4x 4tensor<x14xx4f32, x#linear>32
      %102 = xtt.clampfi 32, %100, #blocked%>cst_18, 
%      cst_23, %propagateNan122 = =  arith.shrui none% 121, :% 118 :tensor< 4x4x1tensor<x4xf32, #blocked>4x
      32%x103 = iarith.fptoui32 , %#blocked>101
       :% 123 = tensor<arith.select4x 4%x1119x, f32, #linear>% 122to,  %tensor<1164x4 : x1tensor<x4xi48, #linear>x
32      x%i1041, # = blocked>arith.fptoui , %tensor<1024x4x :32 tensor<4x4xx1i32, #blocked>x
f32,       #%124 = blocked>arith.maxui  to% 115tensor<,4x4x 1%xcst_14i 8, #blocked>:
       %105 = tensor<arith.addi4x 4%x103, 32%xcst_29i32 , #blocked>: 
tensor<4x4      x%1125x = iarith.subi 8, #linear>%124
      ,% 106% = cst_14arith.addi  %104:,  tensor<%4x4cstx 32: xtensor<i32, #4x4xblocked1>x
i      8, %126 = #blocked>
      arith.shli%107  = %arith.subf 125%,cst_6, % 102% cst_15:  :tensor< 4xtensor<44x4xx321xxi32, f32, #blocked#blocked>>

            %108%127 =  = arith.shrui math.exp2% 123, %%107cst_16  : :tensor<4x 4xtensor<324xxi32, 4x1#blocked>
x      f32, #blocked>%128
 =       arith.ori% 109%126 = , arith.extf %%12778 :  :tensor< 4x4xtensor<324x4xx32i32x, bf16, #blocked> to#blocked> 
      tensor<%4x4x129 = 32arith.addi xf32%, 128, #%blockedcst_11> : 
tensor<      4%x4110 = xtt.broadcast 32%x108i32 :, #blocked>
       tensor<%4130x = 4x1arith.shrui x%f32, #blocked> -> 129, %tensor<cst_114x4x : 32tensor<xf32, #blocked>4x
      4x%32111 = xarith.mulfi 32, %109#blocked>,
       %%110131 =  arith.minui:  %tensor<1304,x 4x%32cst_22x f32, #blocked>:
      %112  = tensor<4tt.bitcastx 4%111x : 32tensor<x4x4xi32, 32#blockedxf32, #blocked> >-> 
tensor<4x4      x%32132 = xarith.shruii 32, #blocked>
      %113, %113 = %arith.andi cst_17%112, % : cst_7tensor< 4:x4x tensor<4x4x3232xix32, #blocked>
i      32, #blocked%>133
       = %114 = arith.ori %arith.shrui132,  %%112131 : ,tensor<4x4x 32%xcst_8i32, #blocked>
       :%134  = tensor<4x4x32arith.truncix i%32133,  #:blocked> 
tensor<      4%115x4x = 32arith.andi x%i11432, #blocked>,  to% cst_9 tensor<:4x 4xtensor<324x4xx32ix8i, #32blocked>, 
#blocked>
      %116       = %arith.andi %112, 135 = %tt.reshape cst_10% : 134tensor< 4:x 4xtensor<324xxi324x, #blocked>
      32%x117i = 8arith.addi,  %115,#blocked> ->  tensor<%4xcst_114x : 16xtensor<4x24xxi328, #blockedx7>i
32, #blocked      >%
      %outLHS118,  = %arith.subioutRHS  = %tt.splitcst_12 ,% 135% : 117tensor< 4x:4 xtensor<16x24x4xx32ix8, i#32, #blockedblocked>
      7>% ->119  = tensor<arith.cmpi4x4 xult, 16%115, x%icst_128,  : #blockedtensor<4x4x2>32
x      i%32, #blocked>
136       = %120 = arith.shliarith.shrui  %%116outRHS,,  %%cst_11cst_2 :  tensor<: 4x4xtensor<432xxi32, #blocked>
      4x%16121 = xarith.orii 8, %#blocked1202>,
       %%cst_13137  = :arith.ori  %tensor<outLHS,4x4 x%32136xi32, # blocked>:
       %tensor<122 = 4xarith.shrui 4x%16121, %118x : tensor<i8, #blocked4x4x2>
32      xi%32138, # = blockedtt.reshape >
      %137% :123 =  arith.selecttensor< 4x%4119x, 16%x122i, 8, %#blocked1162> ->  : tensor<tensor<4x4x644xx32ixi1, #8blocked, #>blocked, 8>tensor<
4x4x      32%x139 = itt.reshape32 , %105#blocked> 
      %124 = :arith.maxui  tensor<%1154,x4x 1%xcst_14i 8, :#linear >tensor<4x4x 32->x i32, #blocked>tensor<
      4x4%x125i = 8arith.subi , %124,#ttg %.slice<{dim = 2, parent = #blocked}>cst_14> : 
tensor<      4x%1404x = 32tt.reshapex i32, #blocked>%
      106%126  = :arith.shli  tensor<%4x1254x,1 x%i8, #blockedcst_15>  :->  tensor<tensor<4x4x44xxi328x, #lineari2>32
, #blocked>      
      %%141127 =  = ttg.convert_layout arith.shrui %%123140,  :%cst_16 :  tensor<4tensor<x44x4x32xxii832, #, blocked>#linear2
> ->       tensor<%4x4128 = xarith.ori i%8, 126#ttg,. slice<{dim = 2, parent = #blocked}>%>127
       : tensor<%4142x4 = xarith.extui32 x%i14132, # blocked>
      :% 129tensor< = 4xarith.addi %128, 4%xcst_11i : 8, tensor<4x4x#32ttg.slice<{dim = 2, parent = #blocked}>x>i 32, #blocked>to
       tensor<%4130 = x4arith.shrui x%129, %icst_1116,  #: ttgtensor<.slice<{dim = 2, parent = #blocked}>4>x4x
32      %x143i = 32, #arith.shli blocked%>142,
       %131% = cst_30arith.minui  : %tensor<1304x,4 x%icst_2216,  #ttg:. slice<{dim = 2, parent = #blocked}>tensor<>4x4x
      32%xi144 = 32tt.bitcast, #blocked> 
      %132%143 =  : arith.shrui %tensor<113,4x4 %xcst_17 i: 16, tensor<#ttg4.slice<{dim = 2, parent = #blocked}>x> ->4 xtensor<432xx4i32, #blocked>x
      bf16%, 133 = #arith.ori ttg.%slice<{dim = 2, parent = #blocked}>132, %131 :> 
tensor<      4x4%x14532 = xtt.expand_dimsi 32, %#blocked144> {
      axis% = 1342 =  : arith.truncii 32}% 133:  tensor<:4x 4tensor<x4x4xbf1632, x#ttgi32, #blocked>.slice<{dim = 2, parent = #blocked}> > -> totensor< 4tensor<x4x14xx4bf16x32, x#blockedi>8, #blocked>

            %%146 = 135tt.broadcast =  %tt.reshape 145% : 134tensor< : 4x4x1tensor<4xx4bf16x, 32#xblockedi> -> 8, #tensor<blocked>4x 4x->32 xtensor<bf16, 4x4x#16x2blockedxi>8
,       #blocked%7>147
 =       tt.reshape %%outLHS146,  %:outRHS  = tensor<tt.split4 x%4x13532 : xtensor<bf16, 4x#4x16blockedx2>x ->i 8, #blockedtensor<7>4x 128->x bf16, tensor<#blocked1>4x4
x      16%148x = i8, amdgpu.scaled_upcast_fp4#blocked 2>%138
       %scale136 =  arith.shli %%147outRHS, { axis = %1cst_2 :  : itensor<32}4 x4x:16 xtensor<i8, #blocked4x2>
      64%x137 = iarith.ori 8, #blocked%8outLHS>,,  %136 : tensor<tensor<4x4x1284xxbf16, 16#blocked1>x i8->,  #blockedtensor<24x>128
      %138 = xtt.reshapebf16,  #blocked1%>137
 :       tensor<%1494x4 = xarith.cmpi16 xieq,8, #blocked2>  %139,->  %tensor<cst_314x :64 xtensor<i84x, #blocked48x>i
8,       #ttg.slice<{dim = 2, parent = #blocked}>%>
139       = %tt.reshape150  = %tt.expand_dims 105% 149: { axis = tensor<4x24x1 : xii32} :8, # lineartensor<> -> 4xtensor<44xx4ix1i, 8, #ttg.#ttgslice<{dim = 2, parent = #blocked}>.slice<{dim = 2, parent = #blocked}>>> ->
       %tensor<1404x = 4tt.reshape x%1061 :x itensor<14x, #blocked4>x1
x      i%8, 151 = #blockedtt.broadcast >%150  :->  tensor<tensor<4x4x44x1xxi8, #ilinear2>1
      , %#blocked141> ->  = tensor<ttg.convert_layout4x 4%140 : xtensor<324xx4i1, xi#blocked8, >
#linear2> ->       %tensor<4x4152 = xtt.reshapei 8%151,  : #ttg.tensor<slice<{dim = 2, parent = #blocked}>4>x4
      x%32142x = i1, #blockedarith.extui>  ->% 141tensor<4x 128:x i1, #blocked1>
tensor<      4x4%153x = i8, arith.select#ttg.slice<{dim = 2, parent = #blocked}> >% 152, to% cst_0tensor<, 4x4%148 : xtensor<i416, x#ttg.slice<{dim = 2, parent = #blocked}>128>x
i      1, #blocked1>%, 143 = tensor<arith.shli4x 128%142, x%bf16cst_30, #blocked1 :>
       tensor<%4154 = x4ttg.local_allocx i%15316,  #:ttg .slice<{dim = 2, parent = #blocked}>(>tensor<
      4%x144128 = xtt.bitcastbf16, #blocked1> )% -> 143 : !tensor<4x4ttgx.imemdesc<4x128xbf16, #shared, #smem>16, 
      #ttg%.slice<{dim = 2, parent = #blocked}>155> ->  = tensor<4x4ttg.local_loadx bf16%154,  #:ttg .!ttgslice<{dim = 2, parent = #blocked}>.memdesc<4x128xbf16, #shared, #smem>>
      %145 =  tt.expand_dims ->% 144tensor<4 {xaxis = 1282x : ibf16, 32#}ttg : .dot_op<{opIdx = 0, parent = #blocked3}>tensor<>4x4
x      bf16%, 156# = ttgarith.extui.slice<{dim = 2, parent = #blocked}> > -> %tensor<704x4 :x 1tensor<x4bf16, x#blocked32>x
      i%146 = 8, tt.broadcast #%ttg145.slice<{dim = 2, parent = #blocked4}> >: to  tensor<4tensor<4xx4x321xxibf1616, , #ttg#.slice<{dim = 2, parent = #blocked4}>blocked>> -> 
tensor<4x4x      32x%157 = bf16, arith.shli#blocked >%
      156, %%147 = cst_19tt.reshape  :% 146tensor< 4:x32 xtensor<i16, 4#x4xttg32xbf16, .#slice<{dim = 2, parent = #blocked4}>blocked>>
 ->       tensor<%158 = 4tt.bitcast x%128157x bf16, #blocked: 1>
      tensor<%1484x = 32amdgpu.scaled_upcast_fp4x i16%, 138# ttgscale.slice<{dim = 2, parent = #blocked4}> > %147->  {tensor<4x32axis = x1bf16,  : #i32ttg}.slice<{dim = 2, parent = #blocked4}> >:
       tensor<4x%64159x = itt.expand_dims 8, #blocked8>%, 158tensor< {4xaxis = 128x2bf16,  : #blocked1>i 32->}  : tensor<tensor<4x4x12832xxbf16, #blocked1>bf16
      , %#ttg149. = slice<{dim = 2, parent = #blocked4}>arith.cmpi > ->eq,  %139, tensor<%4cst_31x32 : xtensor<4x14xbf16, x#blocked4i>8, 
#ttg.slice<{dim = 2, parent = #blocked}>      >
%      160 = %150 = tt.broadcast tt.expand_dims% 159% : 149tensor<4x {32axis = x21 : xibf1632} :,  #blockedtensor<4> ->4x 4tensor<x4ix321, x#ttg32.slice<{dim = 2, parent = #blocked}>x>bf16 -> , tensor<#blocked44x4x1>
x      i1, #blocked%161 = >tt.trans
       %%151 = 160tt.broadcast { order = %array<150i32 : : tensor<4x0, 42x1, x1i1, #blocked>> -> }tensor< 4x: 4x32tensor<x4xi1, 32x32#xblockedbf16, >
      #blocked4> -> %tensor<1524x = 32tt.reshapex 32%x151bf16 , : #tensor<blocked4x4x329>xi
1, #blocked      >% -> 162 = tensor<4xtt.reshape 128%x161i :1, #blocked 1>
tensor<      4x%32153 = xarith.select32 x%bf16, 152#blocked, 9> %-> cst_0tensor<, 128%x32148 : xtensor<bf16, #blocked5>
4      x%128163 = xamdgpu.scaled_upcast_fp4i 1, #blocked1>, %tensor<4x77 scale128 x%bf16, #blocked1>
162       {%axis154 =  = ttg.local_alloc0  : %i32153} :  :tensor< 64(xtensor<324xx128ix8, #blockedbf163>,,  #blocked1>tensor<)128x32 -> x!ttgbf16, .#blockedmemdesc<4x128xbf16, #shared, #smem>5>
       %-> 155tensor< = 128xttg.local_load32 x%154bf16, #blocked5> 
:       %164!ttg = .memdesc<4x128xbf16, #shared, #smem>arith.cmpi  eq,->  %tensor<4x70,128 %xcst_20bf16,  :#ttg .dot_op<{opIdx = 0, parent = #blocked3}>tensor<>4
x32      x%i1568,  = #ttgarith.extui. slice<{dim = 2, parent = #blocked4}>%>70 
: tensor<      4x%32165 = xtt.expand_dims i8, %#ttg164. {slice<{dim = 2, parent = #blocked4}>axis = >2 to : i32} :  tensor<tensor<4x432xx32ix16i, 1, #ttg.slice<{dim = 2, parent = #blocked4}>#ttg>.
slice<{dim = 2, parent = #blocked4}>      >% -> 157 = tensor<arith.shli4x32 x%1156, x%icst_191,  : #tensor<4xblocked4>32
x      i%16616,  = #ttg.slice<{dim = 2, parent = #blocked4}>tt.broadcast>
       %%158 = 165tt.bitcast % 157:  :tensor< 4xtensor<32x4x132xxii116, , #blocked4> -> #ttgtensor<.slice<{dim = 2, parent = #blocked4}>4x32>x ->32 xtensor<i4x1, 32#blockedx4>
bf16,       #%ttg.slice<{dim = 2, parent = #blocked4}>167> = 
      tt.trans% 159% = 166tt.expand_dims  {%order = 158array< {iaxis32 = : 20 : , i232} : , tensor<4x321x>bf16} , : #ttgtensor<.slice<{dim = 2, parent = #blocked4}>4x>32x32 ->x itensor<14x32x, #1blockedx4> ->bf16,  #blockedtensor<4>4x
      32x32%160 = xtt.broadcasti1, # blocked9>%
159       %: 168tensor< = 4tt.reshapex32x1 x%167bf16,  : #blocked4>tensor< 4x-> 32xtensor<4x32x3232xxibf16, 1#, blocked#blocked49> -> >tensor<
      128%161x = 32tt.transx i1%, #blocked1605>
 {      order = %169 = array<arith.selecti 32%: 1680, , 2%, cst_21, 1%>163 : } :tensor< 128tensor<4x32x32xx32i1xbf16, #blocked, 4> ->#blocked 5>tensor<4x, 32xtensor<32128xbf16, #blocked9>
x      32%x162 = bf16tt.reshape,  #blocked5%>161
 :       %170tensor<4 = xttg.local_alloc32 x32%xbf16, 169# blocked: 9>( ->tensor< 128xtensor<32128xx32bf16, xbf16, #blocked#blocked5>)5> -> 
      !%ttg163 = .amdgpu.scaled_upcast_fp4 memdesc<128x32xbf16, #shared, #smem>%
77       scale% 171% = 162ttg.local_load { axis% = 1700 :  : !ttgi32}.memdesc<128x32xbf16, #shared, #smem> : ->  tensor<tensor<128x64x3232xxbf16, i#8, #ttgblocked.dot_op<{opIdx = 1, parent = #blocked3}>3>, >tensor<
128      x%17232 = xtt.dotbf16, #blocked5>  ->%155 ,tensor< 128x%32xbf16171, #blocked5>,
       %164 = %arith.cmpi cst_3eq :,  %tensor<70,4x 128%xcst_20bf16,  #ttg:.dot_op<{opIdx = 0, parent = #blocked3}> >tensor< 4x32* xtensor<i128x328, x#ttgbf16.slice<{dim = 2, parent = #blocked4}>, >#ttg
.dot_op<{opIdx = 1, parent = #blocked3}>      >%165 =  tt.expand_dims->  %tensor<1644 {x32axisx = f322, #blocked : 3>i
32      }% 173: =  arith.addftensor< 4x32%x172i,1,  #ttg.slice<{dim = 2, parent = #blocked4}>%> cst_3->  tensor<:4x32x 1tensor<x4x32ix1, f32, #blocked4>#blocked
3>      
%      166 = %tt.broadcast174  = %arith.truncf165  %: 173tensor< 4x32:x 1tensor<x4xi321, #blocked4> ->x f32tensor<, 4x#blocked3>32x 32tox itensor<14, x#32blocked4>x
      bf16%, 167 = #blocked3>tt.trans
       ler_TP1: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
%166% {175order =  = array<arith.extsii32:  0%, 13 : 2tensor<, 41x>i}32,  :# ttgtensor<.slice<{dim = 1, parent = #blocked3}>4x> to32x 32tensor<4xxi1, #blockedi4>64,  -># ttgtensor<4x.slice<{dim = 1, parent = #blocked3}>32x32>
x      i1, %176 = #arith.extsi blocked9>%
      11% 168:  = i32 to tt.reshapei 64%
167       : %tensor<177 = 4xtt.splat32 x32%x176i1, #blocked9> :  ->i 64 -> tensor<tensor<128x432xxii164, #blocked5>, 
      #%ttg169 = .slice<{dim = 1, parent = #blocked3}>arith.select >%168
,       %%cst_21, 178 = %163arith.addi  : %177,tensor< 128x32%175x : tensor<4ix1i, 64, #blocked#ttg5>, tensor<.slice<{dim = 1, parent = #blocked3}>128x>32
x      bf16%, #blocked5>179
       = %arith.extsi170 =  ttg.local_alloc %%32 169:  tensor<:32 (xtensor<i32, 128#xttg32.slice<{dim = 0, parent = #blocked3}>x>bf16, #blocked5> to)  -> tensor<!ttg32.memdesc<128x32xbf16, #shared, #smem>x
      i%64, 171 = #ttgttg.local_load.slice<{dim = 0, parent = #blocked3}> >%170
 :       !ttg%#.memdesc<128x32xbf16, #shared, #smem>180blocked  =  = ->arith.extsi #ttg %.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>tensor<30
#128 blockedx: 1 = 32i32 to #ttgxi64
      .blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>bf16, %181 = 
##tt.splatblockedttg 2.dot_op<{opIdx = 1, parent = #blocked3}>%180 = > #ttg
      :.% blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>172i
# = 64 blocked3tt.dot-> =   #ttg%tensor<.15532blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>,x
# i64, blocked4%# = 171ttg#ttg,.slice<{dim = 0, parent = #blocked3}>.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}> >
#%
blockedcst_3      5 % = :182 = #ttg arith.addi .tensor<%blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
4x181#128, blocked6x%179 = bf16 #, :ttg# .ttgtensor<blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>.32
dot_op<{opIdx = 0, parent = #blocked3}>x#>iblocked7 *64,  =  #ttg.#ttgtensor<slice<{dim = 0, parent = #blocked3}>.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>128x32>

x      #blockedbf16, %8 = #183#ttg = ttg..dot_op<{opIdx = 1, parent = #blocked3}>arith.muli blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
>%#blocked 7, 9 = ->%6#ttg  :.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>tensor< 
#4i64linear = x
#ttg32      .linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>x%
f32184 = #linear, #blockedtt.addptr 1 = 3%#ttg>arg2, .linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
      %
#%183linear2173 =   = arith.addf:#  ttg.%!ttlinear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>172.ptr<bf16>
#, ,shared = % #ttgcst_3i. 64swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>:

#       smemtensor<% = 4185#ttgx = .shared_memory32tt.expand_dims 
x%modulef32178 attributes {, #blocked {"3axis = tt>1g.
       : nu%i32} :m-174 ct = tensor<4xasarith.truncfi64, " =  #1%ttg : 173.i slice<{dim = 1, parent = #blocked3}>32:> -> , " tensor<tttensor<4x324xg.x1nf32xum, #blocked3>i64, -w #blocked3>arto
ps       " = tensor<4%1864 : x = i3232arith.extsi , x%ttg.targetbf16arg13 = ", #blocked3 hi>: i32 to p:
ig      64fx%175
9 =       50arith.extsi%187"  = , %tt.expand_dims"13 tt %g:175.t  {hrtensor<4xaxiseai = ds321-,  : p#ierttg32-w.slice<{dim = 1, parent = #blocked3}>} :ar> to p" tensor< = 64tensor<4x4 : ii64x32}, i {#ttg.slice<{dim = 1, parent = #blocked3}>64, 
>#ttg  
.tt.func      slice<{dim = 1, parent = #blocked3}> %176 = > -> public arith.extsitensor<@ 4_batched_gemm_afp4_wfp4_pre_quant_kernel%11x( :1xi%arg0 64, : i#!32blockedtt 3.ptr<bf16>to > {i
tt.divisibility = 64      16 : 
%188 = i32      arith.muli}, %177 =  %arg1tt.splat%:  186, %176!tt%176  .::ptr<i8> {  tt.divisibilityii64 = 1664 -> 
 : tensor<      i324%189 = }, xtt.splat %arg2i64, %: !#ttg186tt. .slice<{dim = 1, parent = #blocked3}>:ptr<bf16> {>
       tt.divisibility = %i16 : 178 = 64 ->i32arith.addi  }, %177tensor<%arg3, 4: !%175x1xtt. iptr<i8>:64,  {tt.divisibility #blocked3>
 = tensor<      16 : 4%190 = i32xarith.muli }, i64, %%arg4#ttg.slice<{dim = 1, parent = #blocked3}>189: i>,32
 , %      %arg5: %187i32179 :  {tt.divisibility = tensor<4x1x = 16arith.extsi %i : i32 : 64, 32}tensor<#blocked3>
, %32      arg6x%: i191 = i3232, tt.addptr {# tt.divisibilityttg%184, = . 16 : slice<{dim = 0, parent = #blocked3}>%i32> to 188}, tensor< : %arg732!tt: ix.ptr<bf16>32 {i,tt.divisibility64  = 16, i : #64i32ttg
}, .slice<{dim = 0, parent = #blocked3}>      %arg8>%: 
192i       = 32 {%tt.expand_dims tt.divisibility = 180%16 :  = 182i32arith.extsi  {}, %axis = %300arg9:  :  : i32ii {tt.divisibility3232 =  } : 16 : to tensor<ii323264x}, 
      i64%arg10%, : i181 = #32tt.splatttg { .tt.divisibility%slice<{dim = 0, parent = #blocked3}> = 180>16   : :-> i32 tensor<}, i64 -> 1%arg11tensor<x: 3232xi32xi64, i {#64tt.divisibility = ttg, 16 : .slice<{dim = 0, parent = #blocked3}>#blocked3i32>
      >}, %
%182      arg12:  = %i32arith.addi193 {tt.divisibility % =  = 16181tt.broadcast  : i,%32} 190, %%179 : arg13:  tensor<i32:4 { xtt.divisibilitytensor<1x = 1632i64,  : ix#blocked32}i643, %, > -> arg14: #tensor<4i32ttg.slice<{dim = 0, parent = #blocked3}>x {tt.divisibility>32x = 16
      i : i%6432}183, , % = #blockedarg15: arith.muli 3i32%7, >
      )%% attributes6194 {noinline  =  = :tt.expand_dimsfalse  }i% {64179

 {          axis = %cst%0 = 184 : arith.constant = i tt.addptr32dense< }127>%  : tensor<arg2, :4%183 x4 tensor<x1: 32xi!x8tti64, , #.#ttg.slice<{dim = 0, parent = #blocked3}>blocked>ptr<bf16>> ->
    , % tensor<cst_0 = i641arith.constant
x       32dense<%x0x7FC0185i> = 64 : tt.expand_dims, tensor<4 #blocked3>x128%178
xbf16 {      , #axis = %blocked11195>
 :  =     %itt.broadcastcst_132  = arith.constant} : %194 dense<tensor< 2097152>4x: : tensor<i 4x64tensor<4x, 11x#xittg3232, .x#blockedslice<{dim = 1, parent = #blocked3}>i>
> ->64, #blocked    % 3> -> cst_2tensor<tensor< = arith.constant44 xxdense<1x324ix> : 64, itensor<4#blocked364x>, 4
#x      blocked16x%186 = 3>
i8arith.extsi       , #%%196 = blocked2arg13 : tt.addptr>i 
    32%%c31_i32 to 191,  = arith.constanti%180 :  3164!tt : i
      .ptr<bf16>,32% 
    187i% = 64
cst_3tt.expand_dims       =  %197 = arith.constant%175arith.addi %195, %193 dense< { 0.000000e+00axis:> :  =  tensor<41tensor<x32 : 4xf32i32} : x, #tensor<32blocked4x3xi64, #blocked3>
>
i          %64%c32_i32, 198 =  = arith.constant#arith.extsi ttg 32 : .slice<{dim = 1, parent = #blocked3}>%i>arg432  : i32 to 
    ->i%c4_i32 64 = arith.constanttensor<4
 4x       : 1%i32x199 = 
    itt.splat%true64  = arith.constant, % #198trueblocked 
3:     %>i64 -> c0_i32 = 
tensor<arith.constant      4 0%x1x : i188i32 = 64
    arith.muli , #blocked3>%cst_4%
 = arith.constant186       dense<, %-8388608%200 = > : 176arith.cmpitensor<4  x:slt4x ,1xi i3264%, #
      185,blocked>% %
189199    % =  cst_5tt.splat: = arith.constant   dense<%tensor<42.000000e+00186x> 1 : tensor<: x4iix64 -> 64, #blocked3>
      %2014tensor< = x14arith.extsi xf32x%, 1xarg5#blockedi64,  >
#blocked:     %3icst_6 = >32arith.constant
        dense<%to0.000000e+00190 =  > : arith.muli itensor<4%189, 64
x4%      x1187%202 = xf32 : tt.splat, tensor< #blocked4%>x201
    1 %x:cst_7i  = arith.constant64, i #64 dense<blocked3>
      -> -2147483648>%191 = tensor< : tensor<tt.addptr 14x%184,x32x4x i64, #blocked3>
32x%188       i32:%, # 203blocked>!tt = 
    .arith.cmpi%cst_8ptr<bf16>  = arith.constant,slt  ,dense<i64
      %192 =  23>tt.expand_dims% :  192tensor<4%, %202x4182 x32 {: xiaxis = tensor<32, 01# : xblockedi32>
32} : x    %tensor<icst_9 = 3264arith.constantxi64, , #blocked dense<#ttg.3>
      255>slice<{dim = 0, parent = #blocked3}>%204 =  : tensor<> -> tt.broadcast 4xtensor<%2004x1 32x:x32x iitensor<32644, , x##blocked3>
1blocked      x>
%193 = i    tt.broadcast1%cst_10 ,  = arith.constant%#blocked3>  190-> dense< :tensor<8388607> 4 : tensor<tensor<x32x4x4i4x1, x321x#xiblockedi64, #blocked3>32, 3
#blocked>      > -> %
    tensor<205%4 = cst_11 = xtt.broadcastarith.constant32x  dense<i64%1>, #blocked203 : tensor<3> :4x
 4      tensor<x32%194 = 1xtt.expand_dimsxi32 32x, #%iblocked>1791
 {,     %axis = #blocked3> -> cst_12 = 0tensor<arith.constant : 4 ixdense<32} : 32x127>tensor<i1, #blocked3>
 : tensor<32x      4xi%2064x64,  = 32x#arith.andiittg 32, .slice<{dim = 0, parent = #blocked3}>%204#blocked> -> , %205>
tensor<     %1:cst_13x  = 32xi64, #blocked3>tensor<arith.constant
      4 %xdense<195 = 32x4194304>tt.broadcast i : tensor<%19414x , 4:#x32 blockedxitensor<3>
      32, 1%#blockedx32x207>i64, # = 
blocked3> ->tt.splat %196 :     % !ttcst_14 = tensor<4x.ptr<bf16> -> arith.constant32xi64, #blockedtensor< dense<3>
4126>      x : %32tensor<4196 = xx4tt.addptr!ttx32 .ptr<bf16>, #blockedxi%1913>
32, ,      # %blocked%208 = ler_TP3: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
>
180tt.addptr     : %cst_15 % = !tt207, arith.constant.% ptr<bf16>197 : dense<,tensor<2 i64
4>      x : %32xtensor<4197 = !xarith.addi tt4x%.ptr<bf16>, #blocked32195, 3xi%193>32,  , #blocked: tensor<>tensor<44
    xx%3232cst_16xx = iiarith.constant6464, #blocked3 dense<, #blocked>213
> : >
      %      tensor<4198 = tt.storex4arith.extsi  x32%%xiarg420832,  : ,#i blocked>32 %
    to174%cst_17 , = i arith.constant64%206 
 dense<28      : > : %tensor<tensor<19944 = xxtt.splat324x x32x%!tt.ptr<bf16>, #blocked3i32198 : >, i
#blocked64    }>
 -> 
    %tensor<    cst_18 = 4tt.returnarith.constantx
 dense<1  -1.270000e+02x#}> : i64blocked
tensor<,  = }4##ttg
xblocked3>.
4x
      blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>{-#1x%200 = 
#
f32, arith.cmpi blocked1  #blockedslt = external>
,#_resources: {    % ttg.
cst_19 = %185blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>    arith.constant, 
mlir_reproducer dense<%199#blocked: {7> 2
 : tensor<: =       4x #ttgpipeline32xtensor<.: i4blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>"16, x
bui#ttg1x#ltin.module(op.slice<{dim = 2, parent = #blocked4}>iblockedt>643imize-amd-lds-usage{l
,  = d    ##s%blockedttg-cst_203.lim = arith.constant>blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>i 

#tdense<      blocked4=0 targe-1% = t> : 201 = #-tensor<arith.extsittg.a4x blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
rc32x%#hiarg5blocked5=gfx9508,   = }#:#ttg,ttg. . slice<{dim = 2, parent = #blocked4}>iblocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>triton-scf-to-cf, convert-ind>
32 to i
e    %64#blockedxcst_21 = 
6-arith.constant       = t %#odense<202 = ttg.-l0x7FC0tt.splatblocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>lv> :  
#mtensor<%blocked7{128201 :  = index-x32i64 #bitwidtxbf16-> ttgh=0}, allocate-amdgpu-shared-memor, tensor<.y#1blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
, convertblocked5x#->32xblocked8tr
    i = i%64, #tcst_22#ttg.on- = blockedblocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>aarith.constant3>
      
#m dense<%blockeddgpu-to-llv7>2039m{a : tensor< =  = r4arith.cmpi#cx ttgh=4xslt, .blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>gfx950 ftz=tr32x%
ue}i32192#,, #, linear =  blocked>%#c
202ttg.anonicalize{     % linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}> cst_23:
m = arith.constant #lineara dense<tensor<1x-iterations=10 max-num-1.270000e+02>1x32xi64, #blocked3> = r : 
#etensor<4      ttgwx%204 = .linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>rites4xtt.broadcast
=1 #-x%linear1f32, 200 2 =  #:#rblocked ttg.egion-simplify=normal test>tensor<4linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>-
x
c    %1#onvergence=false top-down=cst_24xisharedt = arith.constant1, #blocked3> ->  = r tensor<#udense<4ttge}, cse, co1.270000e+02>x.nve : tensor<32swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>r4x
txi#-41smem = cx1, #blocked#ttgf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwxf323.i, >
      shared_memoryd#%
tlinear205moduleh>
 =  attributes {=0}, can    %tt.broadcast"ocst_25 ttnicalize{  = arith.constant%g. m 203 nadense<:umx-1.270000e+02> -c- : tensor<tensor<1ti4xatx32xis"erations=10 max-4x1 = n1x, 1uf32, # : m#blockedi-linear3> -> 32rew>
tensor<, r    4"ites=-1 region-s%cst_26xtimplify=norm = arith.constant32xi1ta , #blocked3>
gldense<      .n test-2.000000e+00%umconver>206-gence : tensor< = wa=false top-d4arith.andi rox%pw4204snx, " = =true}, 1%2054 : cse, symbxf32 : io, tensor<4x3232l#x, ttg.target-dce, linear>i1, #blocked3>
       = e
%"nable-l    207 = hiine-info, conver%cst_27tt.splatpt-builtin-fu = arith.constant :gnc-to-llvm{ftz=true}) dense<%f"-8388608196x,> 9
 : : 50      tensor<4!tt"disable_threading: x4.ptr<bf16>, falsex "t,1x->t
i32 g      , #tensor<4x.verify_eachlinear>32t: 
xhtrue    %!ttre
cst_28 = .a    }arith.constantptr<bf16>ds
 , #blocked-  }dense<3>
      pe
2097152>%r#-} : tensor<208 = -w
4xtt.addptr ar4x%207, p1%"x/tmp/torchinductor_root/lo/closwarcf3lxqxnpn5m3rwwhyta67y2azos7vsp3ce5ldch24464.py:18:0197  = i: :6432error:   : i, #Failures have been detected while processing an MLIR pass pipelinetensor<32}linear>
4 
/tmp/torchinductor_root/lo/closwarcf3lxqxnpn5m3rwwhyta67y2azos7vsp3ce5ldch24464.py:18:0x{    %: 32x
cst_29note: !   = Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`tttt.funcarith.constant
.ptr<bf16>  , public dense<#@127blocked_batched_gemm_afp4_wfp4_pre_quant_kernel(> : 3>, %arg0tensor<tensor<: 44!x4xttx32x.ptr<bf16>1i {xi64, tt.divisibility8# = , #blocked3>16linear
       : >tt.storei
 32    %}%cst_30208,  = ,%arith.constant arg1 %: dense<174!7>,tt :  .tensor<%ptr<i8> {4x206tt.divisibility4x  = i16:16,   : i#ttgtensor<32.4}slice<{dim = 2, parent = #blocked}>>x, 
32x%    !tt.ptr<bf16>, #arg2%blocked: cst_313! = arith.constant>tt. dense<
ptr<bf16>-1     {tt.divisibility>} =  : 
16 : tensor<    i324tt.return}x
, 4  %x}arg3: i
!tt8}., 
ptr<i8> {#ttg
tt.divisibility.{-# = 16slice<{dim = 2, parent = #blocked}>
 : >  i32
    external}llvm.intr.assume _resources: {, %
%arg4true    :  mlir_reproduceri:: {32,  i
%[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     a_ptr,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_ptr,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     c_ptr,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_scales_ptr,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     M,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     N,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     K,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_am,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ak,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bk,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bn,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ck,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cm,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cn,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsb,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsn,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsk,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Meta-parameters
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_M: tl.constexpr,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_N: tl.constexpr,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_K: tl.constexpr,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GROUP_SIZE_M: tl.constexpr,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     NUM_KSPLIT: tl.constexpr,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     EVEN_K: tl.constexpr,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GRID_MN: tl.constexpr,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     cache_modifier: tl.constexpr,
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] ):
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """Kernel for computing the matmul C = A x B.
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A_scales and B_scales are in e8m0 format.
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ab > 0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_am > 0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ak > 0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bb > 0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bk > 0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bn > 0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cb > 0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cm > 0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cn > 0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsb > 0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsk > 0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsn > 0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # -----------------------------------------------------------
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Map program ids `pid` to the block of C it should compute.
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # This is done in a grouped ordering to promote L2 data reuse.
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.program_id(axis=0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_unified = tl.program_id(axis=1)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_k = pid_unified % NUM_KSPLIT
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid = pid_unified // NUM_KSPLIT
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if NUM_KSPLIT == 1:
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         remap_xcd(pid, GRID_MN)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     else:
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m = pid // num_pid_n
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_n = pid % num_pid_n
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_batch >= 0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_m >= 0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_n >= 0)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # We assume 32 elements along K share the same scale.
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for first block of A and B input matrices
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         a_ptrs = a_ptr + (
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_ab
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_am[:, None] * stride_am
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split_bf16[None, :] * stride_ak
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_ptrs = b_ptr + (
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_bb
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split[:, None] * stride_bk
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[None, :] * stride_bn
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for the first block of A and B scales
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # B scales are N x K even though B operand is K x N.
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_scale_ptrs = (
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales_ptr
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_bsb
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[:, None] * stride_bsn
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_ks[None, :] * stride_bsk
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales = tl.load(b_scale_ptrs)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # If it is out of bounds, set it to 0.
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             if EVEN_K:
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(a_ptrs)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             else:
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Advance the ptrs to the next K block.
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c = accumulator.to(c_ptr.type.element_ty)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Write back the block of the output matrix C with masks.
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_ptrs = (
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             c_ptr
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_cb
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cm * offs_cm[:, None]
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cn * offs_cn[None, :]
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_k * stride_ck
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         tl.store(c_ptrs, c, mask=c_mask)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 6, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 96, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Traceback (most recent call last):
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     next_module = compile_ir(module, metadata)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pm.run(mod)
[rank6]:E1106 11:04:58.403000 214 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] RuntimeError: PassManager::run failed
1      arg5
pipeline:     : illvm.intr.assume"32 b {%uiltin.modutt.divisibilitytrue l = :e(op16 :  itii1miz32
e}    llvm.intr.assume-am,  %d%true-larg6 d: :s-usai g32i1e {
    {tt.divisibilityllvm.intr.assumel =  %d16true s-limit=0 : :  target-aiir32}1ch=gfx950}, 
,%     tritoarg7llvm.intr.assumen-s:  %cf-to-cf,itrue 32 { :ctt.divisibility onve = ir161t-index-to-llv : 
    m{index-biillvm.intr.assumetwid32 th}%true=0},,   %:allocate-amdgpu-sharedarg8 i-: 1memoi
r32    y, co {llvm.intr.assume nvett.divisibility%r = true t-triton-amdgpu-to-16:l :  liiv321m}
{arch=gfx950 ftz=true}, ,     llvm.intr.assumecanonicalize% %{  marg9truea: i x32 {:-iteratitt.divisibility io = 1ns=10 m16
a :     llvm.intr.assumex-num-rewri %i32truet} e, : s=-%i1arg101 : 
regioni    -32llvm.intr.assumes { itt.divisibility%truemplify=no =  r16:m :  al teiis321t}, 
    -con%llvm.intr.assume vearg11%rgence=false top: true-i do32:wn=true}, cs { iett.divisibility1, convert-cf-to-llvm = 16
{index- :     biillvm.intr.assumetwidth32 =}%true0}, convert-ar,  :it% h-to-llvarg12i1m{: 
ii32    %ndex- {0 = btt.divisibility = tt.get_program_id i16xtwi :  di:t32 h}i32=0}, canonicalize{  max-iterat, 
i%arg13    %ons=10 : 1mi = a32tt.get_program_idx { -nutt.divisibilityym- =  :r16 iewrites=-1 region-sim : 32pi
lify32    =}%2n,  = o%arith.addirarg14:  mi%a32arg5l {,  tt.divisibility = %c31_i32t16 e : : sii32t-converge32
nce=false t},     %op%3 = -doarg15arith.divsiwn=true}, cse, symbol-dce, enab: i l32%2e-line-inf),o, co attributes %nvert {c32_i32-noinline bui = :lfalse it}32in 
    -func{
%-    4t% = arith.extsiocst - = %larith.constantarg7l  :vm{ftdense< z=true})127>i32" :  to,tensor< i
464      x4
    disable_threading: x%5false1x = ,i8arith.extsi
      ,  verify_each#%: trueblockedarg9
> :    }
 
    i32  }% 
cst_0to #-}
 = arith.constanti64 
    dense<%0x7FC06 = /tmp/torchinductor_root/uu/cuubnboef3iux4c3vah5yqq262uvyr33xrzulj7e7pfsaotf7ls6.py:18:0>arith.extsi:  :  error: tensor<%arg11Failures have been detected while processing an MLIR pass pipeline4 
x:/tmp/torchinductor_root/uu/cuubnboef3iux4c3vah5yqq262uvyr33xrzulj7e7pfsaotf7ls6.py:18:0128x i: bf16, 32 note: #blockedto Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`1i
>
64
    %    %cst_1 = 7 = arith.constantarith.extsi  dense<%02097152> : : tensor< i4x32 4xto1x ii3264
, #    %blocked8 = >
arith.divsi     %1%cst_2,  = arith.constant%3 dense< :4> i : tensor<32
4x    %4x9 = 16xarith.remsi i8%1, #, blocked2%3>
 :    % ic31_i3232
 = arith.constant    llvm.intr.assume  %31 : true i: 32
i1    %
    cst_3 = llvm.intr.assume arith.constant%true dense< :0.000000e+00> i : tensor<14x
    32xllvm.intr.assumef32,  %#blockedtrue3> :
     i%c32_i321 = arith.constant
     32%10 : i = arith.cmpi32
     %sgt,c4_i32 =  arith.constant%arg6 4,  : i%c0_i3232
 :    % itrue = 32
arith.constant     truescf.if 
    %10%c0_i32 { = arith.constant
 0      % : 11 = i32arith.muli 
    %8%cst_4,  = arith.constant%c4_i32 dense< :-8388608> i : tensor<324x
      4x%121x = tt.make_rangei32 {end, # = blocked>4 : 
    i32%cst_5, start = arith.constant = 0  : i[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     a_ptr,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_ptr,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     c_ptr,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_scales_ptr,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     M,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     N,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     K,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_am,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ak,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bk,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bn,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ck,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cm,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cn,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsb,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsn,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsk,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Meta-parameters
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_M: tl.constexpr,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_N: tl.constexpr,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_K: tl.constexpr,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GROUP_SIZE_M: tl.constexpr,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     NUM_KSPLIT: tl.constexpr,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     EVEN_K: tl.constexpr,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GRID_MN: tl.constexpr,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     cache_modifier: tl.constexpr,
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] ):
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """Kernel for computing the matmul C = A x B.
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A_scales and B_scales are in e8m0 format.
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ab > 0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_am > 0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ak > 0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bb > 0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bk > 0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bn > 0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cb > 0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cm > 0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cn > 0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsb > 0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsk > 0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsn > 0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # -----------------------------------------------------------
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Map program ids `pid` to the block of C it should compute.
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # This is done in a grouped ordering to promote L2 data reuse.
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.program_id(axis=0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_unified = tl.program_id(axis=1)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_k = pid_unified % NUM_KSPLIT
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid = pid_unified // NUM_KSPLIT
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if NUM_KSPLIT == 1:
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         remap_xcd(pid, GRID_MN)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     else:
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m = pid // num_pid_n
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_n = pid % num_pid_n
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_batch >= 0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_m >= 0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_n >= 0)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # We assume 32 elements along K share the same scale.
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for first block of A and B input matrices
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         a_ptrs = a_ptr + (
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_ab
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_am[:, None] * stride_am
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split_bf16[None, :] * stride_ak
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_ptrs = b_ptr + (
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_bb
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split[:, None] * stride_bk
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[None, :] * stride_bn
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for the first block of A and B scales
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # B scales are N x K even though B operand is K x N.
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_scale_ptrs = (
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales_ptr
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_bsb
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[:, None] * stride_bsn
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_ks[None, :] * stride_bsk
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales = tl.load(b_scale_ptrs)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # If it is out of bounds, set it to 0.
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             if EVEN_K:
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(a_ptrs)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             else:
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Advance the ptrs to the next K block.
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c = accumulator.to(c_ptr.type.element_ty)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Write back the block of the output matrix C with masks.
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_ptrs = (
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             c_ptr
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_cb
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cm * offs_cm[:, None]
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cn * offs_cn[None, :]
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_k * stride_ck
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         tl.store(c_ptrs, c, mask=c_mask)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 5, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 96, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Traceback (most recent call last):
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     next_module = compile_ir(module, metadata)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pm.run(mod)
[rank5]:E1106 11:04:58.406000 213 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] RuntimeError: PassManager::run failed
dense<32}2.000000e+00> : : tensor< 4xtensor<44xxi1x32, f32, ##blockedttg.>
slice<{dim = 1, parent = #blocked1}>>    %
      cst_6 = %13arith.constant = tt.make_range  {enddense< = 40.000000e+00> : i : tensor<32, 4xstart = 4x0 : 1xi32f32, } #blocked: >
tensor<4    %xicst_7 = 32, arith.constant#ttg dense<.slice<{dim = 1, parent = #blocked3}>-2147483648>>
 : tensor<      %4x14 = 4xtt.splat 32x%11i32 :, # iblocked>32 
    -> %cst_8tensor<4 = arith.constantxi 32, dense<#ttg23>.slice<{dim = 1, parent = #blocked1}> : tensor<>
4x      %4x15 = 32xarith.addi i32%14, #, blocked>%12
     :%cst_9 tensor< = arith.constant4x dense<i32255, > : #ttgtensor<4.slice<{dim = 1, parent = #blocked1}>x4>
x32      %xi16 = 32, tt.splat #blocked%arg4>
 :    % icst_10 = 32 arith.constant->  dense<tensor<48388607>xi : tensor<32, 4x#ttg4x.slice<{dim = 1, parent = #blocked1}>32x>
i32      %, 17 = #blockedarith.remsi>
 %    %15,cst_11 =  %arith.constant16  dense<: 1tensor<4> : xitensor<432, x4#ttgx32.slice<{dim = 1, parent = #blocked1}>xi>
32,       %#blocked18>
 = arith.muli    % %cst_12 = 7,arith.constant % dense<4127 :> :  itensor<464
x4      %x3219 = xitt.expand_dims 32, %17#blocked {axis>
 = 1    % : icst_13 = 32}arith.constant : dense< 4194304tensor<4> : xitensor<432, x4#ttgx32.xislice<{dim = 1, parent = #blocked1}>>32,  ->#blocked tensor<>
4x    %1xcst_14 = i32arith.constant, # dense<blocked1126>>
 : tensor<      4x%204x = tt.splat32x %i32arg8 , #blocked>
    %cst_15 = arith.constant dense<2> : :tensor<4 ix432 x32-> xitensor<432, x1#blockedxi>
32,     %#blockedcst_16 = 1>arith.constant
       dense<%2121> = arith.muli :  %tensor<419,x4 %x3220xi :32 tensor<, #4xblocked>1x
    i32%cst_17, # = arith.constantblocked1 dense<>
28>      % : tensor<22 = 4xarith.extsi 4x%2132x :i32 tensor<, #4xblocked>1
    xi%cst_1832,  = #blockedarith.constant1>  todense< tensor<-1.270000e+02>4x : 1xtensor<4i64x4, #x1blocked1xf32>
, #      %blocked>23 = 
    tt.make_range%cst_19 {end = arith.constant = 128 dense< : i7>32,  : tensor<start = 4x0 : 32xi32i16} , : #ttgtensor<128.slice<{dim = 2, parent = #blocked4}>xi>
32,     #ttg%cst_20.slice<{dim = 0, parent = #blocked1}> = arith.constant>
       %dense<24 = -1>tt.expand_dims  : tensor<%234x {axis32x = 0i8 : i, 32}#ttg :.slice<{dim = 2, parent = #blocked4}> tensor<>
128x    i32%cst_21,  = arith.constant#ttg dense<.slice<{dim = 0, parent = #blocked1}>0x7FC0>>  : tensor<-> 128xtensor<132xx128bf16, xi#blocked32, 5>#blocked
    1>%cst_22
       = arith.constant%25 dense< = 7>arith.extsi  : tensor<%244x :4x tensor<32x1xi32128x, #i32blocked>, #
blocked1    %> cst_23to  = arith.constanttensor<1 x128dense<xi1.270000e+02>64,  : tensor<#blocked4x1>4x
      1x%26f32,  = #blockedtt.broadcast >
%22    % :cst_24 =  tensor<arith.constant4x dense<1x1.270000e+02>i64 : tensor<, #4xblocked14x> 1x-> f32, tensor<4#linearx128>
xi    %64, cst_25 = #blockedarith.constant1> dense<
      -1.270000e+02>%27 : tensor< = tt.broadcast4x %4x25 1x: f32, tensor<1#linearx>
128x    %i64cst_26 = , #arith.constantblocked1 dense<> 2.000000e+00-> > : tensor<4tensor<4x128x4xx1i64xf32, #, #blocked1linear>>

          %%cst_2728 =  = arith.constantarith.addi  dense<%-8388608>26, : tensor< %4x274x :1x tensor<i324x, #128xlinear>i64
    , #%cst_28blocked1 = arith.constant>
 dense<      %2097152>29 =  : tensor<tt.addptr4x %4xarg0,1x %i3218 , #: linear>!tt
    .ptr<bf16>%cst_29,  = arith.constanti64 
      dense<%30127> = arith.muli : tensor< %4x9,4x %1xc32_i32i8 , #: linear>i32
    
      %cst_30%31 = arith.constant =  dense<tt.make_range7> {end : tensor< = 324x : i4x32, i16start = , 0 : #ttgi32.slice<{dim = 2, parent = #blocked}>} >
:    % tensor<cst_3132x = arith.constanti32 dense<, -1>#ttg : tensor<.slice<{dim = 0, parent = #blocked6}>4x>
4x      %i832 = , tt.make_range#ttg {end.slice<{dim = 2, parent = #blocked}> = 32>
 : i    32, llvm.intr.assumestart =  %0 : true i32: } i1: 
    tensor<32llvm.intr.assume xi%true32,  :# ittg.1slice<{dim = 0, parent = #blocked3}>>
    
      llvm.intr.assume %33%true = tt.make_range : {end i = 321 : i
    32, llvm.intr.assume start = %true0 :  i32:}  : i1tensor<32
    xillvm.intr.assume 32, %true#ttg :.slice<{dim = 1, parent = #linear1}> i>
1      %
    34 = llvm.intr.assume tt.splat %true%30 : : i i132 
    -> llvm.intr.assume tensor<32%truexi :32,  i#ttg1.slice<{dim = 0, parent = #blocked6}>
    >
llvm.intr.assume       %%true35 =  :tt.splat  i%301 :
     illvm.intr.assume 32 %true->  :tensor<32 ixi132, 
    #ttgllvm.intr.assume .slice<{dim = 1, parent = #linear1}>%true>
 :      % 36 = iarith.addi 1%34
    , llvm.intr.assume %31%true : : tensor< i32x1i32
,     llvm.intr.assume#ttg %.slice<{dim = 0, parent = #blocked6}>true >
:       i1%37
     = arith.addi%0 % = tt.get_program_id35,  %x33 :  i32: 
    tensor<32%1xi = tt.get_program_id32,  y#ttg :.slice<{dim = 1, parent = #linear1}> i>
32
      %    %382 =  = tt.splatarith.addi  %%arg5arg5 , : %c31_i32i32 : -> i tensor<32
32x    %i323 = , arith.divsi#ttg %.slice<{dim = 0, parent = #blocked6}>2,>
 %      %c32_i32 39 = : tt.splat i%arg532
 :    % i4 = 32 arith.extsi ->%arg7 tensor< :32x ii3232 , to #ttgi64.slice<{dim = 1, parent = #linear1}>
    >
%5      % = arith.extsi40 =  %arith.remsi arg9 %36: , i32%38 to : i tensor<64
32x    %i326 = , arith.extsi #ttg%arg11.slice<{dim = 0, parent = #blocked6}> :>
 i      %32 41 = to arith.remsi i64%37
    , %7%39 = arith.extsi : % tensor<0 32x: i32i32,  to#ttg .slice<{dim = 1, parent = #linear1}>i64>

          %%842 =  = arith.divsiarith.muli  %%71,,  %%53  ::  ii3264

          %%943 =  = tt.make_rangearith.remsi {end % = 64 : 1,i32 %, start3  = 0:  : ii3232}
     :llvm.intr.assume tensor< %64xtrue i32: , i1#ttg
    .slice<{dim = 1, parent = #blocked6}>llvm.intr.assume>
 %      %true 44 = : tt.expand_dims i1%43
     {llvm.intr.assume axis = %true1 : : i i32}1 :
     tensor<%1064x = arith.cmpii32 sgt, , #%arg6ttg., slice<{dim = 1, parent = #blocked6}>>%c0_i32 -> : tensor< 64xi321x
    i32scf.if , #%10blocked {6>

            %%4511 =  = arith.extsiarith.muli  %%844 , : %c4_i32tensor<64 :x1 ixi32
32,       %#blocked126> =  tott.make_range tensor< {end64x = 41x : ii6432, , start = #blocked0 : 6>i32
      } %46:  = tt.expand_dimstensor<4 %xi4032,  {axis#ttg = 0.slice<{dim = 1, parent = #blocked1}> : i>
32}      % :13 =  tensor<tt.make_range32x {endi32 = 4,  : i#ttg32, .slice<{dim = 0, parent = #blocked6}>start = > 0 : -> i32tensor<1} x32: xitensor<432, xi#blocked32, 6>#ttg
      .slice<{dim = 1, parent = #blocked3}>%47>
 = tt.splat      % %14 = arg10tt.splat  %: 11 i32:  ->i32 tensor< ->1x tensor<324xxii3232, , ##ttgblocked6.slice<{dim = 1, parent = #blocked1}>>
>
      %      48 = %arith.muli15 =  %arith.addi 46,%14 %, 47%12 : tensor<4xi32, #ttg :.slice<{dim = 1, parent = #blocked1}> tensor<>
1x      %32x16 = i32tt.splat, # %blocked6arg4 >
:       %i3249 =  ->arith.extsi tensor< %4x48 i32: , tensor<1#ttgx32.slice<{dim = 1, parent = #blocked1}>xi>
32,       %#blocked17 = 6>arith.remsi  to%15 , tensor<1%16x32 :x tensor<i644x, #i32blocked6, >
#ttg      %.slice<{dim = 1, parent = #blocked1}>50 = >
tt.broadcast       %%4518 =  arith.muli : %7tensor<64, x1%4xi :64,  i#blocked64
6>       ->%19 tensor< = tt.expand_dims64x %32x17i64 {axis, # = 1blocked6 : i>
32}      % :51 tensor< = tt.broadcast4x %i3249 , : #ttgtensor<1.slice<{dim = 1, parent = #blocked1}>x32> xi-> 64, tensor<4#blockedx16>x ->i32 tensor<, #64xblocked132x>
i64      %, #20 = blocked6tt.splat>
 %      %arg8 52 = : arith.addi i32% ->50, tensor< %4x511 :xi tensor<32, 64x#blocked32x1>i64
      , #%21blocked = arith.muli6> %
      19,%53 % = tt.addptr20 % arg1,:  %tensor<442 x1: xi!tt32, .ptr<i8>#blocked,1> i
      64
%22      % = arith.extsi54 =  %arith.extsi 21 %arg14:  :tensor<4 ix132 xito 32, i64#blocked
      1>%55 to = arith.muli tensor< %4x7,1 %xi5464,  :#blocked i1>64

            %%2356 =  = tt.make_rangett.addptr  {end%arg3 = 128,  : i%5532,  :start =  0 : !tti32.ptr<i8>} , : i64tensor<128
      xi%5732,  = tt.expand_dims#ttg %.41slice<{dim = 0, parent = #blocked1}>> {axis
 = 1      % : i24 = 32}tt.expand_dims : % tensor<2332x {axisi32 = 0,  : i#ttg32}.slice<{dim = 1, parent = #linear1}> :>  tensor<-> 128xtensor<32i32x1, xi#ttg32, .slice<{dim = 0, parent = #blocked1}>#linear> 1>-> 
      tensor<1%58x128 = tt.splatxi %32arg15 , #: blocked1i32> ->
       tensor<%2532x = arith.extsi1x %i3224, # :linear1 tensor<>
1x      %128x59 = i32arith.muli , #%57blocked1, > %58to  :tensor<1 tensor<x12832xxi1x64, i32#blocked, #1>linear1
      >
%26      % = tt.broadcast60 =  %arith.extsi 22 %: 59 tensor<4: x1tensor<32xix164xi, #32, blocked1#linear> 1>-> to tensor< tensor<4x32x128x1xi64i64, #, #blocked1linear1>
>
      %      %27 = 61 = tt.broadcast tt.make_range%25 {end  = 4:  : itensor<132, x128start = xi0 : 64, i32#} blocked1: > tensor<4-> xitensor<432, x128#ttgxi.slice<{dim = 0, parent = #linear1}>64, >
#blocked      %1>62 = 
      tt.broadcast %28% = arith.addi60  %: 26,tensor<32 %x127xi :64,  tensor<#linear4x1128x> i64-> , #tensor<32blocked1x4>
xi      %64, 29 = #lineartt.addptr1> %
      arg0,%63 % = tt.expand_dims18  %: 61!tt {axis.ptr<bf16> = 0,  : ii6432}
       :%30 tensor< = arith.muli4x %i329,,  %#ttgc32_i32.slice<{dim = 0, parent = #linear1}> :>  i-> 32
tensor<1      %x431 = xitt.make_range32,  {end#linear = 321> : i
      32, %64start =  = 0 : tt.broadcast i32%63}  :: tensor< tensor<1x32x4xi32i32, , ##ttglinear1.slice<{dim = 0, parent = #blocked6}>> >
->       %tensor<3232x4 = xitt.make_range32 {end, # = 32linear1 : i>
32,       %start = 65 = 0 : arith.extsi i32%64}  ::  tensor<tensor<3232xxi4x32, i32#ttg, #.slice<{dim = 0, parent = #blocked3}>linear1>
>       %to 33 = tensor<32tt.make_rangex4 {xiend = 64, 32 : #lineari321>, start
       = 0%66 : i = arith.addi32} % :65, tensor< %32x62i32 :,  tensor<#ttg32x.slice<{dim = 1, parent = #linear1}>4x>i64
      , #%34linear1 = tt.splat>
 %      %30 67 = : tt.splat i32%56 -> : tensor< 32x!tti32., ptr<i8> #ttg-> .tensor<32slice<{dim = 0, parent = #blocked6}>x4>
x!      %tt.35 = ptr<i8>, tt.splat #linear%301>
 :      % i6832  = tt.addptr->  %tensor<3267,xi %32, 66 #ttg: .slice<{dim = 1, parent = #linear1}>tensor<32>
x4      %x36 = !ttarith.addi .ptr<i8>%34, #, linear1%31>, : tensor< tensor<32x32x4xi32i64, , #ttg#.slice<{dim = 0, parent = #blocked6}>linear1>
>
      %      %37 = 69 = arith.addi tt.load%35, % %6833 : : tensor< tensor<32x32x4xi32!tt, .ptr<i8>#ttg, #.slice<{dim = 1, parent = #linear1}>linear1>
>      %
      38 = %70tt.splat  = %arg5tt.trans  :%69 i {order32  = -> array<itensor<3232: xi1, 320>, } #ttg: .slice<{dim = 0, parent = #blocked6}>tensor<32>
x4      %xi39 = 8, tt.splat #linear%arg51>  ->:  tensor<i324x ->32x tensor<i32x8, i32#ttg, .slice<{dim = 2, parent = #blocked4}>#ttg>
.slice<{dim = 1, parent = #linear1}>      %>
71 =       %tt.splat 40 = %29arith.remsi : % 36,!tt %.ptr<bf16>38  ->:  tensor<tensor<324xxi128x32, !tt#ttg.ptr<bf16>.slice<{dim = 0, parent = #blocked6}>, #>
blocked1      %>
41 =       %arith.remsi 72 = %37tt.addptr , %71%39,  :%28 tensor< :32 tensor<xi4x32, 128x#ttg!tt.slice<{dim = 1, parent = #linear1}>.ptr<bf16>>
, #      %blocked142 = >,arith.muli  tensor<%74x, 128x%5i64 :, # iblocked164
>
      %      %43 = 73 = tt.make_rangett.load  {end%72 = 64 : : i tensor<32, 4xstart = 128x0 : !tti32.ptr<bf16>} , : #blockedtensor<641>xi
      32, %#ttg74 = .slice<{dim = 1, parent = #blocked6}>tt.splat >
%53      % :44 =  tt.expand_dims! %tt.43ptr<i8> {axis -> = 1 tensor< : i64x32}32x :!tt tensor<.ptr<i8>64x, #i32blocked6, >
#ttg      %.slice<{dim = 1, parent = #blocked6}>75 = tt.addptr %74, %> ->52  tensor<: 64xtensor<641xx32i32x!, #tt.blocked6ptr<i8>, >#blocked
      6>%45,  = tensor<64arith.extsi x%4432x :i64 tensor<, #64xblocked61x>
i32      %, #76 = blocked6tt.load > %75to  tensor<64cacheModifier x1= xicg64,  :# tensor<blocked664x>
32x      %!tt46 = .ptr<i8>tt.expand_dims , #%40blocked6 {axis>
 = 0      % : i77 = 32}ttg.convert_layout  :%76 tensor< :32x tensor<i3264x, 32x#ttgi8.slice<{dim = 0, parent = #blocked6}>, #> blocked6-> > tensor<1-> x32tensor<64xix3232, xi#blocked8, 6>#blocked
      3>%47
       = tt.splat% %78 = arg10 tt.reshape : %73i32 : -> tensor< tensor<4x1x128x32xbf16, i32#blocked, #1>blocked6 ->>
       %tensor<448 = x4arith.muli x32%46xbf16, , #%47blocked> :
       tensor<%791x = math.absf32x %i3278, # :blocked6 tensor<>
4x      %4x49 = 32xarith.extsi bf16, %#blocked48 >
:       %tensor<180 = x32arith.extf xi%7932,  :#blocked tensor<6>4x to4x tensor<32x1xbf16, 32x#i64blocked>, # toblocked6 tensor<>
4x      %4x50 = 32xtt.broadcast f32, %45#blocked :>
 tensor<      %64x81 = 1x"ti64t., #reblocked6du> ce-> "(tensor<64%80x32)xi <64, {axis#blocked = 26> : i
      32}%51> ( = tt.broadcast{
 %      ^bb049 (%: arg16: tensor<1f32, x32%arg17xi: f3264, ):#blocked
6>        % ->209 =  tensor<arith.maxnumf 64x%32xarg16,i64 %, #arg17blocked6 :>
 f32      %
        52 = tt.reduce.return arith.addi %209% 50,:  %f32
51      } :) tensor< : (64xtensor<32x4xi644x, #32xblocked6f32, >
#blocked      %>) -> 53 = tensor<4tt.addptr x4%arg1xf32, , %42#ttg :. slice<{dim = 2, parent = #blocked}>>!tt
      .ptr<i8>%82,  = ttg.convert_layouti64 %
      81 %54:  = arith.extsitensor<4 %x4arg14 xf32: , i32#ttg to.slice<{dim = 2, parent = #blocked}> i> 64
->       %tensor<455 = x4arith.muli xf32%7, , #%ttg.54slice<{dim = 2, parent = #linear}>> :
       i%8364
 = tt.expand_dims      % %56 = 82tt.addptr  {axis%arg3 = 2,  : i%5532} : : ! tensor<tt.4xptr<i8>,4x if32, 64
#ttg      %.slice<{dim = 2, parent = #linear}>57 = > tt.expand_dims -> %41tensor< {axis4x = 14x : i1x32}f32,  :#linear tensor<>
32x      %i3284 = , tt.expand_dims #ttg%.slice<{dim = 1, parent = #linear1}>81>  {axis->  = 2tensor<32 : ix132}xi :32,  tensor<#linear4x1>4x
      f32, %58#ttg = .slice<{dim = 2, parent = #blocked}>tt.splat > %arg15->  :tensor<4 ix432 x1-> xf32tensor<32, #x1blockedxi>
32,       %#linear85 = 1>tt.bitcast 
      %83%59 : = arith.muli tensor< %4x57,4x %1x58f32,  #linear: > tensor<-> 32xtensor<41xx4i32x1, #xilinear132, >
#linear      >
%60      % = arith.extsi86 =  %tt.bitcast 59 %84:  :tensor<32 tensor<x14xxi4x32, 1x#linearf32, 1>#blocked to>  tensor<-> 32xtensor<41xx4i64x1, #xilinear132, >
#      %blocked>61 = 
      tt.make_range%87 {end = arith.addi = 4 % : i85,32,  %start = cst_280 :  :i32 tensor<} 4x: 4xtensor<1x4xi32i32, #, linear>#ttg
      .slice<{dim = 0, parent = #linear1}>%88>
 = arith.addi      % %62 = 86,tt.broadcast  %%60cst_1 : : tensor< tensor<32x4x1x4xi641x, #i32linear1, #>blocked> ->
       tensor<%8932x = tt.bitcast4x %i6487 , #: linear1tensor<4>
x4      %x163 = xitt.expand_dims 32, %#61linear> {axis -> =  tensor<0 : 4xi324x} 1x: i32tensor<4, #xilinear>32, 
      #ttg%90.slice<{dim = 0, parent = #linear1}> = tt.bitcast>  %-> 88 tensor<1: x4tensor<4xix432, x1#linearxi1>32, 
      #blocked%64>  = tt.broadcast->  %tensor<463 x4: x1tensor<1xix432, xi#blocked32, >
#linear      %1>91 =  ->arith.andi  tensor<%8932, x%cst_274x :i tensor<32, 4x#linear4x1>1x
      i32%65, # = arith.extsilinear %>
64       %:92 =  tensor<arith.andi 32x%904x, i32%cst_4, # :linear1 tensor<> 4xto 4xtensor<321x4xixi32, 64, #blocked#linear>
1>      %
      93 = %66tt.bitcast = arith.addi % %91 65: , tensor<4%62x4 :x1 tensor<xi32x32, 4x#lineari64> , #-> linear1tensor<4>
x4      %x167 = xf32tt.splat , #%56linear :>
       %!tt94 = .ptr<i8>tt.bitcast  ->%92 tensor< :32x tensor<4x4x!tt4x.ptr<i8>1x, #i32linear, #1>blocked
      > %68-> = tt.addptr tensor< %4x67,4x %1x66 f32, : #blockedtensor<32>
x4      %x95 = !ttmath.log2.ptr<i8> %, #93linear1 :>, tensor< tensor<4x324xx41xxif32, 64, #linear#linear>
1>      %
      96 = %69math.log2  = %94tt.load  %68: : tensor< tensor<4x32x4x4x1x!ttf32, .ptr<i8>#blocked, #>
linear1      %>
97      % = math.floor70 =  %tt.trans 95%69  {: order = tensor<4array<ix432: x11xf32, 0, #>linear>} 
      : %98tensor<32 = math.floorx4 %xi968,  :#linear tensor<1>4x ->4x 1xtensor<4f32, x32#blockedxi>
8,       %#ttg99.slice<{dim = 2, parent = #blocked4}> = >
arith.subf       %%9771 = , tt.splat %cst_26 %:29  tensor<: 4!ttx4.ptr<bf16>x1 ->xf32 tensor<, 4x#linear128x>!tt
      .ptr<bf16>%100, # = arith.subfblocked1 %>
98,      % %72 = cst_5tt.addptr  :%71 tensor<,4x %4x28 1x:f32,  tensor<#4xblocked>128x
      !tt%101.ptr<bf16> = , #tt.clampf blocked1%99>,,  tensor<%4xcst_25,128x i64%cst_24, #, blocked1propagateNan >
=       %none73 =  :tt.load  tensor<%724x :4x 1xtensor<f324x, #128xlinear>!tt
      .ptr<bf16>%102, # = tt.clampfblocked1 %>
100,      % %74 = cst_18,tt.splat  %%53 cst_23: ,!tt propagateNan.ptr<i8> = -> none tensor< :64x tensor<32x4x!tt4x.ptr<i8>1x, #f32, blocked6#blocked>
>
      %      %75 = 103 = tt.addptrarith.fptoui  %%10174, : % tensor<52 4x: 4xtensor<641xx32f32, x!#tt.linear>ptr<i8>,  to#blocked tensor<6>4x, 4xtensor<641xx32i8xi, #64, linear>#blocked
      6>%104
       = arith.fptoui% %76 = 102 tt.load : %75tensor<4 x4cacheModifier x1= xf32cg, # :blocked> tensor< to64x tensor<324xx4x!tt1x.ptr<i8>i8, #, #blocked6blocked>>

            %%10577 =  = arith.addittg.convert_layout  %%76103, : % tensor<cst_2964x :32x tensor<i84x, #4xblocked61x>i8 ->, # linear>tensor<64
      x32%106xi = arith.addi8,  %#blocked104,3> %
      cst%78 : =  tensor<tt.reshape 4x%734x :1x tensor<i84x, #128xblocked>bf16
      , #%107blocked1 = arith.subf>  %-> cst_6,tensor<4 %x4102x32 :xbf16 tensor<, #4xblocked>4x
      1x%79f32,  = math.absf#blocked %>
78      % :108 =  math.exp2 tensor<4%x4107x32 :xbf16 tensor<, #4xblocked>4x
      1x%80f32,  = #blockedarith.extf >%
      79 %109:  = tensor<4arith.extf x4%78x32 :xbf16 tensor<, #4xblocked>4x to32x tensor<bf16, 4x#blocked4x> 32xto f32, tensor<4#blockedx4>
x32      %xf3281 = , #"tblocked>t.
      re%110du = tt.broadcastce %"(108 %: 80)tensor<4 <{x4axis = x12 : xf32i32, #}>blocked> ({ ->
 tensor<      ^bb04x(%4xarg16: 32xf32, f32%arg17, #: f32blocked>)
      :
%111        % = 209 = arith.mulf arith.maxnumf %109%arg16, , %110%arg17 : : tensor< f324x
        4xtt.reduce.return 32x%209f32,  :#blocked f32>

            %})112 =  : (tt.bitcast tensor<4%111x4 :x32 tensor<xf324x, #4xblocked>32x) -> tensor<f32, 4x#blocked4x> f32, -> #ttgtensor<4.slice<{dim = 2, parent = #blocked}>x4>x32
      xi%8232,  = ttg.convert_layout#blocked >
%81      % :113 =  tensor<arith.andi 4x%1124x, f32, %cst_7#ttg :.slice<{dim = 2, parent = #blocked}> tensor<> 4x-> 4xtensor<432x4xixf3232, , #blocked#ttg>.
      slice<{dim = 2, parent = #linear}>>
%114      % = arith.shrui83 =  %tt.expand_dims 112,%82 % {axiscst_8  = 2:  : itensor<432}x4 :x32 tensor<xi4x32, 4x#blockedf32, >
#ttg      %.slice<{dim = 2, parent = #linear}>115 = > arith.andi -> %114tensor<4, x4%cst_9x1 :xf32 tensor<, #4xlinear>4x
      32x%84i32 = tt.expand_dims, # %blocked>81
       {axis%116 = 2 =  : iarith.andi 32}%112 :,  tensor<%cst_104x :4x tensor<f32, 4x#ttg4x.slice<{dim = 2, parent = #blocked}>32x> i32-> , tensor<4#blockedx4>
x1      %x117 = f32, arith.addi #blocked%115>, 
      %cst_11%85 : = tt.bitcast tensor< %4x83 4x: 32xtensor<4i32x4, #x1blocked>xf32
      , #%118linear> =  ->arith.subi  tensor<%cst_124x, 4x%1171x :i32 tensor<, #4xlinear4x>
32x      %i3286 = , #tt.bitcast blocked>%84
       :%119 tensor< = 4xarith.cmpi 4xult,1x f32, %115#blocked, > %cst_12->  :tensor<4 tensor<x44xx14xxi32x32, i32#blocked, #>
blocked>      %
      87 = %120arith.addi  = arith.shrui%85 %, 116,%cst_28 % :cst_11  tensor<: 4xtensor<44xx41xx32i32xi, #32, linear>#blocked
>
      %      %88 = 121 = arith.addi arith.ori %86%120, , %cst_1%cst_13 : : tensor< tensor<4x4x4x4x1x32xi32i32, #, blocked>#blocked
      >
%89      % = tt.bitcast122 =  %arith.shrui87  : %121tensor<4, x4%118x1 :xi tensor<32, 4x#linear4> x32-> xitensor<432, x4#blockedx1>
xi      %32, 123 = #lineararith.select >
%119      %, %90 = 122, tt.bitcast %116%88 :  :tensor<4 tensor<x44xx324xxi1x1, i32#blocked, #>, blocked>tensor<4 ->x4 tensor<x324xxi4x32, 1x#i32blocked>, #
      blocked>%124
       = %91arith.maxui  = arith.andi%115 %,89, % %cst_14 cst_27 : :tensor<4 tensor<x44xx324xxi1x32, i32#blocked, #>
linear>      %
      125 = %92arith.subi = arith.andi % %124,90,  %%cst_14cst_4  ::  tensor<tensor<44xx44xx132xxii3232, , #blocked#blocked>
>
      %      %93 = 126 = tt.bitcast arith.shli %%12591 , : %cst_15tensor<4 :x4 tensor<x14xix432, x32#linearx> i32-> , #tensor<blocked>4x
      4x%1271 = arith.shruixf32 %, #123,linear> %
      cst_16 %94:  = tt.bitcasttensor<4 %x492 x32: xitensor<324, x4#blockedx1>
xi      %32, 128 = #blockedarith.ori > %126-> , tensor<4%127x4 :x1 tensor<xf324x, #4xblocked>32x
      i32%95, # = blocked>math.log2 
      %93%129 : = arith.addi tensor< %4x128,4x %1xcst_11f32,  :#linear tensor<>
4x      %4x96 = 32xmath.log2 i32%94, # :blocked> tensor<
      4x%1304x = arith.shrui1x %f32, 129,# %blocked>cst_11 
      : %97tensor<4 = math.floorx4 %x3295xi :32,  tensor<#blocked4x>
4x      %1x131 = f32, arith.minui #%130linear>, 
      %cst_22% :98 =  tensor<math.floor4x %4x9632x :i32 , #tensor<4blocked>x4
      x1%132xf32 = arith.shrui, # %blocked>113,
       %%99cst_17  = : arith.subf tensor<4%97x4, x32%cst_26x :i32 tensor<, #4xblocked>4x
      1x%f32, 133 = #lineararith.ori >
%132      %, 100 = %131arith.subf : % tensor<98,4x %4xcst_532x :i32 tensor<, #4xblocked>4x
      1x%134f32,  = arith.trunci#blocked %>
133      % 101 = : tt.clampf tensor<4%99x4, x32%xicst_25,32,  %#blockedcst_24,> propagateNan to = tensor< none4x :4x tensor<32x4xi84x, #1xblocked>f32, 
      #linear%135>
 =       %tt.reshape 102 = %134tt.clampf  :%100 , tensor<4%cst_18x4, x32%cst_23xi, 8, propagateNan #blocked= > none->  :tensor<4 tensor<x4x4x4x16x1x2xf32, i8#blocked, #>
blocked      %7>103 = 
      arith.fptoui%outLHS %, %101 outRHS = : tt.split tensor<%4135 x4:x1 xf32tensor<4, #x4linear>x16 tox2 tensor<xi4x8, 4x#blocked1x7>i8 ->, # tensor<linear>4x
      4x%10416x = arith.fptouii8 %, #102 blocked2: >
tensor<4      x4%136x1 = arith.shlixf32 %, #outRHS,blocked> % tocst_2 tensor< :4x tensor<4x4x1x4xi816x, #i8blocked>, 
      #blocked%1052> = arith.addi
       %137%103 = arith.ori,  %%cst_29outLHS, : % tensor<136 4: x4tensor<4x1x4xxi816x, i8#linear, #>
blocked2      %>
106 =       %arith.addi 138 = %104tt.reshape , %137%cst : : tensor< tensor<4x4x4x4x16x1xi8i8, #, #blocked2blocked>> 
      -> %107tensor< = arith.subf4x %64xcst_6,i8 %, #102blocked8 :>
 tensor<      %4x139 = 4xtt.reshape1x %f32, 105 #blocked: >
tensor<4      x4%108x1 = math.exp2xi %8, 107#linear :>  tensor<-> 4xtensor<44xx41xxif32, 8, #blocked#>
ttg.      %slice<{dim = 2, parent = #blocked}>>109 = 
      arith.extf %140%78 = tt.reshape : % tensor<106 4x: 4xtensor<432xx4bf16, x1#blockedxi>8,  to#blocked tensor<> 4x-> 4xtensor<432xx4f32, xi#8, blocked>#linear
      2>%110
       = tt.broadcast%141 % = ttg.convert_layout108  %: 140 tensor<4: x4tensor<4x1xxf324x, #i8blocked>, # ->linear2 tensor<> 4x-> 4xtensor<432xx4f32, xi#blocked8>
,       %#ttg111 = .slice<{dim = 2, parent = #blocked}>arith.mulf >
%109      %, 142 = %110arith.extui  :%141 tensor< :4x tensor<4x4x32x4xf32, i8#blocked, >
#ttg      %.slice<{dim = 2, parent = #blocked}>112 = > tt.bitcast to %tensor<4111x4 :xi 16, tensor<4#ttgx4.slice<{dim = 2, parent = #blocked}>x32>
xf32      %, #143 = blocked>arith.shli  ->%142 tensor<, 4x%cst_304x :32x i32tensor<4, #x4blocked>xi
      16, %113# = arith.andittg. %slice<{dim = 2, parent = #blocked}>>112,
       %%144cst_7  = tt.bitcast:  %tensor<4143 x4: x32tensor<4xix432, xi#blocked16, >
#ttg      %.slice<{dim = 2, parent = #blocked}>114 = > arith.shrui -> %112tensor<4, x4%cst_8xbf16 :,  tensor<#ttg4x.slice<{dim = 2, parent = #blocked}>4x>
32x      %i32145 = , #tt.expand_dims blocked>%144
       {axis%115 = 2 = arith.andi : i %32}114, : % tensor<cst_9 4x: 4xtensor<4bf16, x4#ttgx32.slice<{dim = 2, parent = #blocked}>xi> 32, -> #blockedtensor<4>
x4      x%1161x = arith.andibf16 %, #112,blocked> %
      cst_10 %146:  = tt.broadcasttensor<4 %x4145 x: 32tensor<4xix432, x1#blockedxbf16>
, #      %blocked>117 =  ->arith.addi  tensor<%1154x, 4x%cst_1132x :bf16,  tensor<#blocked4x>
4x      %32x147 = i32tt.reshape, # %blocked>146 
:       %tensor<4118 = x4arith.subi x32%cst_12xbf16, , #%117blocked> : -> tensor< tensor<4x4x4x128x32xbf16, i32#blocked, #1>blocked>
      
      %148%119 =  = arith.cmpiamdgpu.scaled_upcast_fp4  ult%138,  scale%115 %, 147%cst_12 {axis : = 1 tensor< : i4x32}4x :32x tensor<i324x, #64xblocked>i8
      , %120#blocked = arith.shrui8> %, 116, tensor<4%cst_11x :128x tensor<bf16, 4x#blocked4x1>32x ->i32 tensor<, #4xblocked>128x
      bf16, %121#blocked = arith.ori1> %
      120,% %149cst_13  = arith.cmpi:  tensor<4eq,x4 %x32139,xi %32, cst_31#blocked :>
 tensor<      %4x122 = 4xarith.shrui i8%121, , #ttg%118.slice<{dim = 2, parent = #blocked}> :>
 tensor<      4x%1504x = tt.expand_dims32x %i32149, # {axisblocked> = 2
       : i%12332} = arith.select : % tensor<119, 4x%1224x, i1%116,  : #ttgtensor<4.slice<{dim = 2, parent = #blocked}>x4> x32-> xitensor<41, x4#blockedx1>, xitensor<1, 4x#blocked4x>
32x      %i32151 = , #tt.broadcast blocked>%150
       :%124 tensor< = arith.maxui4x %4x115,1x %i1cst_14 , #: blocked>tensor<4 ->x4 tensor<x324xxi4x32, 32x#blockedi1>
, #      blocked>%125
       = arith.subi%152 % = 124,tt.reshape %cst_14 % :151  tensor<: 4xtensor<44xx432xx32i32xi, #1, blocked>#blocked
      > %126->  = arith.shlitensor<4 %x128125,x %i1cst_15, # :blocked1 tensor<>
4      %x4153 = x32arith.select xi%15232, , %#blockedcst_0, >
%      %148 : 127 = tensor<4arith.shrui x128%123xi, 1, %cst_16#blocked :1> tensor<, tensor<4x4x4x128x32xbf16, i32#blocked, #1>blocked>
      
      %154%128 = ttg.local_alloc = arith.ori % %153126,  %: 127 (tensor<: 4xtensor<4128xx4bf16, x#32xblocked1i32>), # -> blocked>!ttg
      .memdesc<4x128xbf16, #shared, #smem>%129
       = arith.addi%155 % = 128,ttg.local_load  %%cst_11154  :: tensor< 4x!ttg4x.memdesc<4x128xbf16, #shared, #smem>32x ->i32 tensor<, #4xblocked>128x
      bf16, %130#ttg = arith.shrui.dot_op<{opIdx = 0, parent = #blocked3}> %>
129,      % %156 = cst_11 arith.extui : %tensor<470 x4: x32tensor<4xix32, 32x#blockedi8>
,       %#ttg131 = .slice<{dim = 2, parent = #blocked4}>arith.minui > %130to , tensor<4%cst_22x :32x tensor<i164x, 4x#ttg32x.slice<{dim = 2, parent = #blocked4}>i32>
, #      %blocked>157 = 
arith.shli       %%156132 = , arith.shrui %cst_19%113 :,  tensor<%cst_174x :32x tensor<i164x, 4x#ttg32x.slice<{dim = 2, parent = #blocked4}>i32>
, #      blocked>%158
       = tt.bitcast%133 % = arith.ori157  %: 132,tensor<4 %x32131 xi: 16, tensor<4#ttgx4.slice<{dim = 2, parent = #blocked4}>x32> xi-> 32, tensor<4#blockedx>
32x      %bf16, 134 = #ttgarith.trunci .slice<{dim = 2, parent = #blocked4}>%133>
 :      % tensor<159 = 4xtt.expand_dims 4x%15832x {axisi32 = , #2 : blocked>i32 to}  tensor<: 4xtensor<44xx3232xxbf16i8, , ##ttgblocked>.slice<{dim = 2, parent = #blocked4}>
      > %135->  = tt.reshapetensor<4 %x32134 x1: xbf16tensor<4, #x4blocked4x32>
xi      %8, 160 = #blockedtt.broadcast > %159->  :tensor<4 tensor<x44xx1632xx21xixbf168, , ##blockedblocked47>> 
      ->%outLHS tensor<, %4xoutRHS32x = tt.split32x %bf16, 135#blocked :4> tensor<
      4x%1614x = tt.trans16x %2x160i8 {order, # = blocked7array<i> 32: -> 0, tensor<42, x41x16>}xi :8,  tensor<#blocked4x2>32x
      32x%136bf16,  = arith.shli#blocked %4>outRHS, -> % tensor<cst_24x :32x tensor<32x4xbf16, 4x#blocked16x9>i8
      , #%162blocked2 = tt.reshape>
 %      %161 137 = : arith.ori tensor<4%outLHSx32, x32%xbf16136 , #: blocked9tensor<4> x4-> x16tensor<128xix328, xbf16#blocked, #2>blocked5
      >
%138      % = tt.reshape163 =  %amdgpu.scaled_upcast_fp4 137 %77:  scaletensor<4 %x4162x16 {axisxi = 08,  : i#blocked32}2> : -> tensor< tensor<64x4x32x64xi8i8, , ##blockedblocked83>>
,       %tensor<128139 = x32tt.reshape xbf16%105, # :blocked5 tensor<> 4x-> 4xtensor<1x128xi832x, #bf16, linear>#blocked ->5> tensor<
      4x%4x164 = i8, arith.cmpi#ttg eq., slice<{dim = 2, parent = #blocked}>%70>
,       %%cst_20140 =  :tt.reshape  tensor<%1064x :32x tensor<i84x, 4x#ttg1x.slice<{dim = 2, parent = #blocked4}>i8>
,       %#blocked165 = > tt.expand_dims ->%164 tensor< {axis4x = 24x : ii832}, # :linear2 tensor<>
4x      %32x141 = i1ttg.convert_layout,  %#ttg140 .: slice<{dim = 2, parent = #blocked4}>>tensor<4 ->x4 tensor<xi4x8, 32x#linear1x2>i1 ->, # tensor<blocked44x>
4x      %i8166 = , tt.broadcast #ttg%165.slice<{dim = 2, parent = #blocked}> :>
 tensor<      %4x142 = 32xarith.extui1x %i141 1, : #blockedtensor<44>x4 ->xi tensor<8, 4x#ttg32x.slice<{dim = 2, parent = #blocked}>32x> i1to , #tensor<4blocked4x4>
xi      %16, 167 = #ttgtt.trans .slice<{dim = 2, parent = #blocked}>%166>
 {      %order = 143 = array<iarith.shli 32: %1420, , 2, %cst_301> :}  tensor<: 4xtensor<44xx32i16x32, xi#ttg1, .slice<{dim = 2, parent = #blocked}>#blocked>
4>      % ->144 =  tt.bitcast tensor<4%143x :32x tensor<32x4xi14x, i16#blocked, 9>#ttg
      .slice<{dim = 2, parent = #blocked}>%168>  = tt.reshape->  %tensor<4167 x4: xbf16tensor<4, x32#ttgx32.slice<{dim = 2, parent = #blocked}>xi>
1,       %#145 = blocked9tt.expand_dims > %144->  {axistensor< = 128x232x : ii132}, # :blocked5 tensor<>
4x      %4x169 = bf16, arith.select#ttg %.slice<{dim = 2, parent = #blocked}>168, > %cst_21-> , %tensor<4163 : x4tensor<128x1x32xbf16xi, #1, blocked>#blocked
      5>%146, tensor< = tt.broadcast128x %32x145 bf16, : #blockedtensor<45>x4
      x%1701x = ttg.local_allocbf16,  %#blocked169 > : -> (tensor<tensor<4128xx432xx32bf16, xbf16#blocked, 5>#blocked) -> >
!ttg      %.memdesc<128x32xbf16, #shared, #smem>147 = 
      tt.reshape%171 % = 146 ttg.local_load : %170tensor<4 :x4 x32!ttgxbf16.memdesc<128x32xbf16, #shared, #smem>, # ->blocked> tensor< ->128x 32xtensor<4bf16, x128#ttgxbf16.dot_op<{opIdx = 1, parent = #blocked3}>, #>
blocked1      %>
172 =       %tt.dot 148 = %155amdgpu.scaled_upcast_fp4 , %138%171 , scale %cst_3%147 : {axis tensor< = 14x : i12832}xbf16 , : #ttgtensor<4.dot_op<{opIdx = 0, parent = #blocked3}>x64> xi* 8, tensor<128#blockedx8>, tensor<4x32128xbf16x, bf16, #ttg#blocked.dot_op<{opIdx = 1, parent = #blocked3}>1>>  ->->  tensor<4tensor<4x32x128xf32xbf16, #, #blocked3blocked1>
>
      %      %173 = 149 = arith.addf arith.cmpi %172eq,,  %%cst_3139, : % tensor<cst_314x :32x tensor<f32, 4x#blocked4x3>i8
      , %174#ttg = .arith.truncf slice<{dim = 2, parent = #blocked}>>%173
       :%150 tensor< = tt.expand_dims4x %32x149f32,  {axis#blocked = 23> : i to32} tensor< 4x: 32xtensor<4bf16, x4#blockedxi3>1, 
      #ttg%175.slice<{dim = 2, parent = #blocked}> = arith.extsi>  %-> 13 tensor<4: x4tensor<4x1xixi32, 1, #ttg#blocked.slice<{dim = 1, parent = #blocked3}>>
>       to %151tensor<4 = tt.broadcastxi %64, 150 #ttg: .slice<{dim = 1, parent = #blocked3}>tensor<4>
x4      %x1176 = xiarith.extsi1,  %#blocked11 > : -> i32tensor<4 x4to x32i64xi
      1, %177#blocked = tt.splat>
       %%176152 =  :tt.reshape  i%15164  :-> tensor< tensor<4x4x4xi6432x, i1#ttg, #.blocked>slice<{dim = 1, parent = #blocked3}>> ->
       tensor<%1784 = arith.addix128 %x177,i1 %, #175blocked1 :>
 tensor<      %4x153 = i64arith.select , %152#ttg, %.slice<{dim = 1, parent = #blocked3}>cst_0, >%148
       : tensor<%1794x = arith.extsi128x %i132 , #: blocked1tensor<32>, xitensor<432, x128#ttgxbf16.slice<{dim = 0, parent = #blocked3}>, #> blocked1to >
tensor<32      %xi154 = 64, ttg.local_alloc#ttg %.slice<{dim = 0, parent = #blocked3}>153>
 :      % (180 = tensor<4arith.extsi x128%30xbf16 :, # blocked1i32>) to ->  i!ttg64
.memdesc<4x128xbf16, #shared, #smem>      %
      181 = %155tt.splat  = %180ttg.local_load : % i154 64 : -> !ttgtensor<32.memdesc<4x128xbf16, #shared, #smem>x ->i64 tensor<, 4x#ttg128x.slice<{dim = 0, parent = #blocked3}>bf16, >
#ttg      %.dot_op<{opIdx = 0, parent = #blocked3}>182 = >
arith.addi       %%181156 = , arith.extui %179%70 : :  tensor<tensor<432xx32i64xi, 8, #ttg#ttg.slice<{dim = 0, parent = #blocked3}>.slice<{dim = 2, parent = #blocked4}>>
>       %to 183 = tensor<arith.muli 4x%732x, i16%6,  :#ttg i.slice<{dim = 2, parent = #blocked4}>64
>
      %      %184 = 157 = tt.addptr arith.shli %arg2%156, , %183%cst_19 : :  tensor<!tt4x.ptr<bf16>32x, ii6416, 
      #ttg%185. = tt.expand_dimsslice<{dim = 2, parent = #blocked4}>> %
178      % {axis158 =  = 1tt.bitcast  : %157i32 :}  tensor<: 4xtensor<432xxii1664, , #ttg#ttg.slice<{dim = 1, parent = #blocked3}>.slice<{dim = 2, parent = #blocked4}>> > -> -> tensor<4tensor<4x1x32xixbf1664, , #blocked#ttg3>.slice<{dim = 2, parent = #blocked4}>
      >
%186       = arith.extsi%159 % = tt.expand_dimsarg13  : %158i32 {axis to = 2 i : i64
32}       :%187  = tt.expand_dimstensor<4 %x17532x {axisbf16,  = 1#ttg : .slice<{dim = 2, parent = #blocked4}>i32> } -> : tensor<4tensor<4x32xix164, xbf16#ttg, #.slice<{dim = 1, parent = #blocked3}>blocked4> >
->       %tensor<4160 = x1tt.broadcastxi %64, 159 #blocked: 3>tensor<4
      x32%188x1 = arith.mulixbf16 %, #186,blocked4 %> 176->  :tensor<4 ix3264x32
      xbf16%189, # = tt.splatblocked4 %>
186       %:161 =  itt.trans64  -> %160tensor<4 {orderx1 = array<xii3264, #: 0blocked3, 2>
, 1      %>}190 =  :arith.muli  tensor<%4x189,32x %32x187bf16,  :#blocked tensor<4>4x ->1x tensor<i644x, #32blocked3x32>
xbf16      %, #191 = blocked9tt.addptr >
%184      %, 162 = %188tt.reshape  :%161 ! :tt. tensor<ptr<bf16>,4x i32x64
32x      %bf16192 = , #tt.expand_dims blocked9%182>  {axis->  = 0tensor<128 : ix3232}xbf16 :, # tensor<blocked32x5>i64
      , %#ttg163 = .slice<{dim = 0, parent = #blocked3}>amdgpu.scaled_upcast_fp4>  %-> 77 tensor<1scale x32%xi16264,  {#blockedaxis3> = 
      0 : %193i32 = tt.broadcast}  %:190  tensor<: 64xtensor<432xx1i8xi, #64, blocked3#blocked>,3> tensor< ->128x tensor<32x4xbf16, 32x#blockedi645>, # ->blocked3 tensor<>
128x      %32x194 = bf16, tt.expand_dims #blocked%1795 {axis>
 = 0      % : i164 = 32}arith.cmpi  :eq, tensor< %32x70,i64 %, cst_20#ttg :.slice<{dim = 0, parent = #blocked3}> tensor<> 4x-> 32xtensor<1i8x32, xi#ttg64, .slice<{dim = 2, parent = #blocked4}>#blocked>
3>      %
      165 = %195tt.expand_dims =  %tt.broadcast 164%194 {axis : = 2 tensor< : i1x32}32x :i64 tensor<, #4xblocked332x> i1-> , tensor<4#ttgx32.slice<{dim = 2, parent = #blocked4}>xi> 64, -> #blockedtensor<43>x32
      x1%196xi = tt.addptr1,  %#blocked191,4> %
      180 %166:  = !tttt.broadcast.ptr<bf16> %, 165 i64: 
      tensor<4%197x32 = arith.addix1 %xi195,1,  %#blocked1934>  ->:  tensor<tensor<4x4x32x3232xxii164, , ##blockedblocked43>>

            %%198167 =  = arith.extsitt.trans  %%166arg4  {order:  = array<i32i32 to: 0 i, 264
, 1      %>}199 =  :tt.splat  tensor<%1984x :32x i32x64 i1-> , #tensor<4blocked4x1>xi ->64,  tensor<#blocked4x3>32x
      32x%200i1 = , #arith.cmpi blocked9slt,>
 %      %185,168 =  %tt.reshape 199%167  : : tensor<4tensor<4x1x32xix3264, xi#blocked1, 3>#blocked
      9>%201 -> = arith.extsi tensor< %128xarg5 32x: i1i32, # toblocked5 i>
64
      %      %169 = 202 = arith.select tt.splat %%201168,  :%cst_21 i, %64 163 : -> tensor<128tensor<1x32x32xixi1, 64, ##blockedblocked53>, >
tensor<128      %x32203 = xbf16arith.cmpi , #slt,blocked5 >
%192      %, 170 = %202ttg.local_alloc  :%169 tensor< :1x (32xtensor<128i64x32, #xbf16blocked3, #>
blocked      %5>204 = ) -> tt.broadcast !ttg%200.memdesc<128x32xbf16, #shared, #smem> :
       tensor<%1714x = ttg.local_load1x %i1170 , #: blocked!ttg3>.memdesc<128x32xbf16, #shared, #smem> -> -> tensor< tensor<4x128x32x32xi1bf16, , ##ttgblocked3.dot_op<{opIdx = 1, parent = #blocked3}>>
>
      %      %205 = 172 = tt.broadcast tt.dot %203%155 :,  tensor<%1711x, 32x%cst_3i1 :, # tensor<blocked34x> 128x-> bf16, tensor<4#ttgx32.dot_op<{opIdx = 0, parent = #blocked3}>xi>1,  *#blocked tensor<3>128x
      32x%206bf16,  = arith.andi#ttg %.dot_op<{opIdx = 1, parent = #blocked3}>204,>  %-> 205 tensor<4: x32tensor<4xf32x32, #xiblocked31, >
#blocked      3>%173
 = arith.addf      % %207 = 172,tt.splat  %%cst_3196  : : !tttensor<4.ptr<bf16>x32 ->xf32 tensor<, #4xblocked332x>
!tt      %.ptr<bf16>174 = , #arith.truncf blocked3%>
173      % :208 =  tensor<tt.addptr4x %32207,xf32 %, #197 blocked3: > tensor<4to x32tensor<4x!x32tt.xbf16ptr<bf16>, , ##blockedblocked33>>
,       %tensor<4175 = x32arith.extsixi %64, 13 #blocked: 3>tensor<4
      xitt.store 32, %208#ttg, .slice<{dim = 1, parent = #blocked3}>%174> , to %206tensor<4 :xi tensor<64, 4x#ttg32x.slice<{dim = 1, parent = #blocked3}>!tt>
.ptr<bf16>      %, #176 = blocked3arith.extsi >
%11    } 
    : tt.returni32
   to} i
64
}      %
177 = 
{-#tt.splat 
%176  external :_resources: {
 i    mlir_reproducer64 : {
->       tensor<4pipeline: xi"b64, ui#ttglt.slice<{dim = 1, parent = #blocked3}>in>
.m      %od178 = ularith.addie( %op177,t %im175iz :e- tensor<am4xd-i64ld, s-#ttgu.slice<{dim = 1, parent = #blocked3}>sa>
g      %e{179 = ldarith.extsi s-%32li :mi tensor<t=32x0 i32ta, rg#ttget.slice<{dim = 0, parent = #blocked3}>-a> rcto h=tensor<32gfxix964, 50#ttg},.slice<{dim = 0, parent = #blocked3}> t>
ri      %to180 = n-arith.extsi sc%f30ler_TP7: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
-t :o- icf32 , to coi64nv
      er%181t- = tt.splatin %de180 x-: toi64-l ->lv tensor<m{32xini64de, x-#ttgbi.slice<{dim = 0, parent = #blocked3}>tw>
id      %th182 = =0arith.addi },%181 a, ll%179oc at: e-tensor<32amxidg64, pu#ttg-s.slice<{dim = 0, parent = #blocked3}>ha>
re      %d-183 = mearith.muli mo%7ry, , %6co :nv ier64
t-      %tr184 = ittt.addptr o%n-arg2,am %dg183 pu: -!ttto.ptr<bf16>-l, lvi64m{
      ar%185c = tt.expand_dimsh= gf%178x9 {axis50 = 1 f : itz32}=t :ru tensor<e}4x, i64ca, no#ttgni.slice<{dim = 1, parent = #blocked3}>ca> li-> zetensor<4{ #x mblocked1xa = ix#64-ttg, #i.blocked3tblocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>>e

r#blocked      %at1186i =  = o#arith.extsinttg s.%=blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>arg13 1
:0#  blockedim232 a = tox# -ttgi64nu.
m-blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>      r
%ew#187rblocked = i3tt.expand_dimst =  e#%sttg175=-.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}> {1
axis # = 1rblocked : e4ig = 32i#} onttg:-. sblocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>tensor<4i
xm#ipblocked64l5, if = #ttgy#.slice<{dim = 1, parent = #blocked3}>=ttg> n.->oblocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}> r
tensor<4m#xablocked1xl6i64  = , t#ttg#es.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>blocked3t
#>
-cblocked      %on7 = 188 = v#ttgarith.mulie. rblocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>%g
186e#,nblocked %ce8 = 176=# fttg: a.i64lblocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
s
      e#% blocked189t9 = o = tt.splatp# -ttg%d.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>186o
 wn#: =tlinearir = 64 ue#-> },ttgtensor< .4xclinear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>1xs
ie#linear64, , 1#co = blockednv#3ettg>r.
t-linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>      c
%f#190 = -lineararith.mulito2 %- = 189l#,lttg v.%mlinear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>187{i
 :nd#shared e = tensor<x-#4bttgxit.1xwswizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>iid
64, th##blocked=smem30 = >}#
,ttg      % c.191 = oshared_memorytt.addptrnv
 emodule%184rt attributes, - {%ar"188 it: tht!tt-g.t.ptr<bf16>on,-u lmilv-ct64mas
{"      in = %d1192 = e : tt.expand_dimsxi -32%182bi,  {t"axiswt = 0itg.num-w : ida32}trps h":= =  tensor<0}432x,  : icai64, n32#ttgon, .slice<{dim = 0, parent = #blocked3}>icttg.target = >a" ->lh tensor<ii1zpx32e:xi{g64,  fx950#blocked m"3>ax, 
-"      it%tt193eg = r.tt.broadcastatthr ioea%nd190ss-per =-:1w 0 arptensor<m"4xa = 1xx-64inu : 64mi, #-32blocked3r}> e ->w{ tensor<r
4i  x32tett.funcxs i64=public, - #blocked1@3> r_batched_gemm_afp4_wfp4_pre_quant_kernel
      eg(%io%194narg0 = tt.expand_dims-s:  %i!179mtt {axisp. = lptr<bf16>0 : if {i32y=tt.divisibility}n =  :or16 m : tensor<ali32x 32it}64, e, #s%ttgtarg1.-: slice<{dim = 0, parent = #blocked3}>c!> ontt-> v.tensor<1erptr<i8>xg {32ett.divisibilityxn = i64c16, #e : blocked=fi3>a32
l}      s, %e%195 =  arg2: tt.broadcast to!tt%p-.ptr<bf16>194 d {: ott.divisibilitytensor<1wn = x=1632xtr : iui64, e32#}}, blocked,%3 carg3: >s! ett->,.  sptr<i8>tensor<y {4mtt.divisibilityxb = 32xo16i64l- : , di#c32blockede}3, , >e%
      naarg4%bl: 196 = e-itt.addptr l32%i, 191,ne% -arg5%i: 180ni f32:o, {  tt.divisibility!c = tto16.ptr<bf16>nv : ,ei irt3264-}
bu,       i%%larg6197ti:  = niarith.addi-32 %f {195utt.divisibility = ,n16 c : %193-i t32: o}tensor<-, 4l%xlarg732v: xmii{3264f {, ttt.divisibility#blockedz = 3=16>t : 
      rui%198e}32 = arith.extsi)"} ,
, %      disable_threading%arg4: arg8 false: :,i 
32i       {32 verify_each: tt.divisibilitytotrue =  
16i64    } : 
      
i%  }32}, 199
% = #-}arg9: tt.splat 
i%19832 : { itt.divisibility64  = -> /tmp/torchinductor_root/r2/cr2vdhs3b6rpqsjp7l3eiwj6w5etbbommavx4kqjigpzpxlnzxdk.py:18:016tensor<4:  : xerror: i1Failures have been detected while processing an MLIR pass pipeline32xi
}64/tmp/torchinductor_root/r2/cr2vdhs3b6rpqsjp7l3eiwj6w5etbbommavx4kqjigpzpxlnzxdk.py:18:0, , : note: %#Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`arg10blocked
: 3>i
      32%200 { = arith.cmpitt.divisibility slt = , 16%185 : , i%19932 :} tensor<, 4x%1xarg11: i64i, #32blocked3 {>
tt.divisibility      % = 201 = 16arith.extsi  : %arg5i :32} i, 32 %to arg12i64: 
      i%20232 = tt.splat { %tt.divisibility201  = : 16i64 :  ->i tensor<321x}32x, i%64, arg13#blocked: 3>i
      32%203 { = arith.cmpitt.divisibility slt = , 16%192 : ,i %32202},  :% tensor<arg14: 1xi32x32i64 {, #tt.divisibilityblocked3 = >
16      % : 204 = itt.broadcast 32%200} :,  tensor<%4xarg151x: i1i, #32blocked3)>  attributes->  {tensor<4noinlinex32 = xifalse1, }#blocked 3>{
      
%205     = tt.broadcast% %cst203  = : arith.constanttensor<1 x32dense<xi1271, >#blocked : 3>tensor< ->4 tensor<x4x432x[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     a_ptr,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_ptr,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     c_ptr,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_scales_ptr,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     M,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     N,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     K,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_am,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ak,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bk,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bn,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ck,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cm,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cn,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsb,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsn,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsk,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Meta-parameters
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_M: tl.constexpr,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_N: tl.constexpr,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_K: tl.constexpr,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GROUP_SIZE_M: tl.constexpr,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     NUM_KSPLIT: tl.constexpr,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     EVEN_K: tl.constexpr,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GRID_MN: tl.constexpr,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     cache_modifier: tl.constexpr,
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] ):
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """Kernel for computing the matmul C = A x B.
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A_scales and B_scales are in e8m0 format.
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ab > 0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_am > 0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ak > 0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bb > 0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bk > 0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bn > 0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cb > 0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cm > 0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cn > 0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsb > 0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsk > 0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsn > 0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # -----------------------------------------------------------
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Map program ids `pid` to the block of C it should compute.
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # This is done in a grouped ordering to promote L2 data reuse.
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.program_id(axis=0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_unified = tl.program_id(axis=1)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_k = pid_unified % NUM_KSPLIT
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid = pid_unified // NUM_KSPLIT
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if NUM_KSPLIT == 1:
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         remap_xcd(pid, GRID_MN)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     else:
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m = pid // num_pid_n
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_n = pid % num_pid_n
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_batch >= 0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_m >= 0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_n >= 0)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # We assume 32 elements along K share the same scale.
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for first block of A and B input matrices
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         a_ptrs = a_ptr + (
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_ab
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_am[:, None] * stride_am
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split_bf16[None, :] * stride_ak
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_ptrs = b_ptr + (
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_bb
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split[:, None] * stride_bk
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[None, :] * stride_bn
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for the first block of A and B scales
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # B scales are N x K even though B operand is K x N.
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_scale_ptrs = (
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales_ptr
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_bsb
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[:, None] * stride_bsn
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_ks[None, :] * stride_bsk
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales = tl.load(b_scale_ptrs)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # If it is out of bounds, set it to 0.
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             if EVEN_K:
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(a_ptrs)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             else:
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Advance the ptrs to the next K block.
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c = accumulator.to(c_ptr.type.element_ty)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Write back the block of the output matrix C with masks.
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_ptrs = (
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             c_ptr
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_cb
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cm * offs_cm[:, None]
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cn * offs_cn[None, :]
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_k * stride_ck
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         tl.store(c_ptrs, c, mask=c_mask)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 1, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 96, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Traceback (most recent call last):
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     next_module = compile_ir(module, metadata)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pm.run(mod)
[rank1]:E1106 11:04:58.442000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] RuntimeError: PassManager::run failed
xi11, #xblocked3i>
8      %, 206 = #arith.andiblocked %>204,
 %    205 %: cst_0tensor<4 = x32arith.constantxi 1, dense<#blocked0x7FC03>>
       : %207tensor< = tt.splat4 %x196 128: x!ttbf16., ptr<bf16> #-> blockedtensor<41x32>x
!tt    .ptr<bf16>%, #cst_1blocked3 = >
arith.constant      % 208 = dense<tt.addptr 2097152%207>,  : %197tensor< :4 tensor<x4x432xx!tt1.ptr<bf16>x, #iblocked332, #blocked>,> tensor<
4x    32x%i64cst_2 = , #arith.constantblocked3 >
dense<      4tt.store> % : 208,tensor< %4174,x %4206x :16 tensor<x4xi32x8!tt, .ptr<bf16>#, #blockedblocked32>
>    }

        tt.return%
  c31_i32} = 
}arith.constant
 
{-#31
 :   externali_resources: {
32    mlir_reproducer
: {
          %pipeline: cst_3"b = uiarith.constantlt indense<.m0.000000e+00od>ul : e(tensor<op4tixmi32zex-af32md, -l#dsblocked-u3sa>g
e{    ld%c32_i32s- = liarith.constantmi t=320  : tairg32et
-a    r%chc4_i32=g = fxarith.constant95 0}4,  : triit32on
-s    cf%-ttrue = o-arith.constantcf , trueco
nv    er%c0_i32t- = arith.constant 0 : i32
    %cst_4in = dearith.constantx- tdense<o--8388608ll>v : m{tensor<in4dexx-4xbi1twxidith=320}, , #alblockedlo>ca
    te%-cst_5am = dgarith.constantpu -sdense<ha2.000000e+00>re : d-tensor<me4moxry4x, 1coxnvf32er, t-#trblockedit>on
-a    md%gpcst_6 = u-arith.constantto -ldense<lv0.000000e+00m{>ar : chtensor<=g4fxx9540 x1ftxz=f32, tr#ublockede}>, 
ca    no%nicst_7ca = liarith.constantze { dense< m-2147483648ax>-i : tetensor<ra4tixon4sx=1320 xmaix-32nu, m-#blocked>re
    wr%itcst_8 = esarith.constant=- 1 dense<re23gi>on : -stensor<im4plxi4fyx=n32orxmial32, #blocked> t
es    t-%cocst_9nv = erarith.constantg endense<ce255=f>al : setensor< t4xop4x-d32owxni=t32ru, e}#, blocked>cs
e,     c%oncst_10ve = rarith.constantt- cfdense<-t8388607>o- : lltensor<vm4x{i4xnd32exx-biit32wi, dt#h=blocked>0}
,     co%nvcst_11er = t-arith.constantar itdense<h-1to>-l : lvtensor<m{4inxde4x-xbi32twxidith32=0, },# blocked>ca
no    ni%ccst_12al = izarith.constante{   dense<ma127x->it : ertensor<at4ioxns4=1x0 32maxx-inu32m-, re#wblockedri>te
s=    -1% rcst_13eg = ioarith.constantn -sdense<im4194304pl>if : y=tensor<4xno4rmxal32 txesit-32co, n#veblockedrg>en
ce    =f%alcst_14se =  tarith.constantop -ddense<ow126n=>tr : uetensor<},4 cxse4x, 32syxmbiol32-d, ce#, blockeden>ab
le    -l%incst_15e- = inarith.constantfo , dense<co2>n : vetensor<rt4-bxui4xlt32inx-fiun32c-, to#-lblockedlv>m{
    ft%z=cst_16tr = uearith.constant}) "dense<,
21      disable_threading>:  : false,tensor<
      4verify_eachx: true4
    }x
  }32
#-}x
i32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, /tmp/torchinductor_root/73/c73qgnlowaexhmzz5sy72q2odkom2b2djtaa4uxsom2r6zxnjuyg.py:18:0#: ttgerror: Failures have been detected while processing an MLIR pass pipeline.
slice<{dim = 2, parent = #blocked4}>/tmp/torchinductor_root/73/c73qgnlowaexhmzz5sy72q2odkom2b2djtaa4uxsom2r6zxnjuyg.py:18:0: >note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`

    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>> -> tensor<64x1xi32, #blocked6>
      %45 = arith.extsi %44 : tensor<64x1xi32, #blocked6> to tensor<64x1xi64, #blocked6>
      %46 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>> -> tensor<1x32xi32, #blocked6>
      %47 = tt.splat %arg10 : i32 -> tensor<1x32xi32, #blocked6>
      %48 = arith.muli %46, %47 : tensor<1x32xi32, #blocked6>
      %49 = arith.extsi %48 : tensor<1x32xi32, #blocked6> to tensor<1x32xi64, #[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     a_ptr,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_ptr,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     c_ptr,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_scales_ptr,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     M,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     N,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     K,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_am,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ak,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bk,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bn,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ck,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cm,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cn,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsb,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsn,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsk,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Meta-parameters
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_M: tl.constexpr,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_N: tl.constexpr,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_K: tl.constexpr,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GROUP_SIZE_M: tl.constexpr,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     NUM_KSPLIT: tl.constexpr,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     EVEN_K: tl.constexpr,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GRID_MN: tl.constexpr,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     cache_modifier: tl.constexpr,
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] ):
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """Kernel for computing the matmul C = A x B.
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A_scales and B_scales are in e8m0 format.
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ab > 0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_am > 0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ak > 0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bb > 0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bk > 0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bn > 0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cb > 0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cm > 0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cn > 0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsb > 0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsk > 0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsn > 0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # -----------------------------------------------------------
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Map program ids `pid` to the block of C it should compute.
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # This is done in a grouped ordering to promote L2 data reuse.
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.program_id(axis=0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_unified = tl.program_id(axis=1)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_k = pid_unified % NUM_KSPLIT
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid = pid_unified // NUM_KSPLIT
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if NUM_KSPLIT == 1:
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         remap_xcd(pid, GRID_MN)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     else:
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m = pid // num_pid_n
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_n = pid % num_pid_n
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_batch >= 0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_m >= 0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_n >= 0)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # We assume 32 elements along K share the same scale.
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for first block of A and B input matrices
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         a_ptrs = a_ptr + (
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_ab
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_am[:, None] * stride_am
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split_bf16[None, :] * stride_ak
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_ptrs = b_ptr + (
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_bb
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split[:, None] * stride_bk
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[None, :] * stride_bn
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for the first block of A and B scales
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # B scales are N x K even though B operand is K x N.
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_scale_ptrs = (
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales_ptr
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_bsb
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[:, None] * stride_bsn
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_ks[None, :] * stride_bsk
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales = tl.load(b_scale_ptrs)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # If it is out of bounds, set it to 0.
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             if EVEN_K:
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(a_ptrs)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             else:
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Advance the ptrs to the next K block.
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c = accumulator.to(c_ptr.type.element_ty)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Write back the block of the output matrix C with masks.
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_ptrs = (
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             c_ptr
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_cb
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cm * offs_cm[:, None]
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cn * offs_cn[None, :]
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_k * stride_ck
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         tl.store(c_ptrs, c, mask=c_mask)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 3, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 96, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Traceback (most recent call last):
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     next_module = compile_ir(module, metadata)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pm.run(mod)
[rank3]:E1106 11:04:58.446000 211 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] RuntimeError: PassManager::run failed
blocked6>
      %50 = tt.broadcast %45 : tensor<64x1xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %51 = tt.broadcast %49 : tensor<1x32xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %52 = arith.addi %50, %51 : tensor<64x32xi64, #blocked6>
      %53 = tt.addptr %arg1, %42 : !tt.ptr<i8>, i64
      %54 = arith.extsi %arg14 : i32 to i64
      %55 = arith.muli %7, %54 : i64
      %56 = tt.addptr %arg3, %55 : !tt.ptr<i8>, i64
      %57 = tt.expand_dims %41 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<32x1xi32, #linear1>
      %58 = tt.splat %arg15 : i32 -> tensor<32x1xi32, #linear1>
      %59 = arith.muli %57, %58 : tensor<32x1xi32, #linear1>
      %60 = arith.extsi %59 : tensor<32x1xi32, #linear1> to tensor<32x1xi64, #linear1>
      %61 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>>
      %62 = tt.broadcast %60 : tensor<32x1xi64, #linear1> -> tensor<32x4xi64, #linear1>
      %63 = tt.expand_dims %61 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>> -> tensor<1x4xi32, #linear1>
      %64 = tt.broadcast %63 : tensor<1x4xi32, #linear1> -> tensor<32x4xi32, #linear1>
      %65 = arith.extsi %64 : tensor<32x4xi32, #linear1> to tensor<32x4xi64, #linear1>
      %66 = arith.addi %65, %62 : tensor<32x4xi64, #linear1>
      %67 = tt.splat %56 : !tt.ptr<i8> -> tensor<32x4x!tt.ptr<i8>, #linear1>
      %68 = tt.addptr %67, %66 : tensor<32x4x!tt.ptr<i8>, #linear1>, tensor<32x4xi64, #linear1>
      %69 = tt.load %68 : tensor<32x4x!tt.ptr<i8>, #linear1>
      %70 = tt.trans %69 {order = array<i32: 1, 0>} : tensor<32x4xi8, #linear1> -> tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %71 = tt.splat %29 : !tt.ptr<bf16> -> tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %72 = tt.addptr %71, %28 : tensor<4x128x!tt.ptr<bf16>, #blocked1>, tensor<4x128xi64, #blocked1>
      %73 = tt.load %72 : tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %74 = tt.splat %53 : !tt.ptr<i8> -> tensor<64x32x!tt.ptr<i8>, #blocked6>
      %75 = tt.addptr %74, %52 : tensor<64x32x!tt.ptr<i8>, #blocked6>, tensor<64x32xi64, #blocked6>
      %76 = tt.load %75 cacheModifier = cg : tensor<64x32x!tt.ptr<i8>, #blocked6>
      %77 = ttg.convert_layout %76 : tensor<64x32xi8, #blocked6> -> tensor<64x32xi8, #blocked3>
      %78 = tt.reshape %73 : tensor<4x128xbf16, #blocked1> -> tensor<4x4x32xbf16, #blocked>
      %79 = math.absf %78 : tensor<4x4x32xbf16, #blocked>
      %80 = arith.extf %79 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %81 = "tt.reduce"(%80) <{axis = 2 : i32}> ({
      ^bb0(%arg16: f32, %arg17: f32):
        %209 = arith.maxnumf %arg16, %arg17 : f32
        tt.reduce.return %209 : f32
      }) : (tensor<4x4x32xf32, #blocked>) -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>>
      %82 = ttg.convert_layout %81 : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>>
      %83 = tt.expand_dims %82 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>> -> tensor<4x4x1xf32, #linear>
      %84 = tt.expand_dims %81 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xf32, #blocked>
      %85 = tt.bitcast %83 : tensor<4x4x1xf32, #linear> -> tensor<4x4x1xi32, #linear>
      %86 = tt.bitcast %84 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %87 = arith.addi %85, %cst_28 : tensor<4x4x1xi32, #linear>
      %88 = arith.addi %86, %cst_1 : tensor<4x4x1xi32, #blocked>
      %89 = tt.bitcast %87 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xi32, #linear>
      %90 = tt.bitcast %88 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %91 = arith.andi %89, %cst_27 : tensor<4x4x1xi32, #linear>
      %92 = arith.andi %90, %cst_4 : tensor<4x4x1xi32, #blocked>
      %93 = tt.bitcast %91 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xf32, #linear>
      %94 = tt.bitcast %92 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xf32, #blocked>
      %95 = math.log2 %93 : tensor<4x4x1xf32, #linear>
      %96 = math.log2 %94 : tensor<4x4x1xf32, #blocked>
      %97 = math.floor %95 : tensor<4x4x1xf32, #linear>
      %98 = math.floor %96 : tensor<4x4x1xf32, #blocked>
      %99 = arith.subf %97, %cst_26 : tensor<4x4x1xf32, #linear>
      %100 = arith.subf %98, %cst_5 : tensor<4x4x1xf32, #blocked>
      %101 = tt.clampf %99, %cst_25, %cst_24, propagateNan = none : tensor<4x4x1xf32, #linear>
      %102 = tt.clampf %100, %cst_18, %cst_23, propagateNan = none : tensor<4x4x1xf32, #blocked>
      %103 = arith.fptoui %101 : tensor<4x4x1xf32, #linear> to tensor<4x4x1xi8, #linear>
      %104 = arith.fptoui %102 : tensor<4x4x1xf32, #blocked> to tensor<4x4x1xi8, #blocked>
      %105 = arith.addi %103, %cst_29 : tensor<4x4x1xi8, #linear>
      %106 = arith.addi %104, %cst : tensor<4x4x1xi8, #blocked>
      %107 = arith.subf %cst_6, %102 : tensor<4x4x1xf32, #blocked>
      %108 = math.exp2 %107 : tensor<4x4x1xf32, #blocked>
      %109 = arith.extf %78 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %110 = tt.broadcast %108 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x32xf32, #blocked>
      %111 = arith.mulf %109, %110 : tensor<4x4x32xf32, #blocked>
      %112 = tt.bitcast %111 : tensor<4x4x32xf32, #blocked> -> tensor<4x4x32xi32, #blocked>
      %113 = arith.andi %112, %cst_7 : tensor<4x4x32xi32, #blocked>
      %114 = arith.shrui %112, %cst_8 : tensor<4x4x32xi32, #blocked>
      %115 = arith.andi %114, %cst_9 : tensor<4x4x32xi32, #blocked>
      %116 = arith.andi %112, %cst_10 : tensor<4x4x32xi32, #blocked>
      %117 = arith.addi %115, %cst_11 : tensor<4x4x32xi32, #blocked>
      %118 = arith.subi %cst_12, %117 : tensor<4x4x32xi32, #blocked>
      %119 = arith.cmpi ult, %115, %cst_12 : tensor<4x4x32xi32, #blocked>
      %120 = arith.shrui %116, %cst_11 : tensor<4x4x32xi32, #blocked>
      %121 = arith.ori %120, %cst_13 : tensor<4x4x32xi32, #blocked>
      %122 = arith.shrui %121, %118 : tensor<4x4x32xi32, #blocked>
      %123 = arith.select %119, %122, %116 : tensor<4x4x32xi1, #blocked>, tensor<4x4x32xi32, #blocked>
      %124 = arith.maxui %115, %cst_14 : tensor<4x4x32xi32, #blocked>
      %125 = arith.subi %124, %cst_14 : tensor<4x4x32xi32, #blocked>
      %126 = arith.shli %125, %cst_15 : tensor<4x4x32xi32, #blocked>
      %127 = arith.shrui %123, %cst_16 : tensor<4x4x32xi32, #blocked>
      %128 = arith.ori %126, %127 : tensor<4x4x32xi32, #blocked>
      %129 = arith.addi %128, %cst_11 : tensor<4x4x32xi32, #blocked>
      %130 = arith.shrui %129, %cst_11 : tensor<4x4x32xi32, #blocked>
      %131 = arith.minui %130, %cst_22 : tensor<4x4x32xi32, #blocked>
      %132 = arith.shrui %113, %cst_17 : tensor<4x4x32xi32, #blocked>
      %133 = arith.ori %132, %131 : tensor<4x4x32xi32, #blocked>
      %134 = arith.trunci %133 : tensor<4x4x32xi32, #blocked> to tensor<4x4x32xi8, #blocked>
      %135 = tt.reshape %134 : tensor<4x4x32xi8, #blocked> -> tensor<4x4x16x2xi8, #blocked7>
      %outLHS, %outRHS = tt.split %135 : tensor<4x4x16x2xi8, #blocked7> -> tensor<4x4x16xi8, #blocked2>
      %136 = arith.shli %outRHS, %cst_2 : tensor<4x4x16xi8, #blocked2>
      %137 = arith.ori %outLHS, %136 : tensor<4x4x16xi8, #blocked2>
      %138 = tt.reshape %137 : tensor<4x4x16xi8, #blocked2> -> tensor<4x64xi8, #blocked8>
      %139 = tt.reshape %105 : tensor<4x4x1xi8, #linear> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %140 = tt.reshape %106 : tensor<4x4x1xi8, #blocked> -> tensor<4x4xi8, #linear2>
      %141 = ttg.convert_layout %140 : tensor<4x4xi8, #linear2> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %142 = arith.extui %141 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>> to tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %143 = arith.shli %142, %cst_30 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %144 = tt.bitcast %143 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %145 = tt.expand_dims %144 {axis = 2 : i32} : tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xbf16, #blocked>
      %146 = tt.broadcast %145 : tensor<4x4x1xbf16, #blocked> -> tensor<4x4x32xbf16, #blocked>
      %147 = tt.reshape %146 : tensor<4x4x32xbf16, #blocked> -> tensor<4x128xbf16, #blocked1>
      %148 = amdgpu.scaled_upcast_fp4 %138 scale %147 {axis = 1 : i32} : tensor<4x64xi8, #blocked8>, tensor<4x128xbf16, #blocked1> -> tensor<4x128xbf16, #blocked1>
      %149 = arith.cmpi eq, %139, %cst_31 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %150 = tt.expand_dims %149 {axis = 2 : i32} : tensor<4x4xi1, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xi1, #blocked>
      %151 = tt.broadcast %150 : tensor<4x4x1xi1, #blocked> -> tensor<4x4x32xi1, #blocked>
      %152 = tt.reshape %151 : tensor<4x4x32xi1, #blocked> -> tensor<4x128xi1, #blocked1>
      %153 = arith.select %152, %cst_0, %148 : tensor<4x128xi1, #blocked1>, tensor<4x128xbf16, #blocked1>
      %154 = ttg.local_alloc %153 : (tensor<4x128xbf16, #blocked1>) -> !ttg.memdesc<4x128xbf16, #shared, #smem>
      %155 = ttg.local_load %154 : !ttg.memdesc<4x128xbf16, #shared, #smem> -> tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>>
      %156 = arith.extui %70 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>> to tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %157 = arith.shli %156, %cst_19 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %158 = tt.bitcast %157 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %159 = tt.expand_dims %158 {axis = 2 : i32} : tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xbf16, #blocked4>
      %160 = tt.broadcast %159 : tensor<4x32x1xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked4>
      %161 = tt.trans %160 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked9>
      %162 = tt.reshape %161 : tensor<4x32x32xbf16, #blocked9> -> tensor<128x32xbf16, #blocked5>
      %163 = amdgpu.scaled_upcast_fp4 %77 scale %162 {axis = 0 : i32} : tensor<64x32xi8, #blocked3>, tensor<128x32xbf16, #blocked5> -> tensor<128x32xbf16, #blocked5>
      %164 = arith.cmpi eq, %70, %cst_20 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %165 = tt.expand_dims %164 {axis = 2 : i32} : tensor<4x32xi1, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xi1, #blocked4>
      %166 = tt.broadcast %165 : tensor<4x32x1xi1, #blocked4> -> tensor<4x32x32xi1, #blocked4>
      %167 = tt.trans %166 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xi1, #blocked4> -> tensor<4x32x32xi1, #blocked9>
      %168 = tt.reshape %167 : tensor<4x32x32xi1, #blocked9> -> tensor<128x32xi1, #blocked5>
      %169 = arith.select %168, %cst_21, %163 : tensor<128x32xi1, #blocked5>, tensor<128x32xbf16, #blocked5>
      %170 = ttg.local_alloc %169 : (tensor<128x32xbf16, #blocked5>) -> !ttg.memdesc<128x32xbf16, #shared, #smem>
      %171 = ttg.local_load %170 : !ttg.memdesc<128x32xbf16, #shared, #smem> -> tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>
      %172 = tt.dot %155, %171, %cst_3 : tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>> * tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> -> tensor<4x32xf32, #blocked3>
      %173 = arith.addf %172, %cst_3 : tensor<4x32xf32, #blocked3>
      %174 = arith.truncf %173 : tensor<4x32xf32, #blocked3> to tensor<4x32xbf16, #blocked3>
      %175 = arith.extsi %13 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %176 = arith.extsi %11 : i32 to i64
      %177 = tt.splat %176 : i64 -> tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %178 = arith.addi %177, %175 : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %179 = arith.extsi %32 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> to tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %180 = arith.extsi %30 : i32 to i64
      %181 = tt.splat %180 : i64 -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %182 = arith.addi %181, %179 : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %183 = arith.muli %7, %6 : i64
      %184 = tt.addptr %arg2, %183 : !tt.ptr<bf16>, i64
      %185 = tt.expand_dims %178 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %186 = arith.extsi %arg13 : i32 to i64
      %187 = tt.expand_dims %175 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %188 = arith.muli %186, %176 : i64
      %189 = tt.splat %186 : i64 -> tensor<4x1xi64, #blocked3>
      %190 = arith.muli %189, %187 : tensor<4x1xi64, #blocked3>
      %191 = tt.addptr %184, %188 : !tt.ptr<bf16>, i64
      %192 = tt.expand_dims %182 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %193 = tt.broadcast %190 : tensor<4x1xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %194 = tt.expand_dims %179 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %195 = tt.broadcast %194 : tensor<1x32xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %196 = tt.addptr %191, %180 : !tt.ptr<bf16>, i64
      %197 = arith.addi %195, %193 : tensor<4x32xi64, #blocked3>
      %198 = arith.extsi %arg4 : i32 to i64
      %199 = tt.splat %198 : i64 -> tensor<4x1xi64, #blocked3>
      %200 = arith.cmpi slt, %185, %199 : tensor<4x1xi64, #blocked3>
      %201 = arith.extsi %arg5 : i32 to i64
      %202 = tt.splat %201 : i64 -> tensor<1x32xi64, #blocked3>
      %203 = arith.cmpi slt, %192, %202 : tensor<1x32xi64, #blocked3>
      %204 = tt.broadcast %200 : tensor<4x1xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %205 = tt.broadcast %203 : tensor<1x32xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %206 = arith.andi %204, %205 : tensor<4x32xi1, #blocked3>
      %207 = tt.splat %196 : !tt.ptr<bf16> -> tensor<4x32x!tt.ptr<bf16>, #blocked3>
      %208 = tt.addptr %207, %197 : tensor<4x32x!tt.ptr<bf16>, #blocked3>, tensor<4x32xi64, #blocked3>
      tt.store %208, %174, %206 : tensor<4x32x!tt.ptr<bf16>, #blocked3>
    }
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(optimize-amd-lds-usage{lds-limit=0 target-arch=gfx950}, triton-scf-to-cf, convert-index-to-llvm{index-bitwidth=0}, allocate-amdgpu-shared-memory, convert-triton-amdgpu-to-llvm{arch=gfx950 ftz=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-cf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwidth=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info, convert-builtin-func-to-llvm{ftz=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_root/v4/cv42qw44m3jzn5mwwxouo6vmu5tomxgscbyf5eu5tadrozlnzeyn.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_root/v4/cv42qw44m3jzn5mwwxouo6vmu5tomxgscbyf5eu5tadrozlnzeyn.py:18:0: note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     a_ptr,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_ptr,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     c_ptr,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_scales_ptr,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     M,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     N,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     K,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_am,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ak,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bk,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bn,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ck,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cm,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cn,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsb,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsn,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsk,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Meta-parameters
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_M: tl.constexpr,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_N: tl.constexpr,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_K: tl.constexpr,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GROUP_SIZE_M: tl.constexpr,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     NUM_KSPLIT: tl.constexpr,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     EVEN_K: tl.constexpr,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GRID_MN: tl.constexpr,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     cache_modifier: tl.constexpr,
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] ):
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """Kernel for computing the matmul C = A x B.
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A_scales and B_scales are in e8m0 format.
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ab > 0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_am > 0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ak > 0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bb > 0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bk > 0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bn > 0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cb > 0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cm > 0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cn > 0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsb > 0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsk > 0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsn > 0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # -----------------------------------------------------------
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Map program ids `pid` to the block of C it should compute.
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # This is done in a grouped ordering to promote L2 data reuse.
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.program_id(axis=0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_unified = tl.program_id(axis=1)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_k = pid_unified % NUM_KSPLIT
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid = pid_unified // NUM_KSPLIT
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if NUM_KSPLIT == 1:
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         remap_xcd(pid, GRID_MN)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     else:
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m = pid // num_pid_n
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_n = pid % num_pid_n
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_batch >= 0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_m >= 0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_n >= 0)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # We assume 32 elements along K share the same scale.
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for first block of A and B input matrices
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         a_ptrs = a_ptr + (
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_ab
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_am[:, None] * stride_am
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split_bf16[None, :] * stride_ak
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_ptrs = b_ptr + (
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_bb
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split[:, None] * stride_bk
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[None, :] * stride_bn
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for the first block of A and B scales
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # B scales are N x K even though B operand is K x N.
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_scale_ptrs = (
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales_ptr
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_bsb
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[:, None] * stride_bsn
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_ks[None, :] * stride_bsk
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales = tl.load(b_scale_ptrs)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # If it is out of bounds, set it to 0.
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             if EVEN_K:
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(a_ptrs)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             else:
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Advance the ptrs to the next K block.
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c = accumulator.to(c_ptr.type.element_ty)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Write back the block of the output matrix C with masks.
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_ptrs = (
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             c_ptr
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_cb
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cm * offs_cm[:, None]
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cn * offs_cn[None, :]
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_k * stride_ck
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         tl.store(c_ptrs, c, mask=c_mask)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 7, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 96, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Traceback (most recent call last):
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     next_module = compile_ir(module, metadata)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pm.run(mod)
[rank7]:E1106 11:04:58.450000 215 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] RuntimeError: PassManager::run failed
ler_TP2: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
#blocked = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_batched_gemm_afp4_wfp4_pre_quant_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg4: i32, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32) attributes {noinline = false} {
    %cst = arith.constant dense<127> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128xbf16, #blocked1>
    %cst_1 = arith.constant dense<2097152> : tensor<4x4x1xi32, #blocked>
    %cst_2 = arith.constant dense<4> : tensor<4x4x16xi8, #blocked2>
    %c31_i32 = arith.constant 31 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x32xf32, #blocked3>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %true = arith.constant true
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #blocked>
    %cst_5 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_7 = arith.constant dense<-2147483648> : tensor<4x4x32xi32, #blocked>
    %cst_8 = arith.constant dense<23> : tensor<4x4x32xi32, #blocked>
    %cst_9 = arith.constant dense<255> : tensor<4x4x32xi32, #blocked>
    %cst_10 = arith.constant dense<8388607> : tensor<4x4x32xi32, #blocked>
    %cst_11 = arith.constant dense<1> : tensor<4x4x32xi32, #blocked>
    %cst_12 = arith.constant dense<127> : tensor<4x4x32xi32, #blocked>
    %cst_13 = arith.constant dense<4194304> : tensor<4x4x32xi32, #blocked>
    %cst_14 = arith.constant dense<126> : tensor<4x4x32xi32, #blocked>
    %cst_15 = arith.constant dense<2> : tensor<4x4x32xi32, #blocked>
    %cst_16 = arith.constant dense<21> : tensor<4x4x32xi32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>> -> tensor<64x1xi32, #blocked6>
      %45 = arith.extsi %44 : tensor<64x1xi32, #blocked6> to tensor<64x1xi64, #blocked6>
      %46 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>> -> tensor<1x32xi32, #blocked6>
      %47 = tt.splat %arg10 : i32 -> tensor<1x32xi32, #blocked6>
      %48 = arith.muli %46, %47 : tensor<1x32xi32, #blocked6>
      %49 = arith.extsi %48 : tensor<1x32xi32, #blocked6> to tensor<1x32xi64, #blocked6>
      %50 = tt.broadcast %45 : tensor<64x1xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %51 = tt.broadcast %49 : tensor<1x32xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %52 = arith.addi %50, %51 : tensor<64x32xi64, #blocked6>
      %53 = tt.addptr %arg1, %42 : !tt.ptr<i8>, i64
      %54 = arith.extsi %arg14 : i32 to i64
      %55 = arith.muli %7, %54 : i64
      %56 = tt.addptr %arg3, %55 : !tt.ptr<i8>, i64
      %57 = tt.expand_dims %41 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<32x1xi32, #linear1>
      %58 = tt.splat %arg15 : i32 -> tensor<32x1xi32, #linear1>
      %59 = arith.muli %57, %58 : tensor<32x1xi32, #linear1>
      %60 = arith.extsi %59 : tensor<32x1xi32, #linear1> to tensor<32x1xi64, #linear1>
      %61 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>>
      %62 = tt.broadcast %60 : tensor<32x1xi64, #linear1> -> tensor<32x4xi64, #linear1>
      %63 = tt.expand_dims %61 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>> -> tensor<1x4xi32, #linear1>
      %64 = tt.broadcast %63 : tensor<1x4xi32, #linear1> -> tensor<32x4xi32, #linear1>
      %65 = arith.extsi %64 : tensor<32x4xi32, #linear1> to tensor<32x4xi64, #linear1>
      %66 = arith.addi %65, %62 : tensor<32x4xi64, #linear1>
      %67 = tt.splat %56 : !tt.ptr<i8> -> tensor<32x4x!tt.ptr<i8>, #linear1>
      %68 = tt.addptr %67, %66 : tensor<32x4x!tt.ptr<i8>, #linear1>, tensor<32x4xi64, #linear1>
      %69 = tt.load %68 : tensor<32x4x!tt.ptr<i8>, #linear1>
      %70 = tt.trans %69 {order = array<i32: 1, 0>} : tensor<32x4xi8, #linear1> -> tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %71 = tt.splat %29 : !tt.ptr<bf16> -> tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %72 = tt.addptr %71, %28 : tensor<4x128x!tt.ptr<bf16>, #blocked1>, tensor<4x128xi64, #blocked1>
      %73 = tt.load %72 : tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %74 = tt.splat %53 : !tt.ptr<i8> -> tensor<64x32x!tt.ptr<i8>, #blocked6>
      %75 = tt.addptr %74, %52 : tensor<64x32x!tt.ptr<i8>, #blocked6>, tensor<64x32xi64, #blocked6>
      %76 = tt.load %75 cacheModifier = cg : tensor<64x32x!tt.ptr<i8>, #blocked6>
      %77 = ttg.convert_layout %76 : tensor<64x32xi8, #blocked6> -> tensor<64x32xi8, #blocked3>
      %78 = tt.reshape %73 : tensor<4x128xbf16, #blocked1> -> tensor<4x4x32xbf16, #blocked>
      %79 = math.absf %78 : tensor<4x4x32xbf16, #blocked>
      %80 = arith.extf %79 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %81 = "tt.reduce"(%80) <{axis = 2 : i32}> ({
      ^bb0(%arg16: f32, %arg17: f32):
        %209 = arith.maxnumf %arg16, %arg17 : f32
        tt.reduce.return %209 : f32
      }) : (tensor<4x4x32xf32, #blocked>) -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>>
      %82 = ttg.convert_layout %81 : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>>
      %83 = tt.expand_dims %82 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>> -> tensor<4x4x1xf32, #linear>
      %84 = tt.expand_dims %81 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xf32, #blocked>
      %85 = tt.bitcast %83 : tensor<4x4x1xf32, #linear> -> tensor<4x4x1xi32, #linear>
      %86 = tt.bitcast %84 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %87 = arith.addi %85, %cst_28 : tensor<4x4x1xi32, #linear>
      %88 = arith.addi %86, %cst_1 : tensor<4x4x1xi32, #blocked>
      %89 = tt.bitcast %87 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xi32, #linear>
      %90 = tt.bitcast %88 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %91 = arith.andi %89, %cst_27 : tensor<4x4x1xi32, #linear>
      %92 = arith.andi %90, %cst_4 : tensor<4x4x1xi32, #blocked>
      %93 = tt.bitcast %91 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xf32, #linear>
      %94 = tt.bitcast %92 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xf32, #blocked>
      %95 = math.log2 %93 : tensor<4x4x1xf32, #linear>
      %96 = math.log2 %94 : tensor<4x4x1xf32, #blocked>
      %97 = math.floor %95 : tensor<4x4x1xf32, #linear>
      %98 = math.floor %96 : tensor<4x4x1xf32, #blocked>
      %99 = arith.subf %97, %cst_26 : tensor<4x4x1xf32, #linear>
      %100 = arith.subf %98, %cst_5 : tensor<4x4x1xf32, #blocked>
      %101 = tt.clampf %99, %cst_25, %cst_24, propagateNan = none : tensor<4x4x1xf32, #linear>
      %102 = tt.clampf %100, %cst_18, %cst_23, propagateNan = none : tensor<4x4x1xf32, #blocked>
      %103 = arith.fptoui %101 : tensor<4x4x1xf32, #linear> to tensor<4x4x1xi8, #linear>
      %104 = arith.fptoui %102 : tensor<4x4x1xf32, #blocked> to tensor<4x4x1xi8, #blocked>
      %105 = arith.addi %103, %cst_29 : tensor<4x4x1xi8, #linear>
      %106 = arith.addi %104, %cst : tensor<4x4x1xi8, #blocked>
      %107 = arith.subf %cst_6, %102 : tensor<4x4x1xf32, #blocked>
      %108 = math.exp2 %107 : tensor<4x4x1xf32, #blocked>
      %109 = arith.extf %78 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %110 = tt.broadcast %108 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x32xf32, #blocked>
      %111 = arith.mulf %109, %110 : tensor<4x4x32xf32, #blocked>
      %112 = tt.bitcast %111 : tensor<4x4x32xf32, #blocked> -> tensor<4x4x32xi32, #blocked>
      %113 = arith.andi %112, %cst_7 : tensor<4x4x32xi32, #blocked>
      %114 = arith.shrui %112, %cst_8 : tensor<4x4x32xi32, #blocked>
      %115 = arith.andi %114, %cst_9 : tensor<4x4x32xi32, #blocked>
      %116 = arith.andi %112, %cst_10 : tensor<4x4x32xi32, #blocked>
      %117 = arith.addi %115, %cst_11 : tensor<4x4x32xi32, #blocked>
      %118 = arith.subi %cst_12, %117 : tensor<4x4x32xi32, #blocked>
      %119 = arith.cmpi ult, %115, %cst_12 : tensor<4x4x32xi32, #blocked>
      %120 = arith.shrui %116, %cst_11 : tensor<4x4x32xi32, #blocked>
      %121 = arith.ori %120, %cst_13 : tensor<4x4x32xi32, #blocked>
      %122 = arith.shrui %121, %118 : tensor<4x4x32xi32, #blocked>
      %123 = arith.select %119, %122, %116 : tensor<4x4x32xi1, #blocked>, tensor<4x4x32xi32, #blocked>
      %124 = arith.maxui %115, %cst_14 : tensor<4x4x32xi32, #blocked>
      %125 = arith.subi %124, %cst_14 : tensor<4x4x32xi32, #blocked>
      %126 = arith.shli %125, %cst_15 : tensor<4x4x32xi32, #blocked>
      %127 = arith.shrui %123, %cst_16 : tensor<4x4x32xi32, #blocked>
      %128 = arith.ori %126, %127 : tensor<4x4x32xi32, #blocked>
      %129 = arith.addi %128, %cst_11 : tensor<4x4x32xi32, #blocked>
      %130 = arith.shrui %129, %cst_11 : tensor<4x4x32xi32, #blocked>
      %131 = arith.minui %130, %cst_22 : tensor<4x4x32xi32, #blocked>
      %132 = arith.shrui %113, %cst_17 : tensor<4x4x32xi32, #blocked>
      %133 = arith.ori %132, %131 : tensor<4x4x32xi32, #blocked>
      %134 = arith.trunci %133 : tensor<4x4x32xi32, #blocked> to tensor<4x4x32xi8, #blocked>
      %135 = tt.reshape %134 : tensor<4x4x32xi8, #blocked> -> tensor<4x4x16x2xi8, #blocked7>
      %outLHS, %outRHS = tt.split %135 : tensor<4x4x16x2xi8, #blocked7> -> tensor<4x4x16xi8, #blocked2>
      %136 = arith.shli %outRHS, %cst_2 : tensor<4x4x16xi8, #blocked2>
      %137 = arith.ori %outLHS, %136 : tensor<4x4x16xi8, #blocked2>
      %138 = tt.reshape %137 : tensor<4x4x16xi8, #blocked2> -> tensor<4x64xi8, #blocked8>
      %139 = tt.reshape %105 : tensor<4x4x1xi8, #linear> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %140 = tt.reshape %106 : tensor<4x4x1xi8, #blocked> -> tensor<4x4xi8, #linear2>
      %141 = ttg.convert_layout %140 : tensor<4x4xi8, #linear2> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %142 = arith.extui %141 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>> to tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %143 = arith.shli %142, %cst_30 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %144 = tt.bitcast %143 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %145 = tt.expand_dims %144 {axis = 2 : i32} : tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xbf16, #blocked>
      %146 = tt.broadcast %145 : tensor<4x4x1xbf16, #blocked> -> tensor<4x4x32xbf16, #blocked>
      %147 = tt.reshape %146 : tensor<4x4x32xbf16, #blocked> -> tensor<4x128xbf16, #blocked1>
      %148 = amdgpu.scaled_upcast_fp4 %138 scale %147 {axis = 1 : i32} : tensor<4x64xi8, #blocked8>, tensor<4x128xbf16, #blocked1> -> tensor<4x128xbf16, #blocked1>
      %149 = arith.cmpi eq, %139, %cst_31 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %150 = tt.expand_dims %149 {axis = 2 : i32} : tensor<4x4xi1, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xi1, #blocked>
      %151 = tt.broadcast %150 : tensor<4x4x1xi1, #blocked> -> tensor<4x4x32xi1, #blocked>
      %152 = tt.reshape %151 : tensor<4x4x32xi1, #blocked> -> tensor<4x128xi1, #blocked1>
      %153 = arith.select %152, %cst_0, %148 : tensor<4x128xi1, #blocked1>, tensor<4x128xbf16, #blocked1>
      %154 = ttg.local_alloc %153 : (tensor<4x128xbf16, #blocked1>) -> !ttg.memdesc<4x128xbf16, #shared, #smem>
      %155 = ttg.local_load %154 : !ttg.memdesc<4x128xbf16, #shared, #smem> -> tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>>
      %156 = arith.extui %70 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>> to tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %157 = arith.shli %156, %cst_19 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %158 = tt.bitcast %157 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %159 = tt.expand_dims %158 {axis = 2 : i32} : tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xbf16, #blocked4>
      %160 = tt.broadcast %159 : tensor<4x32x1xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked4>
      %161 = tt.trans %160 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked9>
      %162 = tt.reshape %161 : tensor<4x32x32xbf16, #blocked9> -> tensor<128x32xbf16, #blocked5>
      %163 = amdgpu.scaled_upcast_fp4 %77 scale %162 {axis = 0 : i32} : tensor<64x32xi8, #blocked3>, tensor<128x32xbf16, #blocked5> -> tensor<128x32xbf16, #blocked5>
      %164 = arith.cmpi eq, %70, %cst_20 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %165 = tt.expand_dims %164 {axis = 2 : i32} : tensor<4x32xi1, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xi1, #blocked4>
      %166 = tt.broadcast %165 : tensor<4x32x1xi1, #blocked4> -> tensor<4x32x32xi1, #blocked4>
      %167 = tt.trans %166 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xi1, #blocked4> -> tensor<4x32x32xi1, #blocked9>
      %168 = tt.reshape %167 : tensor<4x32x32xi1, #blocked9> -> tensor<128x32xi1, #blocked5>
      %169 = arith.select %168, %cst_21, %163 : tensor<128x32xi1, #blocked5>, tensor<128x32xbf16, #blocked5>
      %170 = ttg.local_alloc %169 : (tensor<128x32xbf16, #blocked5>) -> !ttg.memdesc<128x32xbf16, #shared, #smem>
      %171 = ttg.local_load %170 : !ttg.memdesc<128x32xbf16, #shared, #smem> -> tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>
      %172 = tt.dot %155, %171, %cst_3 : tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>> * tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> -> tensor<4x32xf32, #blocked3>
      %173 = arith.addf %172, %cst_3 : tensor<4x32xf32, #blocked3>
      %174 = arith.truncf %173 : tensor<4x32xf32, #blocked3> to tensor<4x32xbf16, #blocked3>
      %175 = arith.extsi %13 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %176 = arith.extsi %11 : i32 to i64
      %177 = tt.splat %176 : i64 -> tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %178 = arith.addi %177, %175 : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %179 = arith.extsi %32 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> to tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %180 = arith.extsi %30 : i32 to i64
      %181 = tt.splat %180 : i64 -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %182 = arith.addi %181, %179 : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %183 = arith.muli %7, %6 : i64
      %184 = tt.addptr %arg2, %183 : !tt.ptr<bf16>, i64
      %185 = tt.expand_dims %178 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %186 = arith.extsi %arg13 : i32 to i64
      %187 = tt.expand_dims %175 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %188 = arith.muli %186, %176 : i64
      %189 = tt.splat %186 : i64 -> tensor<4x1xi64, #blocked3>
      %190 = arith.muli %189, %187 : tensor<4x1xi64, #blocked3>
      %191 = tt.addptr %184, %188 : !tt.ptr<bf16>, i64
      %192 = tt.expand_dims %182 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %193 = tt.broadcast %190 : tensor<4x1xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %194 = tt.expand_dims %179 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %195 = tt.broadcast %194 : tensor<1x32xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %196 = tt.addptr %191, %180 : !tt.ptr<bf16>, i64
      %197 = arith.addi %195, %193 : tensor<4x32xi64, #blocked3>
      %198 = arith.extsi %arg4 : i32 to i64
      %199 = tt.splat %198 : i64 -> tensor<4x1xi64, #blocked3>
      %200 = arith.cmpi slt, %185, %199 : tensor<4x1xi64, #blocked3>
      %201 = arith.extsi %arg5 : i32 to i64
      %202 = tt.splat %201 : i64 -> tensor<1x32xi64, #blocked3>
      %203 = arith.cmpi slt, %192, %202 : tensor<1x32xi64, #blocked3>
      %204 = tt.broadcast %200 : tensor<4x1xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %205 = tt.broadcast %203 : tensor<1x32xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %206 = arith.andi %204, %205 : tensor<4x32xi1, #blocked3>
      %207 = tt.splat %196 : !tt.ptr<bf16> -> tensor<4x32x!tt.ptr<bf16>, #blocked3>
      %208 = tt.addptr %207, %197 : tensor<4x32x!tt.ptr<bf16>, #blocked3>, tensor<4x32xi64, #blocked3>
      tt.store %208, %174, %206 : tensor<4x32x!tt.ptr<bf16>, #blocked3>
    }
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(optimize-amd-lds-usage{lds-limit=0 target-arch=gfx950}, triton-scf-to-cf, convert-index-to-llvm{index-bitwidth=0}, allocate-amdgpu-shared-memory, convert-triton-amdgpu-to-llvm{arch=gfx950 ftz=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-cf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwidth=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info, convert-builtin-func-to-llvm{ftz=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_root/sh/cshgjrhjmxzfzftlibpws6yqfp27nczcxlzzqrpbu7p3o3kaqtod.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_root/sh/cshgjrhjmxzfzftlibpws6yqfp27nczcxlzzqrpbu7p3o3kaqtod.py:18:0: note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     a_ptr,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_ptr,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     c_ptr,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_scales_ptr,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     M,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     N,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     K,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_am,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ak,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bk,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bn,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ck,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cm,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cn,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsb,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsn,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsk,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Meta-parameters
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_M: tl.constexpr,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_N: tl.constexpr,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_K: tl.constexpr,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GROUP_SIZE_M: tl.constexpr,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     NUM_KSPLIT: tl.constexpr,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     EVEN_K: tl.constexpr,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GRID_MN: tl.constexpr,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     cache_modifier: tl.constexpr,
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] ):
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """Kernel for computing the matmul C = A x B.
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A_scales and B_scales are in e8m0 format.
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ab > 0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_am > 0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ak > 0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bb > 0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bk > 0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bn > 0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cb > 0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cm > 0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cn > 0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsb > 0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsk > 0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsn > 0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # -----------------------------------------------------------
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Map program ids `pid` to the block of C it should compute.
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # This is done in a grouped ordering to promote L2 data reuse.
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.program_id(axis=0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_unified = tl.program_id(axis=1)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_k = pid_unified % NUM_KSPLIT
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid = pid_unified // NUM_KSPLIT
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if NUM_KSPLIT == 1:
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         remap_xcd(pid, GRID_MN)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     else:
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m = pid // num_pid_n
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_n = pid % num_pid_n
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_batch >= 0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_m >= 0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_n >= 0)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # We assume 32 elements along K share the same scale.
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for first block of A and B input matrices
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         a_ptrs = a_ptr + (
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_ab
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_am[:, None] * stride_am
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split_bf16[None, :] * stride_ak
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_ptrs = b_ptr + (
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_bb
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split[:, None] * stride_bk
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[None, :] * stride_bn
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for the first block of A and B scales
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # B scales are N x K even though B operand is K x N.
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_scale_ptrs = (
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales_ptr
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_bsb
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[:, None] * stride_bsn
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_ks[None, :] * stride_bsk
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales = tl.load(b_scale_ptrs)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # If it is out of bounds, set it to 0.
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             if EVEN_K:
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(a_ptrs)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             else:
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Advance the ptrs to the next K block.
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c = accumulator.to(c_ptr.type.element_ty)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Write back the block of the output matrix C with masks.
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_ptrs = (
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             c_ptr
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_cb
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cm * offs_cm[:, None]
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cn * offs_cn[None, :]
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_k * stride_ck
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         tl.store(c_ptrs, c, mask=c_mask)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 2, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 96, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Traceback (most recent call last):
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     next_module = compile_ir(module, metadata)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pm.run(mod)
[rank2]:E1106 11:04:58.481000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] RuntimeError: PassManager::run failed
ler_TP0: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
#blocked = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_batched_gemm_afp4_wfp4_pre_quant_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg4: i32, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32) attributes {noinline = false} {
    %cst = arith.constant dense<127> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128xbf16, #blocked1>
    %cst_1 = arith.constant dense<2097152> : tensor<4x4x1xi32, #blocked>
    %cst_2 = arith.constant dense<4> : tensor<4x4x16xi8, #blocked2>
    %c31_i32 = arith.constant 31 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x32xf32, #blocked3>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %true = arith.constant true
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #blocked>
    %cst_5 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_7 = arith.constant dense<-2147483648> : tensor<4x4x32xi32, #blocked>
    %cst_8 = arith.constant dense<23> : tensor<4x4x32xi32, #blocked>
    %cst_9 = arith.constant dense<255> : tensor<4x4x32xi32, #blocked>
    %cst_10 = arith.constant dense<8388607> : tensor<4x4x32xi32, #blocked>
    %cst_11 = arith.constant dense<1> : tensor<4x4x32xi32, #blocked>
    %cst_12 = arith.constant dense<127> : tensor<4x4x32xi32, #blocked>
    %cst_13 = arith.constant dense<4194304> : tensor<4x4x32xi32, #blocked>
    %cst_14 = arith.constant dense<126> : tensor<4x4x32xi32, #blocked>
    %cst_15 = arith.constant dense<2> : tensor<4x4x32xi32, #blocked>
    %cst_16 = arith.constant dense<21> : tensor<4x4x32xi32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>> -> tensor<64x1xi32, #blocked6>
      %45 = arith.extsi %44 : tensor<64x1xi32, #blocked6> to tensor<64x1xi64, #blocked6>
      %46 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>> -> tensor<1x32xi32, #blocked6>
      %47 = tt.splat %arg10 : i32 -> tensor<1x32xi32, #blocked6>
      %48 = arith.muli %46, %47 : tensor<1x32xi32, #blocked6>
      %49 = arith.extsi %48 : tensor<1x32xi32, #blocked6> to tensor<1x32xi64, #blocked6>
      %50 = tt.broadcast %45 : tensor<64x1xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %51 = tt.broadcast %49 : tensor<1x32xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %52 = arith.addi %50, %51 : tensor<64x32xi64, #blocked6>
      %53 = tt.addptr %arg1, %42 : !tt.ptr<i8>, i64
      %54 = arith.extsi %arg14 : i32 to i64
      %55 = arith.muli %7, %54 : i64
      %56 = tt.addptr %arg3, %55 : !tt.ptr<i8>, i64
      %57 = tt.expand_dims %41 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<32x1xi32, #linear1>
      %58 = tt.splat %arg15 : i32 -> tensor<32x1xi32, #linear1>
      %59 = arith.muli %57, %58 : tensor<32x1xi32, #linear1>
      %60 = arith.extsi %59 : tensor<32x1xi32, #linear1> to tensor<32x1xi64, #linear1>
      %61 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>>
      %62 = tt.broadcast %60 : tensor<32x1xi64, #linear1> -> tensor<32x4xi64, #linear1>
      %63 = tt.expand_dims %61 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>> -> tensor<1x4xi32, #linear1>
      %64 = tt.broadcast %63 : tensor<1x4xi32, #linear1> -> tensor<32x4xi32, #linear1>
      %65 = arith.extsi %64 : tensor<32x4xi32, #linear1> to tensor<32x4xi64, #linear1>
      %66 = arith.addi %65, %62 : tensor<32x4xi64, #linear1>
      %67 = tt.splat %56 : !tt.ptr<i8> -> tensor<32x4x!tt.ptr<i8>, #linear1>
      %68 = tt.addptr %67, %66 : tensor<32x4x!tt.ptr<i8>, #linear1>, tensor<32x4xi64, #linear1>
      %69 = tt.load %68 : tensor<32x4x!tt.ptr<i8>, #linear1>
      %70 = tt.trans %69 {order = array<i32: 1, 0>} : tensor<32x4xi8, #linear1> -> tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %71 = tt.splat %29 : !tt.ptr<bf16> -> tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %72 = tt.addptr %71, %28 : tensor<4x128x!tt.ptr<bf16>, #blocked1>, tensor<4x128xi64, #blocked1>
      %73 = tt.load %72 : tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %74 = tt.splat %53 : !tt.ptr<i8> -> tensor<64x32x!tt.ptr<i8>, #blocked6>
      %75 = tt.addptr %74, %52 : tensor<64x32x!tt.ptr<i8>, #blocked6>, tensor<64x32xi64, #blocked6>
      %76 = tt.load %75 cacheModifier = cg : tensor<64x32x!tt.ptr<i8>, #blocked6>
      %77 = ttg.convert_layout %76 : tensor<64x32xi8, #blocked6> -> tensor<64x32xi8, #blocked3>
      %78 = tt.reshape %73 : tensor<4x128xbf16, #blocked1> -> tensor<4x4x32xbf16, #blocked>
      %79 = math.absf %78 : tensor<4x4x32xbf16, #blocked>
      %80 = arith.extf %79 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %81 = "tt.reduce"(%80) <{axis = 2 : i32}> ({
      ^bb0(%arg16: f32, %arg17: f32):
        %209 = arith.maxnumf %arg16, %arg17 : f32
        tt.reduce.return %209 : f32
      }) : (tensor<4x4x32xf32, #blocked>) -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>>
      %82 = ttg.convert_layout %81 : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>>
      %83 = tt.expand_dims %82 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>> -> tensor<4x4x1xf32, #linear>
      %84 = tt.expand_dims %81 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xf32, #blocked>
      %85 = tt.bitcast %83 : tensor<4x4x1xf32, #linear> -> tensor<4x4x1xi32, #linear>
      %86 = tt.bitcast %84 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %87 = arith.addi %85, %cst_28 : tensor<4x4x1xi32, #linear>
      %88 = arith.addi %86, %cst_1 : tensor<4x4x1xi32, #blocked>
      %89 = tt.bitcast %87 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xi32, #linear>
      %90 = tt.bitcast %88 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %91 = arith.andi %89, %cst_27 : tensor<4x4x1xi32, #linear>
      %92 = arith.andi %90, %cst_4 : tensor<4x4x1xi32, #blocked>
      %93 = tt.bitcast %91 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xf32, #linear>
      %94 = tt.bitcast %92 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xf32, #blocked>
      %95 = math.log2 %93 : tensor<4x4x1xf32, #linear>
      %96 = math.log2 %94 : tensor<4x4x1xf32, #blocked>
      %97 = math.floor %95 : tensor<4x4x1xf32, #linear>
      %98 = math.floor %96 : tensor<4x4x1xf32, #blocked>
      %99 = arith.subf %97, %cst_26 : tensor<4x4x1xf32, #linear>
      %100 = arith.subf %98, %cst_5 : tensor<4x4x1xf32, #blocked>
      %101 = tt.clampf %99, %cst_25, %cst_24, propagateNan = none : tensor<4x4x1xf32, #linear>
      %102 = tt.clampf %100, %cst_18, %cst_23, propagateNan = none : tensor<4x4x1xf32, #blocked>
      %103 = arith.fptoui %101 : tensor<4x4x1xf32, #linear> to tensor<4x4x1xi8, #linear>
      %104 = arith.fptoui %102 : tensor<4x4x1xf32, #blocked> to tensor<4x4x1xi8, #blocked>
      %105 = arith.addi %103, %cst_29 : tensor<4x4x1xi8, #linear>
      %106 = arith.addi %104, %cst : tensor<4x4x1xi8, #blocked>
      %107 = arith.subf %cst_6, %102 : tensor<4x4x1xf32, #blocked>
      %108 = math.exp2 %107 : tensor<4x4x1xf32, #blocked>
      %109 = arith.extf %78 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %110 = tt.broadcast %108 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x32xf32, #blocked>
      %111 = arith.mulf %109, %110 : tensor<4x4x32xf32, #blocked>
      %112 = tt.bitcast %111 : tensor<4x4x32xf32, #blocked> -> tensor<4x4x32xi32, #blocked>
      %113 = arith.andi %112, %cst_7 : tensor<4x4x32xi32, #blocked>
      %114 = arith.shrui %112, %cst_8 : tensor<4x4x32xi32, #blocked>
      %115 = arith.andi %114, %cst_9 : tensor<4x4x32xi32, #blocked>
      %116 = arith.andi %112, %cst_10 : tensor<4x4x32xi32, #blocked>
      %117 = arith.addi %115, %cst_11 : tensor<4x4x32xi32, #blocked>
      %118 = arith.subi %cst_12, %117 : tensor<4x4x32xi32, #blocked>
      %119 = arith.cmpi ult, %115, %cst_12 : tensor<4x4x32xi32, #blocked>
      %120 = arith.shrui %116, %cst_11 : tensor<4x4x32xi32, #blocked>
      %121 = arith.ori %120, %cst_13 : tensor<4x4x32xi32, #blocked>
      %122 = arith.shrui %121, %118 : tensor<4x4x32xi32, #blocked>
      %123 = arith.select %119, %122, %116 : tensor<4x4x32xi1, #blocked>, tensor<4x4x32xi32, #blocked>
      %124 = arith.maxui %115, %cst_14 : tensor<4x4x32xi32, #blocked>
      %125 = arith.subi %124, %cst_14 : tensor<4x4x32xi32, #blocked>
      %126 = arith.shli %125, %cst_15 : tensor<4x4x32xi32, #blocked>
      %127 = arith.shrui %123, %cst_16 : tensor<4x4x32xi32, #blocked>
      %128 = arith.ori %126, %127 : tensor<4x4x32xi32, #blocked>
      %129 = arith.addi %128, %cst_11 : tensor<4x4x32xi32, #blocked>
      %130 = arith.shrui %129, %cst_11 : tensor<4x4x32xi32, #blocked>
      %131 = arith.minui %130, %cst_22 : tensor<4x4x32xi32, #blocked>
      %132 = arith.shrui %113, %cst_17 : tensor<4x4x32xi32, #blocked>
      %133 = arith.ori %132, %131 : tensor<4x4x32xi32, #blocked>
      %134 = arith.trunci %133 : tensor<4x4x32xi32, #blocked> to tensor<4x4x32xi8, #blocked>
      %135 = tt.reshape %134 : tensor<4x4x32xi8, #blocked> -> tensor<4x4x16x2xi8, #blocked7>
      %outLHS, %outRHS = tt.split %135 : tensor<4x4x16x2xi8, #blocked7> -> tensor<4x4x16xi8, #blocked2>
      %136 = arith.shli %outRHS, %cst_2 : tensor<4x4x16xi8, #blocked2>
      %137 = arith.ori %outLHS, %136 : tensor<4x4x16xi8, #blocked2>
      %138 = tt.reshape %137 : tensor<4x4x16xi8, #blocked2> -> tensor<4x64xi8, #blocked8>
      %139 = tt.reshape %105 : tensor<4x4x1xi8, #linear> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %140 = tt.reshape %106 : tensor<4x4x1xi8, #blocked> -> tensor<4x4xi8, #linear2>
      %141 = ttg.convert_layout %140 : tensor<4x4xi8, #linear2> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %142 = arith.extui %141 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>> to tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %143 = arith.shli %142, %cst_30 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %144 = tt.bitcast %143 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %145 = tt.expand_dims %144 {axis = 2 : i32} : tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xbf16, #blocked>
      %146 = tt.broadcast %145 : tensor<4x4x1xbf16, #blocked> -> tensor<4x4x32xbf16, #blocked>
      %147 = tt.reshape %146 : tensor<4x4x32xbf16, #blocked> -> tensor<4x128xbf16, #blocked1>
      %148 = amdgpu.scaled_upcast_fp4 %138 scale %147 {axis = 1 : i32} : tensor<4x64xi8, #blocked8>, tensor<4x128xbf16, #blocked1> -> tensor<4x128xbf16, #blocked1>
      %149 = arith.cmpi eq, %139, %cst_31 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %150 = tt.expand_dims %149 {axis = 2 : i32} : tensor<4x4xi1, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xi1, #blocked>
      %151 = tt.broadcast %150 : tensor<4x4x1xi1, #blocked> -> tensor<4x4x32xi1, #blocked>
      %152 = tt.reshape %151 : tensor<4x4x32xi1, #blocked> -> tensor<4x128xi1, #blocked1>
      %153 = arith.select %152, %cst_0, %148 : tensor<4x128xi1, #blocked1>, tensor<4x128xbf16, #blocked1>
      %154 = ttg.local_alloc %153 : (tensor<4x128xbf16, #blocked1>) -> !ttg.memdesc<4x128xbf16, #shared, #smem>
      %155 = ttg.local_load %154 : !ttg.memdesc<4x128xbf16, #shared, #smem> -> tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>>
      %156 = arith.extui %70 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>> to tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %157 = arith.shli %156, %cst_19 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %158 = tt.bitcast %157 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %159 = tt.expand_dims %158 {axis = 2 : i32} : tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xbf16, #blocked4>
      %160 = tt.broadcast %159 : tensor<4x32x1xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked4>
      %161 = tt.trans %160 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked9>
      %162 = tt.reshape %161 : tensor<4x32x32xbf16, #blocked9> -> tensor<128x32xbf16, #blocked5>
      %163 = amdgpu.scaled_upcast_fp4 %77 scale %162 {axis = 0 : i32} : tensor<64x32xi8, #blocked3>, tensor<128x32xbf16, #blocked5> -> tensor<128x32xbf16, #blocked5>
      %164 = arith.cmpi eq, %70, %cst_20 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %165 = tt.expand_dims %164 {axis = 2 : i32} : tensor<4x32xi1, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xi1, #blocked4>
      %166 = tt.broadcast %165 : tensor<4x32x1xi1, #blocked4> -> tensor<4x32x32xi1, #blocked4>
      %167 = tt.trans %166 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xi1, #blocked4> -> tensor<4x32x32xi1, #blocked9>
      %168 = tt.reshape %167 : tensor<4x32x32xi1, #blocked9> -> tensor<128x32xi1, #blocked5>
      %169 = arith.select %168, %cst_21, %163 : tensor<128x32xi1, #blocked5>, tensor<128x32xbf16, #blocked5>
      %170 = ttg.local_alloc %169 : (tensor<128x32xbf16, #blocked5>) -> !ttg.memdesc<128x32xbf16, #shared, #smem>
      %171 = ttg.local_load %170 : !ttg.memdesc<128x32xbf16, #shared, #smem> -> tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>
      %172 = tt.dot %155, %171, %cst_3 : tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>> * tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> -> tensor<4x32xf32, #blocked3>
      %173 = arith.addf %172, %cst_3 : tensor<4x32xf32, #blocked3>
      %174 = arith.truncf %173 : tensor<4x32xf32, #blocked3> to tensor<4x32xbf16, #blocked3>
      %175 = arith.extsi %13 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %176 = arith.extsi %11 : i32 to i64
      %177 = tt.splat %176 : i64 -> tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %178 = arith.addi %177, %175 : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %179 = arith.extsi %32 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> to tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %180 = arith.extsi %30 : i32 to i64
      %181 = tt.splat %180 : i64 -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %182 = arith.addi %181, %179 : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %183 = arith.muli %7, %6 : i64
      %184 = tt.addptr %arg2, %183 : !tt.ptr<bf16>, i64
      %185 = tt.expand_dims %178 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %186 = arith.extsi %arg13 : i32 to i64
      %187 = tt.expand_dims %175 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %188 = arith.muli %186, %176 : i64
      %189 = tt.splat %186 : i64 -> tensor<4x1xi64, #blocked3>
      %190 = arith.muli %189, %187 : tensor<4x1xi64, #blocked3>
      %191 = tt.addptr %184, %188 : !tt.ptr<bf16>, i64
      %192 = tt.expand_dims %182 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %193 = tt.broadcast %190 : tensor<4x1xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %194 = tt.expand_dims %179 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %195 = tt.broadcast %194 : tensor<1x32xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %196 = tt.addptr %191, %180 : !tt.ptr<bf16>, i64
      %197 = arith.addi %195, %193 : tensor<4x32xi64, #blocked3>
      %198 = arith.extsi %arg4 : i32 to i64
      %199 = tt.splat %198 : i64 -> tensor<4x1xi64, #blocked3>
      %200 = arith.cmpi slt, %185, %199 : tensor<4x1xi64, #blocked3>
      %201 = arith.extsi %arg5 : i32 to i64
      %202 = tt.splat %201 : i64 -> tensor<1x32xi64, #blocked3>
      %203 = arith.cmpi slt, %192, %202 : tensor<1x32xi64, #blocked3>
      %204 = tt.broadcast %200 : tensor<4x1xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %205 = tt.broadcast %203 : tensor<1x32xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %206 = arith.andi %204, %205 : tensor<4x32xi1, #blocked3>
      %207 = tt.splat %196 : !tt.ptr<bf16> -> tensor<4x32x!tt.ptr<bf16>, #blocked3>
      %208 = tt.addptr %207, %197 : tensor<4x32x!tt.ptr<bf16>, #blocked3>, tensor<4x32xi64, #blocked3>
      tt.store %208, %174, %206 : tensor<4x32x!tt.ptr<bf16>, #blocked3>
    }
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(optimize-amd-lds-usage{lds-limit=0 target-arch=gfx950}, triton-scf-to-cf, convert-index-to-llvm{index-bitwidth=0}, allocate-amdgpu-shared-memory, convert-triton-amdgpu-to-llvm{arch=gfx950 ftz=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-cf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwidth=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info, convert-builtin-func-to-llvm{ftz=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_root/gg/cggjcwuvlva23hwbisj5n2gi3dnle4fcbt2uza4aedccsgx47q6u.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_root/gg/cggjcwuvlva23hwbisj5n2gi3dnle4fcbt2uza4aedccsgx47q6u.py:18:0: note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     a_ptr,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_ptr,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     c_ptr,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     b_scales_ptr,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     M,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     N,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     K,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_am,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ak,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bk,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bn,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ck,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cm,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cn,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsb,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsn,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bsk,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Meta-parameters
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_M: tl.constexpr,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_N: tl.constexpr,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     BLOCK_SIZE_K: tl.constexpr,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GROUP_SIZE_M: tl.constexpr,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     NUM_KSPLIT: tl.constexpr,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     EVEN_K: tl.constexpr,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     GRID_MN: tl.constexpr,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     cache_modifier: tl.constexpr,
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] ):
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """Kernel for computing the matmul C = A x B.
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A_scales and B_scales are in e8m0 format.
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     """
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ab > 0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_am > 0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_ak > 0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bb > 0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bk > 0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bn > 0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cb > 0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cm > 0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_cn > 0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsb > 0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsk > 0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(stride_bsn > 0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # -----------------------------------------------------------
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Map program ids `pid` to the block of C it should compute.
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # This is done in a grouped ordering to promote L2 data reuse.
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.program_id(axis=0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_unified = tl.program_id(axis=1)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_k = pid_unified % NUM_KSPLIT
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid = pid_unified // NUM_KSPLIT
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if NUM_KSPLIT == 1:
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         remap_xcd(pid, GRID_MN)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     else:
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_m = pid // num_pid_n
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         pid_n = pid % num_pid_n
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_batch >= 0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_m >= 0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     tl.assume(pid_n >= 0)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     # We assume 32 elements along K share the same scale.
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for first block of A and B input matrices
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         a_ptrs = a_ptr + (
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_ab
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_am[:, None] * stride_am
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split_bf16[None, :] * stride_ak
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_ptrs = b_ptr + (
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             pid_batch * stride_bb
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_k_split[:, None] * stride_bk
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[None, :] * stride_bn
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Create pointers for the first block of A and B scales
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # B scales are N x K even though B operand is K x N.
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         b_scale_ptrs = (
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales_ptr
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_bsb
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_bn[:, None] * stride_bsn
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + offs_ks[None, :] * stride_bsk
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scales = tl.load(b_scale_ptrs)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # If it is out of bounds, set it to 0.
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             if EVEN_K:
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(a_ptrs)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             else:
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 a_bf16 = tl.load(
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 b = tl.load(
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]                 )
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             # Advance the ptrs to the next K block.
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c = accumulator.to(c_ptr.type.element_ty)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         # Write back the block of the output matrix C with masks.
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_ptrs = (
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             c_ptr
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_batch * stride_cb
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cm * offs_cm[:, None]
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + stride_cn * offs_cn[None, :]
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]             + pid_k * stride_ck
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         )
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]         tl.store(c_ptrs, c, mask=c_mask)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] 
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 0, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 96, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] Traceback (most recent call last):
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     next_module = compile_ir(module, metadata)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4]     pm.run(mod)
[rank0]:E1106 11:04:58.628000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/4] RuntimeError: PassManager::run failed
Capturing batches (bs=24 avail_mem=63.66 GB):  87%| | 45/52 [03:28<00:32,  4.63s/it]
[2025-11-06 11:04:59 TP0] Registering 5535 cuda graph addresses
[2025-11-06 11:04:59 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 381, in __init__
    self.capture()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 500, in capture
    ) = self.capture_one_batch_size(bs, forward)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 692, in capture_one_batch_size
    run_once()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 679, in run_once
    logits_output_or_pp_proxy_tensors = forward(
  File "/opt/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 817, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 979, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 963, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1646, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1506, in codegen_and_compile
    compiled_module = graph.compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2318, in compile_to_module
    return self._compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2328, in _compile_to_module
    mod = self._compile_to_module_lines(wrapper_code)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2396, in _compile_to_module_lines
    mod = PyCodeCache.load_by_key_path(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3462, in load_by_key_path
    mod = _reload_python_module(key, path, set_sys_modules=in_toplevel)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 31, in _reload_python_module
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_root/kx/ckxb5ejabllpvvtggznwjk5s2trampl4knibqq4flpoifetq23dc.py", line 38, in <module>
    _batched_gemm_afp4_wfp4_pre_quant_kernel_0 = async_compile.triton('_batched_gemm_afp4_wfp4_pre_quant_kernel', '''
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 486, in triton
    kernel.precompile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 437, in precompile
    self._precompile_worker()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 459, in _precompile_worker
    compile_results.append(self._precompile_config(c))
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
    binary = triton.compile(*compile_args, **compile_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
    next_module = compile_ir(module, metadata)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
    stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
    pm.run(mod)
torch._inductor.exc.InductorError: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2791, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 319, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 237, in __init__
    self._model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 322, in __init__
    self.initialize(min_per_gpu_memory)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 479, in initialize
    self.init_device_graphs()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 1995, in init_device_graphs
    self.graph_runner = graph_runners[self.device](self)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 383, in __init__
    raise Exception(
Exception: Capture cuda graph failed: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

Possible solutions:
1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
3. disable torch compile by not using --enable-torch-compile
4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 


[2025-11-06 11:04:59] Received sigquit from a child process. It usually means the child failed.
