INFO 11-03 11:30:42 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-03 11:30:42] WARNING model_config.py:714: quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-03 11:30:42] WARNING server_args.py:1159: Attention backend not explicitly specified. Use aiter backend by default.
[2025-11-03 11:30:42] WARNING server_args.py:1348: DP attention is enabled. The chunked prefill size is adjusted to 16384 to avoid MoE kernel issues. 
[2025-11-03 11:30:42] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-03 11:30:42] server_args=ServerArgs(model_path='/mnt/raid/models/deepseek-ai/amd-DeepSeek-R1-MXFP4-Preview', tokenizer_path='/mnt/raid/models/deepseek-ai/amd-DeepSeek-R1-MXFP4-Preview', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.68, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=0.3, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=1071228179, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/deepseek-ai/amd-DeepSeek-R1-MXFP4-Preview', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=8, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=16, cuda_graph_bs=[1, 2, 4, 8, 12, 16], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=True, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=-1, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0)
[2025-11-03 11:30:42] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-03 11:30:42] Using default HuggingFace chat template with detected content format: string
INFO 11-03 11:30:49 [__init__.py:241] Automatically detected platform rocm.
INFO 11-03 11:30:49 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-03 11:30:49] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-03 11:30:50] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
INFO 11-03 11:30:57 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-03 11:30:58] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
INFO 11-03 11:30:58 [__init__.py:241] Automatically detected platform rocm.
[2025-11-03 11:30:58 DP3 TP3] Process 512 gpu_id 3 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
[2025-11-03 11:30:58 DP3 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
INFO 11-03 11:30:58 [__init__.py:241] Automatically detected platform rocm.
[2025-11-03 11:30:58 DP3 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-03 11:30:58 DP3 TP3] Init torch distributed begin.
INFO 11-03 11:30:58 [__init__.py:241] Automatically detected platform rocm.
INFO 11-03 11:30:58 [__init__.py:241] Automatically detected platform rocm.
INFO 11-03 11:30:58 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
INFO 11-03 11:30:58 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-03 11:30:58] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
INFO 11-03 11:30:58 [__init__.py:241] Automatically detected platform rocm.
[2025-11-03 11:30:58 DP7 TP7] Process 516 gpu_id 7 is running on CPUs: [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[2025-11-03 11:30:58 DP7 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-11-03 11:30:58 DP7 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-03 11:30:58 DP7 TP7] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-03 11:30:58] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-03 11:30:59] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-03 11:30:59] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-03 11:30:59 DP0 TP0] Process 509 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
[2025-11-03 11:30:59 DP0 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-03 11:30:59] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-03 11:30:59] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-03 11:30:59 DP1 TP1] Process 510 gpu_id 1 is running on CPUs: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
[2025-11-03 11:30:59 DP1 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-03 11:30:59 DP5 TP5] Process 514 gpu_id 5 is running on CPUs: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
[2025-11-03 11:30:59 DP5 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-11-03 11:30:59 DP2 TP2] Process 511 gpu_id 2 is running on CPUs: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
[2025-11-03 11:30:59 DP2 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-03 11:30:59 DP4 TP4] Process 513 gpu_id 4 is running on CPUs: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
[2025-11-03 11:30:59 DP4 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-03 11:30:59] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-03 11:30:59 DP0 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-03 11:30:59 DP0 TP0] Init torch distributed begin.
[2025-11-03 11:30:59 DP6 TP6] Process 515 gpu_id 6 is running on CPUs: [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
[2025-11-03 11:30:59 DP6 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-03 11:30:59 DP1 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-03 11:30:59 DP1 TP1] Init torch distributed begin.
[2025-11-03 11:30:59 DP5 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-03 11:30:59 DP5 TP5] Init torch distributed begin.
[2025-11-03 11:30:59 DP2 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-03 11:30:59 DP2 TP2] Init torch distributed begin.
[2025-11-03 11:30:59 DP4 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-03 11:30:59 DP4 TP4] Init torch distributed begin.
[2025-11-03 11:30:59 DP6 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-03 11:30:59 DP6 TP6] Init torch distributed begin.
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-11-03 11:30:59 DP0 TP0] sglang is using nccl==2.26.6
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2025-11-03 11:31:08 DP7 TP7] Init torch distributed ends. mem usage=3.10 GB
[2025-11-03 11:31:08 DP5 TP5] Init torch distributed ends. mem usage=3.17 GB
[2025-11-03 11:31:08 DP6 TP6] Init torch distributed ends. mem usage=3.11 GB
[2025-11-03 11:31:08 DP4 TP4] Init torch distributed ends. mem usage=3.10 GB
[2025-11-03 11:31:08 DP0 TP0] Init torch distributed ends. mem usage=3.22 GB
[2025-11-03 11:31:08 DP2 TP2] Init torch distributed ends. mem usage=3.24 GB
[2025-11-03 11:31:08 DP3 TP3] Init torch distributed ends. mem usage=3.24 GB
[2025-11-03 11:31:08 DP1 TP1] Init torch distributed ends. mem usage=2.82 GB
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
[2025-11-03 11:31:09 DP4 TP4] Load weight begin. avail mem=284.40 GB
[2025-11-03 11:31:09 DP5 TP5] Load weight begin. avail mem=284.33 GB
[2025-11-03 11:31:09 DP0 TP0] Load weight begin. avail mem=284.28 GB
[2025-11-03 11:31:09 DP0 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
[2025-11-03 11:31:09 DP1 TP1] Load weight begin. avail mem=284.68 GB
[2025-11-03 11:31:09 DP7 TP7] Load weight begin. avail mem=284.40 GB
[2025-11-03 11:31:09 DP2 TP2] Load weight begin. avail mem=284.26 GB
[2025-11-03 11:31:09 DP3 TP3] Load weight begin. avail mem=284.26 GB
[2025-11-03 11:31:09 DP6 TP6] Load weight begin. avail mem=284.39 GB
Loading safetensors checkpoint shards:   0% Completed | 0/73 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/73 [00:00<00:13,  5.45it/s]
Loading safetensors checkpoint shards:   3% Completed | 2/73 [00:00<00:19,  3.72it/s]
Loading safetensors checkpoint shards:   4% Completed | 3/73 [00:00<00:20,  3.43it/s]
Loading safetensors checkpoint shards:   5% Completed | 4/73 [00:01<00:20,  3.30it/s]
Loading safetensors checkpoint shards:   7% Completed | 5/73 [00:01<00:24,  2.79it/s]
Loading safetensors checkpoint shards:   8% Completed | 6/73 [00:01<00:22,  2.97it/s]
Loading safetensors checkpoint shards:  10% Completed | 7/73 [00:02<00:21,  3.05it/s]
Loading safetensors checkpoint shards:  11% Completed | 8/73 [00:02<00:22,  2.88it/s]
Loading safetensors checkpoint shards:  12% Completed | 9/73 [00:02<00:21,  3.03it/s]
Loading safetensors checkpoint shards:  14% Completed | 10/73 [00:03<00:19,  3.28it/s]
Loading safetensors checkpoint shards:  15% Completed | 11/73 [00:03<00:18,  3.31it/s]
Loading safetensors checkpoint shards:  16% Completed | 12/73 [00:03<00:17,  3.41it/s]
Loading safetensors checkpoint shards:  18% Completed | 13/73 [00:03<00:17,  3.51it/s]
Loading safetensors checkpoint shards:  19% Completed | 14/73 [00:04<00:18,  3.15it/s]
Loading safetensors checkpoint shards:  21% Completed | 15/73 [00:05<00:28,  2.04it/s]
Loading safetensors checkpoint shards:  22% Completed | 16/73 [00:05<00:25,  2.25it/s]
Loading safetensors checkpoint shards:  23% Completed | 17/73 [00:05<00:21,  2.64it/s]
Loading safetensors checkpoint shards:  25% Completed | 18/73 [00:06<00:20,  2.65it/s]
Loading safetensors checkpoint shards:  26% Completed | 19/73 [00:06<00:19,  2.70it/s]
Loading safetensors checkpoint shards:  27% Completed | 20/73 [00:06<00:20,  2.64it/s]
Loading safetensors checkpoint shards:  29% Completed | 21/73 [00:07<00:18,  2.76it/s]
Loading safetensors checkpoint shards:  30% Completed | 22/73 [00:07<00:17,  2.99it/s]
Loading safetensors checkpoint shards:  32% Completed | 23/73 [00:07<00:17,  2.91it/s]
Loading safetensors checkpoint shards:  33% Completed | 24/73 [00:08<00:18,  2.70it/s]
Loading safetensors checkpoint shards:  34% Completed | 25/73 [00:08<00:18,  2.66it/s]
Loading safetensors checkpoint shards:  36% Completed | 26/73 [00:09<00:17,  2.69it/s]
Loading safetensors checkpoint shards:  37% Completed | 27/73 [00:09<00:16,  2.79it/s]
Loading safetensors checkpoint shards:  38% Completed | 28/73 [00:09<00:16,  2.71it/s]
Loading safetensors checkpoint shards:  40% Completed | 29/73 [00:10<00:15,  2.82it/s]
Loading safetensors checkpoint shards:  41% Completed | 30/73 [00:10<00:14,  2.90it/s]
Loading safetensors checkpoint shards:  42% Completed | 31/73 [00:10<00:14,  2.86it/s]
Loading safetensors checkpoint shards:  45% Completed | 33/73 [00:11<00:11,  3.56it/s]
Loading safetensors checkpoint shards:  47% Completed | 34/73 [00:11<00:11,  3.46it/s]
Loading safetensors checkpoint shards:  48% Completed | 35/73 [00:12<00:16,  2.29it/s]
Loading safetensors checkpoint shards:  49% Completed | 36/73 [00:12<00:14,  2.57it/s]
Loading safetensors checkpoint shards:  51% Completed | 37/73 [00:12<00:13,  2.65it/s]
Loading safetensors checkpoint shards:  52% Completed | 38/73 [00:13<00:12,  2.75it/s]
Loading safetensors checkpoint shards:  53% Completed | 39/73 [00:13<00:11,  2.99it/s]
Loading safetensors checkpoint shards:  55% Completed | 40/73 [00:13<00:08,  3.68it/s]
Loading safetensors checkpoint shards:  58% Completed | 42/73 [00:13<00:05,  5.63it/s]
Loading safetensors checkpoint shards:  60% Completed | 44/73 [00:14<00:04,  6.80it/s]
Loading safetensors checkpoint shards:  63% Completed | 46/73 [00:14<00:03,  7.97it/s]
Loading safetensors checkpoint shards:  66% Completed | 48/73 [00:14<00:02,  9.65it/s]
Loading safetensors checkpoint shards:  68% Completed | 50/73 [00:14<00:03,  5.92it/s]
Loading safetensors checkpoint shards:  70% Completed | 51/73 [00:15<00:05,  3.80it/s]
Loading safetensors checkpoint shards:  71% Completed | 52/73 [00:16<00:07,  2.68it/s]
Loading safetensors checkpoint shards:  73% Completed | 53/73 [00:17<00:09,  2.09it/s]
Loading safetensors checkpoint shards:  74% Completed | 54/73 [00:18<00:13,  1.38it/s]
Loading safetensors checkpoint shards:  75% Completed | 55/73 [00:19<00:12,  1.49it/s]
Loading safetensors checkpoint shards:  77% Completed | 56/73 [00:19<00:11,  1.53it/s]
Loading safetensors checkpoint shards:  78% Completed | 57/73 [00:20<00:09,  1.75it/s]
Loading safetensors checkpoint shards:  79% Completed | 58/73 [00:20<00:07,  2.02it/s]
Loading safetensors checkpoint shards:  81% Completed | 59/73 [00:20<00:06,  2.18it/s]
Loading safetensors checkpoint shards:  82% Completed | 60/73 [00:21<00:05,  2.22it/s]
Loading safetensors checkpoint shards:  84% Completed | 61/73 [00:21<00:05,  2.26it/s]
Loading safetensors checkpoint shards:  85% Completed | 62/73 [00:22<00:04,  2.42it/s]
Loading safetensors checkpoint shards:  86% Completed | 63/73 [00:22<00:03,  2.54it/s]
Loading safetensors checkpoint shards:  88% Completed | 64/73 [00:22<00:03,  2.44it/s]
Loading safetensors checkpoint shards:  89% Completed | 65/73 [00:23<00:03,  2.52it/s]
Loading safetensors checkpoint shards:  90% Completed | 66/73 [00:23<00:02,  2.64it/s]
Loading safetensors checkpoint shards:  92% Completed | 67/73 [00:23<00:02,  2.55it/s]
Loading safetensors checkpoint shards:  93% Completed | 68/73 [00:24<00:01,  2.70it/s]
Loading safetensors checkpoint shards:  95% Completed | 69/73 [00:24<00:01,  2.62it/s]
Loading safetensors checkpoint shards:  96% Completed | 70/73 [00:25<00:01,  2.72it/s]
Loading safetensors checkpoint shards:  97% Completed | 71/73 [00:25<00:00,  2.64it/s]
Loading safetensors checkpoint shards: 100% Completed | 73/73 [00:26<00:00,  2.92it/s]
Loading safetensors checkpoint shards: 100% Completed | 73/73 [00:26<00:00,  2.81it/s]

[2025-11-03 11:31:36 DP2 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=235.09 GB, mem usage=49.18 GB.
[2025-11-03 11:31:36 DP3 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=235.08 GB, mem usage=49.18 GB.
[2025-11-03 11:31:36 DP5 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=235.15 GB, mem usage=49.18 GB.
[2025-11-03 11:31:36 DP4 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=235.23 GB, mem usage=49.18 GB.
[2025-11-03 11:31:37 DP7 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=235.22 GB, mem usage=49.18 GB.
[2025-11-03 11:31:37 DP6 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=235.21 GB, mem usage=49.18 GB.
[2025-11-03 11:31:37 DP1 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=235.50 GB, mem usage=49.18 GB.
[2025-11-03 11:31:37 DP0 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=235.11 GB, mem usage=49.18 GB.
[2025-11-03 11:31:37 DP0 TP0] Using KV cache dtype: torch.bfloat16
[2025-11-03 11:31:37 DP0 TP0] KV Cache is allocated. #tokens: 2201897, KV size: 144.11 GB
[2025-11-03 11:31:37 DP0 TP0] Memory pool end. avail mem=88.43 GB
[2025-11-03 11:31:37 DP1 TP1] KV Cache is allocated. #tokens: 2201897, KV size: 144.11 GB
[2025-11-03 11:31:37 DP1 TP1] Memory pool end. avail mem=88.82 GB
[2025-11-03 11:31:37 DP3 TP3] KV Cache is allocated. #tokens: 2201897, KV size: 144.11 GB
[2025-11-03 11:31:37 DP3 TP3] Memory pool end. avail mem=88.40 GB
[2025-11-03 11:31:37 DP5 TP5] KV Cache is allocated. #tokens: 2201897, KV size: 144.11 GB
[2025-11-03 11:31:37 DP5 TP5] Memory pool end. avail mem=88.47 GB
[2025-11-03 11:31:37 DP7 TP7] KV Cache is allocated. #tokens: 2201897, KV size: 144.11 GB
[2025-11-03 11:31:37 DP7 TP7] Memory pool end. avail mem=88.54 GB
[2025-11-03 11:31:37 DP4 TP4] KV Cache is allocated. #tokens: 2201897, KV size: 144.11 GB
[2025-11-03 11:31:37 DP4 TP4] Memory pool end. avail mem=88.55 GB
[2025-11-03 11:31:37 DP2 TP2] KV Cache is allocated. #tokens: 2201897, KV size: 144.11 GB
[2025-11-03 11:31:37 DP2 TP2] Memory pool end. avail mem=88.41 GB
[2025-11-03 11:31:37 DP6 TP6] KV Cache is allocated. #tokens: 2201897, KV size: 144.11 GB
[2025-11-03 11:31:37 DP6 TP6] Memory pool end. avail mem=88.53 GB
[2025-11-03 11:31:37 DP5 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=88.34 GB
[2025-11-03 11:31:37 DP4 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=88.42 GB
[2025-11-03 11:31:37 DP6 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=88.40 GB
[2025-11-03 11:31:37 DP7 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=88.42 GB
[2025-11-03 11:31:37 DP3 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=88.27 GB
[2025-11-03 11:31:37 DP1 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=88.70 GB
[2025-11-03 11:31:37 DP2 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=88.28 GB
[2025-11-03 11:31:37 DP0 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=88.30 GB
[2025-11-03 11:31:37 DP0 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16]
  0%|          | 0/6 [00:00<?, ?it/s]Capturing batches (bs=16 avail_mem=88.29 GB):   0%|          | 0/6 [00:00<?, ?it/s]/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
ler_DP5_TP5: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
#blocked = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_batched_gemm_afp4_wfp4_pre_quant_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32) attributes {noinline = false} {
    %cst = arith.constant dense<127> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128xbf16, #blocked1>
    %cst_1 = arith.constant dense<2097152> : tensor<4x4x1xi32, #blocked>
    %cst_2 = arith.constant dense<4> : tensor<4x4x16xi8, #blocked2>
    %c31_i32 = arith.constant 31 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x32xf32, #blocked3>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %true = arith.constant true
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #blocked>
    %cst_5 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_7 = arith.constant dense<-2147483648> : tensor<4x4x32xi32, #blocked>
    %cst_8 = arith.constant dense<23> : tensor<4x4x32xi32, #blocked>
    %cst_9 = arith.constant dense<255> : tensor<4x4x32xi32, #blocked>
    %cst_10 = arith.constant dense<8388607> : tensor<4x4x32xi32, #blocked>
    %cst_11 = arith.constant dense<1> : tensor<4x4x32xi32, #blocked>
    %cst_12 = arith.constant dense<127> : tensor<4x4x32xi32, #blocked>
    %cst_13 = arith.constant dense<4194304> : tensor<4x4x32xi32, #blocked>
    %cst_14 = arith.constant dense<126> : tensor<4x4x32xi32, #blocked>
    %cst_15 = arith.constant dense<2> : tensor<4x4x32xi32, #blocked>
    %cst_16 = arith.constant dense<21> : tensor<4x4x32xi32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>> -> tensor<64x1xi32, #blocked6>
      %45 = arith.extsi %44 : tensor<64x1xi32, #blocked6> to tensor<64x1xi64, #blocked6>
      %46 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>> -> tensor<1x32xi32, #blocked6>
      %47 = tt.splat %arg10 : i32 -> tensor<1x32xi32, #blocked6>
      %48 = arith.muli %46, %47 : tensor<1x32xi32, #blocked6>
      %49 = arith.extsi %48 : tensor<1x32xi32, #blocked6> to tensor<1x32xi64, #blocked6>
      %50 = tt.broadcast %45 : tensor<64x1xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %51 = tt.broadcast %49 : tensor<1x32xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %52 = arith.addi %50, %51 : tensor<64x32xi64, #blocked6>
      %53 = tt.addptr %arg1, %42 : !tt.ptr<i8>, i64
      %54 = arith.extsi %arg14 : i32 to i64
      %55 = arith.muli %7, %54 : i64
      %56 = tt.addptr %arg3, %55 : !tt.ptr<i8>, i64
      %57 = tt.expand_dims %41 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<32x1xi32, #linear1>
      %58 = tt.splat %arg15 : i32 -> tensor<32x1xi32, #linear1>
      %59 = arith.muli %57, %58 : tensor<32x1xi32, #linear1>
      %60 = arith.extsi %59 : tensor<32x1xi32, #linear1> to tensor<32x1xi64, #linear1>
      %61 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>>
      %62 = tt.broadcast %60 : tensor<32x1xi64, #linear1> -> tensor<32x4xi64, #linear1>
      %63 = tt.expand_dims %61 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>> -> tensor<1x4xi32, #linear1>
      %64 = tt.broadcast %63 : tensor<1x4xi32, #linear1> -> tensor<32x4xi32, #linear1>
      %65 = arith.extsi %64 : tensor<32x4xi32, #linear1> to tensor<32x4xi64, #linear1>
      %66 = arith.addi %65, %62 : tensor<32x4xi64, #linear1>
      %67 = tt.splat %56 : !tt.ptr<i8> -> tensor<32x4x!tt.ptr<i8>, #linear1>
      %68 = tt.addptr %67, %66 : tensor<32x4x!tt.ptr<i8>, #linear1>, tensor<32x4xi64, #linear1>
      %69 = tt.load %68 : tensor<32x4x!tt.ptr<i8>, #linear1>
      %70 = tt.trans %69 {order = array<i32: 1, 0>} : tensor<32x4xi8, #linear1> -> tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %71 = tt.splat %29 : !tt.ptr<bf16> -> tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %72 = tt.addptr %71, %28 : tensor<4x128x!tt.ptr<bf16>, #blocked1>, tensor<4x128xi64, #blocked1>
      %73 = tt.load %72 : tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %74 = tt.splat %53 : !tt.ptr<i8> -> tensor<64x32x!tt.ptr<i8>, #blocked6>
      %75 = tt.addptr %74, %52 : tensor<64x32x!tt.ptr<i8>, #blocked6>, tensor<64x32xi64, #blocked6>
      %76 = tt.load %75 cacheModifier = cg : tensor<64x32x!tt.ptr<i8>, #blocked6>
      %77 = ttg.convert_layout %76 : tensor<64x32xi8, #blocked6> -> tensor<64x32xi8, #blocked3>
      %78 = tt.reshape %73 : tensor<4x128xbf16, #blocked1> -> tensor<4x4x32xbf16, #blocked>
      %79 = math.absf %78 : tensor<4x4x32xbf16, #blocked>
      %80 = arith.extf %79 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %81 = "tt.reduce"(%80) <{axis = 2 : i32}> ({
      ^bb0(%arg16: f32, %arg17: f32):
        %209 = arith.maxnumf %arg16, %arg17 : f32
        tt.reduce.return %209 : f32
      }) : (tensor<4x4x32xf32, #blocked>) -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>>
      %82 = ttg.convert_layout %81 : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>>
      %83 = tt.expand_dims %82 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>> -> tensor<4x4x1xf32, #linear>
      %84 = tt.expand_dims %81 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xf32, #blocked>
      %85 = tt.bitcast %83 : tensor<4x4x1xf32, #linear> -> tensor<4x4x1xi32, #linear>
      %86 = tt.bitcast %84 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %87 = arith.addi %85, %cst_28 : tensor<4x4x1xi32, #linear>
      %88 = arith.addi %86, %cst_1 : tensor<4x4x1xi32, #blocked>
      %89 = tt.bitcast %87 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xi32, #linear>
      %90 = tt.bitcast %88 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %91 = arith.andi %89, %cst_27 : tensor<4x4x1xi32, #linear>
      %92 = arith.andi %90, %cst_4 : tensor<4x4x1xi32, #blocked>
      %93 = tt.bitcast %91 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xf32, #linear>
      %94 = tt.bitcast %92 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xf32, #blocked>
      %95 = math.log2 %93 : tensor<4x4x1xf32, #linear>
      %96 = math.log2 %94 : tensor<4x4x1xf32, #blocked>
      %97 = math.floor %95 : tensor<4x4x1xf32, #linear>
      %98 = math.floor %96 : tensor<4x4x1xf32, #blocked>
      %99 = arith.subf %97, %cst_26 : tensor<4x4x1xf32, #linear>
      %100 = arith.subf %98, %cst_5 : tensor<4x4x1xf32, #blocked>
      %101 = tt.clampf %99, %cst_25, %cst_24, propagateNan = none : tensor<4x4x1xf32, #linear>
      %102 = tt.clampf %100, %cst_18, %cst_23, propagateNan = none : tensor<4x4x1xf32, #blocked>
      %103 = arith.fptoui %101 : tensor<4x4x1xf32, #linear> to tensor<4x4x1xi8, #linear>
      %104 = arith.fptoui %102 : tensor<4x4x1xf32, #blocked> to tensor<4x4x1xi8, #blocked>
      %105 = arith.addi %103, %cst_29 : tensor<4x4x1xi8, #linear>
      %106 = arith.addi %104, %cst : tensor<4x4x1xi8, #blocked>
      %107 = arith.subf %cst_6, %102 : tensor<4x4x1xf32, #blocked>
      %108 = math.exp2 %107 : tensor<4x4x1xf32, #blocked>
      %109 = arith.extf %78 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %110 = tt.broadcast %108 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x32xf32, #blocked>
      %111 = arith.mulf %109, %110 : tensor<4x4x32xf32, #blocked>
      %112 = tt.bitcast %111 : tensor<4x4x32xf32, #blocked> -> tensor<4x4x32xi32, #blocked>
      %113 = arith.andi %112, %cst_7 : tensor<4x4x32xi32, #blocked>
      %114 = arith.shrui %112, %cst_8 : tensor<4x4x32xi32, #blocked>
      %115 = arith.andi %114, %cst_9 : tensor<4x4x32xi32, #blocked>
      %116 = arith.andi %112, %cst_10 : tensor<4x4x32xi32, #blocked>
      %117 = arith.addi %115, %cst_11 : tensor<4x4x32xi32, #blocked>
      %118 = arith.subi %cst_12, %117 : tensor<4x4x32xi32, #blocked>
      %119 = arith.cmpi ult, %115, %cst_12 : tensor<4x4x32xi32, #blocked>
      %120 = arith.shrui %116, %cst_11 : tensor<4x4x32xi32, #blocked>
      %121 = arith.ori %120, %cst_13 : tensor<4x4x32xi32, #blocked>
      %122 = arith.shrui %121, %118 : tensor<4x4x32xi32, #blocked>
      %123 = arith.select %119, %122, %116 : tensor<4x4x32xi1, #blocked>, tensor<4x4x32xi32, #blocked>
      %124 = arith.maxui %115, %cst_14 : tensor<4x4x32xi32, #blocked>
      %125 = arith.subi %124, %cst_14 : tensor<4x4x32xi32, #blocked>
      %126 = arith.shli %125, %cst_15 : tensor<4x4x32xi32, #blocked>
      %127 = arith.shrui %123, %cst_16 : tensor<4x4x32xi32, #blocked>
      %128 = arith.ori %126, %127 : tensor<4x4x32xi32, #blocked>
      %129 = arith.addi %128, %cst_11 : tensor<4x4x32xi32, #blocked>
      %130 = arith.shrui %129, %cst_11 : tensor<4x4x32xi32, #blocked>
      %131 = arith.minui %130, %cst_22 : tensor<4x4x32xi32, #blocked>
      %132 = arith.shrui %113, %cst_17 : tensor<4x4x32xi32, #blocked>
      %133 = arith.ori %132, %131 : tensor<4x4x32xi32, #blocked>
      %134 = arith.trunci %133 : tensor<4x4x32xi32, #blocked> to tensor<4x4x32xi8, #blocked>
      %135 = tt.reshape %134 : tensor<4x4x32xi8, #blocked> -> tensor<4x4x16x2xi8, #blocked7>
      %outLHS, %outRHS = tt.split %135 : tensor<4x4x16x2xi8, #blocked7> -> tensor<4x4x16xi8, #blocked2>
      %136 = arith.shli %outRHS, %cst_2 : tensor<4x4x16xi8, #blocked2>
      %137 = arith.ori %outLHS, %136 : tensor<4x4x16xi8, #blocked2>
      %138 = tt.reshape %137 : tensor<4x4x16xi8, #blocked2> -> tensor<4x64xi8, #blocked8>
      %139 = tt.reshape %105 : tensor<4x4x1xi8, #linear> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %140 = tt.reshape %106 : tensor<4x4x1xi8, #blocked> -> tensor<4x4xi8, #linear2>
      %141 = ttg.convert_layout %140 : tensor<4x4xi8, #linear2> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %142 = arith.extui %141 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>> to tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %143 = arith.shli %142, %cst_30 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %144 = tt.bitcast %143 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %145 = tt.expand_dims %144 {axis = 2 : i32} : tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xbf16, #blocked>
      %146 = tt.broadcast %145 : tensor<4x4x1xbf16, #blocked> -> tensor<4x4x32xbf16, #blocked>
      %147 = tt.reshape %146 : tensor<4x4x32xbf16, #blocked> -> tensor<4x128xbf16, #blocked1>
      %148 = amdgpu.scaled_upcast_fp4 %138 scale %147 {axis = 1 : i32} : tensor<4x64xi8, #blocked8>, tensor<4x128xbf16, #blocked1> -> tensor<4x128xbf16, #blocked1>
      %149 = arith.cmpi eq, %139, %cst_31 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %150 = tt.expand_dims %149 {axis = 2 : i32} : tensor<4x4xi1, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xi1, #blocked>
      %151 = tt.broadcast %150 : tensor<4x4x1xi1, #blocked> -> tensor<4x4x32xi1, #blocked>
      %152 = tt.reshape %151 : tensor<4x4x32xi1, #blocked> -> tensor<4x128xi1, #blocked1>
      %153 = arith.select %152, %cst_0, %148 : tensor<4x128xi1, #blocked1>, tensor<4x128xbf16, #blocked1>
      %154 = ttg.local_alloc %153 : (tensor<4x128xbf16, #blocked1>) -> !ttg.memdesc<4x128xbf16, #shared, #smem>
      %155 = ttg.local_load %154 : !ttg.memdesc<4x128xbf16, #shared, #smem> -> tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>>
      %156 = arith.extui %70 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>> to tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %157 = arith.shli %156, %cst_19 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %158 = tt.bitcast %157 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %159 = tt.expand_dims %158 {axis = 2 : i32} : tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xbf16, #blocked4>
      %160 = tt.broadcast %159 : tensor<4x32x1xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked4>
      %161 = tt.trans %160 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked9>
      %162 = tt.reshape %161 : tensor<4x32x32xbf16, #blocked9> -> tensor<128x32xbf16, #blocked5>
      %163 = amdgpu.scaled_upcast_fp4 %77 scale %162 {axis = 0 : i32} : tensor<64x32xi8, #blocked3>, tensor<128x32xbf16, #blocked5> -> tensor<128x32xbf16, #blocked5>
      %164 = arith.cmpi eq, %70, %cst_20 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %165 = tt.expand_dims %164 {axis = 2 : i32} : tensor<4x32xi1, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xi1, #blocked4>
      %166 = tt.broadcast %165 : tensor<4x32x1xi1, #blocked4> -> tensor<4x32x32xi1, #blocked4>
      %167 = tt.trans %166 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xi1, #blocked4> -> tensor<4x32x32xi1, #blocked9>
      %168 = tt.reshape %167 : tensor<4x32x32xi1, #blocked9> -> tensor<128x32xi1, #blocked5>
      %169 = arith.select %168, %cst_21, %163 : tensor<128x32xi1, #blocked5>, tensor<128x32xbf16, #blocked5>
      %170 = ttg.local_alloc %169 : (tensor<128x32xbf16, #blocked5>) -> !ttg.memdesc<128x32xbf16, #shared, #smem>
      %171 = ttg.local_load %170 : !ttg.memdesc<128x32xbf16, #shared, #smem> -> tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>
      %172 = tt.dot %155, %171, %cst_3 : tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>> * tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> -> tensor<4x32xf32, #blocked3>
      %173 = arith.addf %172, %cst_3 : tensor<4x32xf32, #blocked3>
      %174 = arith.truncf %173 : tensor<4x32xf32, #blocked3> to tensor<4x32xbf16, #blocked3>
      %175 = arith.extsi %13 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %176 = arith.extsi %11 : i32 to i64
      %177 = tt.splat %176 : i64 -> tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %178 = arith.addi %177, %175 : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %179 = arith.extsi %32 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> to tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %180 = arith.extsi %30 : i32 to i64
      %181 = tt.splat %180 : i64 -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %182 = arith.addi %181, %179 : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %183 = arith.muli %7, %6 : i64
      %184 = tt.addptr %arg2, %183 : !tt.ptr<bf16>, i64
      %185 = tt.expand_dims %178 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %186 = arith.extsi %arg13 : i32 to i64
      %187 = tt.expand_dims %175 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %188 = arith.muli %186, %176 : i64
      %189 = tt.splat %186 : i64 -> tensor<4x1xi64, #blocked3>
      %190 = arith.muli %189, %187 : tensor<4x1xi64, #blocked3>
      %191 = tt.addptr %184, %188 : !tt.ptr<bf16>, i64
      %192 = tt.expand_dims %182 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %193 = tt.broadcast %190 : tensor<4x1xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %194 = tt.expand_dims %179 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %195 = tt.broadcast %194 : tensor<1x32xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %196 = tt.addptr %191, %180 : !tt.ptr<bf16>, i64
      %197 = arith.addi %195, %193 : tensor<4x32xi64, #blocked3>
      %198 = arith.extsi %arg4 : i32 to i64
      %199 = tt.splat %198 : i64 -> tensor<4x1xi64, #blocked3>
      %200 = arith.cmpi slt, %185, %199 : tensor<4x1xi64, #blocked3>
      %201 = arith.extsi %arg5 : i32 to i64
      %202 = tt.splat %201 : i64 -> tensor<1x32xi64, #blocked3>
      %203 = arith.cmpi slt, %192, %202 : tensor<1x32xi64, #blocked3>
      %204 = tt.broadcast %200 : tensor<4x1xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %205 = tt.broadcast %203 : tensor<1x32xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %206 = arith.andi %204, %205 : tensor<4x32xi1, #blocked3>
      %207 = tt.splat %196 : !tt.ptr<bf16> -> tensor<4x32x!tt.ptr<bf16>, #blocked3>
      %208 = tt.addptr %207, %197 : tensor<4x32x!tt.ptr<bf16>, #blocked3>, tensor<4x32xi64, #blocked3>
      tt.store %208, %174, %206 : tensor<4x32x!tt.ptr<bf16>, #blocked3>
    }
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(optimize-amd-lds-usage{lds-limit=0 target-arch=gfx950}, triton-scf-to-cf, convert-index-to-llvm{index-bitwidth=0}, allocate-amdgpu-shared-memory, convert-triton-amdgpu-to-llvm{arch=gfx950 ftz=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-cf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwidth=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info, convert-builtin-func-to-llvm{ftz=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_root/w2/cw2lz77gyfknswdzrahmgf7qi372of6mx54o2swuti5vcmefwljl.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_root/w2/cw2lz77gyfknswdzrahmgf7qi372of6mx54o2swuti5vcmefwljl.py:18:0: note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """Kernel for computing the matmul C = A x B.
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 5, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank5]:E1103 11:31:40.600000 514 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
ler_DP4_TP4: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
#blocked = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_batched_gemm_afp4_wfp4_pre_quant_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32) attributes {noinline = false} {
    %cst = arith.constant dense<127> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128xbf16, #blocked1>
    %cst_1 = arith.constant dense<2097152> : tensor<4x4x1xi32, #blocked>
    %cst_2 = arith.constant dense<4> : tensor<4x4x16xi8, #blocked2>
    %c31_i32 = arith.constant 31 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x32xf32, #blocked3>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %true = arith.constant true
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #blocked>
    %cst_5 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_7 = arith.constant dense<-2147483648> : tensor<4x4x32xi32, #blocked>
    %cst_8 = arith.constant dense<23> : tensor<4x4x32xi32, #blocked>
    %cst_9 = arith.constant dense<255> : tensor<4x4x32xi32, #blocked>
    %cst_10 = arith.constant dense<8388607> : tensor<4x4x32xi32, #blocked>
    %cst_11 = arith.constant dense<1> : tensor<4x4x32xi32, #blocked>
    %cst_12 = arith.constant dense<127> : tensor<4x4x32xi32, #blocked>
    %cst_13 = arith.constant dense<4194304> : tensor<4x4x32xi32, #blocked>
    %cst_14 = arith.constant dense<126> : tensor<4x4x32xi32, #blocked>
    %cst_15 = arith.constant dense<2> : tensor<4x4x32xi32, #blocked>
    %cst_16 = arith.constant dense<21> : tensor<4x4x32xi32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>> -> tensor<64x1xi32, #blocked6>
      %45 = arith.extsi %44 : tensor<64x1xi32, #blocked6> to tensor<64x1xi64, #blocked6>
      %46 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>> -> tensor<1x32xi32, #blocked6>
      %47 = tt.splat %arg10 : i32 -> tensor<1x32xi32, #blocked6>
      %48 = arith.muli %46, %47 : tensor<1x32xi32, #blocked6>
      %49 = arith.extsi %48 : tensor<1x32xi32, #blocked6> to tensor<1x32xi64, #blocked6>
      %50 = tt.broadcast %45 : tensor<64x1xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %51 = tt.broadcast %49 : tensor<1x32xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %52 = arith.addi %50, %51 : tensor<64x32xi64, #blocked6>
      %53 = tt.addptr %arg1, %42 : !tt.ptr<i8>, i64
      %54 = arith.extsi %arg14 : i32 to i64
      %55 = arith.muli %7, %54 : i64
      %56 = tt.addptr %arg3, %55 : !tt.ptr<i8>, i64
      %57 = tt.expand_dims %41 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<32x1xi32, #linear1>
      %58 = tt.splat %arg15 : i32 -> tensor<32x1xi32, #linear1>
      %59 = arith.muli %57, %58 : tensor<32x1xi32, #linear1>
      %60 = arith.extsi %59 : tensor<32x1xi32, #linear1> to tensor<32x1xi64, #linear1>
      %61 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>>
      %62 = tt.broadcast %60 : tensor<32x1xi64, #linear1> -> tensor<32x4xi64, #linear1>
      %63 = tt.expand_dims %61 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>> -> tensor<1x4xi32, #linear1>
      %64 = tt.broadcast %63 : tensor<1x4xi32, #linear1> -> tensor<32x4xi32, #linear1>
      %65 = arith.extsi %64 : tensor<32x4xi32, #linear1> to tensor<32x4xi64, #linear1>
      %66 = arith.addi %65, %62 : tensor<32x4xi64, #linear1>
      %67 = tt.splat %56 : !tt.ptr<i8> -> tensor<32x4x!tt.ptr<i8>, #linear1>
      %68 = tt.addptr %67, %66 : tensor<32x4x!tt.ptr<i8>, #linear1>, tensor<32x4xi64, #linear1>
      %69 = tt.load %68 : tensor<32x4x!tt.ptr<i8>, #linear1>
      %70 = tt.trans %69 {order = array<i32: 1, 0>} : tensor<32x4xi8, #linear1> -> tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %71 = tt.splat %29 : !tt.ptr<bf16> -> tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %72 = tt.addptr %71, %28 : tensor<4x128x!tt.ptr<bf16>, #blocked1>, tensor<4x128xi64, #blocked1>
      %73 = tt.load %72 : tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %74 = tt.splat %53 : !tt.ptr<i8> -> tensor<64x32x!tt.ptr<i8>, #blocked6>
      %75 = tt.addptr %74, %52 : tensor<64x32x!tt.ptr<i8>, #blocked6>, tensor<64x32xi64, #blocked6>
      %76 = tt.load %75 cacheModifier = cg : tensor<64x32x!tt.ptr<i8>, #blocked6>
      %77 = ttg.convert_layout %76 : tensor<64x32xi8, #blocked6> -> tensor<64x32xi8, #blocked3>
      %78 = tt.reshape %73 : tensor<4x128xbf16, #blocked1> -> tensor<4x4x32xbf16, #blocked>
      %79 = math.absf %78 : tensor<4x4x32xbf16, #blocked>
      %80 = arith.extf %79 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %81 = "tt.reduce"(%80) <{axis = 2 : i32}> ({
      ^bb0(%arg16: f32, %arg17: f32):
        %209 = arith.maxnumf %arg16, %arg17 : f32
        tt.reduce.return %209 : f32
      }) : (tensor<4x4x32xf32, #blocked>) -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>>
      %82 = ttg.convert_layout %81 : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>>
      %83 = tt.expand_dims %82 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>> -> tensor<4x4x1xf32, #linear>
      %84 = tt.expand_dims %81 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xf32, #blocked>
      %85 = tt.bitcast %83 : tensor<4x4x1xf32, #linear> -> tensor<4x4x1xi32, #linear>
      %86 = tt.bitcast %84 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %87 = arith.addi %85, %cst_28 : tensor<4x4x1xi32, #linear>
      %88 = arith.addi %86, %cst_1 : tensor<4x4x1xi32, #blocked>
      %89 = tt.bitcast %87 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xi32, #linear>
      %90 = tt.bitcast %88 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %91 = arith.andi %89, %cst_27 : tensor<4x4x1xi32, #linear>
      %92 = arith.andi %90, %cst_4 : tensor<4x4x1xi32, #blocked>
      %93 = tt.bitcast %91 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xf32, #linear>
      %94 = tt.bitcast %92 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xf32, #blocked>
      %95 = math.log2 %93 : tensor<4x4x1xf32, #linear>
      %96 = math.log2 %94 : tensor<4x4x1xf32, #blocked>
      %97 = math.floor %95 : tensor<4x4x1xf32, #linear>
      %98 = math.floor %96 : tensor<4x4x1xf32, #blocked>
      %99 = arith.subf %97, %cst_26 : tensor<4x4x1xf32, #linear>
      %100 = arith.subf %98, %cst_5 : tensor<4x4x1xf32, #blocked>
      %101 = tt.clampf %99, %cst_25, %cst_24, propagateNan = none : tensor<4x4x1xf32, #linear>
      %102 = tt.clampf %100, %cst_18, %cst_23, propagateNan = none : tensor<4x4x1xf32, #blocked>
      %103 = arith.fptoui %101 : tensor<4x4x1xf32, #linear> to tensor<4x4x1xi8, #linear>
      %104 = arith.fptoui %102 : tensor<4x4x1xf32, #blocked> to tensor<4x4x1xi8, #blocked>
      %105 = arith.addi %103, %cst_29 : tensor<4x4x1xi8, #linear>
      %106 = arith.addi %104, %cst : tensor<4x4x1xi8, #blocked>
      %107 = arith.subf %cst_6, %102 : tensor<4x4x1xf32, #blocked>
      %108 = math.exp2 %107 : tensor<4x4x1xf32, #blocked>
      %109 = arith.extf %78 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %110 = tt.broadcast %108 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x32xf32, #blocked>
      %111 = arith.mulf %109, %110 : tensor<4x4x32xf32, #blocked>
      %112 = tt.bitcast %111 : tensor<4x4x32xf32, #blocked> -> tensor<4x4x32xi32, #blocked>
      %113 = arith.andi %112, %cst_7 : tensor<4x4x32xi32, #blocked>
      %114 = arith.shrui %112, %cst_8 : tensor<4x4x32xi32, #blocked>
      %115 = arith.andi %114, %cst_9 : tensor<4x4x32xi32, #blocked>
      %116 = arith.andi %112, %cst_10 : tensor<4x4x32xi32, #blocked>
      %117 = arith.addi %115, %cst_11 : tensor<4x4x32xi32, #blocked>
      %118 = arith.subi %cst_12, %117 : tensor<4x4x32xi32, #blocked>
      %119 = arith.cmpi ult, %115, %cst_12 : tensor<4x4x32xi32, #blocked>
      %120 = arith.shrui %116, %cst_11 : tensor<4x4x32xi32, #blocked>
      %121 = arith.ori %120, %cst_13 : tensor<4x4x32xi32, #blocked>
      %122 = arith.shrui %121, %118 : tensor<4x4x32xi32, #blocked>
      %123 = arith.select %119, %122, %116 : tensor<4x4x32xi1, #blocked>, tensor<4x4x32xi32, #blocked>
      %124 = arith.maxui %115, %cst_14 : tensor<4x4x32xi32, #blocked>
      %125 = arith.subi %124, %cst_14 : tensor<4x4x32xi32, #blocked>
      %126 = arith.shli %125, %cst_15 : tensor<4x4x32xi32, #blocked>
      %127 = arith.shrui %123, %cst_16 : tensor<4x4x32xi32, #blocked>
      %128 = arith.ori %126, %127 : tensor<4x4x32xi32, #blocked>
      %129 = arith.addi %128, %cst_11 : tensor<4x4x32xi32, #blocked>
      %130 = arith.shrui %129, %cst_11 : tensor<4x4x32xi32, #blocked>
      %131 = arith.minui %130, %cst_22 : tensor<4x4x32xi32, #blocked>
      %132 = arith.shrui %113, %cst_17 : tensor<4x4x32xi32, #blocked>
      %133 = arith.ori %132, %131 : tensor<4x4x32xi32, #blocked>
      %134 = arith.trunci %133 : tensor<4x4x32xi32, #blocked> to tensor<4x4x32xi8, #blocked>
      %135 = tt.reshape %134 : tensor<4x4x32xi8, #blocked> -> tensor<4x4x16x2xi8, #blocked7>
      %outLHS, %outRHS = tt.split %135 : tensor<4x4x16x2xi8, #blocked7> -> tensor<4x4x16xi8, #blocked2>
      %136 = arith.shli %outRHS, %cst_2 : tensor<4x4x16xi8, #blocked2>
      %137 = arith.ori %outLHS, %136 : tensor<4x4x16xi8, #blocked2>
      %138 = tt.reshape %137 : tensor<4x4x16xi8, #blocked2> -> tensor<4x64xi8, #blocked8>
      %139 = tt.reshape %105 : tensor<4x4x1xi8, #linear> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %140 = tt.reshape %106 : tensor<4x4x1xi8, #blocked> -> tensor<4x4xi8, #linear2>
      %141 = ttg.convert_layout %140 : tensor<4x4xi8, #linear2> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %142 = arith.extui %141 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>> to tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %143 = arith.shli %142, %cst_30 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %144 = tt.bitcast %143 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %145 = tt.expand_dims %144 {axis = 2 : i32} : tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xbf16, #blocked>
      %146 = tt.broadcast %145 : tensor<4x4x1xbf16, #blocked> -> tensor<4x4x32xbf16, #blocked>
      %147 = tt.reshape %146 : tensor<4x4x32xbf16, #blocked> -> tensor<4x128xbf16, #blocked1>
      %148 = amdgpu.scaled_upcast_fp4 %138 scale %147 {axis = 1 : i32} : tensor<4x64xi8, #blocked8>, tensor<4x128xbf16, #blocked1> -> tensor<4x128xbf16, #blocked1>
      %149 = arith.cmpi eq, %139, %cst_31 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %150 = tt.expand_dims %149 {axis = 2 : i32} : tensor<4x4xi1, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xi1, #blocked>
      %151 = tt.broadcast %150 : tensor<4x4x1xi1, #blocked> -> tensor<4x4x32xi1, #blocked>
      %152 = tt.reshape %151 : tensor<4x4x32xi1, #blocked> -> tensor<4x128xi1, #blocked1>
      %153 = arith.select %152, %cst_0, %148 : tensor<4x128xi1, #blocked1>, tensor<4x128xbf16, #blocked1>
      %154 = ttg.local_alloc %153 : (tensor<4x128xbf16, #blocked1>) -> !ttg.memdesc<4x128xbf16, #shared, #smem>
      %155 = ttg.local_load %154 : !ttg.memdesc<4x128xbf16, #shared, #smem> -> tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>>
      %156 = arith.extui %70 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>> to tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %157 = arith.shli %156, %cst_19 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %158 = tt.bitcast %157 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %159 = tt.expand_dims %158 {axis = 2 : i32} : tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xbf16, #blocked4>
      %160 = tt.broadcast %159 : tensor<4x32x1xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked4>
      %161 = tt.trans %160 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked9>
      %162 = tt.reshape %161 : tensor<4x32x32xbf16, #blocked9> -> tensor<128x32xbf16, #blocked5>
      %163 = amdgpu.scaled_upcast_fp4 %77 scale %162 {axis = 0 : i32} : tensor<64x32xi8, #blocked3>, tensor<128x32xbf16, #blocked5> -> tensor<128x32xbf16, #blocked5>
      %164 = arith.cmpi eq, %70, %cst_20 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %165 = tt.expand_dims %164 {axis = 2 : i32} : tensor<4x32xi1, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xi1, #blocked4>
      %166 = tt.broadcast %165 : tensor<4x32x1xi1, #blocked4> -> tensor<4x32x32xi1, #blocked4>
      %167 = tt.trans %166 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xi1, #blocked4> -> tensor<4x32x32xi1, #blocked9>
      %168 = tt.reshape %167 : tensor<4x32x32xi1, #blocked9> -> tensor<128x32xi1, #blocked5>
      %169 = arith.select %168, %cst_21, %163 : tensor<128x32xi1, #blocked5>, tensor<128x32xbf16, #blocked5>
      %170 = ttg.local_alloc %169 : (tensor<128x32xbf16, #blocked5>) -> !ttg.memdesc<128x32xbf16, #shared, #smem>
      %171 = ttg.local_load %170 : !ttg.memdesc<128x32xbf16, #shared, #smem> -> tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>
      %172 = tt.dot %155, %171, %cst_3 : tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>> * tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> -> tensor<4x32xf32, #blocked3>
      %173 = arith.addf %172, %cst_3 : tensor<4x32xf32, #blocked3>
      %174 = arith.truncf %173 : tensor<4x32xf32, #blocked3> to tensor<4x32xbf16, #blocked3>
      %175 = arith.extsi %13 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %176 = arith.extsi %11 : i32 to i64
      %177 = tt.splat %176 : i64 -> tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %178 = arith.addi %177, %175 : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %179 = arith.extsi %32 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> to tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %180 = arith.extsi %30 : i32 to i64
      %181 = tt.splat %180 : i64 -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %182 = arith.addi %181, %179 : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %183 = arith.muli %7, %6 : i64
      %184 = tt.addptr %arg2, %183 : !tt.ptr<bf16>, i64
      %185 = tt.expand_dims %178 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %186 = arith.extsi %arg13 : i32 to i64
      %187 = tt.expand_dims %175 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %188 = arith.muli %186, %176 : i64
      %189 = tt.splat %186 : i64 -> tensor<4x1xi64, #blocked3>
      %190 = arith.muli %189, %187 : tensor<4x1xi64, #blocked3>
      %191 = tt.addptr %184, %188 : !tt.ptr<bf16>, i64
      %192 = tt.expand_dims %182 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %193 = tt.broadcast %190 : tensor<4x1xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %194 = tt.expand_dims %179 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %195 = tt.broadcast %194 : tensor<1x32xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %196 = tt.addptr %191, %180 : !tt.ptr<bf16>, i64
      %197 = arith.addi %195, %193 : tensor<4x32xi64, #blocked3>
      %198 = arith.extsi %arg4 : i32 to i64
      %199 = tt.splat %198 : i64 -> tensor<4x1xi64, #blocked3>
      %200 = arith.cmpi slt, %185, %199 : tensor<4x1xi64, #blocked3>
      %201 = arith.extsi %arg5 : i32 to i64
      %202 = tt.splat %201 : i64 -> tensor<1x32xi64, #blocked3>
      %203 = arith.cmpi slt, %192, %202 : tensor<1x32xi64, #blocked3>
      %204 = tt.broadcast %200 : tensor<4x1xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %205 = tt.broadcast %203 : tensor<1x32xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %206 = arith.andi %204, %205 : tensor<4x32xi1, #blocked3>
      %207 = tt.splat %196 : !tt.ptr<bf16> -> tensor<4x32x!tt.ptr<bf16>, #blocked3>
      %208 = tt.addptr %207, %197 : tensor<4x32x!tt.ptr<bf16>, #blocked3>, tensor<4x32xi64, #blocked3>
      tt.store %208, %174, %206 : tensor<4x32x!tt.ptr<bf16>, #blocked3>
    }
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(optimize-amd-lds-usage{lds-limit=0 target-arch=gfx950}, triton-scf-to-cf, convert-index-to-llvm{index-bitwidth=0}, allocate-amdgpu-shared-memory, convert-triton-amdgpu-to-llvm{arch=gfx950 ftz=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-cf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwidth=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info, convert-builtin-func-to-llvm{ftz=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_root/t3/ct3t3fw7xnhqwm57kt7agtaxeqou6t67nnepo76oz4ssegp6addy.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_root/t3/ct3t3fw7xnhqwm57kt7agtaxeqou6t67nnepo76oz4ssegp6addy.py:18:0: note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """Kernel for computing the matmul C = A x B.
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 4, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank4]:E1103 11:31:40.621000 513 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
ler_DP2_TP2: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
#blocked = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_batched_gemm_afp4_wfp4_pre_quant_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32) attributes {noinline = false} {
    %cst = arith.constant dense<127> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128xbf16, #blocked1>
    %cst_1 = arith.constant dense<2097152> : tensor<4x4x1xi32, #blocked>
    %cst_2 = arith.constant dense<4> : tensor<4x4x16xi8, #blocked2>
    %c31_i32 = arith.constant 31 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x32xf32, #blocked3>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %true = arith.constant true
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #blocked>
    %cst_5 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_7 = arith.constant dense<-2147483648> : tensor<4x4x32xi32, #blocked>
    %cst_8 = arith.constant dense<23> : tensor<4x4x32xi32, #blocked>
    %cst_9 = arith.constant dense<255> : tensor<4x4x32xi32, #blocked>
    %cst_10 = arith.constant dense<8388607> : tensor<4x4x32xi32, #blocked>
    %cst_11 = arith.constant dense<1> : tensor<4x4x32xi32, #blocked>
    %cst_12 = arith.constant dense<127> : tensor<4x4x32xi32, #blocked>
    %cst_13 = arith.constant dense<4194304> : tensor<4x4x32xi32, #blocked>
    %cst_14 = arith.constant dense<126> : tensor<4x4x32xi32, #blocked>
    %cst_15 = arith.constant dense<2> : tensor<4x4x32xi32, #blocked>
    %cst_16 = arith.constant dense<21> : tensor<4x4x32xi32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>> -> tensor<64x1xi32, #blocked6>
      %45 = arith.extsi %44 : tensor<64x1xi32, #blocked6> to tensor<64x1xi64, #blocked6>
      %46 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>> -> tensor<1x32xi32, #blocked6>
      %47 = tt.splat %arg10 : i32 -> tensor<1x32xi32, #blocked6>
      %48 = arith.muli %46, %47 : tensor<1x32xi32, #blocked6>
      %49 = arith.extsi %48 : tensor<1x32xi32, #blocked6> to tensor<1x32xi64, #blocked6>
      %50 = tt.broadcast %45 : tensor<64x1xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %51 = tt.broadcast %49 : tensor<1x32xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %52 = arith.addi %50, %51 : tensor<64x32xi64, #blocked6>
      %53 = tt.addptr %arg1, %42 : !tt.ptr<i8>, i64
      %54 = arith.extsi %arg14 : i32 to i64
      %55 = arith.muli %7, %54 : i64
      %56 = tt.addptr %arg3, %55 : !tt.ptr<i8>, i64
      %57 = tt.expand_dims %41 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<32x1xi32, #linear1>
      %58 = tt.splat %arg15 : i32 -> tensor<32x1xi32, #linear1>
      %59 = arith.muli %57, %58 : tensor<32x1xi32, #linear1>
      %60 = arith.extsi %59 : tensor<32x1xi32, #linear1> to tensor<32x1xi64, #linear1>
      %61 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>>
      %62 = tt.broadcast %60 : tensor<32x1xi64, #linear1> -> tensor<32x4xi64, #linear1>
      %63 = tt.expand_dims %61 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>> -> tensor<1x4xi32, #linear1>
      %64 = tt.broadcast %63 : tensor<1x4xi32, #linear1> -> tensor<32x4xi32, #linear1>
      %65 = arith.extsi %64 : tensor<32x4xi32, #linear1> to tensor<32x4xi64, #linear1>
      %66 = arith.addi %65, %62 : tensor<32x4xi64, #linear1>
      %67 = tt.splat %56 : !tt.ptr<i8> -> tensor<32x4x!tt.ptr<i8>, #linear1>
      %68 = tt.addptr %67, %66 : tensor<32x4x!tt.ptr<i8>, #linear1>, tensor<32x4xi64, #linear1>
      %69 = tt.load %68 : tensor<32x4x!tt.ptr<i8>, #linear1>
      %70 = tt.trans %69 {order = array<i32: 1, 0>} : tensor<32x4xi8, #linear1> -> tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %71 = tt.splat %29 : !tt.ptr<bf16> -> tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %72 = tt.addptr %71, %28 : tensor<4x128x!tt.ptr<bf16>, #blocked1>, tensor<4x128xi64, #blocked1>
      %73 = tt.load %72 : tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %74 = tt.splat %53 : !tt.ptr<i8> -> tensor<64x32x!tt.ptr<i8>, #blocked6>
      %75 = tt.addptr %74, %52 : tensor<64x32x!tt.ptr<i8>, #blocked6>, tensor<64x32xi64, #blocked6>
      %76 = tt.load %75 cacheModifier = cg : tensor<64x32x!tt.ptr<i8>, #blocked6>
      %77 = ttg.convert_layout %76 : tensor<64x32xi8, #blocked6> -> tensor<64x32xi8, #blocked3>
      %78 = tt.reshape %73 : tensor<4x128xbf16, #blocked1> -> tensor<4x4x32xbf16, #blocked>
      %79 = math.absf %78 : tensor<4x4x32xbf16, #blocked>
      %80 = arith.extf %79 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %81 = "tt.reduce"(%80) <{axis = 2 : i32}> ({
      ^bb0(%arg16: f32, %arg17: f32):
        %209 = arith.maxnumf %arg16, %arg17 : f32
        tt.reduce.return %209 : f32
      }) : (tensor<4x4x32xf32, #blocked>) -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>>
      %82 = ttg.convert_layout %81 : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>>
      %83 = tt.expand_dims %82 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>> -> tensor<4x4x1xf32, #linear>
      %84 = tt.expand_dims %81 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xf32, #blocked>
      %85 = tt.bitcast %83 : tensor<4x4x1xf32, #linear> -> tensor<4x4x1xi32, #linear>
      %86 = tt.bitcast %84 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %87 = arith.addi %85, %cst_28 : tensor<4x4x1xi32, #linear>
      %88 = arith.addi %86, %cst_1 : tensor<4x4x1xi32, #blocked>
      %89 = tt.bitcast %87 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xi32, #linear>
      %90 = tt.bitcast %88 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %91 = arith.andi %89, %cst_27 : tensor<4x4x1xi32, #linear>
      %92 = arith.andi %90, %cst_4 : tensor<4x4x1xi32, #blocked>
      %93 = tt.bitcast %91 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xf32, #linear>
      %94 = tt.bitcast %92 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xf32, #blocked>
      %95 = math.log2 %93 : tensor<4x4x1xf32, #linear>
      %96 = math.log2 %94 : tensor<4x4x1xf32, #blocked>
      %97 = math.floor %95 : tensor<4x4x1xf32, #linear>
      %98 = math.floor %96 : tensor<4x4x1xf32, #blocked>
      %99 = arith.subf %97, %cst_26 : tensor<4x4x1xf32, #linear>
      %100 = arith.subf %98, %cst_5 : tensor<4x4x1xf32, #blocked>
      %101 = tt.clampf %99, %cst_25, %cst_24, propagateNan = none : tensor<4x4x1xf32, #linear>
      %102 = tt.clampf %100, %cst_18, %cst_23, propagateNan = none : tensor<4x4x1xf32, #blocked>
      %103 = arith.fptoui %101 : tensor<4x4x1xf32, #linear> to tensor<4x4x1xi8, #linear>
      %104 = arith.fptoui %102 : tensor<4x4x1xf32, #blocked> to tensor<4x4x1xi8, #blocked>
      %105 = arith.addi %103, %cst_29 : tensor<4x4x1xi8, #linear>
      %106 = arith.addi %104, %cst : tensor<4x4x1xi8, #blocked>
      %107 = arith.subf %cst_6, %102 : tensor<4x4x1xf32, #blocked>
      %108 = math.exp2 %107 : tensor<4x4x1xf32, #blocked>
      %109 = arith.extf %78 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %110 = tt.broadcast %108 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x32xf32, #blocked>
      %111 = arith.mulf %109, %110 : tensor<4x4x32xf32, #blocked>
      %112 = tt.bitcast %111 : tensor<4x4x32xf32, #blocked> -> tensor<4x4x32xi32, #blocked>
      %113 = arith.andi %112, %cst_7 : tensor<4x4x32xi32, #blocked>
      %114 = arith.shrui %112, %cst_8 : tensor<4x4x32xi32, #blocked>
      %115 = arith.andi %114, %cst_9 : tensor<4x4x32xi32, #blocked>
      %116 = arith.andi %112, %cst_10 : tensor<4x4x32xi32, #blocked>
      %117 = arith.addi %115, %cst_11 : tensor<4x4x32xi32, #blocked>
      %118 = arith.subi %cst_12, %117 : tensor<4x4x32xi32, #blocked>
      %119 = arith.cmpi ult, %115, %cst_12 : tensor<4x4x32xi32, #blocked>
      %120 = arith.shrui %116, %cst_11 : tensor<4x4x32xi32, #blocked>
      %121 = arith.ori %120, %cst_13 : tensor<4x4x32xi32, #blocked>
      %122 = arith.shrui %121, %118 : tensor<4x4x32xi32, #blocked>
      %123 = arith.select %119, %122, %116 : tensor<4x4x32xi1, #blocked>, tensor<4x4x32xi32, #blocked>
      %124 = arith.maxui %115, %cst_14 : tensor<4x4x32xi32, #blocked>
      %125 = arith.subi %124, %cst_14 : tensor<4x4x32xi32, #blocked>
      %126 = arith.shli %125, %cst_15 : tensor<4x4x32xi32, #blocked>
      %127 = arith.shrui %123, %cst_16 : tensor<4x4x32xi32, #blocked>
      %128 = arith.ori %126, %127 : tensor<4x4x32xi32, #blocked>
      %129 = arith.addi %128, %cst_11 : tensor<4x4x32xi32, #blocked>
      %130 = arith.shrui %129, %cst_11 : tensor<4x4x32xi32, #blocked>
      %131 = arith.minui %130, %cst_22 : tensor<4x4x32xi32, #blocked>
      %132 = arith.shrui %113, %cst_17 : tensor<4x4x32xi32, #blocked>
      %133 = arith.ori %132, %131 : tensor<4x4x32xi32, #blocked>
      %134 = arith.trunci %133 : tensor<4x4x32xi32, #blocked> to tensor<4x4x32xi8, #blocked>
      %135 = tt.reshape %134 : tensor<4x4x32xi8, #blocked> -> tensor<4x4x16x2xi8, #blocked7>
      %outLHS, %outRHS = tt.split %135 : tensor<4x4x16x2xi8, #blocked7> -> tensor<4x4x16xi8, #blocked2>
      %136 = arith.shli %outRHS, %cst_2 : tensor<4x4x16xi8, #blocked2>
      %137 = arith.ori %outLHS, %136 : tensor<4x4x16xi8, #blocked2>
      %138 = tt.reshape %137 : tensor<4x4x16xi8, #blocked2> -> tensor<4x64xi8, #blocked8>
      %139 = tt.reshape %105 : tensor<4x4x1xi8, #linear> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %140 = tt.reshape %106 : tensor<4x4x1xi8, #blocked> -> tensor<4x4xi8, #linear2>
      %141 = ttg.convert_layout %140 : tensor<4x4xi8, #linear2> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %142 = arith.extui %141 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>> to tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %143 = arith.shli %142, %cst_30 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %144 = tt.bitcast %143 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %145 = tt.expand_dims %144 {axis = 2 : i32} : tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xbf16, #blocked>
      %146 = tt.broadcast %145 : tensor<4x4x1xbf16, #blocked> -> tensor<4x4x32xbf16, #blocked>
      %147 = tt.reshape %146 : tensor<4x4x32xbf16, #blocked> -> tensor<4x128xbf16, #blocked1>
      %148 = amdgpu.scaled_upcast_fp4 %138 scale %147 {axis = 1 : i32} : tensor<4x64xi8, #blocked8>, tensor<4x128xbf16, #blocked1> -> tensor<4x128xbf16, #blocked1>
      %149 = arith.cmpi eq, %139, %cst_31 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %150 = tt.expand_dims %149 {axis = 2 : i32} : tensor<4x4xi1, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xi1, #blocked>
      %151 = tt.broadcast %150 : tensor<4x4x1xi1, #blocked> -> tensor<4x4x32xi1, #blocked>
      %152 = tt.reshape %151 : tensor<4x4x32xi1, #blocked> -> tensor<4x128xi1, #blocked1>
      %153 = arith.select %152, %cst_0, %148 : tensor<4x128xi1, #blocked1>, tensor<4x128xbf16, #blocked1>
      %154 = ttg.local_alloc %153 : (tensor<4x128xbf16, #blocked1>) -> !ttg.memdesc<4x128xbf16, #shared, #smem>
      %155 = ttg.local_load %154 : !ttg.memdesc<4x128xbf16, #shared, #smem> -> tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>>
      %156 = arith.extui %70 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>> to tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %157 = arith.shli %156, %cst_19 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %158 = tt.bitcast %157 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %159 = tt.expand_dims %158 {axis = 2 : i32} : tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xbf16, #blocked4>
      %160 = tt.broadcast %159 : tensor<4x32x1xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked4>
      %161 = tt.trans %160 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked9>
      %162 = tt.reshape %161 : tensor<4x32x32xbf16, #blocked9> -> tensor<128x32xbf16, #blocked5>
      %163 = amdgpu.scaled_upcast_fp4 %77 scale %162 {axis = 0 : i32} : tensor<64x32xi8, #blocked3>, tensor<128x32xbf16, #blocked5> -> tensor<128x32xbf16, #blocked5>
      %164 = arith.cmpi eq, %70, %cst_20 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %165 = tt.expand_dims %164 {axis = 2 : i32} : tensor<4x32xi1, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xi1, #blocked4>
      %166 = tt.broadcast %165 : tensor<4x32x1xi1, #blocked4> -> tensor<4x32x32xi1, #blocked4>
      %167 = tt.trans %166 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xi1, #blocked4> -> tensor<4x32x32xi1, #blocked9>
      %168 = tt.reshape %167 : tensor<4x32x32xi1, #blocked9> -> tensor<128x32xi1, #blocked5>
      %169 = arith.select %168, %cst_21, %163 : tensor<128x32xi1, #blocked5>, tensor<128x32xbf16, #blocked5>
      %170 = ttg.local_alloc %169 : (tensor<128x32xbf16, #blocked5>) -> !ttg.memdesc<128x32xbf16, #shared, #smem>
      %171 = ttg.local_load %170 : !ttg.memdesc<128x32xbf16, #shared, #smem> -> tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>
      %172 = tt.dot %155, %171, %cst_3 : tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>> * tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> -> tensor<4x32xf32, #blocked3>
      %173 = arith.addf %172, %cst_3 : tensor<4x32xf32, #blocked3>
      %174 = arith.truncf %173 : tensor<4x32xf32, #blocked3> to tensor<4x32xbf16, #blocked3>
      %175 = arith.extsi %13 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %176 = arith.extsi %11 : i32 to i64
      %177 = tt.splat %176 : i64 -> tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %178 = arith.addi %177, %175 : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %179 = arith.extsi %32 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> to tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %180 = arith.extsi %30 : i32 to i64
      %181 = tt.splat %180 : i64 -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %182 = arith.addi %181, %179 : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %183 = arith.muli %7, %6 : i64
      %184 = tt.addptr %arg2, %183 : !tt.ptr<bf16>, i64
      %185 = tt.expand_dims %178 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %186 = arith.extsi %arg13 : i32 to i64
      %187 = tt.expand_dims %175 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %188 = arith.muli %186, %176 : i64
      %189 = tt.splat %186 : i64 -> tensor<4x1xi64, #blocked3>
      %190 = arith.muli %189, %187 : tensor<4x1xi64, #blocked3>
      %191 = tt.addptr %184, %188 : !tt.ptr<bf16>, i64
      %192 = tt.expand_dims %182 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %193 = tt.broadcast %190 : tensor<4x1xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %194 = tt.expand_dims %179 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %195 = tt.broadcast %194 : tensor<1x32xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %196 = tt.addptr %191, %180 : !tt.ptr<bf16>, i64
      %197 = arith.addi %195, %193 : tensor<4x32xi64, #blocked3>
      %198 = arith.extsi %arg4 : i32 to i64
      %199 = tt.splat %198 : i64 -> tensor<4x1xi64, #blocked3>
      %200 = arith.cmpi slt, %185, %199 : tensor<4x1xi64, #blocked3>
      %201 = arith.extsi %arg5 : i32 to i64
      %202 = tt.splat %201 : i64 -> tensor<1x32xi64, #blocked3>
      %203 = arith.cmpi slt, %192, %202 : tensor<1x32xi64, #blocked3>
      %204 = tt.broadcast %200 : tensor<4x1xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %205 = tt.broadcast %203 : tensor<1x32xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %206 = arith.andi %204, %205 : tensor<4x32xi1, #blocked3>
      %207 = tt.splat %196 : !tt.ptr<bf16> -> tensor<4x32x!tt.ptr<bf16>, #blocked3>
      %208 = tt.addptr %207, %197 : tensor<4x32x!tt.ptr<bf16>, #blocked3>, tensor<4x32xi64, #blocked3>
      tt.store %208, %174, %206 : tensor<4x32x!tt.ptr<bf16>, #blocked3>
    }
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(optimize-amd-lds-usage{lds-limit=0 target-arch=gfx950}, triton-scf-to-cf, convert-index-to-llvm{index-bitwidth=0}, allocate-amdgpu-shared-memory, convert-triton-amdgpu-to-llvm{arch=gfx950 ftz=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-cf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwidth=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info, convert-builtin-func-to-llvm{ftz=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_root/os/coskrv2u2s2qfnooggtc5a7exftk3emzbeae7q5c6xdtn6nlefwk.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_root/os/coskrv2u2s2qfnooggtc5a7exftk3emzbeae7q5c6xdtn6nlefwk.py:18:0: note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """Kernel for computing the matmul C = A x B.
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 2, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank2]:E1103 11:31:40.710000 511 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
ler_DP1_TP1: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
#blocked = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_batched_gemm_afp4_wfp4_pre_quant_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32) attributes {noinline = false} {
    %cst = arith.constant dense<127> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128xbf16, #blocked1>
    %cst_1 = arith.constant dense<2097152> : tensor<4x4x1xi32, #blocked>
    %cst_2 = arith.constant dense<4> : tensor<4x4x16xi8, #blocked2>
    %c31_i32 = arith.constant 31 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x32xf32, #blocked3>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %true = arith.constant true
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #blocked>
    %cst_5 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_7 = arith.constant dense<-2147483648> : tensor<4x4x32xi32, #blocked>
    %cst_8 = arith.constant dense<23> : tensor<4x4x32xi32, #blocked>
    %cst_9 = arith.constant dense<255> : tensor<4x4x32xi32, #blocked>
    %cst_10 = arith.constant dense<8388607> : tensor<4x4x32xi32, #blocked>
    %cst_11 = arith.constant dense<1> : tensor<4x4x32xi32, #blocked>
    %cst_12 = arith.constant dense<127> : tensor<4x4x32xi32, #blocked>
    %cst_13 = arith.constant dense<4194304> : tensor<4x4x32xi32, #blocked>
    %cst_14 = arith.constant dense<126> : tensor<4x4x32xi32, #blocked>
    %cst_15 = arith.constant dense<2> : tensor<4x4x32xi32, #blocked>
    %cst_16 = arith.constant dense<21> : tensor<4x4x32xi32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>> -> tensor<64x1xi32, #blocked6>
      %45 = arith.extsi %44 : tensor<64x1xi32, #blocked6> to tensor<64x1xi64, #blocked6>
      %46 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>> -> tensor<1x32xi32, #blocked6>
      %47 = tt.splat %arg10 : i32 -> tensor<1x32xi32, #blocked6>
      %48 = arith.muli %46, %47 : tensor<1x32xi32, #blocked6>
      %49 = arith.extsi %48 : tensor<1x32xi32, #blocked6> to tensor<1x32xi64, #blocked6>
      %50 = tt.broadcast %45 : tensor<64x1xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %51 = tt.broadcast %49 : tensor<1x32xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %52 = arith.addi %50, %51 : tensor<64x32xi64, #blocked6>
      %53 = tt.addptr %arg1, %42 : !tt.ptr<i8>, i64
      %54 = arith.extsi %arg14 : i32 to i64
      %55 = arith.muli %7, %54 : i64
      %56 = tt.addptr %arg3, %55 : !tt.ptr<i8>, i64
      %57 = tt.expand_dims %41 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<32x1xi32, #linear1>
      %58 = tt.splat %arg15 : i32 -> tensor<32x1xi32, #linear1>
      %59 = arith.muli %57, %58 : tensor<32x1xi32, #linear1>
      %60 = arith.extsi %59 : tensor<32x1xi32, #linear1> to tensor<32x1xi64, #linear1>
      %61 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>>
      %62 = tt.broadcast %60 : tensor<32x1xi64, #linear1> -> tensor<32x4xi64, #linear1>
      %63 = tt.expand_dims %61 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>> -> tensor<1x4xi32, #linear1>
      %64 = tt.broadcast %63 : tensor<1x4xi32, #linear1> -> tensor<32x4xi32, #linear1>
      %65 = arith.extsi %64 : tensor<32x4xi32, #linear1> to tensor<32x4xi64, #linear1>
      %66 = arith.addi %65, %62 : tensor<32x4xi64, #linear1>
      %67 = tt.splat %56 : !tt.ptr<i8> -> tensor<32x4x!tt.ptr<i8>, #linear1>
      %68 = tt.addptr %67, %66 : tensor<32x4x!tt.ptr<i8>, #linear1>, tensor<32x4xi64, #linear1>
      %69 = tt.load %68 : tensor<32x4x!tt.ptr<i8>, #linear1>
      %70 = tt.trans %69 {order = array<i32: 1, 0>} : tensor<32x4xi8, #linear1> -> tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %71 = tt.splat %29 : !tt.ptr<bf16> -> tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %72 = tt.addptr %71, %28 : tensor<4x128x!tt.ptr<bf16>, #blocked1>, tensor<4x128xi64, #blocked1>
      %73 = tt.load %72 : tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %74 = tt.splat %53 : !tt.ptr<i8> -> tensor<64x32x!tt.ptr<i8>, #blocked6>
      %75 = tt.addptr %74, %52 : tensor<64x32x!tt.ptr<i8>, #blocked6>, tensor<64x32xi64, #blocked6>
      %76 = tt.load %75 cacheModifier = cg : tensor<64x32x!tt.ptr<i8>, #blocked6>
      %77 = ttg.convert_layout %76 : tensor<64x32xi8, #blocked6> -> tensor<64x32xi8, #blocked3>
      %78 = tt.reshape %73 : tensor<4x128xbf16, #blocked1> -> tensor<4x4x32xbf16, #blocked>
      %79 = math.absf %78 : tensor<4x4x32xbf16, #blocked>
      %80 = arith.extf %79 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %81 = "tt.reduce"(%80) <{axis = 2 : i32}> ({
      ^bb0(%arg16: f32, %arg17: f32):
        %209 = arith.maxnumf %arg16, %arg17 : f32
        tt.reduce.return %209 : f32
      }) : (tensor<4x4x32xf32, #blocked>) -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>>
      %82 = ttg.convert_layout %81 : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>>
      %83 = tt.expand_dims %82 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>> -> tensor<4x4x1xf32, #linear>
      %84 = tt.expand_dims %81 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xf32, #blocked>
      %85 = tt.bitcast %83 : tensor<4x4x1xf32, #linear> -> tensor<4x4x1xi32, #linear>
      %86 = tt.bitcast %84 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %87 = arith.addi %85, %cst_28 : tensor<4x4x1xi32, #linear>
      %88 = arith.addi %86, %cst_1 : tensor<4x4x1xi32, #blocked>
      %89 = tt.bitcast %87 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xi32, #linear>
      %90 = tt.bitcast %88 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %91 = arith.andi %89, %cst_27 : tensor<4x4x1xi32, #linear>
      %92 = arith.andi %90, %cst_4 : tensor<4x4x1xi32, #blocked>
      %93 = tt.bitcast %91 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xf32, #linear>
      %94 = tt.bitcast %92 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xf32, #blocked>
      %95 = math.log2 %93 : tensor<4x4x1xf32, #linear>
      %96 = math.log2 %94 : tensor<4x4x1xf32, #blocked>
      %97 = math.floor %95 : tensor<4x4x1xf32, #linear>
      %98 = math.floor %96 : tensor<4x4x1xf32, #blocked>
      %99 = arith.subf %97, %cst_26 : tensor<4x4x1xf32, #linear>
      %100 = arith.subf %98, %cst_5 : tensor<4x4x1xf32, #blocked>
      %101 = tt.clampf %99, %cst_25, %cst_24, propagateNan = none : tensor<4x4x1xf32, #linear>
      %102 = tt.clampf %100, %cst_18, %cst_23, propagateNan = none : tensor<4x4x1xf32, #blocked>
      %103 = arith.fptoui %101 : tensor<4x4x1xf32, #linear> to tensor<4x4x1xi8, #linear>
      %104 = arith.fptoui %102 : tensor<4x4x1xf32, #blocked> to tensor<4x4x1xi8, #blocked>
      %105 = arith.addi %103, %cst_29 : tensor<4x4x1xi8, #linear>
      %106 = arith.addi %104, %cst : tensor<4x4x1xi8, #blocked>
      %107 = arith.subf %cst_6, %102 : tensor<4x4x1xf32, #blocked>
      %108 = math.exp2 %107 : tensor<4x4x1xf32, #blocked>
      %109 = arith.extf %78 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %110 = tt.broadcast %108 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x32xf32, #blocked>
      %111 = arith.mulf %109, %110 : tensor<4x4x32xf32, #blocked>
      %112 = tt.bitcast %111 : tensor<4x4x32xf32, #blocked> -> tensor<4x4x32xi32, #blocked>
      %113 = arith.andi %112, %cst_7 : tensor<4x4x32xi32, #blocked>
      %114 = arith.shrui %112, %cst_8 : tensor<4x4x32xi32, #blocked>
      %115 = arith.andi %114, %cst_9 : tensor<4x4x32xi32, #blocked>
      %116 = arith.andi %112, %cst_10 : tensor<4x4x32xi32, #blocked>
      %117 = arith.addi %115, %cst_11 : tensor<4x4x32xi32, #blocked>
      %118 = arith.subi %cst_12, %117 : tensor<4x4x32xi32, #blocked>
      %119 = arith.cmpi ult, %115, %cst_12 : tensor<4x4x32xi32, #blocked>
      %120 = arith.shrui %116, %cst_11 : tensor<4x4x32xi32, #blocked>
      %121 = arith.ori %120, %cst_13 : tensor<4x4x32xi32, #blocked>
      %122 = arith.shrui %121, %118 : tensor<4x4x32xi32, #blocked>
      %123 = arith.select %119, %122, %116 : tensor<4x4x32xi1, #blocked>, tensor<4x4x32xi32, #blocked>
      %124 = arith.maxui %115, %cst_14 : tensor<4x4x32xi32, #blocked>
      %125 = arith.subi %124, %cst_14 : tensor<4x4x32xi32, #blocked>
      %126 = arith.shli %125, %cst_15 : tensor<4x4x32xi32, #blocked>
      %127 = arith.shrui %123, %cst_16 : tensor<4x4x32xi32, #blocked>
      %128 = arith.ori %126, %127 : tensor<4x4x32xi32, #blocked>
      %129 = arith.addi %128, %cst_11 : tensor<4x4x32xi32, #blocked>
      %130 = arith.shrui %129, %cst_11 : tensor<4x4x32xi32, #blocked>
      %131 = arith.minui %130, %cst_22 : tensor<4x4x32xi32, #blocked>
      %132 = arith.shrui %113, %cst_17 : tensor<4x4x32xi32, #blocked>
      %133 = arith.ori %132, %131 : tensor<4x4x32xi32, #blocked>
      %134 = arith.trunci %133 : tensor<4x4x32xi32, #blocked> to tensor<4x4x32xi8, #blocked>
      %135 = tt.reshape %134 : tensor<4x4x32xi8, #blocked> -> tensor<4x4x16x2xi8, #blocked7>
      %outLHS, %outRHS = tt.split %135 : tensor<4x4x16x2xi8, #blocked7> -> tensor<4x4x16xi8, #blocked2>
      %136 = arith.shli %outRHS, %cst_2 : tensor<4x4x16xi8, #blocked2>
      %137 = arith.ori %outLHS, %136 : tensor<4x4x16xi8, #blocked2>
      %138 = tt.reshape %137 : tensor<4x4x16xi8, #blocked2> -> tensor<4x64xi8, #blocked8>
      %139 = tt.reshape %105 : tensor<4x4x1xi8, #linear> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %140 = tt.reshape %106 : tensor<4x4x1xi8, #blocked> -> tensor<4x4xi8, #linear2>
      %141 = ttg.convert_layout %140 : tensor<4x4xi8, #linear2> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %142 = arith.extui %141 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>> to tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %143 = arith.shli %142, %cst_30 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %144 = tt.bitcast %143 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %145 = tt.expand_dims %144 {axis = 2 : i32} : tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xbf16, #blocked>
      %146 = tt.broadcast %145 : tensor<4x4x1xbf16, #blocked> -> tensor<4x4x32xbf16, #blocked>
      %147 = tt.reshape %146 : tensor<4x4x32xbf16, #blocked> -> tensor<4x128xbf16, #blocked1>
      %148 = amdgpu.scaled_upcast_fp4 %138 scale %147 {axis = 1 : i32} : tensor<4x64xi8, #blocked8>, tensor<4x128xbf16, #blocked1> -> tensor<4x128xbf16, #blocked1>
      %149 = arith.cmpi eq, %139, %cst_31 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %150 = tt.expand_dims %149 {axis = 2 : i32} : tensor<4x4xi1, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xi1, #blocked>
      %151 = tt.broadcast %150 : tensor<4x4x1xi1, #blocked> -> tensor<4x4x32xi1, #blocked>
      %152 = tt.reshape %151 : tensor<4x4x32xi1, #blocked> -> tensor<4x128xi1, #blocked1>
      %153 = arith.select %152, %cst_0, %148 : tensor<4x128xi1, #blocked1>, tensor<4x128xbf16, #blocked1>
      %154 = ttg.local_alloc %153 : (tensor<4x128xbf16, #blocked1>) -> !ttg.memdesc<4x128xbf16, #shared, #smem>
      %155 = ttg.local_load %154 : !ttg.memdesc<4x128xbf16, #shared, #smem> -> tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>>
      %156 = arith.extui %70 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>> to tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %157 = arith.shli %156, %cst_19 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %158 = tt.bitcast %157 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %159 = tt.expand_dims %158 {axis = 2 : i32} : tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xbf16, #blocked4>
      %160 = tt.broadcast %159 : tensor<4x32x1xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked4>
      %161 = tt.trans %160 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked9>
      %162 = tt.reshape %161 : tensor<4x32x32xbf16, #blocked9> -> tensor<128x32xbf16, #blocked5>
      %163 = amdgpu.scaled_upcast_fp4 %77 scale %162 {axis = 0 : i32} : tensor<64x32xi8, #blocked3>, tensor<128x32xbf16, #blocked5> -> tensor<128x32xbf16, #blocked5>
      %164 = arith.cmpi eq, %70, %cst_20 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %165 = tt.expand_dims %164 {axis = 2 : i32} : tensor<4x32xi1, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xi1, #blocked4>
      %166 = tt.broadcast %165 : tensor<4x32x1xi1, #blocked4> -> tensor<4x32x32xi1, #blocked4>
      %167 = tt.trans %166 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xi1, #blocked4> -> tensor<4x32x32xi1, #blocked9>
      %168 = tt.reshape %167 : tensor<4x32x32xi1, #blocked9> -> tensor<128x32xi1, #blocked5>
      %169 = arith.select %168, %cst_21, %163 : tensor<128x32xi1, #blocked5>, tensor<128x32xbf16, #blocked5>
      %170 = ttg.local_alloc %169 : (tensor<128x32xbf16, #blocked5>) -> !ttg.memdesc<128x32xbf16, #shared, #smem>
      %171 = ttg.local_load %170 : !ttg.memdesc<128x32xbf16, #shared, #smem> -> tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>
      %172 = tt.dot %155, %171, %cst_3 : tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>> * tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> -> tensor<4x32xf32, #blocked3>
      %173 = arith.addf %172, %cst_3 : tensor<4x32xf32, #blocked3>
      %174 = arith.truncf %173 : tensor<4x32xf32, #blocked3> to tensor<4x32xbf16, #blocked3>
      %175 = arith.extsi %13 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %176 = arith.extsi %11 : i32 to i64
      %177 = tt.splat %176 : i64 -> tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %178 = arith.addi %177, %175 : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %179 = arith.extsi %32 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> to tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %180 = arith.extsi %30 : i32 to i64
      %181 = tt.splat %180 : i64 -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %182 = arith.addi %181, %179 : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %183 = arith.muli %7, %6 : i64
      %184 = tt.addptr %arg2, %183 : !tt.ptr<bf16>, i64
      %185 = tt.expand_dims %178 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %186 = arith.extsi %arg13 : i32 to i64
      %187 = tt.expand_dims %175 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %188 = arith.muli %186, %176 : i64
      %189 = tt.splat %186 : i64 -> tensor<4x1xi64, #blocked3>
      %190 = arith.muli %189, %187 : tensor<4x1xi64, #blocked3>
      %191 = tt.addptr %184, %188 : !tt.ptr<bf16>, i64
      %192 = tt.expand_dims %182 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %193 = tt.broadcast %190 : tensor<4x1xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %194 = tt.expand_dims %179 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %195 = tt.broadcast %194 : tensor<1x32xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %196 = tt.addptr %191, %180 : !tt.ptr<bf16>, i64
      %197 = arith.addi %195, %193 : tensor<4x32xi64, #blocked3>
      %198 = arith.extsi %arg4 : i32 to i64
      %199 = tt.splat %198 : i64 -> tensor<4x1xi64, #blocked3>
      %200 = arith.cmpi slt, %185, %199 : tensor<4x1xi64, #blocked3>
      %201 = arith.extsi %arg5 : i32 to i64
      %202 = tt.splat %201 : i64 -> tensor<1x32xi64, #blocked3>
      %203 = arith.cmpi slt, %192, %202 : tensor<1x32xi64, #blocked3>
      %204 = tt.broadcast %200 : tensor<4x1xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %205 = tt.broadcast %203 : tensor<1x32xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %206 = arith.andi %204, %205 : tensor<4x32xi1, #blocked3>
      %207 = tt.splat %196 : !tt.ptr<bf16> -> tensor<4x32x!tt.ptr<bf16>, #blocked3>
      %208 = tt.addptr %207, %197 : tensor<4x32x!tt.ptr<bf16>, #blocked3>, tensor<4x32xi64, #blocked3>
      tt.store %208, %174, %206 : tensor<4x32x!tt.ptr<bf16>, #blocked3>
    }
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(optimize-amd-lds-usage{lds-limit=0 target-arch=gfx950}, triton-scf-to-cf, convert-index-to-llvm{index-bitwidth=0}, allocate-amdgpu-shared-memory, convert-triton-amdgpu-to-llvm{arch=gfx950 ftz=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-cf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwidth=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info, convert-builtin-func-to-llvm{ftz=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_root/y7/cy7x5redwtow3wcql2by2qe6mm5bzoehacdoxkuwodykghgvcuhb.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_root/y7/cy7x5redwtow3wcql2by2qe6mm5bzoehacdoxkuwodykghgvcuhb.py:18:0: note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """Kernel for computing the matmul C = A x B.
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 1, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank1]:E1103 11:31:40.808000 510 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
ler_DP7_TP7: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
#blocked = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_batched_gemm_afp4_wfp4_pre_quant_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32) attributes {noinline = false} {
    %cst = arith.constant dense<127> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128xbf16, #blocked1>
    %cst_1 = arith.constant dense<2097152> : tensor<4x4x1xi32, #blocked>
    %cst_2 = arith.constant dense<4> : tensor<4x4x16xi8, #blocked2>
    %c31_i32 = arith.constant 31 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x32xf32, #blocked3>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %true = arith.constant true
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #blocked>
    %cst_5 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_7 = arith.constant dense<-2147483648> : tensor<4x4x32xi32, #blocked>
    %cst_8 = arith.constant dense<23> : tensor<4x4x32xi32, #blocked>
    %cst_9 = arith.constant dense<255> : tensor<4x4x32xi32, #blocked>
    %cst_10 = arith.constant dense<8388607> : tensor<4x4x32xi32, #blocked>
    %cst_11 = arith.constant dense<1> : tensor<4x4x32xi32, #blocked>
    %cst_12 = arith.constant dense<127> : tensor<4x4x32xi32, #blocked>
    %cst_13 = arith.constant dense<4194304> : tensor<4x4x32xi32, #blocked>
    %cst_14 = arith.constant dense<126> : tensor<4x4x32xi32, #blocked>
    %cst_15 = arith.constant dense<2> : tensor<4x4x32xi32, #blocked>
    %cst_16 = arith.constant dense<21> : tensor<4x4x32xi32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30ler_DP0_TP0: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg#.blocked = slice<{dim = 1, parent = #blocked6}>#ttg>.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>

#      blocked1% = 44#ttg = .blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>tt.expand_dims
# blocked2 = %#ttg.43blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
 {#blockedaxis3 =  = #ttg1.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}> : 
#iblocked432 = } :#ttg .blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>tensor<
#64blocked5x = i#ttg32.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>, 
##blocked6ttg = .slice<{dim = 1, parent = #blocked6}>#ttg> -> .blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>tensor<64
#xblocked71 = x#ttgi.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>32, 
##blocked8blocked = 6#ttg>.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>

#      blocked9% = 45#ttg = .blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>arith.extsi
 #linear% = 44#ttg .linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>:
# linear1tensor< = 64#ttgx.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>1x
#ilinear232 = , #ttg#.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>blocked
#6shared = > to#ttg .swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>tensor<
#64smem = x#ttg1x.shared_memoryi
64module,  attributes {#"tblockedtg6.n>um
-c      %ta46 = s"tt.expand_dims  = %140 :  {i32axis = , "0tt : g.inu32m-} wa: rptensor<32s"x = 4i : i32, 32, #ttg.target = ttg".slice<{dim = 0, parent = #blocked6}>hi>p: -> gftensor<x9150x"32, "xttig.32, th#reblockedad6>s-
pe      r-%wa47 = rptt.splat" =  64 : %i32arg10 :}  {
i  32tt.func ->  public tensor<@1_batched_gemm_afp4_wfp4_pre_quant_kernelx(32x%arg0i: 32, !tt#.ptr<bf16>blocked {6tt.divisibility = >16 : 
i32      }, %%arg148:  = !ttarith.muli %.ptr<i8>46 {tt.divisibility, % = 1647 : i 32}: , %tensor<arg2: 1!ttx.ptr<bf16>32 {tt.divisibilityx = 16i : 32i32, }, #blocked%arg36: !>tt.
ptr<i8> {      tt.divisibility = %16 : 49 = i32arith.extsi }, %%arg448: i 32 {: tt.divisibility = tensor<16 : 1i32x}, 32x%arg5i: i3232 {, tt.divisibility = #blocked616 : > toi32 }, tensor<1x%arg632x: ii32 {64, tt.divisibility = #16 : blockedi326}, >%arg7
: i      32 {%tt.divisibility = 5016 :  = i32tt.broadcast},  %arg8%: i4532 { :tt.divisibility =  16tensor< : i6432}x, %1arg9: xi32i {tt.divisibility64 = 16,  : i#blocked32}6>, % arg10: ->i32  {tt.divisibilitytensor< = 1664 : ix32}32, %xarg11: ii3264 {tt.divisibility,  = 16# : iblocked32}6>, %
arg12:       i32% {tt.divisibility51 = 16 =  : itt.broadcast32} , %%arg13: 49i32 : {tt.divisibility  = 16tensor< : i132}x, %32arg14: xi32i {tt.divisibility64 = 16, #blocked6 : i> ->32} , %tensor<arg15: 64i32x)32 attributes {xnoinline = ifalse64},  {#blocked6>

          %cst% = 52arith.constant =  arith.addidense< 127%> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128x50bf16, ,#blocked 1>%
    51%cst_1  = arith.constant:  dense<tensor<2097152>64 : tensor<x4x32x4xi641x, i32#, #blockedblocked>6
    >%cst_2
 = arith.constant       dense<%53 = 4>tt.addptr : tensor< 4x%4xarg116x,i8 , #%blocked242>
     %:c31_i32 =  arith.constant! tt31 : .i32ptr<i8>
    , %cst_3i = arith.constant64 dense<
0.000000e+00>       : tensor<%4x5432x = f32, arith.extsi#blocked 3>%
    arg14%c32_i32  = arith.constant: 32  : ii32
32    % to c4_i32 = iarith.constant64 4
 : i      32
%    %55true =  = arith.constantarith.muli  true%
    7%c0_i32,  = arith.constant%54 0  : i:32
     %icst_4 = 64arith.constant
 dense<      -8388608>%56 =  : tensor<tt.addptr4x 4x%1xarg3i32,, # blocked>%
    55%cst_5  = arith.constant:  dense<!2.000000e+00>tt : tensor<.ptr<i8>4x,4x 1xif32, 64#blocked
>
          %%cst_6 = 57 = arith.constanttt.expand_dims dense< 0.000000e+00>%41 : tensor< {4xaxis4x = 1x1f32,  : #blockedi>
32    %}cst_7 =  arith.constant: dense< -2147483648>tensor<32x : tensor<i4x324x, 32x#ttg.slice<{dim = 1, parent = #linear1}>i32>, # -> blocked>tensor<
    32%cst_8x = arith.constant1x dense<i32, 23># : tensor<linear4x14x>32x
i32      , #%blocked>58 = 
    tt.splat%cst_9  = arith.constant% dense<arg15255>  : tensor<:4x 4xi32x32 -> i32tensor<, #32blocked>x
    1%cst_10x = arith.constanti dense<328388607>,  : tensor<#4xlinear4x1>32x
i32      , #%blocked>59 = 
    arith.muli%cst_11  = arith.constant% dense<571>, : tensor< 4x%4x5832x :i32 , #tensor<blocked>32
    x%cst_121x = arith.constanti dense<32127>,  : tensor<#linear4x14x>32x
i32      , #%blocked>60
     = %cst_13arith.extsi = arith.constant  dense<%4194304>59 : tensor< 4x: 4xtensor<32x32i32x, #1blocked>x
    i%cst_1432, #linear1> to  = arith.constanttensor< dense<32126>x : tensor<14xx4xi32x64i32, , ##blocked>linear
    1>%cst_15
 = arith.constant       dense<%612> =  : tensor<tt.make_range4x {4xend = 32x4i : 32, i#blocked32>
,     %startcst_16 =  = arith.constant0 dense< : 21>i : tensor<32}4x : 4xtensor<32x4xi32i, #32blocked>, 
    #%cst_17ttg = arith.constant.slice<{dim = 0, parent = #linear1}> dense<>28>
 : tensor<      4x%4x6232x = i32tt.broadcast, # blocked>%
    60%cst_18  = :arith.constant  dense<tensor<-1.270000e+02>32 : tensor<x4x14xx1xif32, 64, #blocked#>
linear    %1> -> cst_19 = tensor<arith.constant32 dense<x7>4x : tensor<i64, #linear4x1>32x
i16      , %#ttg63.slice<{dim = 2, parent = #blocked4}> = >
tt.expand_dims    % cst_20 = %arith.constant61 dense< {-1>axis =  : tensor<04x : 32xii832, } #ttg:.slice<{dim = 2, parent = #blocked4}> >
tensor<    %4cst_21x = arith.constanti32 dense<, 0x7FC0># : tensor<ttg.slice<{dim = 0, parent = #linear1}>128x>32x bf16, ->#blocked 5>tensor<
    1%cst_22x = arith.constant4 dense<x7>i : tensor<324x, #linear4x1>32x
i32      , #%blocked>64
     = %cst_23tt.broadcast = arith.constant  dense<%631.270000e+02> :  : tensor<tensor<4x14xx41xxf32, i32, #blocked#>
linear    %1cst_24 = >arith.constant  dense<-> 1.270000e+02>tensor< : tensor<324xx4x4x1xi32, f32, #linear#linear1>
>    %
cst_25 =       arith.constant% 65dense< = -1.270000e+02>arith.extsi  : tensor<%4x644x 1x: f32, tensor<#linear32>
x    %4cst_26 = xarith.constanti dense<32, #linear2.000000e+001> : >tensor<4 x4to x1tensor<xf3232x, #4linear>x
    i%cst_2764 = arith.constant,  dense<#linear1>
-8388608>       : tensor<%4x664x = 1xarith.addii32 , #%65linear>,
     %cst_28% = arith.constant62 dense< 2097152>:  : tensor<tensor<4x324xx1x4i32x, #i64, #linearlinear1>>

    %      cst_29 = %arith.constant67 =  dense<tt.splat127>  : tensor<%564x : 4x!tt1x.ptr<i8>i8 , #->linear> 
    tensor<32%cst_30x = arith.constant4 dense<x7>! : tensor<tt4x.4xptr<i8>, i16#linear, 1#ttg>.slice<{dim = 2, parent = #blocked}>
>
          %%cst_31 = 68arith.constant =  dense<tt.addptr-1>  : %67tensor<4, x4%xi668,  #ttg:.slice<{dim = 2, parent = #blocked}> >
tensor<    32x4xllvm.intr.assume !tt%true.ptr<i8>,  :# ilinear11
    >llvm.intr.assume ,%true  :tensor< i321x
    4llvm.intr.assume x%truei64, #linear1> :
 i      %69 = 1tt.load
     llvm.intr.assume% %68true  :: i 1tensor<
    32llvm.intr.assume x%true4x :! itt1.ptr<i8>, 
    #linear1>llvm.intr.assume 
%true       :% i701 = 
    tt.transllvm.intr.assume  %true% :69  {i1order
     = llvm.intr.assume array<%truei :32 i: 11
    , llvm.intr.assume 0%true> :} i 1:
     llvm.intr.assume tensor<%true32 :x4 ix1i
    8llvm.intr.assume , #linear1%true> : ->  itensor<14
    llvm.intr.assume %x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      true% :71 i = 1tt.splat
     %0% = tt.get_program_id29 :  x! :tt.ptr<bf16> i 32
->    % 1 = tensor<tt.get_program_id4 yx :128 ix32
!tt    %.2 = ptr<bf16>arith.addi , %arg5#blocked1, >%c31_i32
 :       i%32
72 =     %tt.addptr3 =  arith.divsi %%271, ,%c32_i32  :% i2832
     %: 4 = tensor<arith.extsi 4%arg7x :128 ix32 !to tti64.ptr<bf16>
    , #blocked1>%5, = arith.extsi  %tensor<arg9 4: xi32128 tox ii64
64,     %#6 = blockedarith.extsi 1%arg11> :
 i      32 %to 73i64 = 
    tt.load%7  = arith.extsi% %720  : :i32  totensor< i464
x    %1288 = xarith.divsi !%1tt, .ptr<bf16>%, 3 #: blockedi321
    >%9
 =       arith.remsi %%174,  = %3tt.splat :  i%32
53    llvm.intr.assume  %:true  : !i1tt
    .llvm.intr.assume ptr<i8>%true  :-> i 1tensor<
    64llvm.intr.assume x%true32 :x i!tt.ptr<i8>1, 
    #%10blocked = arith.cmpi6>
 sgt      , %%arg675,  = %c0_i32tt.addptr  :% i7432
,    scf.if  %%10 52{
       %:11 =  arith.muli tensor<%864, x32x%c4_i32!tt.ptr<i8>, #blocked :6 i>32
,      % 12 = tensor<tt.make_range64 {endx = 432x : ii32, 64, start = #0 : blockedi326}> :
 tensor<      %4x76 = i32tt.load,  #ttg%.slice<{dim = 1, parent = #blocked1}>75>
       %cacheModifier13 =  tt.make_range= {end  = 4cg : i 32, : start = tensor<0 : 64i32x} 32: xtensor<4!ttxi.32, ptr<i8>#ttg, .slice<{dim = 1, parent = #blocked3}>#blocked6>>

      %      14 = %tt.splat 77%11 =  :ttg.convert_layout i 32 %-> 76tensor<4 xi:32,  #ttgtensor<.slice<{dim = 1, parent = #blocked1}>64>
x      %3215 = xarith.addi i%148, #blocked, 6%12> :  tensor<->4x i32tensor<, 64#ttgx32x.slice<{dim = 1, parent = #blocked1}>i>
8, #blocked3>      
%16       = %78tt.splat  = %arg4tt.reshape :  i%3273  -> :tensor<4 xitensor<32, 4#ttgx.slice<{dim = 1, parent = #blocked1}>128>
x      %bf1617 = , arith.remsi #%15blocked, 1%16> :  tensor<->4x i32tensor<, 4#ttgx.slice<{dim = 1, parent = #blocked1}>4>
x      %32x18 = bf16arith.muli , %7#, blocked%4> :
 i      64
%      %7919 =  = tt.expand_dims math.absf%17  {axis%78 = 1  : i:32}  :tensor< tensor<44xxi324, x#ttg32.slice<{dim = 1, parent = #blocked1}>x> bf16, -> #tensor<4blocked>x1
      %80xi = 32, arith.extf#blocked 1>%
      79%20  = tt.splat: % arg8 tensor<: 4i32x ->4 tensor<x4x321xxbf16i32, , ##blocked>blocked1 >
to      % 21 = tensor<arith.muli 4x%194, x%2032 :x tensor<f324x, 1x#blocked>i32
, #      blocked1%>
81 =       %"22 = tarith.extsi t.reduce%21" :( tensor<%804x)1x <i32{, #axisblocked1 = > 2to  : tensor<4ix132xi}64, >#blocked (1>{

            %23^bb0 = tt.make_range( {end% = 128arg16:  : if3232, , start = %arg17: 0 : f32i32)} :: 
tensor<128        %xi20932,  = #ttgarith.maxnumf.slice<{dim = 0, parent = #blocked1}> >
%      %arg1624 = ,tt.expand_dims  %23% {axisarg17 = 0  : i:32}  :f32 tensor<
128x        i32tt.reduce.return,  #ttg%.slice<{dim = 0, parent = #blocked1}>209>  -> :tensor<1 x128f32xi
32,       #blocked}1>)
       : %25( = arith.extsitensor<4 %x24 4: xtensor<132x128xxif3232, , #blocked#blocked>1>) ->  totensor< tensor<4x1x4128xxi64f32, , ##ttgblocked1.slice<{dim = 2, parent = #blocked}>>
>      %
26 =       tt.broadcast %%2282 =  :ttg.convert_layout tensor< 4x%1x81i64 , #: blocked1tensor<> 4x4-> xtensor<4f32, x#128xttgi64.slice<{dim = 2, parent = #blocked}>, #>blocked1 ->>
       tensor<%274x = tt.broadcast4 %x25 f32:,  tensor<#1xttg128x.i64slice<{dim = 2, parent = #linear}>, #>blocked1
>       -> %tensor<483x128 = xitt.expand_dims64,  #blocked%1>82
       {%28axis = arith.addi =  %226, :  %i2732 :} : tensor< 4tensor<x1284xix64, 4#blockedx1>f32
      , %29#ttg = tt.addptr.slice<{dim = 2, parent = #linear}> %>arg0, -> % 18 tensor<: 4x4!ttx.1ptr<bf16>,x iler_DP3_TP3: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
f3264
,       %#linear>30 = 
arith.muli       %%9,84 % = c32_i32tt.expand_dims :  i%32
81      % {31 = axis = tt.make_range2 {end :  = 32i : i3232, }start =  0 : :i32 } tensor<: 4tensor<32xxi432, x#ttgf32.slice<{dim = 0, parent = #blocked6}>, >
#      %ttg32 = .slice<{dim = 2, parent = #blocked}>tt.make_range> ->  {endtensor< = 324 : ix432, xstart = 10 : xif32, 32}#blocked> :
 tensor<      32x%i3285 = , tt.bitcast#ttg .slice<{dim = 0, parent = #blocked3}>%>
83      % 33: = tt.make_range  {endtensor< = 324x : i432, xstart = 10 : xi32f32} , : #tensor<32linearxi>32 , ->#ttg .tensor<slice<{dim = 1, parent = #linear1}>>4
      x%344x = tt.splat1 %x30 i: 32i32,  ->#linear tensor<>32
xi      32, %#86 = #ttgtt.bitcastblocked.  = slice<{dim = 0, parent = #blocked6}>%#ttg>
84 : .blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>      %tensor<
354# = xblockedtt.splat 4x11 = %x#30f32ttg :, .blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}> #blocked
#i32> blocked ->2->   = tensor<tensor<4x#32x4ttgix.32, 1blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>#x
ttg.i32, #blockedslice<{dim = 1, parent = #linear1}>>#blocked>3

 =             #ttg%36%. = 87 = blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
arith.addi arith.addi #blocked%34%85, %4 = , cst_28#% ttg31:.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>  
: tensor<#tensor<4blocked532x = x4#ttgi32x1., xblocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#i#ttg32, blocked.#6 = slice<{dim = 0, parent = #blocked6}>linear>#ttg>

.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>      %      
#37 = %88 = blockedarith.addi arith.addi 7%35%86,  = , %#%cst_1ttg.33 blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
 :: # tensor<blockedtensor<4832x = x4#ttgi32x1.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>, x
##ttgi32blocked., 9 = slice<{dim = 1, parent = #linear1}>#blocked>#ttg>
.
      blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>      %
#%3889 = linear = tt.bitcast = tt.splat #ttg %%.arg587linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>  : 
:tensor<# 4linearix1324x1 =  x#->i32, ttg #.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>tensor<32linear> -> 
#xtensor<4xlineari4x232, 1 = #x#ttgittg.32, #.slice<{dim = 0, parent = #blocked6}>linearlinear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>>>

      
#shared%       = 39%# = tt.splat90 = ttg %tt.bitcast .swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>arg5%88
#  smem: :  = itensor<#ttg32 4x4.-> x1shared_memorytensor<x
32xi32, #blocked> modulei32->  attributes {, tensor<"#4tttgxtg.4x1.slice<{dim = 1, parent = #linear1}>xn>ium
32-      , c%40#blocked>t = 
aarith.remsi      s %" = %91136 =  : ,arith.andii32  , %38%89"t :,tg tensor< .n32%uxcst_27m-i32 wa, :r# pttgtensor<s".slice<{dim = 0, parent = #blocked6}>4x = >4x14 : 
      xi%i3241 = 32, #linear>, arith.remsi 
      ttg.target%% = 3792", = h arith.andi i%%p3990:g ,fx: %9 cst_450tensor< "32:, "xi t32tensor<4xt, 4x1g#ttgx..slice<{dim = 1, parent = #linear1}>it>32h
, re      #ad%42blocked>s- = arith.muli
      p %%93 = e7,tt.bitcastr  -%5%w 91a:  ri:p"64
  =       %tensor<64434 : i = tt.make_rangex32 {4x}end1 { = x
64i   : i32, #lineartt.func32> ,  public start-> @ = 0tensor<_batched_gemm_afp4_wfp4_pre_quant_kernel : 4x4x(i321%arg0} x: :f32, #linear! tensor<>tt.64x
ptr<bf16>i32       {, %tt.divisibility#ttg94 = . = 16 : slice<{dim = 1, parent = #blocked6}>tt.bitcasti> 32
%92}       , %:%44 arg1 = tensor<4x4: tt.expand_dims x!%1tt43x. {axisiptr<i8> = 32 {tt.divisibility1,  =  : i#blocked> ->1632}  :  :tensor<4x4i x32tensor<641}xix, 32, f32%#, arg2: ttg.#blocked>!ttslice<{dim = 1, parent = #blocked6}>>
. ->      ptr<bf16> % {tensor<6495 = tt.divisibility = x1math.log216x  : i%i329332},  , #:%arg3blocked6 : >tensor<!
4x4tt      x.%1ptr<i8> {45xtt.divisibility = f32, #linear> = arith.extsi
16 :        i32%%96 = }44math.log2,   %arg4: %: tensor<94i64 32x: {tt.divisibility1x  = itensor<16324x4 : , xi#132blockedx}6f32, >, % to#blocked>arg5 
: tensor<      i64x%32197 {x = tt.divisibilityi64math.floor = ,  16#% : blocked95i6 32>:}
 , %      %tensor<arg6464:  = xitt.expand_dims 4x132%x {tt.divisibility40f32, #linear> =  {
16axis =        : 0 : %98 = i32i32math.floor}, } % :%arg7 96: tensor< i32x:32i  {32tensor<tt.divisibility = , 416 : #xi32ttg.4}slice<{dim = 0, parent = #blocked6}>>x1, % ->xarg8 f32: tensor<, i1#32xblocked> {32
tt.divisibility = xi      1632, % : #99 = iblocked6arith.subf32>
 }      %%, 47 = 97%tt.splat ,arg9% : iarg10%32 :cst_26 { i tt.divisibility32 : = 16->   : tensor<tensor<i3214}x32x, x4%ixarg1032, 1: #xi32blocked6f32 {>
, #linear>tt.divisibility      
 = %      1648% :  = 100 = i32arith.muli arith.subf}%46 , , %%%4798arg11 ,: i: 32 { tensor<%tt.divisibility1xcst_5 = 1632  : x: i32itensor<4x4x1}32, x, %#f32arg12: blocked, i6#32>blocked {tt.divisibility
> =       %
      1649 = % : arith.extsi 101i32%48 = } tt.clampf, %:  arg13: tensor<%i1x993232, {xi tt.divisibility = 32, %16#blockedcst_25 : 6,i> 32 to%},  tensor<cst_24%1x,arg1432 : ixipropagateNan32 {64,  tt.divisibility#= = blocked 16 : 6>nonei
 32      : }, %50tensor<% = 4arg15tt.broadcastx:  4i%x32451) x attributes:f32 { tensor<, #linear>noinline64
       = x%false1102 = }xtt.clampf i {64, %
#100    %blocked,cst6>  =  ->%arith.constant cst_18 tensor<64, dense<x%12732cst_23>xi, propagateNan =  : tensor<64, none4# x4blocked6: x>tensor<1
4xi      %x8514,  = x#tt.broadcast 1blocked%x>
49f32     , %: #blocked>cst_0tensor<1
 = x32      arith.constantx% dense<i1030x7FC064 = >, arith.fptoui : # tensor<blocked%4x6101128> x ->:bf16 tensor< , 64tensor<#x4blocked132xx>i4
64, x    #blocked1%6xcst_1 = >
f32, arith.constant      %#linear 52>dense< =  2097152>arith.addi to : tensor<% 450tensor<x,44 %xx5141 :x1x xitensor<i3264x8, 32, #blockedxi#>64linear
, #>    %blocked6
      cst_2 = >
%104 = arith.constant      arith.fptoui % dense<53 = %4tt.addptr102>   : %:tensor<arg1 4,tensor<x %4x4x1442xx f32, #blocked>16:  to xi!tttensor<8.4x, #ptr<i8>,4x1blocked2 ix>
64
i          8%c31_i32%54,  =  = arith.extsi#blockedarith.constant %> 31arg14 
 : :      i32 %
i105    %32  = cst_3toarith.addi =   arith.constanti% 64103,dense<
 0.000000e+00>      %% : 55cst_29tensor< = arith.muli : 4 %tensor<x7432x, xf32%4, 54x# :1blocked x3ii>648
    
      , %c32_i32%56# =  = lineararith.constanttt.addptr > %
32arg3,       :  %i32%55106
 : =      arith.addi %c4_i32!% = tt104, %arith.constant.ptr<i8>cst 4,   : ii64:32
 
          %tensor<%574true = x = tt.expand_dims4arith.constant %x true411
 {axisx    % = 1ic0_i32 =  : 8arith.constanti,  32}#0 blocked>
 : i:      32
 tensor<%107 =     32xarith.subf%i cst_432% = arith.constant, cst_6 #,dense<ttg. %102-8388608slice<{dim = 1, parent = #linear1}> >>:  :  tensor<tensor<->4x4 4x1xtensor<32x4xf32, x1#1xblocked>xi32
i, #      32, linear1%#>108blocked
       = >%58math.exp2
     = tt.splat % %cst_5 = %arg15107arith.constant   dense<::2.000000e+00> i  : 32tensor<4tensor< x4-> 4xx4tensor<1x32xx11xf32, xf32i32#blocked>, , 
#blocked#linear      >1>%

109    %       = cst_6 = %arith.extfarith.constant59   = arith.muli%dense< 780.000000e+00%57 :>,  :  tensor<tensor<4%584x :x4x324 xxtensor<bf16132, #blocked> toxx1 f32xtensor<, i4#blocked32, x>
#linear4x    1>32%cst_7
      x = arith.constant%f32 60 = , #blocked>
dense<arith.extsi      -2147483648 %>%110 : tensor<59  = 4x:tt.broadcast4x tensor< 3232x%x1108ix 32i:, 32,  #blocked#tensor<4x4x1>linear1x
    > f32, %to#cst_8 tensor<blocked> ->  = 32tensor<arith.constantx4 1xxdense<i423>64, x : #32tensor<linearx41>f32, x
#4x      blocked>32%
x61      i = %32, tt.make_range111 = # {arith.mulfblocked>end =  
    4 : %%i109cst_9 = 32,arith.constant,   dense<start = %2550110> :  :  tensor<4i32:x} 4 tensor<x:4x32 4xtensor<xi43232xx, if32#32, blocked, #blocked>>
#ttg
    .slice<{dim = 0, parent = #linear1}>      %>%cst_10
112 = arith.constant       =  dense<%62tt.bitcast8388607 =  >tt.broadcast% :  %111 :tensor<460 x :tensor<4 4x32tensor<32xxx4i1xx32i6432, #, #xblocked>linear1f32, #blocked>
>      ->%-> cst_11 =  tensor<4xarith.constanttensor<4 32xdense<x4321>xx : i64i32, #blocked>tensor<, 
4x#linear      4x1%32>113xi
       = 32, %arith.andi #63%blocked = 112>tt.expand_dims,
      %%%cst_1261cst_7  =  {axis: arith.constant = 0tensor<  : 4dense<ix412732x> : } 32tensor<: x4xtensor<4i4xi32x32, 32, #x#blockedittg>32, .slice<{dim = 0, parent = #linear1}>
#>      blocked> ->%114 = 
 arith.shrui    %tensor<1 cst_13 = x%arith.constant4x112 i,dense<32 4194304, #%>linearcst_8 : 1> tensor<4
      :x% 464tensor<x = 432tt.broadcast x4x32x%xi3263 i, : 32#tensor<, blocked>1x#blocked>
    4x
%i      %cst_14 = 32, 115arith.constant#linear =  dense<1>arith.andi %114126> ->,  :  %tensor<tensor<cst_9432 : x4x4tensor<4x4x32x32xixx32ii, 3232, #linear, #blocked>#1
blocked>      >

      %    %116%65 = cst_15 = arith.extsiarith.andi =   arith.constant%% 64112dense< :, %2> tensor<cst_10 :  : 32tensor<tensor<x4x4x44x32x4i32xx32, #i32, #blocked>xilinear1
32>       , to%# 117blockedtensor< = >32xarith.addi %115, 
4%    xicst_11%64,  cst_16#:  = arith.constantlinear1tensor< >
4xdense<      421>%66x :  = arith.addi32tensor< x4%ix46532x32,, x #blocked>i32%62
,  :      #blocked %>tensor<118
32x =     4xarith.subi%cst_17i64  = , %arith.constant#cst_12 linear,dense<1 28>>% : 
      117tensor<%67 4 = :xtt.splat 4 %tensor<x56432x :x4i x32, !32#blockedtt.x>ptr<i8>i
 32    %->, cst_18 # = arith.constanttensor<blocked 32>dense<x
-1.270000e+024      >x% : !119tensor<tt = 4.arith.cmpix4ptr<i8>,  x1#linearultxf321>,, 
 #      %blocked>%115
68,     =  %tt.addptr%cst_19 =  %cst_12arith.constant67, : dense< % 7>66 tensor< : :4tensor< x4tensor<4x32x32xx32i164x, xi#!32ttgtt, #blocked>..
slice<{dim = 2, parent = #blocked4}>ptr<i8>      >
, #%    linear120%cst_201> =  = , arith.shruiarith.constanttensor<32  x%dense<4x116, -1i%>64cst_11 : ,  tensor<#:4xlinear 321tensor<x>4i
      x8%4x, 69 = 32#ttgtt.loadx. %islice<{dim = 2, parent = #blocked4}>6832>
 :, #blocked>    % tensor<
cst_2132       = x%arith.constant4x121 =  !arith.oridense<tt 0x7FC0>.ptr<i8>%120 : tensor<, #,128xlinear1 32>%xbf16
cst_13, #       blocked5%70:>
 =      %tt.trans tensor<cst_22%4x4x = 6932arith.constant {x orderidense< = 327array<, > : i#blocked>tensor<32
4:       x1%4x, 0122 = 32>arith.shruix} i %32, :121# ,blockedtensor< %118>32 
    x4: %xtensor<cst_23i4 = 8xarith.constant, #4x dense<linear321.270000e+021x>>i :  32tensor<->, 4 #xtensor<blocked4x4x>132
xxi      f328%, , 123 = ##arith.selectblockedttg. >slice<{dim = 2, parent = #blocked4}>%
>119    
, %cst_24      %% = 71122arith.constant = ,  tt.splat %dense<%1161.270000e+02>29  :  : tensor<: tensor<4!4xttx4x.ptr<bf16>4x1 ->32x tensor<xf324xi, #128x1, #blocked>linear>!tt, 
.tensor<    ptr<bf16>4%, #xcst_25blocked14 = >xarith.constant
32       xdense<%i-1.270000e+0272 = 32, #blocked>> : tt.addptr
tensor< %      471,%124x % = 428arith.maxuix  1x: %115f32, tensor<,#4x linear>128%
    x!cst_14%tt cst_26.: = ptr<bf16> arith.constant, #tensor< blocked4x4xdense<1>322.000000e+00>, x : tensor<i32, tensor<44x#blocked>x128
4x      xi%125 = 1x64, arith.subif32, #blocked %124#1,linear> >

      %    %%cst_14cst_2773  =  = :arith.constanttt.load   tensor<dense<%4-8388608>72x : tensor< :44 xxtensor<4324xx128x1x!ixtt.32i32ptr<bf16>, , , ###blocked1blockedlinear>>>


                %%%cst_2874 = 126 = tt.splat = arith.constant %arith.shli 53 dense< %2097152>: 125 : tensor<!tt,4x.ptr<i8> 4x ->%1 cst_15xtensor<64 ix:32, 32 #xtensor<4linear!x>tt.4
ptr<i8>x    , 32%#xcst_29blockedi = 6>32arith.constant
      ,  %75#blocked>dense< = tt.addptr
127 %      >74% : tensor<, 1274% = x52arith.shrui4  x:%1x tensor<123,i64 8, x%#32cst_16linear>x 
!tt:     .tensor<4%ptr<i8>xcst_30, #4 = blockedxarith.constant632 >xdense<, i327tensor<, > : 64x#tensor<32xblocked>
4xi      4x64, %i#blocked128 = 166arith.ori, > #ttg
      %.%126slice<{dim = 2, parent = #blocked}>76 = ,>
tt.load  %127    %%75 cst_31 : = cacheModifier arith.constant =tensor<  4dense<cgx-1> :4x :  32tensor<tensor<x464ixx32432, xx#i8!blocked, tt.>#ptr<i8>
ttg,       .#%slice<{dim = 2, parent = #blocked}>blocked6129>> = 

arith.addi           %llvm.intr.assume%77128,  =  %ttg.convert_layout %true%76cst_11   :: :  tensor<tensor<i1644
    x32x4xllvm.intr.assumex32 i8x%true, #i blocked32:6,  >#i1 blocked
    -> >llvm.intr.assume tensor<64
%truex32       x%:i8130 =  , arith.shruii1# 
blocked%    llvm.intr.assume3>129,  %
      %true %cst_11: 78 =  itt.reshape:1 % 
73tensor<     4llvm.intr.assume :x4% xtruetensor<32 4xx:128xi ibf16, 321#blocked, 
1#    >blocked>
llvm.intr.assume        ->%% tensor<131 = true4arith.minui x : 4%i1x130
32,    x llvm.intr.assumebf16% , cst_22%# trueblocked: :>  
tensor<i      41%x
    794llvm.intr.assume = x %math.absf32true  %x:78i  32, #blocked>
i:      1 tensor<%
4132    x = llvm.intr.assume4arith.shrui x32 %x%truebf16113 :, , # i1blocked>%

      cst_17    % llvm.intr.assume 80:% =  truearith.extftensor<4x4x  32:%x i79i1 :32
 ,     llvm.intr.assumetensor<4# %xblockedtrue4> x32
:x       ibf16, %1#blocked133
> =      arith.orillvm.intr.assumeto   %%tensor<4132,truex  4x%:32131 ixf32 : 1, tensor<
#4    blockedx%>
4x0 =       %32tt.get_program_id 81 = xi32, #blockedx"> t
: t.      %ir13432e = 
darith.trunci    uc %e"%1331(%  = 80):tt.get_program_id  < y{axistensor<4x : = 24x32  : xii32i32}32
> (, #    %{
blocked2      > = ^bb0( arith.addi%to arg16 %: tensor<arg5f324x4x, , %32%arg17: xc31_i32f32i :)8 i:
, 32        %#blocked>
209
    % =       3arith.maxnumf %135 =  = arith.divsi%arg16tt.reshape , % %2%134, arg17 % :c32_i32 : : tensor< f324i32
x
        4    tt.reduce.returnx%4 %32 = 209xarith.extsi  :i% 8arg7f32,  
#blocked> :      ->  i}tensor<32 )4to : (x4x itensor<416x264xx
4i    x8%32, 5x# = f32blockedarith.extsi , 7>%arg9#
 :blocked       >%i) -> outLHS32tensor<,  to4% xoutRHSi4 = 64
xf32tt.split    ,  %#%6ttg135 = arith.extsi.slice<{dim = 2, parent = #blocked}> : > %
tensor<arg11      %4 82x: = 4 ittg.convert_layoutx16x32 2 to%x 81ii64 :8, #blocked
 7    tensor<>%4 -> 7 = xtensor<arith.extsi 4x4%f32, x0#4 ttg.x:slice<{dim = 2, parent = #blocked}>16 >xii 832->,   tensor<#to4blocked ix42>64x

    f32      %, %8#136 = ttg = arith.divsi .slice<{dim = 2, parent = #linear}>arith.shli%> 1,
      % %outRHS%83, 3 = % tt.expand_dimscst_2:   %:i3282 
 {tensor<    axis4%9 = 2x4 = arith.remsi : ix 32}16% x1:i8, , #blocked %tensor<42>3 x4
:x       f32%i, 13732#ttg.slice<{dim = 2, parent = #linear}>> -> tensor<4x4x1xf32, #linear>
      %84 = tt.expand_dims % = 81arith.ori {
 axis    % = llvm.intr.assume outLHS, 2 : %%itrue13632  }::   :i1tensor< 
    4tensor<llvm.intr.assume x4%true4x :x4 16xixf321i, 
8#    , #blocked2ttgllvm.intr.assume>. 
      slice<{dim = 2, parent = #blocked}>%%>true138  : = ->  tt.reshapetensor<i1 4
    %137x4% x10 = : 1arith.cmpi tensor<4x4xxsgt,16f32 %x, arg6i#blocked,8> , 
%#      %c0_i32blocked85 2 = :>tt.bitcast   i32->%
 83    tensor< scf.if4: x %64tensor<10 x4{i8, x
#blocked4      %8x11>1 = arith.muli
x       f32%8%139, ,  = #linear%c4_i32tt.reshape>   : %->i105 32
 :tensor<      % 412tensor<x = 44tt.make_rangexx {41endx1xi = x324i,  : i8#32, , linearstart#> = 0linear
 : >      i %8632}->  =  tensor<tt.bitcast: 4x tensor<44%xx84i32i , 8: #, tensor<4ttg.#xslice<{dim = 1, parent = #blocked1}>>ttg4
.slice<{dim = 2, parent = #blocked}>x      >1%
x13      f32 = %140, tt.make_range = # {tt.reshapeblocked>end   = %->4106  :  tensor<4i32:x, start 4 = 0tensor<4x4x1x : ix1x32}ii 832: , #blocked, tensor<4> #xi->blocked32 >, tensor<
#4      ttgx%.487slice<{dim = 1, parent = #blocked3}>>x = 
iarith.addi       8, %85%#, 14 = linear%tt.splat2cst_28 > %
:11        %tensor<:1414  = xittg.convert_layout432 x ->%1 140xtensor< : i4tensor<32xi4, 32x#, 4linear#ttgx>.i
slice<{dim = 1, parent = #blocked1}>8      >, %
#88      linear2> = %15 ->arith.addi = arith.addi   %tensor<%14,486, x %%4cst_112x  :i: tensor<8 4, tensor<4x#ttgxi32.4, slice<{dim = 2, parent = #blocked}>x1#>xttg
i32.      , slice<{dim = 1, parent = #blocked1}>>%#
      142blocked>%16 = 
 = arith.extui      tt.splat % %14189%  = tt.bitcastarg4 : :  %i32tensor<87 4 -> x4:tensor<x tensor<4xi4i8x32, 4, #x1#ttgxttg.i.slice<{dim = 1, parent = #blocked1}>slice<{dim = 2, parent = #blocked}>32>
>,        #linear%17to> =   arith.remsitensor<-> 4 %x4tensor<15,x4 ix%16416, x1 :#x ttgi32tensor<4.slice<{dim = 2, parent = #blocked}>, xi>#32, 
linear#ttg      >.%
slice<{dim = 1, parent = #blocked1}>>143      
 = %      %arith.shli90 = 18 =  tt.bitcast arith.muli %142, %%%cst_30887 : , : tensor< %4tensor<4x4x 44:xx i1i16x64, i
#32      %ttg, 19.#blocked = slice<{dim = 2, parent = #blocked}>> tt.expand_dims >->%17
  {axis      tensor< = %41 : 144xi32 = 4x} tt.bitcast1:  xtensor<%i414332x , i32:#,  blocked#tensor<>ttg.4
slice<{dim = 1, parent = #blocked1}>x      >4% x91->i =  tensor<16arith.andi4,  %x1#89xittg,32, .slice<{dim = 2, parent = #blocked}> #blocked>%1> ->cst_27
  :      %tensor< 204tensor< = tt.splatx44 xx%arg8bf164 , x1:#ttgx .slice<{dim = 2, parent = #blocked}>i32i32>,  ->
#       lineartensor<4%145 = >x1tt.expand_dims 
      xi%%32, 14492# { = arith.andiblocked1axis > = %90
      2,%21 :   = i%arith.muli32cst_4  %}:19 : tensor<, 4 %tensor<4x4x20x4 bf16x:, 1 #xitensor<ttg324.slice<{dim = 2, parent = #blocked}>, x>#1 ->blockedx >
itensor<      324%93, #x4x = blocked1tt.bitcast 1x%>bf1691 
      , :%#blocked 22>tensor< = 
4arith.extsi      x4 %%146x21 = 1 tt.broadcastxi:  32tensor<%, 4145#linearx1 >x: ->i32  , tensor<tensor<#4x44blockedx1x1x4>bf16, x1 to#blockedxf32 >, tensor< #4->linear>x 
1tensor<      x4%94i64x = , 4xtt.bitcast#32 blockedx%1bf1692>,  
#:      blocked tensor<%>423
x =       4tt.make_range%x {1471end =  = x128 : tt.reshapeii32 32, %, start146# = 0 blocked : : >itensor< 324->}x  :4xtensor< 324tensor<xx128bf16, 4x#xiblocked>132,  x#ttg->f32.slice<{dim = 0, parent = #blocked1}> , >tensor<#
4blocked      %x>24128
 = x      tt.expand_dims bf16%%23, 95 {axis# =  = 0blockedmath.log2 : 1 i>%32
93}        :%: tensor<148 =  tensor<128xamdgpu.scaled_upcast_fp44xi32 4x, %1#138xf32ttg. , #slice<{dim = 0, parent = #blocked1}>scalelinear>>  
->%      % tensor<14796 = 1 {math.log2xaxis =  1281%x : 94i32i , 32:#} blocked1 tensor<>:4
       x%tensor<4254x = x1arith.extsi 64x%xf3224i,  :8, # #blockedblockedtensor<8>
1>      x,%128 97xtensor< = i324math.floor, x %#12895blocked1x > bf16:to,  tensor< tensor<#blocked41x1>x128 4x->xi64 1, tensor<x#4f32blockedx, 1128#>xlinear
bf16, #blocked1>>      

%            %26%98 = 149 =  = math.floortt.broadcast arith.cmpi  %%22eq96 , : : % tensor<139tensor<4,4x x1x%4icst_31x64 1, :x# f32blockedtensor<, 14#>x4blocked x>->i
       8%tensor<4, 99x128# = xittgarith.subf64, .slice<{dim = 2, parent = #blocked}> #blocked>%1>
97
            , %%%27 = 150cst_26tt.broadcast =  : %tt.expand_dims  tensor<25 %4:149x  {4tensor<1axis = xx21128x : xiif32, 64, 32} ##blocked: linear1tensor<>>4
       x%->4100 tensor<x = 4xiarith.subf 1281%x, 98i#,64ttg , #.slice<{dim = 2, parent = #blocked}>%blocked> ->cst_51  >tensor<:
      4x %4tensor<28 = x4arith.addi 1x%26x4,ix 11%, #blockedx27>f32 
, :      # tensor<%151blocked4 = >
xtt.broadcast      128 %x%101i150 = 64,  tt.clampf#: blocked1 %>tensor<99
      4x4x1, %29x% = icst_25tt.addptr1, ,  %arg0#%,blockedcst_24 %> , 18->propagateNan   =:tensor<  4none!x tt.4:ptr<bf16>,x  i32tensor<64x4
      i1x%30, 4 = #xarith.muli blocked1%>x9
f32,       %152, %c32_i32 = #linear :tt.reshape > i%
32
151             %%:102 = 31 tt.clampf  = tt.make_rangetensor<% {4100endx, = 324  : x%i32cst_1832x,, starti  = 1%0, cst_23 : i#,32blocked }>propagateNan   :-> = tensor< tensor<324xnonexi128 :32, x #itensor<ttg14.slice<{dim = 0, parent = #blocked6}>, x>
#blocked4x      %1132 = >xtt.make_range
f32,  {      #end%blocked> = 153
      32 :  = %103iarith.select = 32,  arith.fptouistart%  = 152%0, 101  : i%:32cst_0 } , tensor<:%4 148xtensor< : 4x32tensor<1xi4x32xf32, 128, #x#ttgilinear.slice<{dim = 0, parent = #blocked3}>1, > >#to 
      blockedtensor<4%331x = >4tt.make_range, x {endtensor<1 = 4xx32 : 128ii32x8, , startbf16# = 0, linear : i#blocked1>
      >32%
      }154%104 : =  =  tensor<ttg.local_allocarith.fptoui32  %xi%10232153 :,   tensor<#ttg: 4x.slice<{dim = 1, parent = #linear1}>(4>tensor<x
41x      xf32%34128,  = tt.splatx# bf16blocked>%,  to30#  blockedtensor<: 14i>x32 )4x-> -> 1 !xtensor<32ttgi8x., imemdesc<4x128xbf16, #shared, #smem>#blocked32
>,       
#ttg%      .155%slice<{dim = 0, parent = #blocked6}>> = 105 = 
      ttg.local_loadarith.addi % %10335 = %154,tt.splat   %:%30  cst_29:! : ittg 32.tensor< memdesc<4x128xbf16, #shared, #smem>4->  x4tensor<32->xxi 1x32, tensor<i#48ttg.x128, #slice<{dim = 1, parent = #linear1}>xlinear>bf16>
, 
      #      %ttg%36.dot_op<{opIdx = 0, parent = #blocked3}>106 = > = arith.addiarith.addi
 %       104%%,34156 =  ,arith.extui% % cst31%  :70:  :  tensor<tensor<tensor<324x4xi32x432, xx#ttgi1.8xslice<{dim = 0, parent = #blocked6}>, i>#ttg8
., #      slice<{dim = 2, parent = #blocked4}>blocked%37>>
 = arith.addi to      % % 107 = 35,tensor<4xarith.subf 32 %33x% :icst_6 16,tensor<,  32#ttg%x.slice<{dim = 2, parent = #blocked4}>102i> :32
 ,       tensor<#ttg%4.slice<{dim = 1, parent = #linear1}>157x>
 = 4      arith.shlix1% x38 = %f32tt.splat156,, # % blockedarg5%> cst_19
      : :%  108itensor< = 32 4math.exp2-> x tensor<32%32xx107ii 3216: , , tensor<4x#ttg#4.ttgxslice<{dim = 0, parent = #blocked6}>.slice<{dim = 2, parent = #blocked4}>1>>xf32
      
, %      #39%blocked = tt.splat158 = >
 %tt.bitcast      arg5  %:%157109 i :  = 32tensor<arith.extf 4x ->32%78 tensor<x 32i:xi16 32, , tensor<4##xttgttg4.slice<{dim = 1, parent = #linear1}>.slice<{dim = 2, parent = #blocked4}>x>>32
       -> x%tensor<bf16, 40 = 4#arith.remsixblocked %32>36x ,bf16to ,  tensor<%#438 ttg.x: slice<{dim = 2, parent = #blocked4}>4tensor<>x32
      32xi%x32159f32,  = , #ttgtt.expand_dims#. blockedslice<{dim = 0, parent = #blocked6}>%>>158
      
 {%      axis = 110 = %412tt.broadcast =  :  arith.remsii% %3210837} , : : tensor<% 439tensor<x 44: x32xtensor<x132bf16xxi, f3232, #, #ttgttg#..blockedslice<{dim = 1, parent = #linear1}>slice<{dim = 2, parent = #blocked4}>>>> 
       ->->%  tensor<42tensor<4x4 = 32xxarith.muli 14%7xx,bf16, 32 %#blocked4>x5
f32 :      ,  i%160#64 = blocked>
      tt.broadcast
%43        = tt.make_range%% {159111end  =  = :arith.mulf64 :   %i32tensor<109, start4, = 0x  : 32%ix110321 :} x :bf16tensor< tensor<, 4x64#4xiblocked4> -> x32, tensor<32#4xxf32ttg.32x32, slice<{dim = 1, parent = #blocked6}>>x#
      bf16, blocked>%44#blocked4
 = tt.expand_dims>       
%%      11243% =  {161 = tt.bitcast axistt.trans% =  1111 : %160 i {:32order =  }array<tensor< :i4 tensor<32x464x: xi32032, , x#2f32ttg., , slice<{dim = 1, parent = #blocked6}>1#>>blocked ->}> tensor< :  64xtensor<->14 xixtensor<32324, xx#324blockedxx6>bf16, #32x
      blocked4i%>3245 ,  = arith.extsi-> # %tensor<4xblocked4432> x
: 32      tensor<64x%xbf161131,  = x#blockedarith.andii9 32, >%#
112blocked      ,6% > 162 = %cst_7to tt.reshape tensor< %:64161 x tensor<41:xx 4itensor<x64, 432#xxblocked632xi>
3232      %xbf16, , 46 = #blocked9> #tt.expand_dims -> blocked%tensor<>
40128       {x%axis32114 = 0x =  : bf16arith.shruii,  32#%}blocked112 :5, tensor<> 32
%x      cst_8i% 32163:,  =  tensor<#ttgamdgpu.scaled_upcast_fp44.slice<{dim = 0, parent = #blocked6}> x> %4->77x32 tensor< xi1xscale3232x , i%#32162blocked>,  {
#axis =       blocked0%6 : 115>
i =       32arith.andi%47}  = tt.splat % %:114arg10 ,  :tensor<% i64x32cst_932x : ->i  tensor<8tensor<1, 4x#x32xblocked4i3x32>,32,  x#tensor<iblocked612832>
x32,       x#%bf16blocked>48, #blocked
       = 5%arith.muli>116   = arith.andi%-> 46 %,tensor<112 128,%x 4732% xcst_10: bf16, #blocked tensor<5:1> x
tensor<32      4xi%x32164 = 4, arith.cmpix32# xblockedeqi6,32> , #
      %blocked%4970> = arith.extsi,
 %       48%% :cst_20117   = tensor<:arith.addi1  xtensor<%115324,xx i32%32xcst_11, i #blocked8:6,  >#tensor< ttg4to.slice<{dim = 2, parent = #blocked4}>x4 tensor<>
x1      32x32%xx165 = ii64tt.expand_dims32, # , blocked6%#>164blocked
 {>      axis
% =       50 = 2%tt.broadcast : 118 i = %32} arith.subi45 :  %: tensor<cst_12tensor<644x,x32 %1x117xii 641: , ler_DP6_TP6: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
, tensor<#blocked#ttg.46>slice<{dim = 2, parent = #blocked4}>x ->> -> 4 tensor<tensor<4xx643232xx32xix132, i64x#, iblocked#1>blocked, #blocked4
6>      %>

      119      % = %51166 = arith.cmpi  = tt.broadcasttt.broadcastult  ,%49%165  : :% 115 tensor<tensor<,14x x32%32xxcst_12i1 64x:, i #1tensor<blocked6, 4>#x4 ->blockedx 432tensor<64>xx i3232x-> , itensor<#644blocked, #x>blocked32
6x      >32%
x120 =       %iarith.shrui521,   = #%116arith.addi blocked4>,%
 50      %, %cst_11#%167 blocked51 = : =  tt.trans tensor<#:  4ttgtensor<64%x.x1664xblocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>32 {32x
xiorder = i32#blocked64, array<, 1 = #i#blocked#blocked632>ttg>
: 
      .blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>      0%
%53, 121# = tt.addptr2 = blocked , arith.ori2%1 % = arg1>120#,},ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}> %  
#42 : %cst_13blocked:tensor< 3 4x32x32:  = !xtensor<4#ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>tt.i1, x4
#blocked4 = ptr<i8>#blockedx#,432ttg i> -> x.64
tensor<4x32xiblocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>      3232
%x, #blocked5 = 54i##ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}> = 1blocked
#blocked6 = arith.extsi , #blocked>#ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = %9
#ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>arg14 >      %
#blocked:
122 = 8 =        arith.shrui #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blockedi%%9 = 32 168 = 121,#ttgtott.reshape .blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}> i %
#linear64
%118  =       167: #ttg% tensor<.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>55:4x
#linear1 =  = arith.muli 4x#ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}> tensor<32x
#linear2 = %4i#7,x32, ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}> %32#
#shared = 54x32blocked>#ttg :x
. ii1, #blocked      swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>649%

      >123#smem%  = arith.select = 56 = -> #ttgtt.addptr %. tensor<119shared_memory%arg3128x32, %
,x122, module i1, #% attributes%blocked116 :  {55 5>tensor<": 
4ttg.num-c!      xtastt.%4"ptr<i8>169x32 = ,  = xi1i64arith.select 1 : 
      %, i%57168#blocked32 = tt.expand_dims, >,  %%, tensor<"t41cst_214tg.num-warps {axis, %163x" =  : 4 = 1 : tensor<x324 : i32i32128x, }xittg.target :3232,  =  tensor<x#"32xiblockedhi1>ip:gfx9532, #blocked
0, 5>      "#ttg, %124, .slice<{dim = 1, parent = #linear1}>tensor< = ">128xarith.maxuittg.threads-per-warp" =  32 64-> x%115 : i32tensor<32bf16, #blocked,}x5>  1x
%{i      cst_14 
32%170 = :   , ttg.local_alloc tensor<4tt.func#linear%x 11694public>
 x32       %:x@58 i32_batched_gemm_afp4_wfp4_pre_quant_kernel = tt.splat(, ( tensor<#%%arg15128x32blockedarg0 :x>:  bf16
!i,       tt32 #%.->blocked125ptr<bf16> 5 =  {tensor<32>)arith.subi tt.divisibility = x1 -> %16x!ttg124, : i32i32.memdesc<128x32xbf16, #shared, #smem> %}, #
cst_14, linear       %1%:arg1: >
171 =  !tt.ptr<i8> {      %ttg.local_load tensor<4tt.divisibility = 59 = %x416arith.muli170x : i32 % :32x}57, i,  %!ttg32%arg258., #:  memdesc<128x32xbf16, #shared, #smem>blocked>!tt.ptr<bf16> {:  
tt.divisibility = tensor<32->      16x %126 : i321xtensor<128x32 = arith.shli}i32x %, , bf16125%arg3: #linear, , !tt.1>#ttg%cst_15ptr<i8>
.  {      dot_op<{opIdx = 1, parent = #blocked3}>: tt.divisibility = %60>tensor<416 : i32},  = arith.extsi
x% %      4arg459%x:  :17232i  = xi32 {tensor<32tt.dot32, tt.divisibility = x1 #16x%blocked : i32155>
i32}, , #,      %%arg5: linear1 127i>% = 32 to171arith.shrui { tensor<, %tt.divisibility = 32x 123,161x%  : i64cst_3%cst_16i32}, %arg6: , #  ilinear::321   {>tensor<tensor<4tt.divisibility = 
      4xx16%611284 :  = tt.make_rangexxi32}, %arg7:  {endbf16, 32xi32 { = 4#itt.divisibility =  : ittg321632.,  : i32}, %arg8: i32 {, startdot_op<{opIdx = 0, parent = #blocked3}>#tt.divisibility = >blocked = 0 :  >16 : i32}, i*
%32       arg9: }tensor<%i32 { :128128tt.divisibility x =  = tensor<432arith.ori 16 : i32}, xx%%i32bf16, 126,arg10: , #ttg i32 {#ttg.%tt.divisibility = .slice<{dim = 0, parent = #linear1}>dot_op<{opIdx = 1, parent = #blocked3}>12716>
>  : i32}, %       : arg11%->tensor<: 62 4i32 { = tt.broadcasttensor<x4tt.divisibility =  %4x1660x32 : i :32xi32 tensor<x32, }32xf32#, 1x, #blockedblocked%i643>>
arg12: , 
      i#      %12932 {linear% = tt.divisibility = 1>173 = arith.addi 16 arith.addf% : i32}, ->  128%tensor<%,arg13: i32 {32x172 %tt.divisibility4,cst_11 = xi  16 : i32}, %arg14: i64%: 32 {, #cst_3tensor<tt.divisibilitylinear 4 = 1>:x416
       x32 : %63tensor<xi = tt.expand_dims4xi32},  3232%%x, arg15: i3261f32, #) {#blocked> attributesaxis = blocked
 {0 : 3      %noinline = i32>130false}
 = arith.shrui} :         %%{
tensor<4174 = 129    xiarith.truncf, %32 %cst, %173cst_11 = #  arith.constantttg:: .slice<{dim = 0, parent = #linear1}>  tensor<dense<>tensor<4127 ->4x> tensor<x4 : 1x32xtensor<4x324xif32, #blocked3xx32, >i324x1#linear , #x1>toblocked>i
 
      8      %tensor<4x%, 6432131#blocked = tt.broadcastx = > %bf16arith.minui
63 , #blocked3>     : 
%%cst_0 = tensor<1      130arith.constantx4%, xi175 =  %dense<32arith.extsicst_220x7FC0, #  :>linear%  : 113tensor<tensor<>  44->:xx  4x128tensor<tensor<32x324xibf16, #xx32blocked1>4xi, #
    i3232, blocked>%cst_1 = , ##
arith.constantlinearttg      % 1>.132dense<
      slice<{dim = 1, parent = #blocked3}> = 2097152%65> to arith.shrui> = tensor<  : tensor<4x4x1arith.extsi 4%x%x113i64 i,32: 64 %, #blockedtensor<32, cst_17 >x4#ttg:
    x. %cst_2 = i32slice<{dim = 1, parent = #blocked3}>tensor<arith.constant, >4 #linear
x4dense<1>      x4> :  to%176 = 32xtensor<4x4x tensor<arith.extsii1632x 32, xi8, #blocked4%#2>xi11blocked
    64,  >
%#linear:      c31_i321 % = >
i133arith.constant      %32 to = arith.ori 66 =   %31arith.addii132 :  64, i32%65
%131
    ,       % %%:cst_362177  =  : = tensor<4arith.constant tensor<tt.splatx 32 4dense<x%x0.000000e+00417632x>x : i32 : i64i, #tensor<4x32x, #64blockedf32linear1 -> >
, #blocked3>
    >tensor<      %
      4%134c32_i32 = %67x = arith.constant = tt.splati64arith.trunci   %, %1333256#  :  ttg: i32: .slice<{dim = 1, parent = #blocked3}>tensor<4
    !tt>x%.
4xc4_i32 = ptr<i8>       32arith.constant-> %x tensor<178i32432x = , # : 4arith.addiblockedix >32!% 
tt177,to     . tensor<4%ptr<i8>, %175x4true#linear x = 1: 32arith.constant>tensor<4xi 
      x8, true
    %68i64#% = tt.addptr, blocked>c0_i32 =  %#
arith.constant67ttg       ,.slice<{dim = 1, parent = #blocked3}>%1350 :  > = tt.reshapei32
    %66
       % :%179%cst_4  = 134 = tensor<32arith.extsi arith.constantx :  4x%tensor<dense<!tt324-8388608. x4> : ptr<i8>:xtensor<, # 32x4x4x1xlinear1tensor<ii>328, 32, #blocked>
    ,x#%cst_5 =  tensor<iblockedarith.constant3232> x4,  ->dense<xi#ttg.slice<{dim = 0, parent = #blocked3}> tensor<2.000000e+00>64>4 : , # to x4tensor<4x4x1lineartensor<xx13216f32>
xx,       %i642x#blocked>
    %69 = , i8cst_6 = tt.load #, arith.constant%ttg# 68.blockeddense< slice<{dim = 0, parent = #blocked3}>70.000000e+00: >>>tensor<

 : 32            tensor<x4%%4x!180 = outLHS, x4x1xtt.arith.extsi %outRHSf32, ptr<i8>, % = #blocked>#30tt.split
    linear  %%1:135cst_7>  : = 
      i arith.constant%32tensor< 70 =  to 4xdense<tt.transi4-2147483648 %64x16>69
x :  {      2tensor<4x4xorder%x32 = 181ixarray< = 8, i32, #blocked>i32tt.splat#
    %cst_8 = :  blocked7arith.constant1%>  , 180-> dense<0> : tensor<23}i4> :64 x :  ->4tensor<tensor<32 x164xx4tensor<x4x32xi8xii64, , #328, #ttgblockedx#.2>i32, #blocked>linear1slice<{dim = 0, parent = #blocked3}>

> >          ->
%% tensor<      136cst_9 = 4x%182 = arith.constant32 = arith.shli xiarith.addi %dense<8,  outRHS,255>#ttg%  : .181%cst_2tensor<slice<{dim = 2, parent = #blocked4}>, 4x4x32x>
 :i32, #blocked>      %% 
71179tensor<     =  4%cst_10 = tt.splat : xarith.constant%tensor<4 2932xdense< :x168388607 ix>!64i : tensor<4x4x32tt, 8, xi32, #blocked>.ptr<bf16>##
 ->ttgblocked     .slice<{dim = 0, parent = #blocked3}>2%tensor<>>cst_114x

       = 128x      %137arith.constant!% =  tt.183arith.ori dense<ptr<bf16> = %1, #arith.muli outLHS,> : tensor<4x4x32xblocked1% i>7%32
,136,         :#blocked>%72% tensor<
     = tt.addptr64%cst_12 =  % xarith.constant71:4x ,  16dense<%28i64xi127> :
      8,  : tensor<4x4x32 tensor<%184#x4x = blocked2i128xtt.addptr>32!tt 
      , #blocked>.%%138
ptr<bf16>arg2, =     , # tt.reshape%blocked1% cst_13>,183 % =  tensor<: 137 arith.constant4x!tt:  128x.tensor<dense<iptr<bf16>44194304>64, , x4 : tensor<4x4x32xi32, ##blockedi64xblocked1>
16>
            x
%%i    73 = 185 = 8, %cst_14 = tt.load tt.expand_dims#arith.constant%72 blocked  :%2dense< 178>126tensor< { >4axis = ->  : x1tensor<tensor<4x4x32x128x : 4i!ix6432, #tt.32}xblockedptr<bf16> i>, #: 8, 
    blocked1tensor<#%>
4xiblockedcst_15      648 = %74, >
arith.constant = tt.splat#        %ttg%dense<53 .slice<{dim = 1, parent = #blocked3}>1392>: > =  : ! tt.reshapetensor<4x4x32tt->  %xi32, #blocked>.ptr<i8>tensor<105
 ->4      tensor<x1xi64, :%64# cst_16 = xblockedtensor<arith.constant32x3>4x !tt
4dense<.ptr<i8>      x121, %x>#blocked186i : 6> = 8tensor<4
      arith.extsi, x4x32% #x75%lineari32, #blocked> = tt.addptrarg13>
    %cst_17 =    arith.constant%74:->  ,  tensor<dense<%52i428> 32x : : 4xtensor<4x4x32x tensor<toi8i32, #blocked>64x , 
32i#ttg    x64.slice<{dim = 2, parent = #blocked}>%!tt
>cst_18.ptr<i8>      
       = , %%arith.constant#187140 blocked =  = tt.reshapedense<6tt.expand_dims -1.270000e+02>> % : tensor<4x4x1,%106x tensor<175 :f32, #blocked>64x { 
32axis = tensor<4    %x1xcst_19 = i64 : 4arith.constant, ix #blocked321dense<6}xi7> : tensor<4x32x>
 8i16,       :, #ttg%76 #. = tt.loadtensor<4xblockedslice<{dim = 2, parent = #blocked4}> i> >%7564->
     ,  %cst_20 = cacheModifier #ttgtensor<arith.constant=.4  cgslice<{dim = 1, parent = #blocked3}>xdense< >4-1>:  ->x : tensor<4x32tensor< ixi8, 64xtensor<8#324, ttgxx#linear.slice<{dim = 2, parent = #blocked4}>!tt1x2>>
    %cst_21 = .ptr<i8>i
arith.constant, 64      % #blocked, 141dense<6>#blocked3> = ttg.convert_layout0x7FC0>

 % : tensor<      %      140 128x3277%:x = ttg.convert_layout188 bf16 % = tensor<, 76 arith.muli4#: xblocked tensor<%4564x186x>32x,i
i 8    8, %, %#176#cst_22 = blocked6 lineararith.constant>:2  -> >dense< i64 ->7>tensor<64
  : x32      tensor<4tensor<4x4x32x%xxi81894i32, #blocked>, # = xi
    blockedtt.splat8%cst_23 = 3> , arith.constant
      %186#ttg %78 :.dense< =  slice<{dim = 2, parent = #blocked}>1.270000e+02tt.reshapei>> %64 ->
 : 73       tensor<4x4x1 tensor<%x:4142 = f32, #blocked> tensor<xarith.extui 
41%141    xx %128xi:cst_24bf1664  = , , tensor<arith.constant##4 blockedblockedxdense<1341.270000e+02> >x>-> 
i : tensor<4      8, tensor<4x4x1x%#x4x190 = ttg.f32, #linear>32xarith.mulislice<{dim = 2, parent = #blocked}>
bf16,  >     %cst_25 = #blocked%toarith.constant>
189,        % tensor<4dense<79%x-1.270000e+02 = 1874x> : tensor<4x4x1xmath.absf  if32, #linear>%78:16, 
    %  #cst_26: tensor<4ttg. = tensor<4xslice<{dim = 2, parent = #blocked}>arith.constantx1> 4xx
dense<32i      2.000000e+00> : xbf1664, %143tensor<4x4x1, # = arith.shlix#blocked %f32, #linear>
blocked>3>142    
      
,%%       cst_2780%% =  = 191cst_30arith.constantarith.extf  =   %tt.addptr:dense<79  -8388608> %tensor< : : 184,4tensor<4x4x1tensor<4 x4xx%xi4188i32, #linear>
    %x32 16cst_28xbf16: ,  = , #!tt#arith.constantblocked>.ptr<bf16>ttg.  to,slice<{dim = 2, parent = #blocked}>>dense< tensor< 
20971524i      >x464%144 : tensor<4x4x1xx32
 = ix      tt.bitcast 32, #linear>
    %f32, %%cst_29 = #192143 arith.constantblocked> = :  
      tt.expand_dims tensor<4dense<%81%x127 = 1824>" {xi : ttaxis = 16, tensor<4x4.r0#xe : ttg1dui.xi8c32slice<{dim = 2, parent = #blocked}>, #linear>e"} > 
    %(:-> cst_30 = %80 tensor<4arith.constant)tensor<x  <324dense<{xx7axisibf16> = 64,  : 2, #ttgtensor<4x4xi16,  : i#.#32}ttg.slice<{dim = 2, parent = #blocked}>>ttg> (slice<{dim = 0, parent = #blocked3}>
      .slice<{dim = 2, parent = #blocked}>{
>%145>      ^bb0  = 
(%->tt.expand_dims    arg16  %: tensor<%144cst_31 = f32, 1 {arith.constant%xaxis =  arg1732x2dense<: i : -1f3264i>), 32} : tensor<4x4:#blocked3> :x

 i8,         %      %tensor<#2091934ttg =  = x.arith.maxnumftt.broadcast4slice<{dim = 2, parent = #blocked}> % xbf16>arg16,%, 
     %190#ttgllvm.intr.assumearg17 .  :slice<{dim = 2, parent = #blocked}>%: >true tensor<  f324-> :
        x1tensor< tt.reduce.return x4i1%ix4
209 64x    :, #1llvm.intr.assume blockedx f323bf16%
      >, true} ->#blocked : ) >i : tensor<
      1(4%
tensor<x146    432 = llvm.intr.assume x4xtt.broadcast%xi true : i13264%
    xf32, #blocked145 llvm.intr.assume , 3: %true : #blocked>tensor<4i1>) -> 
x
    tensor<4      4llvm.intr.assumex4%194 = x1 xf32tt.expand_dims xbf16%true : i1, %, 
#179#blocked    ttg. {>llvm.intr.assumeslice<{dim = 2, parent = #blocked}>axis -> > =  tensor<%true
      04 %82 : x4: i1 = ttg.convert_layouti32x
 } 32    %81:xllvm.intr.assume  bf16 :tensor<, % 32#truetensor<xblocked 4i>:x64, 
       i14x#%
f32ttg147    , . = llvm.intr.assume %true#ttgslice<{dim = 0, parent = #blocked3}>tt.reshape  .>%146: i1slice<{dim = 2, parent = #blocked}>> ->  :
 tensor< tensor<    -> 14xllvm.intr.assumetensor<x4 432xx%xi32xtrue464bf16,  : x, #if32#blocked>1, blocked3 
#>->    ttg
 llvm.intr.assume.      tensor<4 %slice<{dim = 2, parent = #linear}>>%xtrue
195128        = x:%tt.broadcastbf16 83 %, #i = 194blocked11tt.expand_dims  >
%:
    82       llvm.intr.assume %true {tensor<%148 : axis = 1 = i2 : xamdgpu.scaled_upcast_fp41i32 
32x%    } i64, #blocked3>138llvm.intr.assume %true : :  ->  scalei1tensor<tensor< 
4x4%    4x147%xf3232 {0, xaxis = #ttgi = tt.get_program_id.641 :  slice<{dim = 2, parent = #linear}>>, ix ->#blocked3>32}  
 :tensor<4      :  x4%196 = tensor<4i32x1tt.addptr x64
xf32%x    , #191i%linear,8, 1 = >
 #tt.get_program_id      %%blocked 841808y = tt.expand_dims :>  % , : 81!tttensor<i {.432axis = ptr<bf16>x
    2,128% :  xbf162 = i32i, arith.addi} 64#blocked : 
1%tensor<      >arg54x% ,4x197-> f32,  =  %#arith.additensor<c31_i32ttg. 4x slice<{dim = 2, parent = #blocked}>>%128: 195x -> ,bf16itensor<4 , 32x4%193#blocked
x 1    1x:>%3 = f32,  
      arith.divsi#blockedtensor<% >4149%2
      x = arith.cmpi,%8532   = tt.bitcastxeq% %i64,c32_i3283 , #blocked  : 3%139:tensor<>, 4x
 %i32
    4x      cst_31%4 = 1x% arith.extsif32, 198 = :  #lineararith.extsitensor<%> 4arg7 %x4 ->arg4xi: tensor< 8,  4x: #ttgi324xi32. 1 slice<{dim = 2, parent = #blocked}>>toxito 
       32, i%i#linear6415064>
 = 
    
            tt.expand_dims%5 = %86% arith.extsi = 199% tt.bitcast = 149% %tt.splat {arg984 axis =  : i32 to  :%1982i tensor< :  : i64
    4i32%x464 ->} 6 = x1 :arith.extsi %xtensor< arg11f324tensor< : , x4i#1x32 to i64
blockedx4    > ix%->64i7 , 1 = tensor<4#, arith.extsixblocked#ttg 4x3>.slice<{dim = 2, parent = #blocked}>%0 : 1x
>ii32       ->32 to , #% tensor<i64
    blocked2004x%8 = >
 = 4arith.divsi       %arith.cmpi x1%1, %387sltx : i32
     = ,i%9 = arith.addi  1, arith.remsi%%# 85,185blocked% %,>
1cst_28       , %199%151 :  = tt.broadcast%3 tensor<:  4x %:4xtensor<4150  1x: i32x1tensor<4
i32xx4    , #ix1llvm.intr.assume linear>64x%
      , itrue%88#1,  : i1 = arith.addiblocked3>#
 
blocked    %      > llvm.intr.assume 86,%->% %201 truecst_1 = tensor<  :arith.extsi4: tensor< x4 %4xx32iarg54xx1 1xi
:i1     32, #llvm.intr.assumei, #blocked> 32 to blocked
%i>
      %true64      %152 =  
89 = tt.reshape :      tt.bitcast% %202 =  %151 itt.splat87 : 1 :tensor<
% tensor<4x    2014x4x% 43210: xxi = i1x1, arith.cmpi64 -> i# tensor<32blockedsgt1, >,x#  32xlinear->%i> arg664 tensor<,, #blocked->4 3> tensor<x128%
4xc0_i32      x4i1 %203 = x1, #:arith.cmpixblocked  i1islt32, >
32,#linear      %
 >153    %
 = scf.if192,       %arith.select  %90%%202 = tt.bitcast15210  , % :%cst_0{ 88, 
tensor< :%148      1  : %x32xtensor<4tensor<411i64, #blockedxx = 34128arith.muli>xx 
1i%      xi18%32, ,204, ##blocked  = blocked1%tt.broadcast >>, c4_i32% tensor< 200->4: :  x tensor<4tensor<4128xixxbf163214x, #
x1xblocked1      %12 = ii32>
tt.make_range1, #blocked,        {3> #%end->blocked>154 =  =  
ttg.local_alloc4tensor<        : 4%%ix911533232x =  , iarith.andi: start1,  (tensor< = #%894x0blocked, 128x : 3>%bf16i
cst_27, #32       blocked}%:1 205 tensor<>): = 4x ->  tt.broadcast4!tensor< x1ttg.4%203xmemdesc<4x128xbf16, #shared, #smem>x i
i: 32,       %32tensor<#linear155 = , 1>ttg.local_load#x
 ttg32x      %%154.i92 slice<{dim = 1, parent = #blocked1}>1, # = arith.andi: >blocked3> -> %!ttg
      %13 =  90,.memdesc<4x128xbf16, #shared, #smem>tt.make_rangetensor<   {4%cst_4-> endx tensor< = 32: 4x4xtensor<4128x : ix4bf16, i1, #blocked3>x#ttg32
1x.,       i32dot_op<{opIdx = 0, parent = #blocked3}>>start%206 = , #
       = arith.andiblocked%0 >
156 : %      % = arith.extuii20493 32, = tt.bitcast%}  %70  %91 : :205:tensor<   tensor<4xtensor<:4x324 4xxtensor<4xiix32x1832ix, , 1, i###blocked32, ttgttg3#.slice<{dim = 2, parent = #blocked4}>.>linear> slice<{dim = 1, parent = #blocked3}>
> to >      -> tensor<4
%tensor<x      207 = 4x32x%tt.splat4i14 x116,  = %x#ttgtt.splat196 : f32, .slice<{dim = 2, parent = #blocked4}> !tt#linear>
%.>
      11ptr<bf16>      %%157  94 = :->  = arith.shli tensor<tt.bitcast i4 %32x32%156 x92,->! : % tt tensor<cst_19tensor<.ptr<bf16>, 4 4#blockedx:x3>4x i
1xtensor<432      i32x32, %, #xi#208 = blocked16ttgtt.addptr>, . %207 #slice<{dim = 1, parent = #blocked1}>,-> ttg.> tensor<4slice<{dim = 2, parent = #blocked4}>>
      %15 = %197x
arith.addi 4       :x%% 115814tensor<xf32 = tt.bitcast,4x,   32x#%%!blocked15712tt>  .ptr<bf16>, 
::#blocked        3>%tensor<4tensor<, 95 = x324tensor<math.log2xx4 %i16ix93, 3232 :#ttg, x tensor<.slice<{dim = 2, parent = #blocked4}>#i4>ttg64, #blocked3>
x .      4x-> slice<{dim = 1, parent = #blocked1}>tt.store1tensor<> xf324x
%, 32      208#x%,linearbf1616 >
,  = %      #tt.splat174%96ttg. , = math.log2slice<{dim = 2, parent = #blocked4}>>%  
arg4%%       20694%159:   =  :: tt.expand_dims i tensor<4%32tensor<4x158 x4 {->32xx1axis =  !x2tensor<tt.f32,  : i4ptr<bf16>, #32}x#blocked iblocked>:323
       tensor<, >%974x#
 = 32ttg    math.floor xbf16.}%, slice<{dim = 1, parent = #blocked1}>
95#>     :ttg.
tt.return tensor<slice<{dim = 2, parent = #blocked4}>>      
4x %  }4->17
x  = }1xtensor<4arith.remsi
f32x32 
, x1%{-##linearx15
>
bf16, ,        # external%98blocked%_resources: { = math.floor4>16
 %
           96%:mlir_reproducer 160 : {:  = tt.broadcasttensor<
tensor< 4      4x%159xpipeline4 :i: x 32"1xtensor<4, bf32x#uiltin.module(op, #32ttgtblockedx.i>1slice<{dim = 1, parent = #blocked1}>mize-amd
x>-lds-      %bf16, 
u99 = #blocked      sagearith.subf4%{lds- >18l%  = i97->arith.mulimit=0 target-arch=gfx,  9 tensor<4%50}, trito%x327n-scf-to-cst_26x,c 32x f, c:bf16%onver , 4ttensor<# -4xblocked4:> 
ii      4xn64%1d
      %19 = 161 = xf32ett.expand_dimstt.trans , #x %160linear-%17 {>
t {order =       %oaxisarray<i100 = - = 32: arith.subf l10%98l : , , vi322%m}, cst_5{ :1 i >:ntensor<}  d4:tensor<4ex xxitensor<4-324xx1b, 32xi#ttg.slice<{dim = 1, parent = #blocked1}>xf32t>32x, w bf16#i->, blocked>d #blocked
      ttensor<4%h4>101=x1xi32,   = 0#->tt.clampf}blocked  %,1tensor<499, >x32 %a
xcst_25l      32x, l%bf16, %cst_24o20 = #blocked, ctt.splat 9propagateNana%>
 =targ8      % nonee : 162 -i32 ->  = : atensor<4x1xi32, tt.reshapetensor<m#blocked 4d1>
      %21 = %xgarith.muli %19, %201614p  xu: :1x-tensor<4 f32, sx1xi32tensor<4#linearh, #blockedx32>
a1x      r>32x%102e
bf16,  = tt.clampfd      # -%22 = blocked%marith.extsi 9>100,e%  m21 : -> %cst_18otensor<tensor<128, r4x32%yx1xxcst_23,ibf16, 32, ,  c##propagateNanoblockedblocked n15=v> to> nonee 
 rtensor<4      :tx% -1x163 = tensor<4tiamdgpu.scaled_upcast_fp4xr64, #blocked1>
      % 4i23%x1t = 77 xf32ott.make_rangescale , #n {%blocked-end = 162>
a128 {      m : iaxis = %103d320 = g,  : iarith.fptoui pstart32%u = }101-0  t : i32: : o}tensor<64tensor<4- xxl: 324ltensor<xxv128i1mx8x{i, f32, a32#blocked#r, 3>linear>c#ttg.slice<{dim = 0, parent = #blocked1}>,  toh>tensor< =
      128tensor<g%24 = x324xftt.expand_dimsxbf164xx , 19%23#xi5 {blocked8, 0axis5#  = > linear>f0->
t : i32} :  tensor<      %ztensor<128104=128x = tx32arith.fptoui rix%u32, bf16, 102 e##:}ttg.slice<{dim = 0, parent = #blocked1}>> -> blocked ,tensor<5tensor< 1>4cx128xi32, 
x4a#      xnblocked%1o1>164xn
       = f32i%25 = arith.cmpi , carith.extsieq#a , blocked>l%24 : % itensor<170tozx, e128 tensor<{x%4x icst_204 32, #blocked1> to  :x1mtensor< xa1x128xtensor<4i8xix, -6432#i, #blocked1>
      xiblocked>t%268
      e = , %105rtt.broadcast# = a ttg.arith.addi t%slice<{dim = 2, parent = #blocked4}>%103i22>
, o : tensor<4x      %n1x%165cst_29si =  =64tt.expand_dims:1, #blocked1> ->  % tensor<0tensor<41644x x {4m128axisxax = 21xxi64, #blocked1> : ii8-
      %27 = 32}, #ntt.broadcast linearu : >
m%tensor<      -25 : 4%rtensor<x106e1x128x32 = wixarith.addi r64, #blocked1> -> i1%104itensor<4, , tx#%e128xttgcstsi. =64slice<{dim = 2, parent = #blocked4}>: -, >tensor<41# x blocked1>->4r
 x1e      tensor<xg%28 = 4iiarith.addi x328, o%x#n26, %271blocked>- xi
      s: 1, %107itensor<4#blocked = arith.subfmx4 p128xi64>%l, #blocked1>
      cst_6,i
      %29 = %166 %ftt.addptr = 102y tt.broadcast =% %: narg0165tensor<o, 4r :x4m% x1a18tensor<xf32l 4,  :x32#blockedt x>e!tt.ptr<bf16>1
      s,x%t i1108 = -i, #math.exp2 c64blocked%o
4107n      >  :v%-> tensor<e30 4xr = tensor<4garith.muli %9, %c32_i324xx1e 32xn:xf32c 32x, #eiiblocked=321>f
, 
a      %31 = #blocked      %ltt.make_range4109 = s {>
arith.extfeend =       % % 32167 = 78 t : tt.trans:oi32  p, %tensor<4-start166xd =  {order4xo0 = 32w : i32} : array<ixbf16ntensor<32, =32x: #blockedti0> r32, , tou#2 ettg, tensor<}.1>4x,slice<{dim = 0, parent = #blocked6}>}4 > :x32c
 xs      tensor<f32e%32 = 4, ,tt.make_rangex32#blocked  {end = x32>c32 : i32, x
      %110ostart = i1 = tt.broadcastn0,  v : i32#%e} : blocked108rtensor<4 :t32> -x tensor<4ci->xf32,  tensor<4-#4xtttgx1xo.32xf32, -slice<{dim = 0, parent = #blocked3}>32#l>xiblocked>l
      1,  ->v%# tensor<m33 = blocked94x{tt.make_range>4xi {end = 
      32xn32%168f32, d :  = #eitt.reshape blocked>x32%
-, start = 167       %b0:111 = i :  arith.mulf ti32}tensor<%w :4109i x32, dtensor<x%t3232x110hxi1 :=i, # tensor<032blocked4}, 9>x4,# x ttg.slice<{dim = 1, parent = #linear1}>-> 32xc>tensor<f32o
      %34 = 128, ntt.splatx#v 32blockede%x>r30 : i
      ti32 -> 1, %112-tensor<# = a32blockedtt.bitcastrxi32, 5> %i#
111tttg.slice<{dim = 0, parent = #blocked6}>>
      %35 =       % htt.splat 169 = : -%arith.selecttensor<t30 :  4oi32 ->%168x4- , xltensor<%32l32cst_21xvx, %f32, mi163#{32 : blockedi, tensor<>n#ttg.slice<{dim = 1, parent = #linear1}>128 d>
      %36x->e = 32 tensor<xarith.addix4x- i4b%1xi34, %31, #32xt blockediw: 5>32, itensor<, #d32xtensor<128blocked>tix
h32, 32x      %=#bf16, 113 = 0ttg#arith.andi}.slice<{dim = 0, parent = #blocked6}>blocked ,>
      %37 = 5% arith.addi %35, >
112,c%       a33%170%cst_7n  =  o:ttg.local_alloc: n  tensor<itensor<%4xc32xi32, 169 4xa#:32lttg.slice<{dim = 1, parent = #linear1}>>
      %38 =  (xiitt.splattensor<32, z 128x#blockede%32x>
{arg5bf16        : , % i#114 = m32 -> blocked5arith.shrui atensor<>%112x32xi32, ) -> ,-#ttg.slice<{dim = 0, parent = #blocked6}>! i>
      ttg.%cst_8t%memdesc<128x32xbf16, #shared, #smem> e39 = 
      : rtt.splat%tensor<a 171 = 4t%ttg.local_loadxiarg5 :  4oi32 -> %xntensor<3217032xsx i=i32, : 321#!, 0ttgttg# ..blocked>mslice<{dim = 1, parent = #linear1}>memdesc<128x32xbf16, #shared, #smem>
      a> %x
      -> 115 = -%tensor<arith.andin40 = 128 uarith.remsi %36,x%m %38 : tensor<32xi32, 32114-#x,rttg.slice<{dim = 0, parent = #blocked6}>bf16 %e>, cst_9w
      # r%ttg:i41. t = dot_op<{opIdx = 1, parent = #blocked3}>>tensor<4earith.remsi
x4s %37, %39 :       x=tensor<%32x-32xi32, 172 = i321#tt.dot , # ttg%155blocked>r.,
eslice<{dim = 1, parent = #linear1}>>
             g%42 = %171%116iarith.muli ,  = arith.andio%% n7cst_3%-, 112s :,i%  m5tensor<4%cst_10p : x li64128:i
      x f%43 = bf16, tensor<ytt.make_range#ttg4= {.dot_op<{opIdx = 0, parent = #blocked3}>x4nend>xo =  *32xr64 im : tensor<12832, ai32, x#lstart32blocked  = xbf16>
t0 : i32} : ,       etensor<64xi32, #ttg%117s#.dot_op<{opIdx = 1, parent = #blocked3}> = arith.additttg> -.slice<{dim = 1, parent = #blocked6}>> ->%115c
 ,o      tensor< n%44 = 4x%cst_11vtt.expand_dims %4332x :e {f32 raxis = , #tensor<4g1blockedx4e : i3xn32>32c} : 
      xietensor<%32=64173, fx = #aiarith.addfblockedl32,  >s#ttg.slice<{dim = 1, parent = #blocked6}>>%172
      e ,% ->  118ttensor<64% = arith.subiox1xcst_3 pi32,  :%cst_12-#blocked6> tensor<, d
4%o      %45 = x117warith.extsi 32x n%44f32, : = #tensor<t:blocked4r 3xutensor<64x1xi32, #blocked6> to >
4xetensor<64x1x      %32x}i174i,64 = 32 , #blocked6>arith.truncf, #c
       blockeds%%>e46173
      , =  % tt.expand_dims : 119 = s%tensor<arith.cmpiy404x ultm {32,baxis = x o0 : i32} : f32%115ltensor<, , -32#%cst_12dxi32, blocked3 :c#> ettg totensor<4,. x4 slice<{dim = 0, parent = #blocked6}>tensor<x32e>4xn -> xiatensor<3232b1x32xx, li32, bf16#e#, blocked-blocked6>#blocked>
l
3      %i      >120 = n%
arith.shruie47       - = %175%116itt.splat = arith.extsi, n  %%cst_11f%13  :oarg10: , : i32 ->  tensor<tensor< tensor<44c1x32xi32, #blocked6>
      %48 = xxoarith.muli %46, i324xn%, 32v47#ttgxe. irt-slice<{dim = 1, parent = #blocked3}>: 32, b> tensor<#uiltin-funto1blocked>c-to-ll x
      vtensor<32%m4x121{ftz=trxi32, #blocked6>
 = ue})i64      %49 = arith.ori ", arith.extsi %120,#%48 : , 
ttgtensor<1%cst_13      .slice<{dim = 1, parent = #blocked3}>x32xi32, #blocked :disable_threading>6> : 
 to tensor<4false      %tensor<1x,176x4
 = 32x      arith.extsi x32xverify_each: true%ii
116432,     } :, #
 #blocked>  }i32blocked6>
      %50 = 
      
#-} tott.broadcast %
 i%4512264  = arith.shrui
:  %/tmp/torchinductor_root/5f/c5fay6dh2cyu6vjiqilgbytbqkqd5di6bm3dut2fs5f7zdeffehg.py:18:0      %tensor<121: 177 = 64x1x,error: tt.splat i64, #blocked6> ->  %Failures have been detected while processing an MLIR pass pipeline%176tensor<64x32xi64, #blocked6>
      118 
 %:/tmp/torchinductor_root/5f/c5fay6dh2cyu6vjiqilgbytbqkqd5di6bm3dut2fs5f7zdeffehg.py:18:0: 51 tensor<: i64 = 4xnote:  ->tt.broadcast %49 : 4xPipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
 tensor<tensor<1x32x32x4ii32xi64, #, 64blocked6> -> #blocked, tensor<>#ttg64x32x
.i      %slice<{dim = 1, parent = #blocked3}>>64, #blocked123
6 = arith.select      %> %178
119 =       , %arith.addi %52 = 122, %177arith.addi %50%,,116   : %%tensor<175514  x4: : xtensor<tensor<32464xxxi1i6432x, #, iblocked>#ttg64, tensor<.slice<{dim = 1, parent = #blocked3}>, 4>#x
blocked4      6x32%179>x = 
i32arith.extsi      , # %%53 = blocked>32 tt.addptr
:        %%124tensor<32arg1 = arith.maxuixi, %42 %32 115,, : %#ttg cst_14.! slice<{dim = 0, parent = #blocked3}>tt:>. tensor< to[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """Kernel for computing the matmul C = A x B.
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 7, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank7]:E1103 11:31:40.937000 516 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
ptr<i8>, 4x tensor<i64432
xx      32i%xi64, 54 = 32#arith.extsi, #ttg blocked>.slice<{dim = 0, parent = #blocked3}>%
>arg14 :       %
      i32 to125 = % arith.subi180 = i64 %arith.extsi 
      %124%55,30 =  % :arith.muli %cst_14 i7 :32,   tensor<to%4x i54464 : x32
      i64x%
i32181 =       %56 = , #tt.splat tt.addptr %blocked%arg3, %55 : >180!
       :tt%126 i.ptr<i8>, i64
       = 64 %57 = arith.shli -> tt.expand_dims %tensor<%4112532 {,xaxis =  %i641 : i32cst_15, } :# :  ttgtensor<tensor<.324slice<{dim = 0, parent = #blocked3}>xx>i4x
      32, 32x%182#ttg.slice<{dim = 1, parent = #linear1}>i32 = arith.addi> -> ,  tensor<#blocked%32>181x
,1      % %x127179i = arith.shrui :32,  % tensor<#linear1>123,32x
      %58 =  %itt.splatcst_16 64,  :#% tensor<ttg.arg15 : 4slice<{dim = 0, parent = #blocked3}>ix4>32x
 32x      -> i%tensor<32, 183 = 32x1xi32, #arith.muli#linear1>
      %59 = blocked arith.muli >%%57, 
7%58      , %128 %:  = arith.ori6tensor<32  x%:1126, ixi32, #linear1> %64

      %60 = 127       %arith.extsi %59 : tensor<32:184x1  = xtensor<tt.addptri32, #linear1> to 4x %tensor<32x1x4xarg2,i64, #linear32x %1>i183
32       , :%# 61blocked! = >tttt.make_range
. {      ptr<bf16>end%, = 129 4 = i : arith.addi64i32 
, %128      %start = , 185 = 0%tt.expand_dims : cst_11 %i32 178} : :  {axistensor<tensor< = 44x1 : xi32, 4i32#ttg.slice<{dim = 0, parent = #linear1}>x32}>x 
      %62 = i32: tt.broadcast , #tensor<4%60blockedx >i:
64       , tensor<%#32130ttgx = .slice<{dim = 1, parent = #blocked3}>1xarith.shrui>i % ->64129 , , tensor<#%cst_114xlinear1> :1x ->  tensor<i64tensor<32x4x4x, #i64, #linear1>4xblocked3
      %63 = 32>
tt.expand_dims %61x       {i32%axis, #186 = blocked> = arith.extsi0 : i32
       %} : %arg13tensor<131 =  :4xi32, arith.minui  i#ttg.slice<{dim = 0, parent = #linear1}>> %32->130, to  % itensor<cst_2264
1x4x :      %i32, #linear1 tensor<187 = >4tt.expand_dims
x4 %      x175%64 = 32 {tt.broadcastxaxis =  i1%32 : i63 , #32:blocked} >
 tensor<      %: 1x4xi32, #linear1> -> 132tensor<4tensor< = arith.shruixi32x4xi32, #linear1> %64, 
      %65 = 113#arith.extsi,ttg.  %slice<{dim = 1, parent = #blocked3}>>%64cst_17   -> : : tensor<4tensor<32x4xi32tensor<4x, x1#4xxilinear1> 32x64toi,  32, #tensor<#blockedblocked332>>
x4
            %x%188i133 =  = arith.muli64, #linear1>arith.ori  
%%186      %66 = 132,, arith.addi  %%%17665131  ,::  tensor< i%62464 x4
      :x% 32189 = tensor<xtt.splat32i x32%4, #186 xblocked>:i
 64      %i64, 134 =  #arith.trunci-> linear1>
      %67 %tensor<4 = 133xtt.splat 1 : x%tensor<i564x64,  4x#blocked: 323!x>tti32
      .ptr<i8> -> , #%190tensor<32x4xblocked = !>arith.mulitt  .to%ptr<i8> 189, tensor<4,#x linear4%1>
      %68x187 = 32 tt.addptr xi:%8 67, %66 : , #tensor<4tensor<32blockedxx>
1x4x      %i64!tt.ptr<i8>, #linear135 = , 1tt.reshape #>, %blockedtensor<134332 >x4x: 
      i64, #linear1>
      %69 = tensor<4%191tt.loadx4 = tt.addptr x %32x%18468i,  8%:, 188 # tensor<blocked:32x4x> ! !tt->tt.ptr<i8>, #linear1> tensor<.ptr<bf16>
      %70 = 4x, tt.trans 4xi64%6916
 {x      order2% = xi192 = array<8tt.expand_dimsi,  32#blocked%: 7>1821
 {,       axis0%outLHS = >, 0} : tensor<32x4x% : ioutRHSi8 = 32}, tt.split # :linear% 1>135tensor<  :32x->  itensor<tensor<464, 4x32xx4#ttgi8, x.#ttg16xslice<{dim = 0, parent = #blocked3}>>.2 slice<{dim = 2, parent = #blocked4}>xi->>8 
, tensor<      #blocked1x%71732 = >xtt.splat i -> 64, %tensor<#blocked29 : 4x3>!tt.ptr<bf16> 4
      ->x16%193 xi = tt.broadcasttensor<8,  %4x#190128xblocked2 :!tt>
 .      %tensor<4ptr<bf16>136x, #blocked1> = 1
arith.shlix      %72 =  %itt.addptroutRHS,64,   #%%cst_2blocked71,  3%: > 28 : tensor<->tensor<4x 44xtensor<4x16x128x32xi8xi!tt, #64, .blocked2#ptr<bf16>>blocked, 
3#      %>blocked1>, 137 = 
      tensor<arith.ori%4 %194 = xoutLHStt.expand_dims128,  %x%179i136  {64, #blocked1>: axis
tensor< =       40%73 = x : tt.load4xi32 16}%xi :728  , tensor<:#32 blocked2xitensor<>644
, x      %#128138 = ttgxtt.reshape .slice<{dim = 0, parent = #blocked3}>!tt.%137> ptr<bf16>, #blocked1> :-> 
       tensor<tensor<1%4xx744x32 = 16xtt.splatxi i64, %53 8#blocked: , 3!tt.ptr<i8> -> #>tensor<blocked2
      64x> %19532x-> = ! tensor<tt.broadcasttt4x .ptr<i8>, 64x%194#blocked6>
      i8 :%75 = , # tensor<tt.addptr blocked81x%74,>
32x %52 : tensor<64x32x      %i64!tt.ptr<i8>, #blocked6>, tensor<64139, x = tt.reshape#blocked32x 3i%105>64,   #:->blocked tensor< tensor<6>
      %76 = 4x4xtt.load 4x32x%751xi i864, cacheModifier, # #linearblocked3= >>cg 
 ->      :  tensor<%tensor<419664x = x324xtt.addptr xi%!8, 191,tt# .ttg%ptr<i8>, .slice<{dim = 2, parent = #blocked}>180#> blocked
:6      % !>140tt
 = tt.reshape.ptr<bf16>      %77 =  %, ttg.convert_layout 106 i64%76:
        :tensor<4%197 x = tensor<4xarith.addi 64x32x1%ix1958, i8,#, # %blocked6> blocked193->>  : -> tensor< tensor<tensor<464x324xx32x4xixii648, #blocked3>8, 
, #      #linearblocked%2378>
>
 =             tt.reshape%141%198  =  = %73ttg.convert_layout arith.extsi  : %%tensor<140arg44  x: : 128xtensor<ibf164x32 , #blocked1>4xto  i8i64->, 
 #linear      %tensor<21994> = x tt.splat4-> x tensor<%324198xx4 :bf16, #blocked>xi i
      %79 = 8, 64 math.absf#-> ttg. %slice<{dim = 2, parent = #blocked}>>tensor<478
x       %1x:142 = i64 arith.extui, tensor< #4%blockedx14134 >x32:
x       bf16tensor<4%200, #x4 = arith.cmpiblocked>
      %80 = xi sltarith.extf8, ,  #ttg%185%79.slice<{dim = 2, parent = #blocked}>,  > %199:to  : tensor<4 tensor<xtensor<44x4xx4x32i1x16, xibf16, #blocked>#ttg64,  .#toslice<{dim = 2, parent = #blocked}>>blocked3 
      >
tensor<4x4x%      32143%x = 201f32arith.shli = ,  arith.extsi#blocked% >142,%arg5
      %81 =  % :"cst_30 t :i32t.r tensor< e4xtod4x uiic1664e, 
      "#ttg%202(. = %slice<{dim = 2, parent = #blocked}>>tt.splat80
       )%% <144 = 201{tt.bitcast  :axis = %143 i2 64 : : i tensor<->324 }x4tensor<1>xx (i32{16x
, i      #64^bb0ttg, #(.slice<{dim = 2, parent = #blocked}>blocked3%arg16: > >
f32->      ,  %%tensor<203arg17: f324 = arith.cmpi)x :4xslt,
bf16,          #%%209 = ttg.192,arith.maxnumfslice<{dim = 2, parent = #blocked}>> % 
      202%%145 :arg16 =  ,tt.expand_dims tensor<1 %%144xarg17 {32 axisx: = 2i64  : , f32i32#blocked
        } 3>tt.reduce.return:
        %tensor<%2094204 x = :4tt.broadcast  x%f32bf16, 200 
#ttg:       .tensor<}slice<{dim = 2, parent = #blocked}>>4) x : -> 1x(tensor<itensor<41, 4x4x32x4#blockedxx13>f32, #blocked>x ) -> bf16-> tensor<4x4x, #tensor<4f32, blockedx#>32ttg
      x.%146islice<{dim = 2, parent = #blocked}> = 1>tt.broadcast, 
 #      %blocked%145 3>82 = : 
ttg.convert_layouttensor<       4x%205%814 =  xtt.broadcast:1  xbf16%203tensor<4x4, # xblocked:f32> tensor<,  ->1x#ttg.slice<{dim = 2, parent = #blocked}> 32>tensor<4xi -> x1tensor<4, 4x32#xxblocked4bf163x, #> f32, blocked>-> #ttg.slice<{dim = 2, parent = #linear}>
tensor<>
      %83 =       4tt.expand_dims %x32%82147 = xi {tt.reshape1axis =  %, 2146# : i32 blocked} : tensor<4x4xf32, :3#ttg tensor<>
.slice<{dim = 2, parent = #linear}>4      >x4%206 ->x32 = arith.andi tensor<4x4x1x xf32, bf16%#linear>, 204,
      #blocked %84>% =  205tt.expand_dims->  %81 tensor<: {4x tensor<axis = 12842xbf16x : , #32xi32} : blocked1i1tensor<4x4>, xf32, 
      #blocked#ttg.slice<{dim = 2, parent = #blocked}>%3>148> ->  = 
tensor<4x4x1amdgpu.scaled_upcast_fp4      x %%207f32, 138 = #blocked> scalett.splat 
       %%196%85 = 147 :tt.bitcast {  axis = !tt%831 : .ptr<bf16> :i  32->tensor<} 4x4x1 tensor<x:4f32 x32, tensor<x#4x!ttlinear>64.ptr<bf16> x, #->iblocked 8, 3tensor<#>4blocked
x8>      4,%x1 208xtensor< = i32, #linear>4tt.addptr 
      x128%207%86 = x, tt.bitcast bf16%197%84 : , # :tensor<4x4x1xf32, #blockedblocked1 >> tensor<4 -> tensor<4x4x1->xx 32itensor<x!32, #blocked>4xtt.
      %87 = 128xptr<bf16>arith.addibf16, , # %85, %#blockedblocked3cst_281>>, : 
       tensor<tensor<4x4x1x%1494xi32,  = 32#arith.cmpixlinear eqi64>
      %88 = ,, arith.addi %#blocked 1393%, >
86, %      %cst_31tt.storecst_1   :%208: ,  tensor<%tensor<4174,4x4x1xx i4x%20632i8 :, #blocked>
      ,  %#tensor<89 = ttg4tt.bitcast.slice<{dim = 2, parent = #blocked}>x32 >x%
!87      %tt :150.  = ptr<bf16>tensor<4x4x1xtt.expand_dims , #i32, #linear> -> tensor<4x4x1%blockedxi32, #linear>1493>
      %90 =  {axis
    tt.bitcast  = 2}%88 : tensor<4x4x1x : i
    i3232tt.return, }
#   blocked> -> :}tensor<4x4x1 
xtensor<4}i32x
, 4x
{-##blocked>i

1        , external_resources: {%91 = #ttg
    arith.andi.slice<{dim = 2, parent = #blocked}>mlir_reproducer: { > 
      %->pipeline89 tensor<: ,4x"b 4u%xicst_271xlt ii:1n , #.mtensor<4x4x1xblocked>odi32, #linear>
      
u%      %le92 = 151(arith.andi = tt.broadcastop %90,  ti%%mcst_4 : tensor<150iz4x4x1 e-xi32, #blocked>:a
 tensor<md      %93 = 4x-ltt.bitcast %91 : 4xdstensor<4x4x1xi32, #linear> -> 1x-utensor<4x4x1xisf321a, , g##elinearblocked>{l>
      %94 =  dtt.bitcast->s  -%92 : tensor<ltensor<4i4x4x1xi32, #x4miblocked> -> tensor<4x4x1xxtf3232x=0, #blocked>
      %95 = i tmath.log21a , r%#g93blockedet >
-a:      r %ctensor<152 = h4tt.reshape=x4x1 gfx%x9f32, #linear>
      1515% :096 tensor<}, = 4 math.log2x4tr xi%32to94xn i1-:, sc #ftensor<blocked-t4x4x> o1-> -xtensor<cf324xf, 128,#blocked>
      x %ico97 = 1nmath.floor, ve #r%blockedt951- >
in:      %d 153etensor<4x = arith.selectx4 -x%152to1, -x%lf32, #linear>cst_0, lv
      %98 = %148m{math.floor  : in%96tensor<d 4xex: tensor<4x4x1128x-xi1bif32, #blocked, #tw>
      %99 = blocked1idarith.subf>, th tensor<=%4x0}97128x,,bf16,   #a%blocked1llcst_26>o 
      c:%a 154 = tetensor<4x4x1ttg.local_alloc-x af32, #linear>%m
153dg       p%:u100 (-s = tensor<4haarith.subfx128re xd%bf16-98, #m,blockede 1>mo%)rycst_5 -> , !ttg c: .memdesc<4x128xbf16, #shared, #smem>ontensor<4x4
      vex1%rx155tf32 = -, #blocked>
ttg.local_load tr      %101 = %itt.clampf154t  o%:n99 -a, !ttgmd%cst_25, .gp%cst_24memdesc<4x128xbf16, #shared, #smem>u, - ->tpropagateNan o tensor<-=4ll x128vmnonex{a bf16r:, c #h=tensor<4ttggx4x1.fxdot_op<{opIdx = 0, parent = #blocked3}>x9f32, #linear>>5

0             %ft%102 = 156ztt.clampf = = arith.extui t%%70ru100, % ecst_18: },, tensor< %4xcacst_23, propagateNan = 32nononexn : iitensor<8, ca4x#l4ttgix.slice<{dim = 2, parent = #blocked4}>ze1>{x  f32to , #blocked>
      % tensor<m103 = 4xaxarith.fptoui32x-i i16te%101, ra #t:ttgi .slice<{dim = 2, parent = #blocked4}>ontensor<4x4x1xf32, #linear>
s>      = %15710to =   arith.shlimtensor< %ax4x4156-x, nu1%mxcst_19-ri e8:w,  tensor<ri#linear>4t
xe      32xs=%104 = i16-arith.fptoui , 1 %#r102ttg.e slice<{dim = 2, parent = #blocked4}>g:>
io tensor<4x4x1      nx%-f32158 = si, tt.bitcastm# %pblocked157l>  :ifto tensor<4x4x1 yxtensor<=i4n8x32o, xir#16mblocked, al># 
      ttgt%.slice<{dim = 2, parent = #blocked4}>es105 = > t-arith.addi %103, %-> cocst_29tensor<n 4v:xe 32rtensor<xge4x4x1xbf16, ni#c8ttg.e, slice<{dim = 2, parent = #blocked4}>>=f#
      allinear%s>159e
 = tt.expand_dims t       o%106 = %158p-arith.addi  {axisdo%104, % = wcst2 : n= : ittensor<32}r4 :ux e4tensor<4},x1xx i32xcs8, #blockedbf16, e,>
      %107 = # arith.subf ttgc%.slice<{dim = 2, parent = #blocked4}>oncst_6, >v%102 e : ->rttensor<4x tensor<-4x14cfxf32, #blocked>x-
      32to%108 = x1-lmath.exp2xlv bf16, m%#{107blockedin 4d:>
ex       %-btensor<4x4x1160ix = tt.broadcasttwf32, #blocked> %i
159 dt      :h% tensor<=0109 = 4x},arith.extf32  x1c%xo78bf16n , #ve:blockedr 4>t-tensor<4x a4x->r32 ixtensor<tbf164h, x32-t#blocked>xo 32-toxll bf16vtensor<, #m4blocked4{x>i4x32
nx      %def32, #blocked>161 = x-
      tt.trans bi%110 = %160twtt.broadcast  {i%order = dt108array<h i=:32: 0} 0,tensor<,  42cx4x1, ax1nf32, #blocked> -> >otensor<} n4:ix tensor<c44axxl3232ixxzf32, #blocked>
      %11132e{ = xbf16  arith.mulf, m #blockedax%1094>-i, ->te  tensor<ra%1104t xi:32o xntensor<4x4x32s=32x10xf32, #blocked>
      %112bf16  = , #matt.bitcast %111 blocked9x-: >nutensor<4x4x32
m-xf32, #blocked> ->       %retensor<4x4x32162wrx = iitt.reshapet32 es, #blocked>
      %=%161-113 1 = :  rarith.anditensor<4eg %112, %cst_7 :x32io tensor<4x4x32xx32n-i32, xbf16si#blocked>, #mp
      blocked9li%114 = >farith.shrui y -> =%112tensor<128no,xr 32xm%bf16acst_8, l # :blocked5te >stensor<
t4      -x%163c4x32 = amdgpu.scaled_upcast_fp4onx vi%e32, #blocked>77 r
scale ge      %162n% {c115 = axis = e=arith.andi0fa %114, %cst_9 : tensor<4x : l4i32sx} e 32:tox pi32, tensor<-d#blocked>64xow
      %116 = 32narith.andi %112, %x=cst_10it : 8rtensor<, #ue4x4x32blocked}xi323>, , #blocked>
      , cs%tensor<e,117 = 128x sarith.addi 32xym%115,bf16, bo %#lcst_11blocked-d : 5ctensor<> e,4x4x32x-> i32, #blocked tensor<en>128xa
32xb      %118 = bf16, learith.subi#-l blockedi%5>necst_12
      -i,%n 164f% = o117arith.cmpi,  eq c:,on  vetensor<4%70rtx4x32,-xi %bu32, #blocked>cst_20il
      %119 :ti =  narith.cmpitensor<4-f xuult32xnc, i8-t%115, %cst_12, o #-:ttgl tensor<4x4x.l32slice<{dim = 2, parent = #blocked4}>>vmxi32, #blocked>
      {f
      %120 = %tarith.shrui165 = z tt.expand_dims=% t116%164r, {u axis = e}%2 : )"cst_11 :i32, }
tensor<       4:disable_threading: x4x32xi32,  tensor<false#4x,blocked>
      %121 = 32x
      arith.orii1verify_each:  , true%#
120ttg    },.
  } slice<{dim = 2, parent = #blocked4}>>
#-}% ->
cst_13 tensor< 4x:32x 1xtensor<i14, #/tmp/torchinductor_root/ax/caxufmlaky4jkk2x4oqaq64msnnhbq2fec4pb5glmnyuchr3clyw.py:18:0xblocked: 44>error: Failures have been detected while processing an MLIR pass pipelinex
      
32%/tmp/torchinductor_root/ax/caxufmlaky4jkk2x4oqaq64msnnhbq2fec4pb5glmnyuchr3clyw.py:18:0x166: i = tt.broadcastnote: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`32, #blocked> %

      165 %122 = : arith.shrui %121, %118 : tensor<4x4x32tensor<4xx32i32, #blocked>
      %123 = x1arith.selectxi 1, %119#blocked, 4>%122 ->,  tensor<%1164x : 32xtensor<32x4x4x32i1xi, #1, #blocked>blocked4, >
tensor<      %4x4x32xi32, #blocked>167 = 
      tt.trans %%124 = 166arith.maxui {order  = array<%115i32,: 0 , 2%, 1cst_14>}  :: tensor< 4xtensor<4x432xx32x32i1x, #i32, #blockedblocked4>> 
      %125 = -> arith.subi tensor<4%124x32, x32%cst_14xi :1,  tensor<4x#blocked4x329>xi32
      , #blocked>%168
 = tt.reshape       %%167 126 = : arith.shlitensor<4 x32%125x32,x i1%, #[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """Kernel for computing the matmul C = A x B.
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 0, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank0]:E1103 11:31:40.953000 509 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
cst_15blocked9 > :->  tensor<128tensor<4x4x32xi32, #blockedx32>xi
1,       %#blocked127 = 5>arith.shrui %123, 
      %%169cst_16 :  = tensor<4x4x32xi32, #blocked>
      arith.select %128 = %168arith.ori %126, %127 : tensor<4x4x32, %xi32, #blocked>
      cst_21, %%163129 =  : tensor<arith.addi %128x128, %cst_1132x : i1tensor<4x4x32, #xblocked5i32, #blocked>>, 
tensor<128      x32%130xbf16 = , #arith.shrui blocked5%>
129      %, %170 = cst_11 : tensor<4x4x32ttg.local_alloc xi32, #blocked>%169
       :%131 =  (arith.minuitensor<128 x32%130xbf16,, # blocked5%>)cst_22 ->  !ttg:.memdesc<128x32xbf16, #shared, #smem> 
      tensor<4x%1714x32xi32, #blocked> = ttg.local_load
       %%132 = 170 arith.shrui : %113, %!ttgcst_17 : .memdesc<128x32xbf16, #shared, #smem>tensor<4x4x32xi32, # ->blocked tensor<>128x
32x      bf16, %133 = #ttgarith.ori .dot_op<{opIdx = 1, parent = #blocked3}>%>
132      %, 172 = %131 : tt.dot tensor<4x4x32%155xi32, , #blocked>%171
      , %%cst_3134 : =  tensor<arith.trunci4x 128x%bf16, 133#ttg .dot_op<{opIdx = 0, parent = #blocked3}>:>  * tensor<4tensor<128xx324x32xbf16xi, 32, #blocked>#ttg .dot_op<{opIdx = 1, parent = #blocked3}>to>  -> tensor<tensor<4x4x32x4xi8, #blocked>
      %135 = 32xtt.reshape f32, %134#blocked : 3>tensor<4x4x32
      xi8, #blocked> ->%173  = arith.addftensor< %4x4x16x2xi8, #blocked172,7> %
      cst_3% :outLHS tensor<, 4x%32xoutRHSf32,  = #tt.split %135blocked3 : >
tensor<4x4x16x2x      %i8, #blocked174 = 7> -> arith.truncf tensor<4x4x%17316 :x tensor<i8, #blocked2>
      4x%32x136 = f32, arith.shli #%outRHS, blocked3%cst_2>  : to tensor<4x4tensor<4x16xi8, #x32blocked2>xbf16
,       #blocked%137 = 3>arith.ori 
      %%175outLHS, = arith.extsi  %%136 13 : : tensor<4x4x16xi8, tensor<4#xiblocked2>32, 
      #ttg%.slice<{dim = 1, parent = #blocked3}>138>  = to tt.reshapetensor<4 xi%13764,  :#ttg .slice<{dim = 1, parent = #blocked3}>tensor<4x4x>
16      %xi8, #blocked2>176 =  arith.extsi -> %11tensor< :4x64 ixi8, 32 #to blockedi648>
      
      %139 = %177tt.reshape  = tt.splat%105 % : 176 tensor<4x4x1x: i8, #lineari64> -> ->  tensor<tensor<4x4x4xi8, i64#ttg.slice<{dim = 2, parent = #blocked}>>
      %, 140#ttg = .slice<{dim = 1, parent = #blocked3}>tt.reshape >
%      %106178 =  arith.addi : tensor<4x4x1xi8, #blocked%177> -> tensor<4x4x, i%1758 :, #linear2> tensor<
      %141 = 4xttg.convert_layout %140 i64: , tensor<4x4#ttgxi8, #linear.slice<{dim = 1, parent = #blocked3}>2>>
 -> tensor<4      %x4179 = xiarith.extsi 8%32,  :#ttg.slice<{dim = 2, parent = #blocked}> tensor<>32x
      %142 = i32arith.extui,  #ttg%141.slice<{dim = 0, parent = #blocked3}> > :to  tensor<32tensor<4x4xi8, xi#64, ttg.slice<{dim = 2, parent = #blocked}>>#ttg .slice<{dim = 0, parent = #blocked3}>to>
       %tensor<4x4180 = xi16, arith.extsi #ttg.slice<{dim = 2, parent = #blocked}>%30> :
      %143 =  iarith.shli %142, %32 cst_30to  : i64tensor<4x4xi16, 
      #ttg.slice<{dim = 2, parent = #blocked}>>
      %181%144 =  = tt.splattt.bitcast %143 : tensor<4x4xi16,  %#ttg180 .slice<{dim = 2, parent = #blocked}>> -> : tensor<4xi644 ->x tensor<bf16, 32x#ttg.slice<{dim = 2, parent = #blocked}>>
i64      , %145 = #ttgtt.expand_dims %144.slice<{dim = 0, parent = #blocked3}> {>
axis =       %2182 =  : i32} : arith.addi tensor<%1814x4x, bf16, %179# :ttg tensor<.slice<{dim = 2, parent = #blocked}>32x> -> i64tensor<4x4x1, x#ttgbf16, .slice<{dim = 0, parent = #blocked3}>#blocked>>

      %146 =       %tt.broadcast 183 = %145arith.muli  : %7tensor<4, x%64 :x1 ix64
bf16,       %#blocked184 = > -> tt.addptr tensor<4x4x32xbf16, #blocked>
      %arg2%147 = , tt.reshape %183%146 : :  tensor<4x4x32xbf16, #blocked> -> !tttensor<4x128.ptr<bf16>xbf16, #blocked, 1>
      %148 = i64amdgpu.scaled_upcast_fp4
       %%138185 =  tt.expand_dims scale%178  {axis%147 = 1 { : iaxis = 32}1 :  :i32}  tensor<:4x i64tensor<4x, 64#ttgx.slice<{dim = 1, parent = #blocked3}>i8, > #blocked-> 8>tensor<4,x1 xitensor<4x128xbf16, #blocked1> 64, ->#blocked 3>tensor<4x128x
      bf16, #blocked1>%186
      %149 =  = arith.extsiarith.cmpi % arg13 eq, : %139, %i32cst_31 to :  itensor<4x4x64
i      %8, 187 = #ttg.slice<{dim = 2, parent = #blocked}>tt.expand_dims >
      %150 = %175tt.expand_dims {axis  = 1% : i14932} { :axis tensor< = 4x2i64 : i32} : , tensor<4x4#ttgx.slice<{dim = 1, parent = #blocked3}>i1, > #ttg.slice<{dim = 2, parent = #blocked}>-> >tensor<4 -> x1tensor<xi4x464, x1#blockedx3>i1, #blocked
      >%188
 = arith.muli      %151 =  %tt.broadcast186, %150 % : 176tensor<4x4x1 :x ii1, #blocked> -> 64
tensor<      %4x189 = 4tt.splat x32%186xi1 :,  i#blocked>
      64 %152 = -> tt.reshape tensor<4%151x1 xi: 64, tensor<4x4x32#blockedxi1, #blocked> -> 3>tensor<4x128x
      i1%190, #blocked1> = arith.muli
      %153 % = 189,arith.select  %%152, %187cst_0, %148 : :  tensor<tensor<4x4x1x128i64x, #i1, #blockedblocked31>>
,       %tensor<4x128x191 = bf16, #blocked1>tt.addptr 
%184      , %154 = %188ttg.local_alloc :  !%153tt. ptr<bf16>,: i 64
(      %tensor<4x128192 = xtt.expand_dims bf16, %182#blocked1> {axis) = 0 ->  : i!ttg32}.memdesc<4x128xbf16, #shared, #smem> :
       tensor<%155 = 32xttg.local_loadi64 , %154#ttg .slice<{dim = 0, parent = #blocked3}>:>  -> !tensor<1ttgx32.memdesc<4x128xbf16, #shared, #smem>xi 64, ->#blocked 3>tensor<4x128x
      bf16, %193# = tt.broadcastttg %.dot_op<{opIdx = 0, parent = #blocked3}>190 >: 
tensor<4      x1%156 = xiarith.extui 64, %70 :#blocked tensor<4x3>32 ->x tensor<i4x8, 32x#ttgi64.slice<{dim = 2, parent = #blocked4}>, #> toblocked3 >
tensor<4      %x194 = 32tt.expand_dims x%179i {axis16,  = 0#ttg : i.slice<{dim = 2, parent = #blocked4}>>32
}       : %157 = tensor<32arith.shli %156xi, 64%Capturing batches (bs=16 avail_mem=88.29 GB):   0%|          | 0/6 [00:02<?, ?it/s], cst_19
#ttg : tensor<4x32xi16, .slice<{dim = 0, parent = #blocked3}>#ttg> .slice<{dim = 2, parent = #blocked4}>-> >tensor<1
      %158 = xtt.bitcast32x %157 : tensor<4x32ixi16, 64, #ttg.slice<{dim = 2, parent = #blocked4}>> -> #blockedtensor<4x323>x
bf16,       %#ttg.slice<{dim = 2, parent = #blocked4}>195 = >tt.broadcast 
      %159 = %194tt.expand_dims %158 : { tensor<axis = 1x232x : i32} : i64tensor<4x32x, #bf16, blocked3#ttg.slice<{dim = 2, parent = #blocked4}>> > -> tensor<4x-> 32x1tensor<4xx32bf16, #xiblocked4>64, 
      %160 = #blockedtt.broadcast %1593> : 
      tensor<4x32x%1961 = tt.addptrx %bf16, 191,#blocked4> ->  %tensor<180 4: x!tt32.ptr<bf16>x, 32i64x
      bf16, #blocked4>%197
      %161 =  = arith.additt.trans % %160195, { %order = 193array< :i32:  tensor<0, 4x232x, i641, #>} : blocked3tensor<>
4      %x198 = 32x32arith.extsi x%arg4bf16, #blocked4> ->  :tensor< i4x32x3232 xbf16, to #blocked9>i64
      %162 = 
      tt.reshape %161%199 :  = tt.splattensor<4x %32x32198 xbf16, : #i64blocked9 ->> ->  tensor<tensor<4x128x321xxi64bf16, #blocked, #5>blocked3
      %163 = >
amdgpu.scaled_upcast_fp4       %%200 = 77arith.cmpi  slt,scale % 185,% %162199 { :axis =  tensor<04x : 1xi32} i64: tensor<64x32x, #iblocked38, #blocked3>, >
tensor<      %128201 = x32arith.extsi x%arg5bf16 :, #blocked5> -> tensor<128x32 ixbf16, 32 #blocked5>to 
      %164 = i64arith.cmpi 
      eq, %70, %cst_20%202 : tensor< = tt.splat4x32x %i8, 201 #ttg.slice<{dim = 2, parent = #blocked4}>: >
      %165 = i64tt.expand_dims %164 -> { tensor<axis = 1x232x : i32}i64 : , #tensor<blocked34x32x>
i      %1, 203 = #ttg.slice<{dim = 2, parent = #blocked4}>>arith.cmpi  -> slt,tensor<4x32 %x192,1 %x202i1, #blocked4> :
       tensor<%166 = 1xtt.broadcast %16532x : i64tensor<4x32, #xblocked31>
x      %i1, #blocked2044> ->  = tt.broadcasttensor<4x32x32xi1, #blocked4> %
      %167 = 200 tt.trans : %tensor<4166x1 {xiorder = 1, array<#blockedi3>32:  ->0,  tensor<24x, 32x1i1>} : , #tensor<4x32x32blocked3xi1, #>
blocked4> ->       %tensor<205 = 4x32x32tt.broadcast x%203i1,  :# tensor<blocked1x932x>i1
, #      %168 = blocked3tt.reshape %167>  : -> tensor<4x32x32tensor<4xi1, x32#blocked9> -> tensor<xi128x321, x#blockedi3>1
      , %206# = arith.andiblocked %5>204,
      % %169205  = : arith.select tensor<4%x32168xi, %cst_21, %163 : 1, tensor<#blocked128x323>x
      i%2071, #blocked5>,  = tt.splattensor< %128x32196 x: bf16, #blocked5>
      !tt%170 = .ptr<bf16>ttg.local_alloc ->  tensor<%1694x 32x: !tt(tensor<128x32.ptr<bf16>x, #bf16, #blocked3blocked5>>
)      % -> 208 = !ttg.memdesc<128x32xbf16, #shared, #smem>tt.addptr 
      %171 = %207ttg.local_load,  %197%170 : :  tensor<!ttg4x.memdesc<128x32xbf16, #shared, #smem> 32x-> !tttensor<128x32.ptr<bf16>x, #bf16, blocked3#ttg>,. tensor<dot_op<{opIdx = 1, parent = #blocked3}>4x>32x
      i64%172 = , #tt.dotblocked3 >
%      tt.store155 %,208,  %%171,174,  %%206cst_3 :  tensor<:4x 32tensor<4x128xx!ttbf16, .ptr<bf16>#ttg, #.dot_op<{opIdx = 0, parent = #blocked3}>>blocked3 >
*     }tensor<
    128x32xbf16, tt.return
#ttg  }.dot_op<{opIdx = 1, parent = #blocked3}>
}>
 
{-#->
   externaltensor<_resources: {
4x32x    mlir_reproducerf32, #blocked3>: {

            pipeline: %173 = "barith.addfui lt%in172.m,od ul%e(cst_3op ti:mi zetensor<-a4x32mdxf32, #blocked3>-l
ds      %-u174 = saarith.truncfge {l%173ds -l:im ittensor<4x32xf32, #blocked3>=0  ttoar getensor<t-4x32arxchbf16, =g#blocked3>fx
      95%175 = 0}arith.extsi , %tr13it : ontensor<4-sxcfi-t32, o-#ttg.slice<{dim = 1, parent = #blocked3}>cf> to , tensor<4coxnvier64, t-#ttg.slice<{dim = 1, parent = #blocked3}>>in
      %176 = dearith.extsi x-%to11-l : lvi32 to m{iin64de
      %177 = x-tt.splat bi%tw176 : idi64 -> thtensor<=04x},i64,  a#ttg.slice<{dim = 1, parent = #blocked3}>>
      %ll178oc = atarith.addi %177, e-%am175dg : putensor<4x-si64, ha#ttg.slice<{dim = 1, parent = #blocked3}>re>d-
      %179 = mearith.extsi %32 : motensor<ry32x, i32, co#ttg.slice<{dim = 0, parent = #blocked3}>> to nvtensor<er32xt-i64, tr#ttg.slice<{dim = 0, parent = #blocked3}>>
      %180 = itarith.extsi on%-a30 : mdi32 to gpi64u-
      to%181 = -ltt.splat lv%180m{ : ari64 ->ch tensor<=g32xfxi64, 95#ttg0 .slice<{dim = 0, parent = #blocked3}>ft>z=
      tr%182 = uearith.addi },%181, %179 c : tensor<32xi64, an#ttg.slice<{dim = 0, parent = #blocked3}>>
      on%183 = icarith.muli %7, %6al : izi64e{
        %ma184x- = ittt.addptrer at%ioarg2, ns%183 : =1!tt0 .ptr<bf16>ma, x-i64nu
      %185 = m-tt.expand_dims re%wr178it {axis = es1=- : i32} : 1 tensor<4xi64, re#ttggi.slice<{dim = 1, parent = #blocked3}>on>-s -> imtensor<pl4x1xifiy=64, #noblocked3>rm
      %al186 =  tarith.extsi es%t-arg13 : coi32 to nvi64er
      %ge187 = nctt.expand_dims e=%175fa {axis = ls1e  : i32} :to p-tensor<4xi64, do#ttg.slice<{dim = 1, parent = #blocked3}>> ->wn =ttensor<ru4e}x, 1xcsi64, e,#blocked3> c
      on%188 = vearith.muli %186, %176rt : -ci64f-
      %to189-l = lvtt.splat m{%in186de : i64 x--> bitensor<tw4x1xi64, id#blocked3>th
      %190 = =0arith.muli %189},, %187 c : ontensor<4x1xvei64, rt#blocked-a3>ri
      %191 = thtt.addptr -t%o-184ll, vm%188 : {i!ttnd.ptr<bf16>ex, -bi64it
      %192 = witt.expand_dims %182dt {h=axis = 0}0,  : i32} ca: notensor<ni32xi64, ca#ttg.slice<{dim = 0, parent = #blocked3}>li> ->ze { tensor< m1xax32xi64, -i#blocked3>te
      %ra193 = titt.broadcast on%190s= : 10tensor< m4axx-n1umxi64, #blocked3> -> -rtensor<ew4x32xi64, #riblocked3>
      te%194 = s=tt.expand_dims-1  r%179eg {ioaxis = n-0 : sii32}mp : litensor<32xfyi64, =n#ttg.orslice<{dim = 0, parent = #blocked3}>ma> -> l tensor<1x32xi64, #blocked3>te
      %195 = sttt.broadcast -c%194on : vetensor<rg1x32xi64, en#ceblocked3> -> =ftensor<4x32xi64, #alblocked3>se
       t%196 = optt.addptr-d ow%191, n=%180 : tr!tt.ptr<bf16>ue, i64},
      %197 =  carith.addi %195, %193se : , tensor<4x32xi64sy, #blocked3>
      %198 = mbarith.extsi ol%-darg4 : cei32,  to i64en
      ab%199 = lett.splat -l%198 : inie-64 -> tensor<4x1xi64, #blocked3>
      in%200 = farith.cmpi o,slt, %185,  c%199on : tensor<4x1xi64, ve#blocked3>rt
      %201 = -barith.extsi ui%ltarg5 : ini32-f toun i64
      %202c- = tott.splat -l%201 lv: m{i64 -> fttensor<1xz=32xi64, #blocked3>tr
      %203 = uearith.cmpi })slt," %192,,
       disable_threading%202: false : ,
tensor<1x32xi64      verify_each, #: trueblocked
    }3>
  }

#-}      %204 = 
tt.broadcast %200 : tensor<4x1xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %205 = tt.broadcast %203 : tensor<1x32xi1, #blocked3> -> tensor<4x32xi1, #/tmp/torchinductor_root/r2/cr2saybjh4nkyntgqdq4pentfojw7pkmneebzvf62mp734xev2c6.py:18:0: blocked3>
      %206 = error: Failures have been detected while processing an MLIR pass pipelinearith.andi %204, 
%/tmp/torchinductor_root/r2/cr2saybjh4nkyntgqdq4pentfojw7pkmneebzvf62mp734xev2c6.py:18:0: 205 : note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`tensor<4x32x
i1, #blocked3>
      %207 = tt.splat %196 : !tt.ptr<bf16> -> tensor<4x32x!tt.ptr<bf16>, #blocked3>
      %208 = tt.addptr %207, %197 : tensor<4x32x!tt.ptr<bf16>, #blocked3>, tensor<4x32xi64, #blocked3>
      tt.store %208, %174, %206 : tensor<4x32x!tt.ptr<bf16>, #blocked3>
    }
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(optimize-amd-lds-usage{lds-limit=0 target-arch=gfx950}, triton-scf-to-cf, convert-index-to-llvm{index-bitwidth=0}, allocate-amdgpu-shared-memory, convert-triton-amdgpu-to-llvm{arch=gfx950 ftz=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-cf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwidth=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info, convert-builtin-func-to-llvm{ftz=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_root/mt/cmte7uqiegy3xzybpv3snopvoa4gjnqcdjusmrtprrx2tq2xn7qt.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_root/mt/cmte7uqiegy3xzybpv3snopvoa4gjnqcdjusmrtprrx2tq2xn7qt.py:18:0: note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """Kernel for computing the matmul C = A x B.
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 3, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank3]:E1103 11:31:40.965000 512 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """Kernel for computing the matmul C = A x B.
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bk': 'constexpr', 'stride_bn': 'i32', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 6, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (12,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank6]:E1103 11:31:40.965000 515 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
[2025-11-03 11:31:40 DP0 TP0] Registering 0 cuda graph addresses
[2025-11-03 11:31:41 DP0 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 381, in __init__
    self.capture()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 500, in capture
    ) = self.capture_one_batch_size(bs, forward)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 688, in capture_one_batch_size
    run_once()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 675, in run_once
    logits_output_or_pp_proxy_tensors = forward(
  File "/opt/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 817, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 979, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 963, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1646, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1506, in codegen_and_compile
    compiled_module = graph.compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2318, in compile_to_module
    return self._compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2328, in _compile_to_module
    mod = self._compile_to_module_lines(wrapper_code)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2396, in _compile_to_module_lines
    mod = PyCodeCache.load_by_key_path(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3462, in load_by_key_path
    mod = _reload_python_module(key, path, set_sys_modules=in_toplevel)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 31, in _reload_python_module
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_root/qg/cqgsnoaf7qv573naauflw76hoe7lt2j53i52yzx23qgjtfr4l44p.py", line 38, in <module>
    _batched_gemm_afp4_wfp4_pre_quant_kernel_0 = async_compile.triton('_batched_gemm_afp4_wfp4_pre_quant_kernel', '''
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 486, in triton
    kernel.precompile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 437, in precompile
    self._precompile_worker()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 459, in _precompile_worker
    compile_results.append(self._precompile_config(c))
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
    binary = triton.compile(*compile_args, **compile_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
    next_module = compile_ir(module, metadata)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
    stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
    pm.run(mod)
torch._inductor.exc.InductorError: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2786, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 319, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 235, in __init__
    self._model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 322, in __init__
    self.initialize(min_per_gpu_memory)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 479, in initialize
    self.init_device_graphs()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 1995, in init_device_graphs
    self.graph_runner = graph_runners[self.device](self)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 383, in __init__
    raise Exception(
Exception: Capture cuda graph failed: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

Possible solutions:
1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
3. disable torch compile by not using --enable-torch-compile
4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 


[2025-11-03 11:31:41 DP1 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 381, in __init__
    self.capture()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 500, in capture
    ) = self.capture_one_batch_size(bs, forward)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 688, in capture_one_batch_size
    run_once()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 675, in run_once
    logits_output_or_pp_proxy_tensors = forward(
  File "/opt/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 817, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 979, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 963, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1646, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1506, in codegen_and_compile
    compiled_module = graph.compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2318, in compile_to_module
    return self._compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2328, in _compile_to_module
    mod = self._compile_to_module_lines(wrapper_code)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2396, in _compile_to_module_lines
    mod = PyCodeCache.load_by_key_path(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3462, in load_by_key_path
    mod = _reload_python_module(key, path, set_sys_modules=in_toplevel)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 31, in _reload_python_module
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_root/u4/cu4fnpc62wbtrsq3q6m4vrhrnsfq72pxqfvfwbdlgyuapc53vmbi.py", line 38, in <module>
    _batched_gemm_afp4_wfp4_pre_quant_kernel_0 = async_compile.triton('_batched_gemm_afp4_wfp4_pre_quant_kernel', '''
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 486, in triton
    kernel.precompile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 437, in precompile
    self._precompile_worker()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 459, in _precompile_worker
    compile_results.append(self._precompile_config(c))
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
    binary = triton.compile(*compile_args, **compile_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
    next_module = compile_ir(module, metadata)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
    stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
    pm.run(mod)
torch._inductor.exc.InductorError: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2786, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 319, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 235, in __init__
    self._model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 322, in __init__
    self.initialize(min_per_gpu_memory)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 479, in initialize
    self.init_device_graphs()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 1995, in init_device_graphs
    self.graph_runner = graph_runners[self.device](self)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 383, in __init__
    raise Exception(
Exception: Capture cuda graph failed: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

Possible solutions:
1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
3. disable torch compile by not using --enable-torch-compile
4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 


[2025-11-03 11:31:41 DP5 TP5] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 381, in __init__
    self.capture()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 500, in capture
    ) = self.capture_one_batch_size(bs, forward)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 688, in capture_one_batch_size
    run_once()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 675, in run_once
    logits_output_or_pp_proxy_tensors = forward(
  File "/opt/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 817, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 979, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 963, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1646, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1506, in codegen_and_compile
    compiled_module = graph.compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2318, in compile_to_module
    return self._compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2328, in _compile_to_module
    mod = self._compile_to_module_lines(wrapper_code)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2396, in _compile_to_module_lines
    mod = PyCodeCache.load_by_key_path(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3462, in load_by_key_path
    mod = _reload_python_module(key, path, set_sys_modules=in_toplevel)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 31, in _reload_python_module
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_root/tm/ctmceenu2imu4zi3rrqe5pil7qagahbhgnw5fnxnwfpfmryd77bb.py", line 38, in <module>
    _batched_gemm_afp4_wfp4_pre_quant_kernel_0 = async_compile.triton('_batched_gemm_afp4_wfp4_pre_quant_kernel', '''
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 486, in triton
    kernel.precompile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 437, in precompile
    self._precompile_worker()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 459, in _precompile_worker
    compile_results.append(self._precompile_config(c))
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
    binary = triton.compile(*compile_args, **compile_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
    next_module = compile_ir(module, metadata)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
    stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
    pm.run(mod)
torch._inductor.exc.InductorError: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2786, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 319, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 235, in __init__
    self._model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 322, in __init__
    self.initialize(min_per_gpu_memory)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 479, in initialize
    self.init_device_graphs()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 1995, in init_device_graphs
    self.graph_runner = graph_runners[self.device](self)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 383, in __init__
    raise Exception(
Exception: Capture cuda graph failed: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

Possible solutions:
1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
3. disable torch compile by not using --enable-torch-compile
4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 


[2025-11-03 11:31:41 DP7 TP7] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 381, in __init__
    self.capture()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 500, in capture
    ) = self.capture_one_batch_size(bs, forward)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 688, in capture_one_batch_size
    run_once()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 675, in run_once
    logits_output_or_pp_proxy_tensors = forward(
  File "/opt/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 817, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 979, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 963, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1646, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1506, in codegen_and_compile
    compiled_module = graph.compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2318, in compile_to_module
    return self._compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2328, in _compile_to_module
    mod = self._compile_to_module_lines(wrapper_code)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2396, in _compile_to_module_lines
    mod = PyCodeCache.load_by_key_path(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3462, in load_by_key_path
    mod = _reload_python_module(key, path, set_sys_modules=in_toplevel)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 31, in _reload_python_module
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_root/ak/cakbedn2f7cj4eqs3al5ygu2rdzforicyweesoc6i3q2nhz66hnv.py", line 38, in <module>
    _batched_gemm_afp4_wfp4_pre_quant_kernel_0 = async_compile.triton('_batched_gemm_afp4_wfp4_pre_quant_kernel', '''
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 486, in triton
    kernel.precompile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 437, in precompile
    self._precompile_worker()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 459, in _precompile_worker
    compile_results.append(self._precompile_config(c))
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
    binary = triton.compile(*compile_args, **compile_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
    next_module = compile_ir(module, metadata)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
    stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
    pm.run(mod)
torch._inductor.exc.InductorError: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2786, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 319, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 235, in __init__
    self._model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 322, in __init__
    self.initialize(min_per_gpu_memory)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 479, in initialize
    self.init_device_graphs()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 1995, in init_device_graphs
    self.graph_runner = graph_runners[self.device](self)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 383, in __init__
    raise Exception(
Exception: Capture cuda graph failed: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

Possible solutions:
1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
3. disable torch compile by not using --enable-torch-compile
4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 


[2025-11-03 11:31:41 DP4 TP4] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 381, in __init__
    self.capture()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 500, in capture
    ) = self.capture_one_batch_size(bs, forward)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 688, in capture_one_batch_size
    run_once()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 675, in run_once
    logits_output_or_pp_proxy_tensors = forward(
  File "/opt/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 817, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 979, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 963, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1646, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1506, in codegen_and_compile
    compiled_module = graph.compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2318, in compile_to_module
    return self._compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2328, in _compile_to_module
    mod = self._compile_to_module_lines(wrapper_code)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2396, in _compile_to_module_lines
    mod = PyCodeCache.load_by_key_path(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3462, in load_by_key_path
    mod = _reload_python_module(key, path, set_sys_modules=in_toplevel)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 31, in _reload_python_module
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_root/kb/ckbholskekd5monmzoob5oljoxgfz4xlzonoap6m2ll232d6zm3s.py", line 38, in <module>
    _batched_gemm_afp4_wfp4_pre_quant_kernel_0 = async_compile.triton('_batched_gemm_afp4_wfp4_pre_quant_kernel', '''
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 486, in triton
    kernel.precompile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 437, in precompile
    self._precompile_worker()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 459, in _precompile_worker
    compile_results.append(self._precompile_config(c))
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
    binary = triton.compile(*compile_args, **compile_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
    next_module = compile_ir(module, metadata)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
    stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
    pm.run(mod)
torch._inductor.exc.InductorError: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2786, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 319, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 235, in __init__
    self._model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 322, in __init__
    self.initialize(min_per_gpu_memory)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 479, in initialize
    self.init_device_graphs()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 1995, in init_device_graphs
    self.graph_runner = graph_runners[self.device](self)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 383, in __init__
    raise Exception(
Exception: Capture cuda graph failed: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

Possible solutions:
1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
3. disable torch compile by not using --enable-torch-compile
4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 


[2025-11-03 11:31:41 DP6 TP6] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 381, in __init__
    self.capture()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 500, in capture
    ) = self.capture_one_batch_size(bs, forward)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 688, in capture_one_batch_size
    run_once()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 675, in run_once
    logits_output_or_pp_proxy_tensors = forward(
  File "/opt/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 817, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 979, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 963, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1646, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1506, in codegen_and_compile
    compiled_module = graph.compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2318, in compile_to_module
    return self._compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2328, in _compile_to_module
    mod = self._compile_to_module_lines(wrapper_code)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2396, in _compile_to_module_lines
    mod = PyCodeCache.load_by_key_path(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3462, in load_by_key_path
    mod = _reload_python_module(key, path, set_sys_modules=in_toplevel)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 31, in _reload_python_module
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_root/pj/cpjly2vl4nqwtzgee66y5vt32vu6dgbpxnwply3m77qu7msp2omg.py", line 38, in <module>
    _batched_gemm_afp4_wfp4_pre_quant_kernel_0 = async_compile.triton('_batched_gemm_afp4_wfp4_pre_quant_kernel', '''
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 486, in triton
    kernel.precompile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 437, in precompile
    self._precompile_worker()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 459, in _precompile_worker
    compile_results.append(self._precompile_config(c))
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
    binary = triton.compile(*compile_args, **compile_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
    next_module = compile_ir(module, metadata)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
    stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
    pm.run(mod)
torch._inductor.exc.InductorError: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2786, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 319, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 235, in __init__
    self._model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 322, in __init__
    self.initialize(min_per_gpu_memory)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 479, in initialize
    self.init_device_graphs()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 1995, in init_device_graphs
    self.graph_runner = graph_runners[self.device](self)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 383, in __init__
    raise Exception(
Exception: Capture cuda graph failed: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

Possible solutions:
1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
3. disable torch compile by not using --enable-torch-compile
4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 


[2025-11-03 11:31:41 DP2 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 381, in __init__
    self.capture()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 500, in capture
    ) = self.capture_one_batch_size(bs, forward)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 688, in capture_one_batch_size
    run_once()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 675, in run_once
    logits_output_or_pp_proxy_tensors = forward(
  File "/opt/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 817, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 979, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 963, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1646, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1506, in codegen_and_compile
    compiled_module = graph.compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2318, in compile_to_module
    return self._compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2328, in _compile_to_module
    mod = self._compile_to_module_lines(wrapper_code)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2396, in _compile_to_module_lines
    mod = PyCodeCache.load_by_key_path(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3462, in load_by_key_path
    mod = _reload_python_module(key, path, set_sys_modules=in_toplevel)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 31, in _reload_python_module
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_root/xv/cxvgv42f65hkiltj6b5iwoiz2l5sndhmjhvwbjdojjhantp37qny.py", line 38, in <module>
    _batched_gemm_afp4_wfp4_pre_quant_kernel_0 = async_compile.triton('_batched_gemm_afp4_wfp4_pre_quant_kernel', '''
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 486, in triton
    kernel.precompile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 437, in precompile
    self._precompile_worker()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 459, in _precompile_worker
    compile_results.append(self._precompile_config(c))
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
    binary = triton.compile(*compile_args, **compile_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
    next_module = compile_ir(module, metadata)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
    stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
    pm.run(mod)
torch._inductor.exc.InductorError: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2786, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 319, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 235, in __init__
    self._model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 322, in __init__
    self.initialize(min_per_gpu_memory)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 479, in initialize
    self.init_device_graphs()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 1995, in init_device_graphs
    self.graph_runner = graph_runners[self.device](self)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 383, in __init__
    raise Exception(
Exception: Capture cuda graph failed: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

Possible solutions:
1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
3. disable torch compile by not using --enable-torch-compile
4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 


[2025-11-03 11:31:41 DP3 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 381, in __init__
    self.capture()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 500, in capture
    ) = self.capture_one_batch_size(bs, forward)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 688, in capture_one_batch_size
    run_once()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 675, in run_once
    logits_output_or_pp_proxy_tensors = forward(
  File "/opt/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 817, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 979, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 963, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1646, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1506, in codegen_and_compile
    compiled_module = graph.compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2318, in compile_to_module
    return self._compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2328, in _compile_to_module
    mod = self._compile_to_module_lines(wrapper_code)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2396, in _compile_to_module_lines
    mod = PyCodeCache.load_by_key_path(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3462, in load_by_key_path
    mod = _reload_python_module(key, path, set_sys_modules=in_toplevel)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 31, in _reload_python_module
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_root/fp/cfplyp6avjs7k66evomdrkblvk3x7vmq4xbwl45377k5xadqi4oz.py", line 38, in <module>
    _batched_gemm_afp4_wfp4_pre_quant_kernel_0 = async_compile.triton('_batched_gemm_afp4_wfp4_pre_quant_kernel', '''
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 486, in triton
    kernel.precompile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 437, in precompile
    self._precompile_worker()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 459, in _precompile_worker
    compile_results.append(self._precompile_config(c))
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
    binary = triton.compile(*compile_args, **compile_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
    next_module = compile_ir(module, metadata)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
    stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
    pm.run(mod)
torch._inductor.exc.InductorError: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2786, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 319, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 235, in __init__
    self._model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 322, in __init__
    self.initialize(min_per_gpu_memory)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 479, in initialize
    self.init_device_graphs()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 1995, in init_device_graphs
    self.graph_runner = graph_runners[self.device](self)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 383, in __init__
    raise Exception(
Exception: Capture cuda graph failed: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

Possible solutions:
1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
3. disable torch compile by not using --enable-torch-compile
4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 


[rank0]:[W1103 11:31:42.342062975 ProcessGroupNCCL.cpp:1522] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W1103 11:31:42.458757820 ProcessGroupNCCL.cpp:1522] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W1103 11:31:42.468856514 ProcessGroupNCCL.cpp:1522] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W1103 11:31:42.468984741 ProcessGroupNCCL.cpp:1522] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W1103 11:31:42.469099309 ProcessGroupNCCL.cpp:1522] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W1103 11:31:42.469170988 ProcessGroupNCCL.cpp:1522] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W1103 11:31:42.479225912 ProcessGroupNCCL.cpp:1522] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W1103 11:31:42.489202987 ProcessGroupNCCL.cpp:1522] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
