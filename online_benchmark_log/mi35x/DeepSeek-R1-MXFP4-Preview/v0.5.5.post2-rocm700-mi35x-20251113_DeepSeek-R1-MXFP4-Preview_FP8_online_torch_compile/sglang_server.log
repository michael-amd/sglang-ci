merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-13 09:50:40 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-13 09:50:40] WARNING model_config.py:723: quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:40] WARNING server_args.py:1204: Attention backend not explicitly specified. Use aiter backend by default.
[2025-11-13 09:50:40] INFO trace.py:69: opentelemetry package is not installed, tracing disabled
[2025-11-13 09:50:41] WARNING memory_pool_host.py:36: Current platform not support pin_memory
[2025-11-13 09:50:41] server_args=ServerArgs(model_path='/mnt/raid/models/deepseek-ai/amd-DeepSeek-R1-MXFP4-Preview', tokenizer_path='/mnt/raid/models/deepseek-ai/amd-DeepSeek-R1-MXFP4-Preview', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.68, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=579775675, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/mnt/raid/models/deepseek-ai/amd-DeepSeek-R1-MXFP4-Preview', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=16, cuda_graph_bs=[1, 2, 4, 8, 12, 16], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, decrypted_config_file=None, decrypted_draft_config_file=None)
[2025-11-13 09:50:41] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:41] Using default HuggingFace chat template with detected content format: string
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-13 09:50:49 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-13 09:50:49 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-13 09:50:49 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-13 09:50:49 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-13 09:50:49 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-13 09:50:49 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-13 09:50:49 [__init__.py:241] Automatically detected platform rocm.
[2025-11-13 09:50:49] INFO trace.py:69: opentelemetry package is not installed, tracing disabled
[2025-11-13 09:50:49] WARNING memory_pool_host.py:36: Current platform not support pin_memory
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-13 09:50:49] INFO trace.py:69: opentelemetry package is not installed, tracing disabled
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-13 09:50:49 [__init__.py:241] Automatically detected platform rocm.
[2025-11-13 09:50:49] WARNING memory_pool_host.py:36: Current platform not support pin_memory
[2025-11-13 09:50:49] INFO trace.py:69: opentelemetry package is not installed, tracing disabled
[2025-11-13 09:50:49] WARNING memory_pool_host.py:36: Current platform not support pin_memory
[2025-11-13 09:50:49 TP5] Process 208 gpu_id 5 is running on CPUs: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-13 09:50:49 [__init__.py:241] Automatically detected platform rocm.
[2025-11-13 09:50:49 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:49 TP0] Process 203 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
[2025-11-13 09:50:49 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:49 TP6] Process 209 gpu_id 6 is running on CPUs: [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
[2025-11-13 09:50:49 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:49 TP5] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:49 TP5] Init torch distributed begin.
[2025-11-13 09:50:49 TP0] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:49 TP0] Init torch distributed begin.
[2025-11-13 09:50:49 TP6] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:49 TP6] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-11-13 09:50:49] INFO trace.py:69: opentelemetry package is not installed, tracing disabled
[2025-11-13 09:50:49] WARNING memory_pool_host.py:36: Current platform not support pin_memory
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-13 09:50:49] INFO trace.py:69: opentelemetry package is not installed, tracing disabled
[2025-11-13 09:50:49] WARNING memory_pool_host.py:36: Current platform not support pin_memory
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-11-13 09:50:50] INFO trace.py:69: opentelemetry package is not installed, tracing disabled
[2025-11-13 09:50:50] WARNING memory_pool_host.py:36: Current platform not support pin_memory
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-11-13 09:50:50] INFO trace.py:69: opentelemetry package is not installed, tracing disabled
[2025-11-13 09:50:50] WARNING memory_pool_host.py:36: Current platform not support pin_memory
[2025-11-13 09:50:50 TP3] Process 206 gpu_id 3 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-13 09:50:50 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:50 TP2] Process 205 gpu_id 2 is running on CPUs: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
[2025-11-13 09:50:50 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-13 09:50:50] INFO trace.py:69: opentelemetry package is not installed, tracing disabled
[2025-11-13 09:50:50] WARNING memory_pool_host.py:36: Current platform not support pin_memory
[2025-11-13 09:50:50 TP4] Process 207 gpu_id 4 is running on CPUs: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
[2025-11-13 09:50:50 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:50] INFO trace.py:69: opentelemetry package is not installed, tracing disabled
[2025-11-13 09:50:50] WARNING memory_pool_host.py:36: Current platform not support pin_memory
[2025-11-13 09:50:50 TP1] Process 204 gpu_id 1 is running on CPUs: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
[2025-11-13 09:50:50 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:50 TP7] Process 210 gpu_id 7 is running on CPUs: [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
[2025-11-13 09:50:50 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:50 TP3] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:50 TP3] Init torch distributed begin.
[2025-11-13 09:50:50 TP2] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:50 TP2] Init torch distributed begin.
[2025-11-13 09:50:50 TP4] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:50 TP4] Init torch distributed begin.
[2025-11-13 09:50:50 TP1] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:50 TP1] Init torch distributed begin.
[2025-11-13 09:50:50 TP7] quark quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-13 09:50:50 TP7] Init torch distributed begin.
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-11-13 09:50:50 TP0] sglang is using nccl==2.26.6
[2025-11-13 09:50:55 TP4] Using AiterCustomAllreduce for ROCm.
[2025-11-13 09:50:55 TP6] Using AiterCustomAllreduce for ROCm.
[2025-11-13 09:50:55 TP7] Using AiterCustomAllreduce for ROCm.
[2025-11-13 09:50:55 TP5] Using AiterCustomAllreduce for ROCm.
[2025-11-13 09:50:55 TP0] Using AiterCustomAllreduce for ROCm.
[2025-11-13 09:50:55 TP1] Using AiterCustomAllreduce for ROCm.
[2025-11-13 09:50:55 TP3] Using AiterCustomAllreduce for ROCm.
[2025-11-13 09:50:55 TP2] Using AiterCustomAllreduce for ROCm.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-11-13 09:50:56 TP0] Init torch distributed ends. mem usage=3.32 GB
[2025-11-13 09:50:56 TP7] Init torch distributed ends. mem usage=3.20 GB
[2025-11-13 09:50:56 TP6] Init torch distributed ends. mem usage=3.21 GB
[2025-11-13 09:50:56 TP5] Init torch distributed ends. mem usage=3.27 GB
[2025-11-13 09:50:56 TP4] Init torch distributed ends. mem usage=3.19 GB
[2025-11-13 09:50:56 TP3] Init torch distributed ends. mem usage=3.34 GB
[2025-11-13 09:50:56 TP2] Init torch distributed ends. mem usage=3.33 GB
[2025-11-13 09:50:56 TP1] Init torch distributed ends. mem usage=2.92 GB
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
[2025-11-13 09:50:57 TP0] Load weight begin. avail mem=284.18 GB
[2025-11-13 09:50:57 TP4] Load weight begin. avail mem=284.31 GB
[2025-11-13 09:50:57 TP7] Load weight begin. avail mem=284.30 GB
[2025-11-13 09:50:57 TP0] Shared experts fusion optimization enabled.
[2025-11-13 09:50:57 TP5] Load weight begin. avail mem=284.23 GB
[2025-11-13 09:50:57 TP2] Load weight begin. avail mem=284.17 GB
[2025-11-13 09:50:57 TP3] Load weight begin. avail mem=284.16 GB
Loading safetensors checkpoint shards:   0% Completed | 0/73 [00:00<?, ?it/s]
[2025-11-13 09:50:57 TP1] Load weight begin. avail mem=284.58 GB
[2025-11-13 09:50:57 TP6] Load weight begin. avail mem=284.29 GB
Loading safetensors checkpoint shards:   1% Completed | 1/73 [00:00<00:13,  5.45it/s]
Loading safetensors checkpoint shards:   3% Completed | 2/73 [00:00<00:18,  3.77it/s]
Loading safetensors checkpoint shards:   4% Completed | 3/73 [00:00<00:18,  3.86it/s]
Loading safetensors checkpoint shards:   5% Completed | 4/73 [00:01<00:19,  3.50it/s]
Loading safetensors checkpoint shards:   7% Completed | 5/73 [00:01<00:21,  3.15it/s]
Loading safetensors checkpoint shards:   8% Completed | 6/73 [00:01<00:18,  3.53it/s]
Loading safetensors checkpoint shards:  10% Completed | 7/73 [00:02<00:21,  3.07it/s]
Loading safetensors checkpoint shards:  11% Completed | 8/73 [00:02<00:19,  3.37it/s]
Loading safetensors checkpoint shards:  12% Completed | 9/73 [00:02<00:17,  3.72it/s]
Loading safetensors checkpoint shards:  14% Completed | 10/73 [00:02<00:13,  4.62it/s]
Loading safetensors checkpoint shards:  15% Completed | 11/73 [00:03<00:18,  3.29it/s]
Loading safetensors checkpoint shards:  16% Completed | 12/73 [00:03<00:18,  3.26it/s]
Loading safetensors checkpoint shards:  18% Completed | 13/73 [00:04<00:24,  2.43it/s]
Loading safetensors checkpoint shards:  19% Completed | 14/73 [00:04<00:21,  2.76it/s]
Loading safetensors checkpoint shards:  21% Completed | 15/73 [00:04<00:21,  2.74it/s]
Loading safetensors checkpoint shards:  22% Completed | 16/73 [00:04<00:19,  2.95it/s]
Loading safetensors checkpoint shards:  23% Completed | 17/73 [00:05<00:17,  3.13it/s]
Loading safetensors checkpoint shards:  25% Completed | 18/73 [00:05<00:18,  2.97it/s]
Loading safetensors checkpoint shards:  26% Completed | 19/73 [00:05<00:16,  3.22it/s]
Loading safetensors checkpoint shards:  27% Completed | 20/73 [00:06<00:15,  3.47it/s]
Loading safetensors checkpoint shards:  29% Completed | 21/73 [00:06<00:14,  3.58it/s]
Loading safetensors checkpoint shards:  30% Completed | 22/73 [00:06<00:13,  3.81it/s]
Loading safetensors checkpoint shards:  32% Completed | 23/73 [00:06<00:14,  3.44it/s]
Loading safetensors checkpoint shards:  33% Completed | 24/73 [00:07<00:15,  3.07it/s]
Loading safetensors checkpoint shards:  34% Completed | 25/73 [00:07<00:14,  3.29it/s]
Loading safetensors checkpoint shards:  36% Completed | 26/73 [00:07<00:13,  3.57it/s]
Loading safetensors checkpoint shards:  37% Completed | 27/73 [00:08<00:12,  3.56it/s]
Loading safetensors checkpoint shards:  38% Completed | 28/73 [00:08<00:12,  3.47it/s]
Loading safetensors checkpoint shards:  40% Completed | 29/73 [00:08<00:10,  4.27it/s]
Loading safetensors checkpoint shards:  42% Completed | 31/73 [00:09<00:12,  3.29it/s]
Loading safetensors checkpoint shards:  45% Completed | 33/73 [00:09<00:08,  4.87it/s]
Loading safetensors checkpoint shards:  48% Completed | 35/73 [00:09<00:06,  6.26it/s]
Loading safetensors checkpoint shards:  51% Completed | 37/73 [00:09<00:04,  7.94it/s]
Loading safetensors checkpoint shards:  53% Completed | 39/73 [00:09<00:03,  9.79it/s]
Loading safetensors checkpoint shards:  56% Completed | 41/73 [00:10<00:04,  7.11it/s]
Loading safetensors checkpoint shards:  59% Completed | 43/73 [00:12<00:12,  2.42it/s]
Loading safetensors checkpoint shards:  60% Completed | 44/73 [00:13<00:17,  1.63it/s]
Loading safetensors checkpoint shards:  62% Completed | 45/73 [00:15<00:21,  1.30it/s]
Loading safetensors checkpoint shards:  63% Completed | 46/73 [00:15<00:21,  1.28it/s]
Loading safetensors checkpoint shards:  64% Completed | 47/73 [00:16<00:21,  1.20it/s]
Loading safetensors checkpoint shards:  66% Completed | 48/73 [00:17<00:18,  1.34it/s]
Loading safetensors checkpoint shards:  67% Completed | 49/73 [00:17<00:15,  1.56it/s]
Loading safetensors checkpoint shards:  68% Completed | 50/73 [00:18<00:16,  1.43it/s]
Loading safetensors checkpoint shards:  70% Completed | 51/73 [00:18<00:12,  1.74it/s]
Loading safetensors checkpoint shards:  71% Completed | 52/73 [00:19<00:10,  2.02it/s]
Loading safetensors checkpoint shards:  73% Completed | 53/73 [00:19<00:09,  2.22it/s]
Loading safetensors checkpoint shards:  74% Completed | 54/73 [00:19<00:07,  2.42it/s]
Loading safetensors checkpoint shards:  75% Completed | 55/73 [00:20<00:06,  2.73it/s]
Loading safetensors checkpoint shards:  77% Completed | 56/73 [00:20<00:05,  2.93it/s]
Loading safetensors checkpoint shards:  78% Completed | 57/73 [00:20<00:05,  3.14it/s]
Loading safetensors checkpoint shards:  79% Completed | 58/73 [00:21<00:04,  3.24it/s]
Loading safetensors checkpoint shards:  81% Completed | 59/73 [00:21<00:04,  3.22it/s]
Loading safetensors checkpoint shards:  82% Completed | 60/73 [00:21<00:04,  3.23it/s]
Loading safetensors checkpoint shards:  84% Completed | 61/73 [00:21<00:03,  3.35it/s]
Loading safetensors checkpoint shards:  85% Completed | 62/73 [00:22<00:03,  3.57it/s]
Loading safetensors checkpoint shards:  86% Completed | 63/73 [00:22<00:03,  3.27it/s]
Loading safetensors checkpoint shards:  88% Completed | 64/73 [00:22<00:03,  2.98it/s]
Loading safetensors checkpoint shards:  89% Completed | 65/73 [00:23<00:02,  3.21it/s]
Loading safetensors checkpoint shards:  90% Completed | 66/73 [00:23<00:01,  3.56it/s]
Loading safetensors checkpoint shards:  92% Completed | 67/73 [00:23<00:01,  3.19it/s]
Loading safetensors checkpoint shards:  93% Completed | 68/73 [00:23<00:01,  3.69it/s]
Loading safetensors checkpoint shards:  96% Completed | 70/73 [00:24<00:00,  5.81it/s]
Loading safetensors checkpoint shards:  99% Completed | 72/73 [00:24<00:00,  6.52it/s]
Loading safetensors checkpoint shards: 100% Completed | 73/73 [00:24<00:00,  3.94it/s]
Loading safetensors checkpoint shards: 100% Completed | 73/73 [00:24<00:00,  2.93it/s]

[2025-11-13 09:51:25 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=241.43 GB, mem usage=42.87 GB.
[2025-11-13 09:51:25 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=241.43 GB, mem usage=42.87 GB.
[2025-11-13 09:51:25 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=241.36 GB, mem usage=42.87 GB.
[2025-11-13 09:51:25 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=241.71 GB, mem usage=42.87 GB.
[2025-11-13 09:51:25 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=241.29 GB, mem usage=42.87 GB.
[2025-11-13 09:51:25 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=241.31 GB, mem usage=42.87 GB.
[2025-11-13 09:51:25 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=241.29 GB, mem usage=42.87 GB.
[2025-11-13 09:51:25 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=241.42 GB, mem usage=42.87 GB.
[2025-11-13 09:51:25 TP0] Using KV cache dtype: torch.bfloat16
[2025-11-13 09:51:25 TP0] KV Cache is allocated. #tokens: 2297187, KV size: 150.34 GB
[2025-11-13 09:51:25 TP2] KV Cache is allocated. #tokens: 2297187, KV size: 150.34 GB
[2025-11-13 09:51:25 TP4] KV Cache is allocated. #tokens: 2297187, KV size: 150.34 GB
[2025-11-13 09:51:25 TP5] KV Cache is allocated. #tokens: 2297187, KV size: 150.34 GB
[2025-11-13 09:51:25 TP7] KV Cache is allocated. #tokens: 2297187, KV size: 150.34 GB
[2025-11-13 09:51:25 TP0] Memory pool end. avail mem=90.31 GB
[2025-11-13 09:51:25 TP2] Memory pool end. avail mem=90.29 GB
[2025-11-13 09:51:25 TP3] KV Cache is allocated. #tokens: 2297187, KV size: 150.34 GB
[2025-11-13 09:51:25 TP4] Memory pool end. avail mem=90.43 GB
[2025-11-13 09:51:25 TP7] Memory pool end. avail mem=90.43 GB
[2025-11-13 09:51:25 TP5] Memory pool end. avail mem=90.36 GB
[2025-11-13 09:51:25 TP3] Memory pool end. avail mem=90.29 GB
[2025-11-13 09:51:25 TP1] KV Cache is allocated. #tokens: 2297187, KV size: 150.34 GB
[2025-11-13 09:51:25 TP1] Memory pool end. avail mem=90.71 GB
[2025-11-13 09:51:25 TP6] KV Cache is allocated. #tokens: 2297187, KV size: 150.34 GB
[2025-11-13 09:51:25 TP6] Memory pool end. avail mem=90.42 GB
[2025-11-13 09:51:26 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=90.30 GB
[2025-11-13 09:51:26 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=90.18 GB
[2025-11-13 09:51:26 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=90.23 GB
[2025-11-13 09:51:26 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=90.30 GB
[2025-11-13 09:51:26 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16]
[2025-11-13 09:51:26 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=90.29 GB
[2025-11-13 09:51:26 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=90.58 GB
[2025-11-13 09:51:26 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=90.16 GB
[2025-11-13 09:51:26 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=90.16 GB
  0%|          | 0/6 [00:00<?, ?it/s]Capturing batches (bs=16 avail_mem=90.15 GB):   0%|          | 0/6 [00:00<?, ?it/s]/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/opt/venv/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1652: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
ler_TP5: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
#blocked = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_batched_gemm_afp4_wfp4_pre_quant_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32) attributes {noinline = false} {
    %cst = arith.constant dense<127> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128xbf16, #blocked1>
    %cst_1 = arith.constant dense<2097152> : tensor<4x4x1xi32, #blocked>
    %cst_2 = arith.constant dense<4> : tensor<4x4x16xi8, #blocked2>
    %c31_i32 = arith.constant 31 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x32xf32, #blocked3>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %true = arith.constant true
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #blocked>
    %cst_5 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_7 = arith.constant dense<-2147483648> : tensor<4x4x32xi32, #blocked>
    %cst_8 = arith.constant dense<23> : tensor<4x4x32xi32, #blocked>
    %cst_9 = arith.constant dense<255> : tensor<4x4x32xi32, #blocked>
    %cst_10 = arith.constant dense<8388607> : tensor<4x4x32xi32, #blocked>
    %cst_11 = arith.constant dense<1> : tensor<4x4x32xi32, #blocked>
    %cst_12 = arith.constant dense<127> : tensor<4x4x32xi32, #blocked>
    %cst_13 = arith.constant dense<4194304> : tensor<4x4x32xi32, #blocked>
    %cst_14 = arith.constant dense<126> : tensor<4x4x32xi32, #blocked>
    %cst_15 = arith.constant dense<2> : tensor<4x4x32xi32, #blocked>
    %cst_16 = arith.constant dense<21> : tensor<4x4x32xi32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %trueler_TP3: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
 : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>> -> tensor<64x1xi32, #blocked6>
      %45 = arith.extsi %44 : tensor<64x1xi32, #blocked6> to tensor<64x1xi64, #blocked6>
      %46 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>> -> tensor<1x32xi32, #blocked6>
      %47 = tt.splat %arg10 : i32 -> tensor<1x32xi32, #blocked6>
      %48 = arith.muli %46#,blocked =  #ttg%47.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}> 
#:blocked1  = tensor<#ttg1.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>x
#32xiblocked232, #blocked6>
       = %#ttg49 = .blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>arith.extsi
# blocked3%48 =  #ttg: .blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>tensor<
#1blocked4x = 32xi#ttg32, #blocked6> to tensor<.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>1
#xblocked532xi64, #blocked6>
       = %#ttg50 = .blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>tt.broadcast %45
# : blocked6tensor< = 64#ttgx.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>1xi64, #blocked6> -> 
#tensor<blocked764 = x#ttg32x.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>i64, #blocked6>
      %
#51 = blocked8tt.broadcast =  #ttg%.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>49
# blocked9:  = tensor<1x32xi64, #blocked6> #-> ttg.tensor<blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
64x32xi64, #linear#blocked = 6>
      %52 = #ttgarith.addi %.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>50, %
#51linear1 :  = tensor<#ttg64.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>x
#32xlinear2i64,  = ##ttgblocked.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>6>
      
#%shared = 53 = #ttgtt.addptr.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}> 
#%smem = arg1, #ttg%.shared_memory42
 : module!tt attributes {.ptr<i8>, "ti64
tg      .n%um54 = -carith.extsi ta%s"arg14 :  = i32 to 1i64
       : %55 = i32arith.muli , "%t7tg,.n um%54-w : ari64
      %56 = pstt.addptr " = %4 : arg3, i32%55 : , ttg.target!tt.ptr<i8>, i64
       = %"h57ip = :gtt.expand_dims fx%4195 {0"axis = , "1tt : g.i32} :th retensor<32xi32, ad#ttgs-.slice<{dim = 1, parent = #linear1}>> -> petensor<32xr-1waxi32, rp#linear1>" = 
      64 : %58 = i32tt.splat %} arg15 : i32 {
->   tt.functensor<32x1xi32, # linear1public >@
      _batched_gemm_afp4_wfp4_pre_quant_kernel%59 = (arith.muli %57, %58%arg0 : : tensor<32!xtt.1xi32, ptr<bf16> {#lineartt.divisibility = 1>
      %60 = 16 : arith.extsi i32%}, 59 : %arg1tensor<32: x!tt1x.ptr<i8>i {tt.divisibility32 = 16,  : i#linear1> 32}to , %tensor<32arg2: x!tt1.ptr<bf16>xi64, #linear1> {tt.divisibility
      %61 =  = 16tt.make_range : i {end = 32}4, % : arg3: i!tt32, .ptr<i8>start =  {tt.divisibility0 = 16 :  : ii32} : 32}tensor<, %4xi32, arg4: #ttgi32.slice<{dim = 0, parent = #linear1}> {tt.divisibility> = 16
       : i%62 = 32}tt.broadcast, % arg5: %60i32 : {tt.divisibility  = 16tensor< : i3232}x1xi, %64arg6: , i32# {tt.divisibilitylinear = 161> ->  : itensor<32}32x, %4arg7: xi64, #linear1i32> {tt.divisibility
 = 16      % : i63 = 32}tt.expand_dims %61, % {arg8: axis = i320 { : tt.divisibility = i3216 : } : i32tensor<}, 4x%arg9i32, : i#ttg32 {.tt.divisibility = slice<{dim = 0, parent = #linear1}>16> : i -> 32}tensor<, %1arg10: x4xi32, #linear1>i32
 {tt.divisibility       = 16%64 =  : itt.broadcast32} , %%63arg11:  i32: {tt.divisibility  = 16tensor< : i1x432}x, %i32, arg12: #i32linear1> {tt.divisibility ->  = 16tensor<32x4xi32,  : i#linear1>32}
      , %%65 = arg13: arith.extsi i32%64  {tt.divisibility: tensor<32 = 16x : i4xi32}32, %, #lineararg14: 1i32> to  {tt.divisibilitytensor< = 1632x4xi64, #linear : i1>32}
      %, %66 = arg15: arith.addi i32%65,)  attributes {%noinline = 62false : }tensor< {32
x4x    i%cst64, #linear1> = 
      %67 = arith.constanttt.splat  %dense<56 :127>  : tensor<!4xtt4x.1xptr<i8> -> i8tensor<32x4x, #!ttblocked>.ptr<i8>, #linear1>
      
    %%cst_068 =  = arith.constanttt.addptr dense< 0x7FC0%67, > : %tensor<466 :x128 xbf16tensor<32, #xblocked14x>
!tt.ptr<i8>, #linear    %1cst_1 = >arith.constant,  dense<tensor<2097152>32x4xi : tensor<644x, 4x#linear1>1x
i32      , #%69 = blocked>tt.load
     %cst_2% = arith.constant68 dense< 4>: : tensor< 4xtensor<4x3216xxi84, #xblocked2!tt>
.ptr<i8>, #linear    %1>c31_i32 = 
      arith.constant%70 =  tt.trans31 :  i32%
    69%cst_3 { = arith.constantorder dense< = 0.000000e+00>array< : tensor<i4x3232x: f32, 1#blocked, 3>0
    >%c32_i32} :  = arith.constanttensor<32x4x 32i : i832
, #linear1> ->     %tensor<c4_i32 = 4arith.constantx 432x : ii32
8,     %#ttg.slice<{dim = 2, parent = #blocked4}>>true = 
      %71 = arith.constanttt.splat  true%29
     : %c0_i32!tt = arith.constant.ptr<bf16> -> 0  : itensor<32
4    %xcst_4 = 128xarith.constant!tt.ptr<bf16>,  dense<#blocked-8388608>1> : tensor<
      %4x72 = 4xtt.addptr 1x%71, i32%28 : , #tensor<blocked>4
    x%cst_5128x = arith.constant!tt.ptr<bf16> dense<, 2.000000e+00># : tensor<blocked1>4x, 4xtensor<1x4f32, x#blocked128x>
i64, #blocked1>
      %73 =     %tt.load cst_6 = %arith.constant72 dense< : 0.000000e+00>tensor< : tensor<44xx4x128x1x!tt.ptr<bf16>, #blocked1>f32, 
      #blocked%74>
 =     %tt.splat cst_7 = %53 : arith.constant!tt.ptr<i8> ->  dense<tensor<-2147483648>64 : tensor<x4x32x4x!tt.ptr<i8>, #blocked6>32x
i32      , #%75 = blocked>tt.addptr 
    %74, %52 : %cst_8tensor< = arith.constant64 dense<x32x23>!tt.ptr<i8>, #blocked : tensor<6>, 4xtensor<4x6432xx32xi64, #blocked6>
      %76 = i32tt.load, # blocked>%
    75%cst_9  = arith.constantcacheModifier dense< 255>= : tensor< 4xcg4x : 32xtensor<i3264, #x32xblocked>!tt.ptr<i8>, #blocked6>
    
      %77 = %cst_10ttg.convert_layout = arith.constant  dense<%768388607>  : tensor<:4x 4xtensor<32x64i32x32x, #iblocked>8
    , #blocked%cst_116 = arith.constant>  dense<->1 > : tensor<tensor<644xx32x4xi32x8, #blocked3>i32
, #      %78 = blocked>tt.reshape
     %cst_12% = arith.constant73  dense<:127>  : tensor<tensor<4x44xx128x32xbf16, #blocked1> i32->, # blocked>tensor<
    4x4x%cst_1332x = arith.constantbf16 dense<, 4194304>#blocked : tensor<>4x
      %4x79 = 32xmath.absfi32 , #%blocked>78
     %cst_14: = arith.constant  dense<tensor<126>4 : tensor<x4x44xx32xbf1632x, #blockedi32>
      , #%blocked>80
     = %cst_15arith.extf = arith.constant  dense<%2>79 : tensor< 4x:4x 32xtensor<i324, #xblocked>4x32
    x%cst_16bf16, #blocked> = arith.constant  dense<to21>  : tensor<tensor<4x44xx32x4x32i32x, #f32, blocked>#blocked>
    
      %%cst_1781 =  = arith.constant" dense<tt.r28>educe : tensor<"4x(4x%32x80i32), # <blocked>{
    axis%cst_18 =  = arith.constant2 dense< : -1.270000e+02>i : tensor<324x}4x>1x (f32, {
#blocked      >
^bb0    %(cst_19% = arith.constantarg16 dense<: 7>f32 : tensor<, 4x%32xarg17: i16f32, )#ttg:.slice<{dim = 2, parent = #blocked4}>
>
        %    %209 = cst_20 = arith.maxnumfarith.constant  dense<%-1>arg16 : tensor<,4x 32x%i8arg17,  #ttg:.slice<{dim = 2, parent = #blocked4}> >
f32    %
        cst_21 = tt.reduce.returnarith.constant  dense<%0x7FC0>209 : tensor< 128x: 32xf32bf16, 
#blocked      5>}
    )%cst_22 :  = arith.constant( dense<tensor<7>4 : tensor<x4x4x324xx32xf32i32, #blocked>) -> , #tensor<blocked>4x4
    x%cst_23f32,  = arith.constant#ttg dense<.slice<{dim = 2, parent = #blocked}>1.270000e+02>> : tensor<
      %82 = 4xttg.convert_layout4x 1x%f32, 81 : #blockedtensor<4>
x    %4xcst_24 = f32, arith.constant#ttg dense<.slice<{dim = 2, parent = #blocked}>1.270000e+02>> ->  : tensor<tensor<4x44xx1x4f32, x#linearf32, >
#ttg    %.slice<{dim = 2, parent = #linear}>cst_25 = >arith.constant
      % dense<83 = -1.270000e+02>tt.expand_dims % : tensor<824x {4xaxis = 1x2f32,  : #lineari32>
}     %: cst_26 = tensor<4xarith.constant4 dense<xf322.000000e+00>,  : tensor<#ttg4x.4xslice<{dim = 2, parent = #linear}>1x>f32,  -> #lineartensor<>
4    %x4xcst_27 = 1arith.constantx dense<f32, -8388608>#linear> : tensor<
      %84 = 4xtt.expand_dims4x 1x%81i32 {, #axislinear> = 
    2%cst_28 :  = arith.constanti dense<32} : 2097152>tensor<4x4 : tensor<xf32, 4x#4xttg1x.i32slice<{dim = 2, parent = #blocked}>, #>linear> -> 
    tensor<%cst_294x4x1 = arith.constantxf32,  dense<#blocked>
      %85 = 127>tt.bitcast : tensor< 4x%4x831x i8:, # linear>tensor<4x
    4%cst_30x = arith.constant1 dense<x7>f32, # : tensor<linear4x> 4->xi 16, tensor<#ttg4x.slice<{dim = 2, parent = #blocked}>4x1>
x    %icst_3132 = arith.constant, # linear>dense<
-1      %86 = > : tt.bitcasttensor<4 x4%84xi 8, :#ttg .slice<{dim = 2, parent = #blocked}>tensor<4x4x1>
xf32, #blocked> ->     llvm.intr.assumetensor< %4x4x1true x: i32, #blockedi1>
    
      llvm.intr.assume %%87true  = : arith.addii1 
    %85,llvm.intr.assume  %true% :cst_28 i 1: 
    tensor<llvm.intr.assume 4x%true4 :x i11x
    illvm.intr.assume32 %, #linear>
      true %88 = : arith.addii1 
%86, %    llvm.intr.assumecst_1 % : true tensor<: 4i1x4x1
    xllvm.intr.assume i32, #blocked>
      %89 = %truett.bitcast :  i%87 : tensor<4x41x
1    llvm.intr.assumex %i32, #linear> ->true  : tensor<i14
    xllvm.intr.assume 4x1%truexi32, #linear>
      % 90:  = i1tt.bitcast
     llvm.intr.assume %88 : tensor<4x4x1%truex :i i32, #blocked>1 -> 
    tensor<4x4x1xi32, #blockedllvm.intr.assume >
      %91 = %truearith.andi :  i%891,
     llvm.intr.assume %%truecst_27 :  i:1 
    tensor<%04x4x1 = tt.get_program_idx xi32,  :#linear>
      %92 =  iarith.andi 32
%90, %    %cst_4 :1 =  tt.get_program_id tensor<4x4x1yxi :32, #blocked>
       i%32
93 =     %tt.bitcast2 =  arith.addi %91 : tensor<4x4x1%arg5xi32, #linear> -> , tensor<4x4x1%c31_i32x :f32, # ilinear32
>    %
3 =       arith.divsi %94 = %2tt.bitcast,  %c32_i32%92 : tensor<4x4x1 :x i32, #blocked>i32 -> 
    tensor<4x4x1x%4f32 = arith.extsi,  %#arg7 blocked>
      : %95 = i32math.log2 to  i%9364
     %:5 =  arith.extsi tensor<4x4x1%arg9x :f32 i, #linear>32 
to       %96i64 = 
    math.log2%6  = arith.extsi% %94arg11  : : i32tensor< to4x i4x164
x    %f327 = , arith.extsi #%0blocked :>
 i      32 %to 97i64 = 
    math.floor%8  = arith.divsi% %951,  %:3  : tensor<i324x4x1
    x%9f32, #linear> = arith.remsi
 %      %98 = 1,math.floor % 3 %96:  i32:
     llvm.intr.assume tensor<4x4x1%truexf32, #blocked :>
       i%99 = 1arith.subf
     llvm.intr.assume %97%true, :  i%1cst_26
     llvm.intr.assume :%true  :tensor<4x i4x11x
    f32, #linear>%10
 = arith.cmpi      % sgt100 = , arith.subf%arg6 , %98, %c0_i32% cst_5:  i32:
     tensor<4x4x1scf.if xf32, #blocked%10> {

            %%101 = 11 = tt.clampfarith.muli  %8%99,,  %c4_i32% :cst_25 i,32
       %%12 = cst_24,tt.make_range  {endpropagateNan = 4  : =i32 , startnone = 0  : i:32}  :tensor<4x4x1xf32, #linear>
       tensor<%4x102 = i32tt.clampf,  %100, %#ttgcst_18.slice<{dim = 1, parent = #blocked1}>, >
%      %cst_23, propagateNan = 13 = nonett.make_range  {end:  = 4tensor< : i4x4x1xf32, #blocked>32, 
      start = %0 : 103 = i32arith.fptoui}  : %101tensor<4 xi:32,  #ttgtensor<4x4.slice<{dim = 1, parent = #blocked3}>x1>
xf32, #linear>      % 14 = tott.splat  %11tensor<4x4x1 :x ii32 8, #-> linear>tensor<4
      %104 = xiarith.fptoui 32, %102 : #ttgtensor<.slice<{dim = 1, parent = #blocked1}>4x4x1>
xf32, #blocked> to tensor<      %4x4x15 = 1arith.addi x%i14,8, #blocked>
       %%105 = 12arith.addi : %103, tensor< %4xcst_29i32 : , tensor<#4xttg.4xslice<{dim = 1, parent = #blocked1}>>1
      x%16i = tt.splat8, # %lineararg4 >: 
      %106 = i32arith.addi  ->%104,  tensor<%4xcsti32 , :#ttg .slice<{dim = 1, parent = #blocked1}>tensor<4x4x1>
x      %i8, #blocked>17 = 
      %107 = arith.remsi arith.subf%15 %, cst_6, %16%102 : :  tensor<tensor<4x4x14xxf32, #blocked>i32
      , %#ttg108 = .slice<{dim = 1, parent = #blocked1}>math.exp2>
       %%10718 =  arith.muli :%7 , tensor<4x%44x1 :x if32, #blocked>64

      %      %109 = 19 = arith.extf %tt.expand_dims 78%17  {axis: tensor<4 = 1x4x : i3232}x :bf16, #blocked> to tensor< tensor<4x4x4x32i32x, f32, #blocked>#ttg
      %110 = .slice<{dim = 1, parent = #blocked1}>tt.broadcast > %-> 108tensor<4 : x1tensor<xi432, x#blocked41>x
      1%20x = tt.splatf32 %, arg8 #blocked> -> : tensor<i324x4x ->32 tensor<xf32, #blocked>4x
      %111 = 1xarith.mulfi32 , #%109blocked1,>
       %%11021 =  arith.muli :%19 , tensor<%204x4x :32 tensor<xf32, #blocked>
      %112 = 4xtt.bitcast1x i32%111 : , #tensor<4x4xblocked132>
xf32, #blocked> ->       %tensor<22 = 4x4arith.extsi x%2132 :x tensor<i4x321x, i32#blocked>
      , #%blocked1113>  = to arith.anditensor<4 %x1112xi,64,  #blocked%1>cst_7
       : %23tensor<4x4x32 = tt.make_rangex {endi32, #blocked = 128>
      %114 : i = 32, arith.shruistart =  0% : i11232}, :  tensor<%128xcst_8i32 , :#ttg .slice<{dim = 0, parent = #blocked1}>tensor<>
4x4x      %3224 = xtt.expand_dimsi %32, 23#blocked> {axis
 = 0      % : i115 = 32}arith.andi :  tensor<%114128x,i32 , %#ttgcst_9 :.slice<{dim = 0, parent = #blocked1}> > tensor<-> 4x4xtensor<132x128xxii32, #blocked>32, 
      %#blocked116 = 1>arith.andi
       %112, %%25cst_10 : = arith.extsi  %tensor<4x4x24 32xi32, #blocked>
      : %tensor<1117x128 = xiarith.addi 32, %#blocked115, %1>cst_11 to : tensor< 1xtensor<4x4x128x32i64xi32, #, blocked>
      #blocked%1118 = >
arith.subi      % 26 = %tt.broadcast cst_12%22, : % tensor<1174x 1x:i64 , #tensor<blocked14> x-> 4xtensor<432xxi32, #blocked>
      %119 = 128xarith.cmpii64 , #ultblocked1,>
       %%115,27 =  tt.broadcast %%25cst_12 : : tensor<4x4x32 tensor<x1xi32, 128x#i64blocked>
      , #%120 = blocked1arith.shrui>  %116, %-> cst_11tensor<4 : x128tensor<xi4x4x64, 32xi32, #blocked>
      %121 = #blockedarith.ori1> 
      %%28120 = arith.addi, % 26,% %cst_1327  :: tensor< 4xtensor<128x4i64x4x, #32blocked1x>
i32, #      %blocked>29 = 
      tt.addptr %%arg0122 = , arith.shrui %121%18,  :% 118 !tt: .ptr<bf16>tensor<, 4x4xi6432
      x%30i32, #blocked>
      %123 =  = arith.muliarith.select % 9,%119 %, c32_i32% :122 i, %32
116      % : 31 = tensor<4x4x32tt.make_rangex {endi = 321, #blocked> : i, 32, tensor<4start = x40 : xi3232x} i32, #blocked>: 
      tensor<32%xi12432,  = #ttgarith.maxui.slice<{dim = 0, parent = #blocked6}> >
%      %11532 = ,tt.make_range  {%end = cst_1432 :  i32:, start  = 0tensor<4x4x32 : ix32}i :32, #blocked>
       tensor<%32x125 = i32arith.subi,  #ttg%.slice<{dim = 0, parent = #blocked3}>124>
,      % 33% = tt.make_rangecst_14 { end = : tensor<4x432 : x32xi32i32, #blocked>, start
       = 0%126 =  : iarith.shli32}  :% tensor<12532x,i32 , %#ttgcst_15.slice<{dim = 1, parent = #linear1}> >
:      % 34 = tensor<tt.splat 4%30x4 :x i3232 x-> i32, #blocked>tensor<32
xi      %32, 127#ttg = .slice<{dim = 0, parent = #blocked6}>arith.shrui>
       %%35 = 123, tt.splat %%30cst_16 : tensor<4x4x32 :x ii32 32, #blocked>
      -> %128 = tensor<32arith.ori %xi126, %127 :32 , tensor<#ttg4.slice<{dim = 1, parent = #linear1}>x>
4x      %3236 = xi32, #blocked>arith.addi 
      %34%, 129 = %31arith.addi  :% tensor<128, %32xcst_11i32 : , tensor<4#ttgx4x.slice<{dim = 0, parent = #blocked6}>32>
x      %i32, #blocked>37 = 
      arith.addi %130 = %35arith.shrui,  %33% :129, tensor< 32x%i32cst_11 : , tensor<#ttg4x4x.slice<{dim = 1, parent = #linear1}>32>
xi32, #blocked>      %
      %131 = 38 = arith.minuitt.splat  %arg5% :130 i,32  -> %tensor<32cst_22xi 32, :#ttg .slice<{dim = 0, parent = #blocked6}>tensor<>
4      %x4x3239 = xi32, #blocked>tt.splat 
      %arg5% :132 i = 32 arith.shrui->  tensor<32%113, %xicst_17 : 32, tensor<4x4x#ttg32.slice<{dim = 1, parent = #linear1}>x>
i32, #blocked>
            %%40 = 133 = arith.remsi arith.ori %36%132, %131 : tensor<4x4x, 32%38xi32, #blocked :> tensor<
32x      i32%, 134 = #ttgarith.trunci.slice<{dim = 0, parent = #blocked6}> >
%133      % 41 = :arith.remsi  %tensor<4x37,4 %x3239 x: i32, #blockedtensor<32>xi 32, to#ttg .slice<{dim = 1, parent = #linear1}>tensor<>
4      %x4x42 = 32arith.muli x%7i, 8%5, #blocked>
      % :135 i = 64
tt.reshape       %43%134 = tt.make_range  {end: = 64  : itensor<32, 4x4xstart = 320 : xi32i} 8, #: blockedtensor<64>xi ->32,  #ttgtensor<.slice<{dim = 1, parent = #blocked6}>4x>
4x      %16x244 = xtt.expand_dims i8, #blocked%437> {axis
       = 1% : ioutLHS32},  :% tensor<outRHS64x = i32tt.split,  #ttg%.slice<{dim = 1, parent = #blocked6}>135>  : -> tensor<4x4x16x2xtensor<64i8, #blockedx17> ->xi 32, tensor<#blocked4x4x166>x
i8, #blocked2>      %
      45%136 =  = arith.extsiarith.shli  %44% :outRHS, tensor< %cst_264x 1x: tensor<i324, #xblocked4x166>x toi8, #blocked2>
       %tensor<64137 = x1arith.orixi 64, %#blockedoutLHS, 6>%
      136%46 : = tt.expand_dims tensor<4x4x16 %x40i {axis8, #blocked2> = 0
       : i%32}138 : =  tensor<tt.reshape %13732 x: i32tensor<, 4x4x#ttg16xi8, #blocked.slice<{dim = 0, parent = #blocked6}>2> -> > tensor<4x-> 64tensor<x1i8, #blocked8>x32
x      i32%139 = , #tt.reshape blocked6%>
105      % 47 = :tt.splat  %arg10tensor< :4x4x1 ix32 i8, #linear> ->->  tensor<1tensor<x324xxi432, x#blockedi68, >
#ttg.slice<{dim = 2, parent = #blocked}>      %>48 = 
arith.muli       %46%, 140 = %47tt.reshape  :%106 tensor< : tensor<4x1x4x132xxi32i8, #blocked> ->, # tensor<4x4blocked6xi8, #linear>
2>      %
49 =       arith.extsi %%48141 : =  tensor<ttg.convert_layout1x 32x%140i32 , #: blocked6tensor<> 4to xtensor<14xi8, x32#linearxi2>64,  #blocked->6> 
      tensor<4x4%50xi8,  = tt.broadcast#ttg %.slice<{dim = 2, parent = #blocked}>>45 
:       tensor<64%x1142xi = 64, arith.extui#blocked 6>%141 ->  tensor<:64x 32xtensor<i644x4, #xblocked6i>
8,       %#51 = ttgtt.broadcast. %slice<{dim = 2, parent = #blocked}>49 > to tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>:> tensor<
1x      32x%i64143 = , #arith.shliblocked6 > %-> 142, %tensor<cst_3064x : 32xtensor<4xi464, x#blockedi6>16, 
      #%52ttg = arith.addi. %slice<{dim = 2, parent = #blocked}>>
      50,%144 =  %tt.bitcast 51% :143 tensor< : 64xtensor<4x432xxii6416, , ##ttgblocked.slice<{dim = 2, parent = #blocked}>6>> -> 
      tensor<4x4%53x = tt.addptrbf16,  %#ttgarg1,. %slice<{dim = 2, parent = #blocked}>42 >: 
!tt      .ptr<i8>%145 = , tt.expand_dimsi64 
      %%54144 = arith.extsi { %axis = arg14 2:  : i32i32} :  totensor<4x4 ix64
bf16,       %#55 = ttg.slice<{dim = 2, parent = #blocked}>arith.muli > ->%7 , tensor<%544x :4x i1x64
bf16,       %#56 = blockedtt.addptr >%arg3
      %146 = , tt.broadcast%55 %145 : :  !tensor<tt.4xptr<i8>,4x1x ibf16, 64
#blocked> ->       %tensor<4x457 = xtt.expand_dims 32%41x {axisbf16,  = 1#blocked>
       : i%147 = 32}tt.reshape  :%146 tensor< : 32xtensor<i324, x4x32xbf16, ##ttgblocked>.slice<{dim = 1, parent = #linear1}> > -> ->tensor< tensor<432xx1x128i32x, #bf16linear1, #blocked>
1>
            %%58 = 148 = tt.splat amdgpu.scaled_upcast_fp4%arg15  :% i138 32 scale->  tensor<32%147x1 {xiaxis = 32, 1#linear : i32}1> 
      :%59  = arith.mulitensor< %4x57,64 %x58i :8,  tensor<#blocked32x8>1x,i32 , #tensor<linear14x>
128xbf16, #blocked      %1> 60 = ->arith.extsi  %59tensor<4x :128 xtensor<32bf16x1, xi#blocked32, 1#linear>1>
       to%149 =  tensor<arith.cmpi 32xeq, %1x139i64, , #%linear1cst_31>
       %: 61 = tensor<4x4xtt.make_rangei8,  {end# = 4ttg.slice<{dim = 2, parent = #blocked}> : i>32, 
      start = %150 = 0 : tt.expand_dims i32%} 149:  {tensor<4axisxi = 32, 2#ttg : i32} : tensor<4x4.slice<{dim = 0, parent = #linear1}>x>
i      %1, 62 = #ttgtt.broadcast .slice<{dim = 2, parent = #blocked}>%> -> 60 tensor<4x: 4x1tensor<32xx1ixi1, #blocked64, >#linear
      1>%151 =  ->tt.broadcast tensor< 32x%1504x :i64 tensor<4x4x1, #xlinear1i1, #>
blocked      %>63 =  tt.expand_dims -> %tensor<614x4x32x {axisi = 01, #blocked> : i
      %152 = 32}tt.reshape :  tensor<%1514x :i32 , tensor<4x#ttg4x.32slice<{dim = 0, parent = #linear1}>>x ->i1, #blocked> ->  tensor<tensor<4x1x1284xxi32i1, #blocked, #1>linear1
>
            %%64 = 153tt.broadcast  = %63arith.select  :% tensor<152, %1xcst_04x, %148 : i32tensor<4x, #128linear1x> i1-> , #tensor<32blockedx41>xi, 32, tensor<4x128#linearx1>bf16, #blocked
      1>%65
 = arith.extsi      % %154 = 64 ttg.local_alloc:  tensor<32%153x4 xi:32,  #linear(1>tensor<4x to128 tensor<x32xbf16, #blocked1>4x)i64 -> , #!ttglinear1.memdesc<4x128xbf16, #shared, #smem>>

      %155 =       %ttg.local_load66 =  arith.addi %%65154,  %62: :  tensor<!ttg32x.memdesc<4x128xbf16, #shared, #smem>4x i64->, # linear1tensor<>
4x128      %x67 = bf16, tt.splat #ttg%56. :dot_op<{opIdx = 0, parent = #blocked3}> !>tt.
ptr<i8>       -> %tensor<32156x4 = xarith.extui!tt .ptr<i8>%, #70linear1 >
:      % 68 = tensor<4xtt.addptr 32%67x, i%668,  :#ttg tensor<.slice<{dim = 2, parent = #blocked4}>32x> 4xto!tt .ptr<i8>tensor<, #4x32linear1x>,i tensor<16, 32x#ttg4x.i64slice<{dim = 2, parent = #blocked4}>, #>linear1
      %157 = >
arith.shli      % 69 = %tt.load 156,%68  :% tensor<cst_1932x : 4xtensor<!tt4x32.ptr<i8>x, #ilinear116, >
#ttg      %.slice<{dim = 2, parent = #blocked4}>70 = >tt.trans 
      %69%158 =  {ordertt.bitcast %157 : =  array<itensor<32: 4x1, 320>x} i16, : #ttg.slice<{dim = 2, parent = #blocked4}>tensor<32>x4 -> tensor<4xix328, x#linearbf161>,  -># tensor<ttg4x.32xslice<{dim = 2, parent = #blocked4}>i8>, 
#ttg      .slice<{dim = 2, parent = #blocked4}>%159 = >
tt.expand_dims       %%15871 =  {tt.splat axis = %292 : :  !i32} : tt.tensor<ptr<bf16> 4-> x32tensor<4xx128bf16, x!#tt.ttg.ptr<bf16>, slice<{dim = 2, parent = #blocked4}>#blocked> -> 1>tensor<4
      x32x1%72x = tt.addptrbf16,  %#blocked4>71,
      %160 =  %tt.broadcast %15928  : : tensor<4xtensor<432x1x128xx!bf16, tt.#blockedptr<bf16>, 4>#blocked 1>-> , tensor<4xtensor<432x32x128xxibf16, 64, #blocked4>#blocked
      %1>161
       = %73tt.trans = tt.load  %%72160 : { order = tensor<4array<x128ix!32tt.: ptr<bf16>, 0, #blocked21>, 
      1%74>} :  = tt.splattensor<4x %3253 x: 32!ttxbf16, .ptr<i8>#blocked4> ->  tensor<->64x 32xtensor<!tt4x32x32xbf16, #.ptr<i8>blocked, #9blocked6>>

      %      75 = %tt.addptr 162 = %74tt.reshape,  %%52 161:  tensor<64:x32 x!tensor<tt.4xptr<i8>, 32x#blocked326>x, bf16, #blockedtensor<649>x32 ->xi 64, tensor<#blocked128x326>xbf16, #blocked5>
      
%76       = tt.load%163 =  %amdgpu.scaled_upcast_fp475  cacheModifier% =77 scale cg  :% tensor<16264x {32axis = x!0tt. : ptr<i8>, i#blocked326>}
       :%77  = ttg.convert_layouttensor< %64x76 32: xtensor<64ix328xi, 8, #blocked3>, #blockedtensor<6>128x32 ->xbf16, #blocked5>  tensor<->64x 32xtensor<i8128, #x32blocked3x>
bf16, #blocked      %5>78 = 
tt.reshape       %73%164 =  :arith.cmpi  tensor<eq, 4x%128x70, %bf16, cst_20#blocked : 1>tensor<4x32 ->x tensor<i4x8, 4x#ttg32x.slice<{dim = 2, parent = #blocked4}>bf16, >#blocked
      %165 = >
tt.expand_dims      % 79 = %math.absf 164%78 { :axis =  tensor<24 : x4i32} : x32tensor<4x32xbf16x, #i1, blocked>#
      ttg%80.slice<{dim = 2, parent = #blocked4}> = arith.extf> % -> 79tensor< :4x tensor<324xx14xx32xibf16, 1#blocked, > #blockedto 4tensor<4>x4
x32      xf32%166 = , #tt.broadcastblocked> 
%165      % 81 = :"t t.tensor<re4duxce32x"(1%80x)i1, #blocked4> ->  <tensor<{axis4x32x32 = 2x : ii1, #blocked4>32}
      %167 = > (tt.trans{
       ^bb0%166(% {arg16: orderf32,  = %arg17array<: f32i):32
:         %0, 209 = 2arith.maxnumf , %arg161, >} : %arg17tensor<4x32x32 :x f32i
        1, #blockedtt.reduce.return 4%209> ->  :tensor< f324x
      32x32})x : (i1, #blocked9>tensor<4
      x4%x32168 = xf32tt.reshape, # %167blocked> : ) -> tensor<tensor<4x4x32x324xxi1, #blocked9> -> f32, tensor<#ttg128x32xi1, #blocked5>.slice<{dim = 2, parent = #blocked}>
      %169 = >
arith.select       %%82 = 168ttg.convert_layout , %%81 cst_21, : %tensor<4163 : x4tensor<xf32128x32, x#ttgi.slice<{dim = 2, parent = #blocked}>1> , #blocked5>, -> tensor<tensor<4128x32x4xxf32bf16, #blocked, 5>#ttg
      %170 = .slice<{dim = 2, parent = #linear}>ttg.local_alloc>
       %%83 = 169tt.expand_dims  :%82  {axis( = 2tensor< : i12832}x :32 tensor<x4xbf16, #blocked5>)4x -> f32, !ttg.memdesc<128x32xbf16, #shared, #smem>#ttg
.slice<{dim = 2, parent = #linear}>      > %-> 171 = tensor<4ttg.local_load %x4170 : x1!ttgx.memdesc<128x32xbf16, #shared, #smem>f32,  #linear->>
       tensor<%12884 = x32tt.expand_dims x%81bf16,  {axis#ttg = 2.dot_op<{opIdx = 1, parent = #blocked3}>> : i
32}       :%172 =  tensor<tt.dot4x 4x%f32, 155#ttg,.slice<{dim = 2, parent = #blocked}> > %-> 171tensor<4,x4 x1%xf32cst_3, # blocked>:
       %85tensor< = tt.bitcast4x128 %x83 bf16, : #ttgtensor<4.x4dot_op<{opIdx = 0, parent = #blocked3}>x1>xf32 * , #tensor<128x32linear>x ->bf16,  tensor<#ttg.dot_op<{opIdx = 1, parent = #blocked3}>4x>4x 1x->i32 , #tensor<linear>4x32x
      f32%86, #blocked3> = tt.bitcast
 %      84 %: 173 = tensor<4arith.addfx4 x1%xf32172, #,blocked>  ->% tensor<cst_34x 4x:1x i32tensor<4, #x32blocked>x
      f32, #blocked3>%87
 = arith.addi       %%85,174 =  %arith.truncfcst_28  :%173 tensor< 4x:4x 1xtensor<i324x32, #xf32, #blockedlinear>3
      >%88  = arith.addito % 86,tensor< %4cst_1x :32 tensor<x4xbf16, #blocked3>4x
1x      i32%175 = , #arith.extsi blocked>%13
       %89:  = tt.bitcasttensor< %487 x: i32, tensor<4#ttg.slice<{dim = 1, parent = #blocked3}>>x4 to x1tensor<xi432, x#lineari> 64-> , tensor<4#x4ttgx1.slice<{dim = 1, parent = #blocked3}>>xi
      %176 = 32, arith.extsi#linear >
%      %1190 =  : tt.bitcast i32 to %88i :64 tensor<
4x      4x%177 = 1xtt.splati32 , #%blocked>176 ->  tensor<:4x 4xi1x64 -> i32tensor<4x, #i64, blocked>#ttg.slice<{dim = 1, parent = #blocked3}>>
      %178 = 
      arith.addi %91% = arith.andi177,  %%17589, : % cst_27 tensor<4x: itensor<464, x4#x1ttgxi.32, slice<{dim = 1, parent = #blocked3}>#linear>>

            %%92 = 179arith.andi  = %90arith.extsi,  %cst_4% :32 :  tensor<tensor<4x32x4xi1x32, i32#ttg, #.blocked>slice<{dim = 0, parent = #blocked3}>
      > to %93tensor< = tt.bitcast32xi64,  %#ttg.slice<{dim = 0, parent = #blocked3}>91 >: 
      tensor<4%180 = x4arith.extsix1 xi%32, 30#linear : > i-> 32 to tensor<4ix464x1
xf32      , #%linear>181 = 
      tt.splat %180 : %94i64 ->  = tt.bitcasttensor< %32x92 i: 64tensor<4, x4#ttgx1.xislice<{dim = 0, parent = #blocked3}>32, >
      %182 = #blockedarith.addi>  -> %tensor<4181x4, %179x1 :xf32 tensor<, #32blocked>xi64, 
#ttg      %.slice<{dim = 0, parent = #blocked3}>>95 = 
      %183 = math.log2 arith.muli%93  :% tensor<7,4x 4x%1x6f32,  #linear: >i
      64%96
       = math.log2%184 =  %tt.addptr %94arg2 :, %183 tensor< : 4x!tt.ptr<bf16>4x,1x f32, i64#blocked
      %185 = >
tt.expand_dims      % 97 = %178math.floor  {%95axis =  :1 tensor< : 4xi324x}1x : f32, tensor<#linear4xi64>, 
      #ttg%98.slice<{dim = 1, parent = #blocked3}> = math.floor> ->  %tensor<964 :x tensor<1xi64, 4x#blocked3>4x
      %186 = 1xarith.extsi f32, %#blockedarg13 : >
i      %3299 =  arith.subf to %97i, 64%cst_26
 :       tensor<%4x187 = 4xtt.expand_dims1x f32, %#linear175>
 {      %axis = 100 = 1arith.subf  : %98i32, }%cst_5 : tensor<4x :i64,  tensor<#ttg.slice<{dim = 1, parent = #blocked3}>4x> 4x-> 1xtensor<4f32, x#blocked1xi64, >
#blocked3>      %
      101 = %tt.clampf 188%99 = , arith.muli%cst_25 , %186, %cst_24%176,  : propagateNan i= 64none
       :%189 =  tensor<tt.splat %186 : i64 -> 4xtensor<44xx11xxf32, i64, #linear#blocked3>>

      %      102 = %tt.clampf 190 = %100arith.muli,  %cst_18%189, , %%cst_23187,  propagateNan : = tensor<4x1xnonei64,  :#blocked3>
      %191 =  tensor<tt.addptr4x 4x%1x184,f32,  #blocked%>
188      % 103 = : arith.fptoui !tt.ptr<bf16>%101, :  tensor<i644x
      %192 = 4xtt.expand_dims %1x182f32,  {#linearaxis = > 0to  : tensor<i4x32} :4x 1xtensor<i832xi64, , ##ttglinear>.slice<{dim = 0, parent = #blocked3}>
      > -> %104tensor< = arith.fptoui1 %x102 32:x tensor<i4x64, 4x#blocked3>
      1x%f32, 193 = #blockedtt.broadcast>  to %tensor<4190x4 : x1tensor<xi48, x1xi#blocked64>
, #blocked3>      % 105 = ->arith.addi  %103tensor<, 4%cst_29x 32: xtensor<4ix464, #blocked3>x1
xi      8, %#linear194 = >
tt.expand_dims      % 106 = %179arith.addi  {%104axis = , 0%cst :  :i tensor<32} : 4xtensor<4x32xi1x64, i8#ttg.slice<{dim = 0, parent = #blocked3}>, #> -> tensor<1x32xblocked>i64, #blocked3>
      
      %107%195 =  = arith.subftt.broadcast % %194cst_6,  %: 102tensor< :1 tensor<x32x4xi64, #blocked3> ->4x 1xtensor<4f32, x#blocked32xi64, #>
blocked3>      %
      108 = %196 = math.exp2 tt.addptr %107%191,  :%180 :  tensor<!tt.ptr<bf16>4x, i64
      4x%1x197 = f32, arith.addi#blocked %195,>
       %%193109 =  arith.extf :%78  :tensor<4 tensor<x4x32x4xi64, #blocked332x>bf16, 
      #blocked%198 = > arith.extsito  tensor<4%x4arg4x32 : xf32i, #32 to i64blocked>
      
      %%110199 =  = tt.broadcasttt.splat % 108 %198 : : itensor<464 -> tensor<4x1xi64, #blocked3>
      x4%200 = x1arith.cmpixf32 , #slt,blocked>  ->% tensor<185,4x 4x%19932x : f32, tensor<#blocked4x1xi64, #>
blocked3>
            %%201 = 111 = arith.extsi arith.mulf %%109arg5,  %110: :  tensor<i4x324x to i64
      32x%f32, 202 = #blockedtt.splat >
%201 :       %i64 -> tensor<1112 = x32xitt.bitcast 64, %111# :blocked3>
       tensor<%4x203 = 4xarith.cmpi32x f32, slt, #blocked%> 192,->  tensor<4%x4202x32 xi: 32, tensor<1x32xi64#blocked, #>
blocked      %3>113 = 
      arith.andi %%112204,  = %cst_7tt.broadcast  :%200 tensor< : 4xtensor<4x4x32x1xi32i, #1, #blocked3> -> blocked>tensor<
      4%114x32xi1, #blocked3>
       = arith.shrui% %205 = 112,tt.broadcast  %%cst_8 203:  :tensor<4 x4tensor<x321xix32xi32, 1, #blocked3> -> #blockedtensor<>
4      %x32xi115 = 1, #arith.andi blocked%1143, >
      %cst_9%206 =  :arith.andi tensor< 4x%204, %205 : 4xtensor<4x32xi1, #blocked3>32x
      i32%207 = , #tt.splatblocked> 
      %196 : %116!tt.ptr<bf16> ->  = arith.anditensor<4x32x %!tt112,. %ptr<bf16>cst_10 , : #tensor<4blockedx43>
      x32%208 = xitt.addptr 32, %207, #blocked%>
197 :       %tensor<4x32x117 = !tt.ptr<bf16>arith.addi , #blocked3>%115,,  %cst_11tensor< :4 tensor<x4x32xi64, #blocked3>4x
      32xtt.storei32 , #%208, blocked>%
      174%118,  = arith.subi%206 % :cst_12,  %tensor<1174x32x :!tt.ptr<bf16>, #blocked3> tensor<
4x    4x}32x
i32    , #tt.returnblocked>

        }%119
 = arith.cmpi} ult
,
 %{-#115,
 %  cst_12external :_resources: { tensor<
4x    4xmlir_reproducer32x: {
i32      , #pipelineblocked>: 
      "%120b = arith.shruiu %iltin.module(optimi116,ze-amd %-lds-usage{cst_11 lds-limit=0 target-arch: =gfx950tensor<4}, x4tx32rixiton-sc32, f-to-cf#blocked, >
con      %ve121 = rarith.ori t-index-to-l%120lvm{i, nde%cst_13x- :b tensor<itwidth=0}, allocate-amd4xgpu-sh4xared-memory, convert-triton-32xami32d, #gpublocked>-to-llv
      m{arch=%122g = arith.shruifx950 % ft121,z=true}, cano %nical118 ize{  m: ax-iterationstensor<4=10 max-num-rewx4rx32ites=-1 region-simxiplify=normal test-co32, nve#blockedrgence=fa>
lse top      %-do123 = warith.select n=tr%119ue}, cse,, % convert-cf-to-122, l%116lv : tensor<m{index-bitwidt4xh=04x}32x,i1 convert-arith-to, #-llvm{indblocked>ex-, tensor<bi4xt4xwidth=0}, canonicali32xzei32{ , # maxblocked>-iteratio
      n%124s=10 max-num-rewrites=-1 re = arith.maxuigion-simpli %fy=norma115,l test %-cst_14 convergence=false top-do: wn=true},tensor<4 cse, symx4bx32ol-dce, enable-line-infoxi,32,  co#blockednv>
e      %rt125 = -builtin-fuarith.subi nc-%124to-llvm{ftz=true}), "%cst_14, :
 tensor<      4xdisable_threading4x: 32xfalsei32,
      , #verify_each: trueblocked>
    }
      
%126  } = arith.shli
#-} %
125, %cst_15 : tensor<4x4x32xi32, #blocked>
      %127 = arith.shrui %123, %cst_16 : tensor<4x4x32xi32, #blocked>
      %128 = arith.ori %126, %127 /tmp/torchinductor_root/5t/c5tivjrxlnefwufzfsnnt3ioqxuu7jyf3dsmzbslrtlwh4vc4cdn.py:18:0: : tensor<4error: x4Failures have been detected while processing an MLIR pass pipelinex32
xi/tmp/torchinductor_root/5t/c5tivjrxlnefwufzfsnnt3ioqxuu7jyf3dsmzbslrtlwh4vc4cdn.py:18:032, : #blockednote: >
Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
      %129 = arith.addi %128, %cst_11 : tensor<4x4x32xi32, #blocked>
      %130 = arith.shrui %129, %cst_11 : tensor<4x4x32xi32, #blocked>
      %131 = arith.minui %130, %cst_22 : tensor<4x4x32xi32, #blocked>
      %132 = arith.shrui %113, %cst_17 : tensor<4x4x32xi32, #blocked>
      %133 = arith.ori %132, %131 : tensor<4x4x32xi32, #blocked>
      %134 = arith.trunci %133 : tensor<4x4x32xi32, #blocked> to tensor<4x4x32xi8, #blocked>
      %135 = tt.reshape %134 : tensor<4x4x32xi8, #blocked> -> tensor<4x4x16x2xi8, #blocked7>
      %outLHS, %outRHS = tt.split %135 : tensor<4x4x16x2xi8, #blocked7> -> tensor<4x4x16xi8, #blocked2>
      %136 = arith.shli %outRHS, %cst_2 : tensor<4x4x16xi8, #blocked2>
      %137 = arith.ori %outLHS, %136 : tensor<4x4x16xi8, #blocked2>
      %138 = tt.reshape %137 : tensor<4x4x16xi8, #blocked2> -> tensor<4x64xi8, #blocked8>
      %139 = tt.reshape %105 : tensor<4x4x1xi8, #linear> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %140 = tt.reshape %106 : tensor<4x4x1xi8, #blocked> -> tensor<4x4xi8, #linear2>
      %141 = ttg.convert_layout %140 : tensor<4x4xi8, #linear2> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %142 = arith.extui %141 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>> to tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %143 = arith.shli %142, %cst_30 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %144 = tt.bitcast %143 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %145 = tt.expand_dims %144 {axis = 2 : i32} : tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xbf16, #blocked>
      %146 = tt.broadcast %145 : tensor<4x4x1xbf16, #blocked> -> tensor<4x4x32xbf16, #blocked>
      %147 = tt.reshape %146 : tensor<4x4x32xbf16, #blocked> -> tensor<4x128xbf16, #blocked1>
      %148 = amdgpu.scaled_upcast_fp4 %138 scale %147 {axis = 1 : i32} : tensor<4x64xi8, #blocked8>, tensor<4x128xbf16, #blocked1> -> tensor<4x128xbf16, #blocked1>
      %149 = arith.cmpi eq, %139, %cst_31 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %150 = tt.expand_dims %149 {axis = 2 : i32} : tensor<4x4xi1, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xi1, #blocked>
      %151 = tt.broadcast %150 : tensor<4x4x1xi1, #blocked> -> tensor<4x4x32xi1, #blocked>
      %152 = tt.reshape %151 : tensor<4x4x32xi1, #blocked> -> tensor<4x128xi1, #blocked1>
      %153 = arith.select %152, %cst_0, %148 : tensor<4x128xi1, #blocked1>, tensor<4x128xbf16, #blocked1>
      %154 = ttg.local_alloc %153 : (tensor<4x128xbf16, #blocked1>) -> !ttg.memdesc<4x128xbf16, #shared, #smem>
      %155 = ttg.local_load %154 : !ttg.memdesc<4x128xbf16, #shared, #smem> -> tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>>
      %156 = arith.extui %70 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>> to tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %157 = arith.shli %156, %cst_19 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %158 = tt.bitcast %157 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %159 = tt.expand_dims %158 {axis = 2 : i32} : tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xbf16, #blocked4>
      %160 = tt.broadcast %159 : tensor<4x32x1xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked4>
      %161 = tt.trans %160 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked9>
      %162 = tt.reshape %161 : tensor<4x32x32xbf16, #blocked9> -> tensor<128x32xbf16, #blocked5>
      %163 = amdgpu.scaled_upcast_fp4 %77 scale %162 {axis = 0 : i32} : tensor<64x32xi8, #blocked3>, tensor<128x32xbf16, #blocked5> -> tensor<128x32xbf16, #blocked5>
      %164 = arith.cmpi eq, %70, %cst_20 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %165 = tt.expand_dims %164 {axis = 2 : i32} : tensor<4x32xi1, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xi1, #blocked4>
      %166 = tt.broadcast %165 : tensor<4x32x1xi1, #blocked4> -> tensor<4x32x32xi1, #blocked4>
      %167 = tt.trans %166 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xi1, #blocked4> -> tensor<4x32x32xi1, #blocked9>
      %168 = tt.reshape %[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     Kernel for computing the matmul C = A x B.
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bn': 'i32', 'stride_bk': 'constexpr', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 5, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank5]:E1113 09:51:30.593000 208 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
167 : tensor<4x32x32xi1, #blocked9> -> tensor<128x32xi1, #blocked5>
      %169 = arith.select %168, %cst_21, %163 : tensor<128x32xi1, #blocked5>, tensor<128x32xbf16, #blocked5>
      %170 = ttg.local_alloc %169 : (tensor<128x32xbf16, #blocked5>) -> !ttg.memdesc<128x32xbf16, #shared, #smem>
      %171 = ttg.local_load %170 : !ttg.memdesc<128x32xbf16, #shared, #smem> -> tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>
      %172 = tt.dot %155, %171, %cst_3 : tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>> * tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> -> tensor<4x32xf32, #blocked3>
      %173 = arith.addf %172, %cst_3 : tensor<4x32xf32, #blocked3>
      %174 = arith.truncf %173 : tensor<4x32xf32, #blocked3> to tensor<4x32xbf16, #blocked3>
      %175 = arith.extsi %13 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %176 = arith.extsi %11 : i32 to i64
      %177 = tt.splat %176 : i64 -> tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %178 = arith.addi %177, %175 : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %179 = arith.extsi %32 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> to tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %180 = arith.extsi %30 : i32 to i64
      %181 = tt.splat %180 : i64 -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %182 = arith.addi %181, %179 : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %183 = arith.muli %7, %6 : i64
      %184 = tt.addptr %arg2, %183 : !tt.ptr<bf16>, i64
      %185 = tt.expand_dims %178 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %186 = arith.extsi %arg13 : i32 to i64
      %187 = tt.expand_dims %175 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %188 = arith.muli %186, %176 : i64
      %189 = tt.splat %186 : i64 -> tensor<4x1xi64, #blocked3>
      %190 = arith.muli %189, %187 : tensor<4x1xi64, #blocked3>
      %191 = tt.addptr %184, %188 : !tt.ptr<bf16>, i64
      %192 = tt.expand_dims %182 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %193 = tt.broadcast %190 : tensor<4x1xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %194 = tt.expand_dims %179 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %195 = tt.broadcast %194 : tensor<1x32xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %196 = tt.addptr %191, %180 : !tt.ptr<bf16>, i64
      %197 = arith.addi %195, %193 : tensor<4x32xi64, #blocked3>
      %198 = arith.extsi %arg4 : i32 to i64
      %199 = tt.splat %198 : i64 -> tensor<4x1xi64, #blocked3>
      %200 = arith.cmpi slt, %185, %199 : tensor<4x1xi64, #blocked3>
      %201 = arith.extsi %arg5 : i32 to i64
      %202 = tt.splat %201 : i64 -> tensor<1x32xi64, #blocked3>
      %203 = arith.cmpi slt, %192, %202 : tensor<1x32xi64, #blocked3>
      %204 = tt.broadcast %200 : tensor<4x1xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %205 = tt.broadcast %203 : tensor<1x32xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %206 = arith.andi %204, %205 : tensor<4x32xi1, #blocked3>
      %207 = tt.splat %196 : !tt.ptr<bf16> -> tensor<4x32x!tt.ptr<bf16>, #blocked3>
      %208 = tt.addptr %207, %197 : tensor<4x32x!tt.ptr<bf16>, #blocked3>, tensor<4x32xi64, #blocked3>
      tt.store %208, %174, %206 : tensor<4x32x!tt.ptr<bf16>, #blocked3>
    }
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(optimize-amd-lds-usage{lds-limit=0 target-arch=gfx950}, triton-scf-to-cf, convert-index-to-llvm{index-bitwidth=0}, allocate-amdgpu-shared-memory, convert-triton-amdgpu-to-llvm{arch=gfx950 ftz=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-cf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwidth=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info, convert-builtin-func-to-llvm{ftz=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_root/px/cpxh6tlfziesw4zmln4hzywrh5c4cggbbjkod3iqlvdmpodslwoj.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_root/px/cpxh6tlfziesw4zmln4hzywrh5c4cggbbjkod3iqlvdmpodslwoj.py:18:0: note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     Kernel for computing the matmul C = A x B.
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bn': 'i32', 'stride_bk': 'constexpr', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 3, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank3]:E1113 09:51:30.595000 206 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
ler_TP4: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
#blocked = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_batched_gemm_afp4_wfp4_pre_quant_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32) attributes {noinline = false} {
    %cst = arith.constant dense<127> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128xbf16, #blocked1>
    %cst_1 = arith.constant dense<2097152> : tensor<4x4x1xi32, #blocked>
    %cst_2 = arith.constant dense<4> : tensor<4x4x16xi8, #blocked2>
    %c31_i32 = arith.constant 31 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x32xf32, #blocked3>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %true = arith.constant true
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #blocked>
    %cst_5 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_7 = arith.constant dense<-2147483648> : tensor<4x4x32xi32, #blocked>
    %cst_8 = arith.constant dense<23> : tensor<4x4x32xi32, #blocked>
    %cst_9 = arith.constant dense<255> : tensor<4x4x32xi32, #blocked>
    %cst_10 = arith.constant dense<8388607> : tensor<4x4x32xi32, #blocked>
    %cst_11 = arith.constant dense<1> : tensor<4x4x32xi32, #blocked>
    %cst_12 = arith.constant dense<127> : tensor<4x4x32xi32, #blocked>
    %cst_13 = arith.constant dense<4194304> : tensor<4x4x32xi32, #blocked>
    %cst_14 = arith.constant dense<126> : tensor<4x4x32xi32, #blocked>
    %cst_15 = arith.constant dense<2> : tensor<4x4x32xi32, #blocked>
    %cst_16 = arith.constant dense<21> : tensor<4x4x32xi32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>> -> tensor<64x1xi32, #blocked6>
      %45 = arith.extsi %44 : tensor<64x1xi32, #blocked6> to tensor<64x1xi64, #blocked6>
      %46 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>> -> tensor<1x32xi32, #blocked6>
      %47 = tt.splat %arg10 : i32 -> tensor<1x32xi32, #blocked6>
      %48 = arith.muli %46, %47 : tensor<1x32xi32, #blocked6>
      %49 = arith.extsi %48 : tensor<1x32xi32, #blocked6> to tensor<1x32xi64, #blocked6>
      %50 = tt.broadcast %45 : tensor<64x1xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %51 = tt.broadcast %49 : tensor<1x32xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %52 = arith.addi %50, %51 : tensor<64x32xi64, #blocked6>
      %53 = tt.addptr %arg1, %42 : !tt.ptr<i8>, i64
      %54 = arith.extsi %arg14 : i32 to i64
      %55 = arith.muli %7, %54 : i64
      %56 = tt.addptr %arg3, %55 : !tt.ptr<i8>, i64
      %57 = tt.expand_dims %41 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<32x1xi32, #linear1>
      %58 = tt.splat %arg15 : i32 -> tensor<32x1xi32, #linear1>
      %59 = arith.muli %57, %58 : tensor<32x1xi32, #linear1>
      %60 = arith.extsi %59 : tensor<32x1xi32, #linear1> to tensor<32x1xi64, #linear1>
      %61 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>>
      %62 = tt.broadcast %60 : tensor<32x1xi64, #linear1> -> tensor<32x4xi64, #linear1>
      %63 = tt.expand_dims %61 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>> -> tensor<1x4xi32, #linear1>
      %64 = tt.broadcast %63 : tensor<1x4xi32, #linear1> -> tensor<32x4xi32, #linear1>
      %65 = arith.extsi %64 : tensor<32x4xi32, #linear1> to tensor<32x4xi64, #linear1>
      %66 = arith.addi %65, %62 : tensor<32x4xi64, #linear1>
      %67 = tt.splat %56 : !tt.ptr<i8> -> tensor<32x4x!tt.ptr<i8>, #linear1>
      %68 = tt.addptr %67, %66 : tensor<32x4x!tt.ptr<i8>, #linear1>, tensor<32x4xi64, #linear1>
      %69 = tt.load %68 : tensor<32x4x!tt.ptr<i8>, #linear1>
      %70 = tt.trans %69 {order = array<i32: 1, 0>} : tensor<32x4xi8, #linear1> -> tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %71 = tt.splat %29 : !tt.ptr<bf16> -> tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %72 = tt.addptr %71, %28 : tensor<4x128x!tt.ptr<bf16>, #blocked1>, tensor<4x128xi64, #blocked1>
      %73 = tt.load %72 : tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %74 = tt.splat %53 : !tt.ptr<i8> -> tensor<64x32x!tt.ptr<i8>, #blocked6>
      %75 = tt.addptr %74, %52 : tensor<64x32x!tt.ptr<i8>, #blocked6>, tensor<64x32xi64, #blocked6>
      %76 = tt.load %75 cacheModifier = cg : tensor<64x32x!tt.ptr<i8>, #blocked6>
      %77 = ttg.convert_layout %76 : tensor<64x32xi8, #blocked6> -> tensor<64x32xi8, #blocked3>
      %78 = tt.reshape %73 : tensor<4x128xbf16, #blocked1> -> tensor<4x4x32xbf16, #blocked>
      %79 = math.absf %78 : tensor<4x4x32xbf16, #blocked>
      %80 = arith.extf %79 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %81 = "tt.reduce"(%80) <{axis = 2 : i32}> ({
      ^bb0(%arg16: f32, %arg17: f32):
        %209 = arith.maxnumf %arg16, %arg17 : f32
        tt.reduce.return %209 : f32
      }) : (tensor<4x4x32xf32, #blocked>) -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>>
      %82 = ttg.convert_layout %81 : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>>
      %83 = tt.expand_dims %82 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>> -> tensor<4x4x1xf32, #linear>
      %84 = tt.expand_dims %81 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xf32, #blocked>
      %85 = tt.bitcast %83 : tensor<4x4x1xf32, #linear> -> tensor<4x4x1xi32, #linear>
      %86 = tt.bitcast %84 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %87 = arith.addi %85, %cst_28 : tensor<4x4x1xi32, #linear>
      %88 = arith.addi %86, %cst_1 : tensor<4x4x1xi32, #blocked>
      %89 = tt.bitcast %87 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xi32, #linear>
      %90 = tt.bitcast %88 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %91 = arith.andi %89, %cst_27 : tensor<4x4x1xi32, #linear>
      %92 = arith.andi %90, %cst_4 : tensor<4x4x1xi32, #blocked>
      %93 = tt.bitcast %91 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xf32, #linear>
      %94 = tt.bitcast %92 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xf32, #blocked>
      %95 = math.log2 %93 : tensor<4x4x1xf32, #linear>
      %96 = math.log2 %94 : tensor<4x4x1xf32, #blocked>
      %97 = math.floor %95 : tensor<4x4x1xf32, #linear>
      %98 = math.floor %96 : tensor<4x4x1xf32, #blocked>
      %99 = arith.subf %97, %cst_26 : tensor<4x4x1xf32, #linear>
      %100 = arith.subf %98, %cst_5 : tensor<4x4x1xf32, #blocked>
      %101 = tt.clampf %99, %cst_25, %cst_24, propagateNan = none : tensor<4x4x1xf32, #linear>
      %102 = tt.clampf %100, %cst_18, %cst_23, propagateNan = none : tensor<4x4x1xf32, #blocked>
      %103 = arith.fptoui %101 : tensor<4x4x1xf32, #linear> to tensor<4x4x1xi8, #linear>
      %104 = arith.fptoui %102 : tensor<4x4x1xf32, #blocked> to tensor<4x4x1xi8, #blocked>
      %105 = arith.addi %103, %cst_29 : tensor<4x4x1xi8, #linear>
      %106 = arith.addi %104, %cst : tensor<4x4x1xi8, #blocked>
      %107 = arith.subf %cst_6, %102 : tensor<4x4x1xf32, #blocked>
      %108 = math.exp2 %107 : tensor<4x4x1xf32, #blocked>
      %109 = arith.extf %78 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %110 = tt.broadcast %108 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x32xf32, #blocked>
      %111 = arith.mulf %109, %110 : tensor<4x4x32xf32, #blocked>
      %112 = tt.bitcast %111 : tensor<4x4x32xf32, #blocked> -> tensor<4x4x32xi32, #blocked>
      %113 = arith.andi %112, %cst_7 : tensor<4x4x32xi32, #blocked>
      %114 = arith.shrui %112, %cst_8 : tensor<4x4x32xi32, #blocked>
      %115 = arith.andi %114, %cst_9 : tensor<4x4x32xi32, #blocked>
      %116 = arith.andi %112, %cst_10 : tensor<4x4x32xi32, #blocked>
      %117 = arith.addi %115, %cst_11 : tensor<4x4x32xi32, #blocked>
      %118 = arith.subi %cst_12, %117 : tensor<4x4x32xi32, #blocked>
      %119 = arith.cmpi ult, %115, %cst_12 : tensor<4x4x32xi32, #blocked>
      %120 = arith.shrui %116, %cst_11 : tensor<4x4x32xi32, #blocked>
      %121 = arith.ori %120, %cst_13 : tensor<4x4x32xi32, #blocked>
      %122 = arith.shrui %121, %118 : tensor<4x4x32xi32, #blocked>
      %123 = arith.select %119, %122, %116 : tensor<4x4x32xi1, #blocked>, tensor<4x4x32xi32, #blocked>
      %124 = arith.maxui %115, %cst_14 : tensor<4x4x32xi32, #blocked>
      %125 = arith.subi %124, %cst_14 : tensor<4x4x32xi32, #blocked>
      %126 = arith.shli %125, %cst_15 : tensor<4x4x32xi32, #blocked>
      %127 = arith.shrui %123, %cst_16 : tensor<4x4x32xi32, #blocked>
      %128 = arith.ori %126, %127 : tensor<4x4x32xi32, #blocked>
      %129 = arith.addi %128, %cst_11 : tensor<4x4x32xi32, #blocked>
      %130 = arith.shrui %129, %cst_11 : tensor<4x4x32xi32, #blocked>
      %131 = arith.minui %130, %cst_22 : tensor<4x4x32xi32, #blocked>
      %132 = arith.shrui %113, %cst_17 : tensor<4x4x32xi32, #blocked>
      %133 = arith.ori %132, %131 : tensor<4x4x32xi32, #blocked>
      %134 = arith.trunci %133 : tensor<4x4x32xi32, #blocked> to tensor<4x4x32xi8, #blocked>
      %135 = tt.reshape %134 : tensor<4x4x32xi8, #blocked> -> tensor<4x4x16x2xi8, #blocked7>
      %outLHS, %outRHS = tt.split %135 : tensor<4x4x16x2xi8, #blocked7> -> tensor<4x4x16xi8, #blocked2>
      %136 = arith.shli %outRHS, %cst_2 : tensor<4x4x16xi8, #blocked2>
      %137 = arith.ori %outLHS, %136 : tensor<4x4x16xi8, #blocked2>
      %138 = tt.reshape %137 : tensor<4x4x16xi8, #blocked2> -> tensor<4x64xi8, #blocked8>
      %139 = tt.reshape %105 : tensor<4x4x1xi8, #linear> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %140 = tt.reshape %106 : tensor<4x4x1xi8, #blocked> -> tensor<4x4xi8, #linear2>
      %141 = ttg.convert_layout %140 : tensor<4x4xi8, #linear2> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %142 = arith.extui %141 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>> to tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %143 = arith.shli %142, %cst_30 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %144 = tt.bitcast %143 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %145 = tt.expand_dims %144 {axis = 2 : i32} : tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xbf16, #blocked>
      %146 = tt.broadcast %145 : tensor<4x4x1xbf16, #blocked> -> tensor<4x4x32xbf16, #blocked>
      %147 = tt.reshape %146 : tensor<4x4x32xbf16, #blocked> -> tensor<4x128xbf16, #blocked1>
      %148 = amdgpu.scaled_upcast_fp4 %138 scale %147 {axis = 1 : i32} : tensor<4x64xi8, #blocked8>, tensor<4x128xbf16, #blocked1> -> tensor<4x128xbf16, #blocked1>
      %149 = arith.cmpi eq, %139, %cst_31 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %150 = tt.expand_dims %149 {axis = 2 : i32} : tensor<4x4xi1, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xi1, #blocked>
      %151 = tt.broadcast %150 : tensor<4x4x1xi1, #blocked> -> tensor<4x4x32xi1, #blocked>
      %152 = tt.reshape %151 : tensor<4x4x32xi1, #blocked> -> tensor<4x128xi1, #blocked1>
      %153 = arith.select %152, %cst_0, %148 : tensor<4x128xi1, #blocked1>, tensor<4x128xbf16, #blocked1>
      %154 = ttg.local_alloc %153 : (tensor<4x128xbf16, #blocked1>) -> !ttg.memdesc<4x128xbf16, #shared, #smem>
      %155 = ttg.local_load %154 : !ttg.memdesc<4x128xbf16, #shared, #smem> -> tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>>
      %156 = arith.extui %70 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>> to tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %157 = arith.shli %156, %cst_19 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %158 = tt.bitcast %157 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %159 = tt.expand_dims %158 {axis = 2 : i32} : tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xbf16, #blocked4>
      %160 = tt.broadcast %159 : tensor<4x32x1xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked4>
      %161 = tt.trans %160 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked9>
      %162 = tt.reshape %161 : tensor<4x32x32xbf16, #blocked9> -> tensor<128x32xbf16, #blocked5>
      %163 = amdgpu.scaled_upcast_fp4 %77 scale %162 {axis = 0 : i32} : tensor<64x32xi8, #blocked3>, tensor<128x32xbf16, #blocked5> -> tensor<128x32xbf16, #blocked5>
      %164 = arith.cmpi eq, %70, %cst_20 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %165 = tt.expand_dims %164 {axis = 2 : i32} : tensor<4x32xi1, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xi1, #blocked4>
      %166 = tt.broadcast %165 : tensor<4x32x1xi1, #blocked4> -> tensor<4x32x32xi1, #blocked4>
      %167 = tt.trans %166 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xi1, #blocked4> -> tensor<4x32x32xi1, #blocked9>
      %168 = tt.reshape %167 : tensor<4x32x32xi1, #blocked9> -> tensor<128x32xi1, #blocked5>
      %169 = arith.select %168, %cst_21, %163 : tensor<128x32xi1, #blocked5>, tensor<128x32xbf16, #blocked5>
      %170 = ttg.local_alloc %169 : (tensor<128x32xbf16, #blocked5>) -> !ttg.memdesc<128x32xbf16, #shared, #smem>
      %171 = ttg.local_load %170 : !ttg.memdesc<128x32xbf16, #shared, #smem> -> tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>
      %172 = tt.dot %155, %171, %cst_3 : tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>> * tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> -> tensor<4x32xf32, #blocked3>
      %173 = arith.addf %172, %cst_3 : tensor<4x32xf32, #blocked3>
      %174 = arith.truncf %173 : tensor<4x32xf32, #blocked3> to tensor<4x32xbf16, #blocked3>
      %175 = arith.extsi %13 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %176 = arith.extsi %11 : i32 to i64
      %177 = tt.splat %176 : i64 -> tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %178 = arith.addi %177, %175 : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %179 = arith.extsi %32 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> to tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %180 = arith.extsi %30 : i32 to i64
      %181 = tt.splat %180 : i64 -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %182 = arith.addi %181, %179 : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %183 = arith.muli %7, %6 : i64
      %184 = tt.addptr %arg2, %183 : !tt.ptr<bf16>, i64
      %185 = tt.expand_dims %178 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %186 = arith.extsi %arg13 : i32 to i64
      %187 = tt.expand_dims %175 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %188 = arith.muli %186, %176 : i64
      %189 = tt.splat %186 : i64 -> tensor<4x1xi64, #blocked3>
      %190 = arith.muli %189, %187 : tensor<4x1xi64, #blocked3>
      %191 = tt.addptr %184, %188 : !tt.ptr<bf16>, i64
      %192 = tt.expand_dims %182 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %193 = tt.broadcast %190 : tensor<4x1xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %194 = tt.expand_dims %179 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %195 = tt.broadcast %194 : tensor<1x32xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %196 = tt.addptr %191, %180 : !tt.ptr<bf16>, i64
      %197 = arith.addi %195, %193 : tensor<4x32xi64, #blocked3>
      %198 = arith.extsi %arg4 : i32 to i64
      %199 = tt.splat %198 : i64 -> tensor<4x1xi64, #blocked3>
      %200 = arith.cmpi slt, %185, %199 : tensor<4x1xi64, #blocked3>
      %201 = arith.extsi %arg5 : i32 to i64
      %202 = tt.splat %201 : i64 -> tensor<1x32xi64, #blocked3>
      %203 = arith.cmpi slt, %192, %202 : tensor<1x32xi64, #blocked3>
      %204 = tt.broadcast %200 : tensor<4x1xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %205 = tt.broadcast %203 : tensor<1x32xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %206 = arith.andi %204, %205 : tensor<4x32xi1, #blocked3>
      %207 = tt.splat %196 : !tt.ptr<bf16> -> tensor<4x32x!tt.ptr<bf16>, #blocked3>
      %208 = tt.addptr %207, %197 : tensor<4x32x!tt.ptr<bf16>, #blocked3>, tensor<4x32xi64, #blocked3>
      tt.store %208, %174, %206 : tensor<4x32x!tt.ptr<bf16>, #blocked3>
    }
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(optimize-amd-lds-usage{lds-limit=0 target-arch=gfx950}, triton-scf-to-cf, convert-index-to-llvm{index-bitwidth=0}, allocate-amdgpu-shared-memory, convert-triton-amdgpu-to-llvm{arch=gfx950 ftz=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-cf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwidth=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info, convert-builtin-func-to-llvm{ftz=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_root/6y/c6yonyzacqpfur2fzxomoeu34pa7aty4hzt2576ke6zxcx7znqyi.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_root/6y/c6yonyzacqpfur2fzxomoeu34pa7aty4hzt2576ke6zxcx7znqyi.py:18:0: note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     Kernel for computing the matmul C = A x B.
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bn': 'i32', 'stride_bk': 'constexpr', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 4, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank4]:E1113 09:51:30.646000 207 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
ler_TP6: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
#blocked = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_batched_gemm_afp4_wfp4_pre_quant_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32) attributes {noinline = false} {
    %cst = arith.constant dense<127> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128xbf16, #blocked1>
    %cst_1 = arith.constant dense<2097152> : tensor<4x4x1xi32, #blocked>
    %cst_2 = arith.constant dense<4> : tensor<4x4x16xi8, #blocked2>
    %c31_i32 = arith.constant 31 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x32xf32, #blocked3>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %true = arith.constant true
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #blocked>
    %cst_5 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_7 = arith.constant dense<-2147483648> : tensor<4x4x32xi32, #blocked>
    %cst_8 = arith.constant dense<23> : tensor<4x4x32xi32, #blocked>
    %cst_9 = arith.constant dense<255> : tensor<4x4x32xi32, #blocked>
    %cst_10 = arith.constant dense<8388607> : tensor<4x4x32xi32, #blocked>
    %cst_11 = arith.constant dense<1> : tensor<4x4x32xi32, #blocked>
    %cst_12 = arith.constant dense<127> : tensor<4x4x32xi32, #blocked>
    %cst_13 = arith.constant dense<4194304> : tensor<4x4x32xi32, #blocked>
    %cst_14 = arith.constant dense<126> : tensor<4x4x32xi32, #blocked>
    %cst_15 = arith.constant dense<2> : tensor<4x4x32xi32, #blocked>
    %cst_16 = arith.constant dense<21> : tensor<4x4x32xi32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>> -> tensor<64x1xi32, #blocked6>
      %45 = arith.extsi %44 : tensor<64x1xi32, #blocked6> to tensor<64x1xi64, #blocked6>
      %46 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>> -> tensor<1x32xi32, #blocked6>
      %47 = tt.splat %arg10 : i32 -> tensor<1x32xi32, #blocked6>
      %48 = arith.muli %46, %47 : tensor<1x32xi32, #blocked6>
      %49 = arith.extsi %48 : tensor<1x32xi32, #blocked6> to tensor<1x32xi64, #blocked6>
      %50 = tt.broadcast %45 : tensor<64x1xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %51 = tt.broadcast %49 : tensor<1x32xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %52 = arith.addi %50, %51 : tensor<64x32xi64, #blocked6>
      %53 = tt.addptr %arg1, %42 : !tt.ptr<i8>, i64
      %54 = arith.extsi %arg14 : i32 to i64
      %55 = arith.muli %7, %54 : i64
      %56 = tt.addptr %arg3, %55 : !tt.ptr<i8>, i64
      %57 = tt.expand_dims %41 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<32x1xi32, #linear1>
      %58 = tt.splat %arg15 : i32 -> tensor<32x1xi32, #linear1>
      %59 = arith.muli %57, %58 : tensor<32x1xi32, #linear1>
      %60 = arith.extsi %59 : tensor<32x1xi32, #linear1> to tensor<32x1xi64, #linear1>
      %61 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>>
      %62 = tt.broadcast %60 : tensor<32x1xi64, #linear1> -> tensor<32x4xi64, #linear1>
      %63 = tt.expand_dims %61 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>> -> tensor<1x4xi32, #linear1>
      %64 = tt.broadcast %63 : tensor<1x4xi32, #linear1> -> tensor<32x4xi32, #linear1>
      %65 = arith.extsi %64 : tensor<32x4xi32, #linear1> to tensor<32x4xi64, #linear1>
      %66 = arith.addi %65, %62 : tensor<32x4xi64, #linear1>
      %67 = tt.splat %56 : !tt.ptr<i8> -> tensor<32x4x!tt.ptr<i8>, #linear1>
      %68 = tt.addptr %67, %66 : tensor<32x4x!tt.ptr<i8>, #linear1>, tensor<32x4xi64, #linear1>
      %69 = tt.load %68 : tensor<32x4x!tt.ptr<i8>, #linear1>
      %70 = tt.trans %69 {order = array<i32: 1, 0>} : tensor<32x4xi8, #linear1> -> tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %71 = tt.splat %29 : !tt.ptr<bf16> -> tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %72 = tt.addptr %71, %28 : tensor<4x128x!tt.ptr<bf16>, #blocked1>, tensor<4x128xi64, #blocked1>
      %73 = tt.load %72 : tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %74 = tt.splat %53 : !tt.ptr<i8> -> tensor<64x32x!tt.ptr<i8>, #blocked6>
      %75 = tt.addptr %74, %52 : tensor<64x32x!tt.ptr<i8>, #blocked6>, tensor<64x32xi64, #blocked6>
      %76 = tt.load %75 cacheModifier = cg : tensor<64x32x!tt.ptr<i8>, #blocked6>
      %77 = ttg.convert_layout %76 : tensor<64x32xi8, #blocked6> -> tensor<64x32xi8, #blocked3>
      %78 = tt.reshape %73 : tensor<4x128xbf16, #blocked1> -> tensor<4x4x32xbf16, #blocked>
      %79 = math.absf %78 : tensor<4x4x32xbf16, #blocked>
      %80 = arith.extf %79 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %81 = "tt.reduce"(%80) <{axis = 2 : i32}> ({
      ^bb0(%arg16: f32, %arg17: f32):
        %209 = arith.maxnumf %arg16, %arg17 : f32
        tt.reduce.return %209 : f32
      }) : (tensor<4x4x32xf32, #blocked>) -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>>
      %82 = ttg.convert_layout %81 : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>>
      %83 = tt.expand_dims %82 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>> -> tensor<4x4x1xf32, #linear>
      %84 = tt.expand_dims %81 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xf32, #blocked>
      %85 = tt.bitcast %83 : tensor<4x4x1xf32, #linear> -> tensor<4x4x1xi32, #linear>
      %86 = tt.bitcast %84 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %87 = arith.addi %85, %cst_28 : tensor<4x4x1xi32, #linear>
      %88 = arith.addi %86, %cst_1 : tensor<4x4x1xi32, #blocked>
      %89 = tt.bitcast %87 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xi32, #linear>
      %90 = tt.bitcast %88 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %91 = arith.andi %89, %cst_27 : tensor<4x4x1xi32, #linear>
      %92 = arith.andi %90, %cst_4 : tensor<4x4x1xi32, #blocked>
      %93 = tt.bitcast %91 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xf32, #linear>
      %94 = tt.bitcast %92 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xf32, #blocked>
      %95 = math.log2 %93 : tensor<4x4x1xf32, #linear>
      %96 = math.log2 %94 : tensor<4x4x1xf32, #blocked>
      %97 = math.floor %95 : tensor<4x4x1xf32, #linear>
      %98 = math.floor %96 : tensor<4x4x1xf32, #blocked>
      %99 = arith.subf %97, %cst_26 : tensor<4x4x1xf32, #linear>
      %100 = arith.subf %98, %cst_5 : tensor<4x4x1xf32, #blocked>
      %101 = tt.clampf %99, %cst_25, %cst_24, propagateNan = none : tensor<4x4x1xf32, #linear>
      %102 = tt.clampf %100, %cst_18, %cst_23, propagateNan = none : tensor<4x4x1xf32, #blocked>
      %103 = arith.fptoui %101 : tensor<4x4x1xf32, #linear> to tensor<4x4x1xi8, #linear>
      %104 = arith.fptoui %102 : tensor<4x4x1xf32, #blocked> to tensor<4x4x1xi8, #blocked>
      %105 = arith.addi %103, %cst_29 : tensor<4x4x1xi8, #linear>
      %106 = arith.addi %104, %cst : tensor<4x4x1xi8, #blocked>
      %107 = arith.subf %cst_6, %102 : tensor<4x4x1xf32, #blocked>
      %108 = math.exp2 %107 : tensor<4x4x1xf32, #blocked>
      %109 = arith.extf %78 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %110 = tt.broadcast %108 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x32xf32, #blocked>
      %111 = arith.mulf %109, %110 : tensor<4x4x32xf32, #blocked>
      %112 = tt.bitcast %111 : tensor<4x4x32xf32, #blocked> -> tensor<4x4x32xi32, #blocked>
      %113 = arith.andi %112, %cst_7 : tensor<4x4x32xi32, #blocked>
      %114 = arith.shrui %112, %cst_8 : tensor<4x4x32xi32, #blocked>
      %115 = arith.andi %114, %cst_9 : tensor<4x4x32xi32, #blocked>
      %116 = arith.andi %112, %cst_10 : tensor<4x4x32xi32, #blocked>
      %117 = arith.addi %115, %cst_11 : tensor<4x4x32xi32, #blocked>
      %118 = arith.subi %cst_12, %117 : tensor<4x4x32xi32, #blocked>
      %119 = arith.cmpi ult, %115, %cst_12 : tensor<4x4x32xi32, #blocked>
      %120 = arith.shrui %116, %cst_11 : tensor<4x4x32xi32, #blocked>
      %121 = arith.ori %120, %cst_13 : tensor<4x4x32xi32, #blocked>
      %122 = arith.shrui %121, %118 : tensor<4x4x32xi32, #blocked>
      %123 = arith.select %119, %122, %116 : tensor<4x4x32xi1, #blocked>, tensor<4x4x32xi32, #blocked>
      %124 = arith.maxui %115, %cst_14 : tensor<4x4x32xi32, #blocked>
      %125 = arith.subi %124, %cst_14 : tensor<4x4x32xi32, #blocked>
      %126 = arith.shli %125, %cst_15 : tensor<4x4x32xi32, #blocked>
      %127 = arith.shrui %123, %cst_16 : tensor<4x4x32xi32, #blocked>
      %128 = arith.ori %126, %127 : tensor<4x4x32xi32, #blocked>
      %129 = arith.addi %128, %cst_11 : tensor<4x4x32xi32, #blocked>
      %130 = arith.shrui %129, %cst_11 : tensor<4x4x32xi32, #blocked>
      %131 = arith.minui %130, %cst_22 : tensor<4x4x32xi32, #blocked>
      %132 = arith.shrui %113, %cst_17 : tensor<4x4x32xi32, #blocked>
      %133 = arith.ori %132, %131 : tensor<4x4x32xi32, #blocked>
      %134 = arith.trunci %133 : tensor<4x4x32xi32, #blocked> to tensor<4x4x32xi8, #blocked>
      %135 = tt.reshape %134 : tensor<4x4x32xi8, #blocked> -> tensor<4x4x16x2xi8, #blocked7>
      %outLHS, %outRHS = tt.split %135 : tensor<4x4x16x2xi8, #blocked7> -> tensor<4x4x16xi8, #blocked2>
      %136 = arith.shli %outRHS, %cst_2 : tensor<4x4x16xi8, #blocked2>
      %137 = arith.ori %outLHS, %136 : tensor<4x4x16xi8, #blocked2>
      %138 = tt.reshape %137 : tensor<4x4x16xi8, #blocked2> -> tensor<4x64xi8, #blocked8>
      %139 = tt.reshape %105 : tensor<4x4x1xi8, #linear> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %140 = tt.reshape %106 : tensor<4x4x1xi8, #blocked> -> tensor<4x4xi8, #linear2>
      %141 = ttg.convert_layout %140 : tensor<4x4xi8, #linear2> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %142 = arith.extui %141 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>> to tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %143 = arith.shli %142, %cst_30 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %144 = tt.bitcast %143 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %145 = tt.expand_dims %144 {axis = 2 : i32} : tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xbf16, #blocked>
      %146 = tt.broadcast %145 : tensor<4x4x1xbf16, #blocked> -> tensor<4x4x32xbf16, #blocked>
      %147 = tt.reshape %146 : tensor<4x4x32xbf16, #blocked> -> tensor<4x128xbf16, #blocked1>
      %148 = amdgpu.scaled_upcast_fp4 %138 scale %147 {axis = 1 : i32} : tensor<4x64xi8, #blocked8>, tensor<4x128xbf16, #blocked1> -> tensor<4x128xbf16, #blocked1>
      %149 = arith.cmpi eq, %139, %cst_31 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %150 = tt.expand_dims %149 {axis = 2 : i32} : tensor<4x4xi1, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xi1, #blocked>
      %151 = tt.broadcast %150 : tensor<4x4x1xi1, #blocked> -> tensor<4x4x32xi1, #blocked>
      %152 = tt.reshape %151 : tensor<4x4x32xi1, #blocked> -> tensor<4x128xi1, #blocked1>
      %153 = arith.select %152, %cst_0, %148 : tensor<4x128xi1, #blocked1>, tensor<4x128xbf16, #blocked1>
      %154 = ttg.local_alloc %153 : (tensor<4x128xbf16, #blocked1>) -> !ttg.memdesc<4x128xbf16, #shared, #smem>
      %155 = ttg.local_load %154 : !ttg.memdesc<4x128xbf16, #shared, #smem> -> tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>>
      %156 = arith.extui %70 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>> to tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %157 = arith.shli %156, %cst_19 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %158 = tt.bitcast %157 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %159 = tt.expand_dims %158 {axis = 2 : i32} : tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xbf16, #blocked4>
      %160 = tt.broadcast %159 : tensor<4x32x1xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked4>
      %161 = tt.trans %160 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked9>
      %162 = tt.reshape %161 : tensor<4x32x32xbf16, #blocked9> -> tensor<128x32xbf16, #blocked5>
      %163 = amdgpu.scaled_upcast_fp4 %77 scale %162 {axis = 0 : i32} : tensor<64x32xi8, #blocked3>, tensor<128x32xbf16, #blocked5> -> tensor<128x32xbf16, #blocked5>
      %164 = arith.cmpi eq, %70, %cst_20 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %165 = tt.expand_dims %164 {axis = 2 : i32} : tensor<4x32xi1, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xi1, #blocked4>
      %166 = tt.broadcast %165 : tensor<4x32x1xi1, #blocked4> -> tensor<4x32x32xi1, #blocked4>
      %167 = tt.trans %166 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xi1, #blocked4> -> tensor<4x32x32xi1, #blocked9>
      %168 = tt.reshape %167 : tensor<4x32x32xi1, #blocked9> -> tensor<128x32xi1, #blocked5>
      %169 = arith.select %168, %cst_21, %163 : tensor<128x32xi1, #blocked5>, tensor<128x32xbf16, #blocked5>
      %170 = ttg.local_alloc %169 : (tensor<128x32xbf16, #blocked5>) -> !ttg.memdesc<128x32xbf16, #shared, #smem>
      %171 = ttg.local_load %170 : !ttg.memdesc<128x32xbf16, #shared, #smem> -> tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>
      %172 = tt.dot %155, %171, %cst_3 : tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>> * tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> -> tensor<4x32xf32, #blocked3>
      %173 = arith.addf %172, %cst_3 : tensor<4x32xf32, #blocked3>
      %174 = arith.truncf %173 : tensor<4x32xf32, #blocked3> to tensor<4x32xbf16, #blocked3>
      %175 = arith.extsi %13 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %176 = arith.extsi %11 : i32 to i64
      %177 = tt.splat %176 : i64 -> tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %178 = arith.addi %177, %175 : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %179 = arith.extsi %32 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> to tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %180 = arith.extsi %30 : i32 to i64
      %181 = tt.splat %180 : i64 -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %182 = arith.addi %181, %179 : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %183 = arith.muli %7, %6 : i64
      %184 = tt.addptr %arg2, %183 : !tt.ptr<bf16>, i64
      %185 = tt.expand_dims %178 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %186 = arith.extsi %arg13 : i32 to i64
      %187 = tt.expand_dims %175 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %188 = arith.muli %186, %176 : i64
      %189 = tt.splat %186 : i64 -> tensor<4x1xi64, #blocked3>
      %190 = arith.muli %189, %187 : tensor<4x1xi64, #blocked3>
      %191 = tt.addptr %184, %188 : !tt.ptr<bf16>, i64
      %192 = tt.expand_dims %182 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %193 = tt.broadcast %190 : tensor<4x1xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %194 = tt.expand_dims %179 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %195 = tt.broadcast %194 : tensor<1x32xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %196 = tt.addptr %191, %180 : !tt.ptr<bf16>, i64
      %197 = arith.addi %195, %193 : tensor<4x32xi64, #blocked3>
      %198 = arith.extsi %arg4 : i32 to i64
      %199 = tt.splat %198 : i64 -> tensor<4x1xi64, #blocked3>
      %200 = arith.cmpi slt, %185, %199 : tensor<4x1xi64, #blocked3>
      %201 = arith.extsi %arg5 : i32 to i64
      %202 = tt.splat %201 : i64 -> tensor<1x32xi64, #blocked3>
      %203 = arith.cmpi slt, %192, %202 : tensor<1x32xi64, #blocked3>
      %204 = tt.broadcast %200 : tensor<4x1xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %205 = tt.broadcast %203 : tensor<1x32xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %206 = arith.andi %204, %205 : tensor<4x32xi1, #blocked3>
      %207 = tt.splat %196 : !tt.ptr<bf16> -> tensor<4x32x!tt.ptr<bf16>, #blocked3>
      %208 = tt.addptr %207, %197 : tensor<4x32x!tt.ptr<bf16>, #blocked3>, tensor<4x32xi64, #blocked3>
      tt.store %208, %174, %206 : tensor<4x32x!tt.ptr<bf16>, #blocked3>
    }
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(optimize-amd-lds-usage{lds-limit=0 target-arch=gfx950}, triton-scf-to-cf, convert-index-to-llvm{index-bitwidth=0}, allocate-amdgpu-shared-memory, convert-triton-amdgpu-to-llvm{arch=gfx950 ftz=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-cf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwidth=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info, convert-builtin-func-to-llvm{ftz=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_root/m2/cm2ckktdb3uenk5b5wo2lwcnzljexisdl665k2pxm63tt2sjbteb.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_root/m2/cm2ckktdb3uenk5b5wo2lwcnzljexisdl665k2pxm63tt2sjbteb.py:18:0: note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     Kernel for computing the matmul C = A x B.
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bn': 'i32', 'stride_bk': 'constexpr', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 6, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank6]:E1113 09:51:30.666000 209 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
ler_TP2: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
#blocked = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_batched_gemm_afp4_wfp4_pre_quant_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32) attributes {noinline = false} {
    %cst = arith.constant dense<127> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128xbf16, #blocked1>
    %cst_1 = arith.constant dense<2097152> : tensor<4x4x1xi32, #blocked>
    %cst_2 = arith.constant dense<4> : tensor<4x4x16xi8, #blocked2>
    %c31_i32 = arith.constant 31 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x32xf32, #blocked3>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %true = arith.constant true
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #blocked>
    %cst_5 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_7 = arith.constant dense<-2147483648> : tensor<4x4x32xi32, #blocked>
    %cst_8 = arith.constant dense<23> : tensor<4x4x32xi32, #blocked>
    %cst_9 = arith.constant dense<255> : tensor<4x4x32xi32, #blocked>
    %cst_10 = arith.constant dense<8388607> : tensor<4x4x32xi32, #blocked>
    %cst_11 = arith.constant dense<1> : tensor<4x4x32xi32, #blocked>
    %cst_12 = arith.constant dense<127> : tensor<4x4x32xi32, #blocked>
    %cst_13 = arith.constant dense<4194304> : tensor<4x4x32xi32, #blocked>
    %cst_14 = arith.constant dense<126> : tensor<4x4x32xi32, #blocked>
    %cst_15 = arith.constant dense<2> : tensor<4x4x32xi32, #blocked>
    %cst_16 = arith.constant dense<21> : tensor<4x4x32xi32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>> -> tensor<64x1xi32, #blocked6>
      %45 = arith.extsi %44 : tensor<64x1xi32, #blocked6> to tensor<64x1xi64, #blocked6>
      %46 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>> -> tensor<1x32xi32, #blocked6>
      %47 = tt.splat %arg10 : i32 -> tensor<1x32xi32, #blocked6>
      %48 = arith.muli %46, %47 : tensor<1x32xi32, #blocked6>
      %49 = arith.extsi %48 : tensor<1x32xi32, #blocked6> to tensor<1x32xi64, #blocked6>
      %50 = tt.broadcast %45 : tensor<64x1xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %51 = tt.broadcast %49 : tensor<1x32xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %52 = arith.addi %50, %51 : tensor<64x32xi64, #blocked6>
      %53 = tt.addptr %arg1, %42 : !tt.ptr<i8>, i64
      %54 = arith.extsi %arg14 : i32 to i64
      %55 = arith.muli %7, %54 : i64
      %56 = tt.addptr %arg3, %55 : !tt.ptr<i8>, i64
      %57 = tt.expand_dims %41 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<32x1xi32, #linear1>
      %58 = tt.splat %arg15 : i32 -> tensor<32x1xi32, #linear1>
      %59 = arith.muli %57, %58 : tensor<32x1xi32, #linear1>
      %60 = arith.extsi %59 : tensor<32x1xi32, #linear1> to tensor<32x1xi64, #linear1>
      %61 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>>
      %62 = tt.broadcast %60 : tensor<32x1xi64, #linear1> -> tensor<32x4xi64, #linear1>
      %63 = tt.expand_dims %61 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>> -> tensor<1x4xi32, #linear1>
      %64 = tt.broadcast %63 : tensor<1x4xi32, #linear1> -> tensor<32x4xi32, #linear1>
      %65 = arith.extsi %64 : tensor<32x4xi32, #linear1> to tensor<32x4xi64, #linear1>
      %66 = arith.addi %65, %62 : tensor<32x4xi64, #linear1>
      %67 = tt.splat %56 : !tt.ptr<i8> -> tensor<32x4x!tt.ptr<i8>, #linear1>
      %68 = tt.addptr %67, %66 : tensor<32x4x!tt.ptr<i8>, #linear1>, tensor<32x4xi64, #linear1>
      %69 = tt.load %68 : tensor<32x4x!tt.ptr<i8>, #linear1>
      %70 = tt.trans %69 {order = array<i32: 1, 0>} : tensor<32x4xi8, #linear1> -> tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %71 = tt.splat %29 : !tt.ptr<bf16> -> tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %72 = tt.addptr %71, %28 : tensor<4x128x!tt.ptr<bf16>, #blocked1>, tensor<4x128xi64, #blocked1>
      %73 = tt.load %72 : tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %74 = tt.splat %53 : !tt.ptr<i8> -> tensor<64x32x!tt.ptr<i8>, #blocked6>
      %75 = tt.addptr %74, %52 : tensor<64x32x!tt.ptr<i8>, #blocked6>, tensor<64x32xi64, #blocked6>
      %76 = tt.load %75 cacheModifier = cg : tensor<64x32x!tt.ptr<i8>, #blocked6>
      %77 = ttg.convert_layout %76 : tensor<64x32xi8, #blocked6> -> tensor<64x32xi8, #blocked3>
      %78 = tt.reshape %73 : tensor<4x128xbf16, #blocked1> -> tensor<4x4x32xbf16, #blocked>
      %79 = math.absf %78 : tensor<4x4x32xbf16, #blocked>
      %80 = arith.extf %79 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %81 = "tt.reduce"(%80) <{axis = 2 : i32}> ({
      ^bb0(%arg16: f32, %arg17: f32):
        %209 = arith.maxnumf %arg16, %arg17 : f32
        tt.reduce.return %209 : f32
      }) : (tensor<4x4x32xf32, #blocked>) -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>>
      %82 = ttg.convert_layout %81 : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>>
      %83 = tt.expand_dims %82 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>> -> tensor<4x4x1xf32, #linear>
      %84 = tt.expand_dims %81 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xf32, #blocked>
      %85 = tt.bitcast %83 : tensor<4x4x1xf32, #linear> -> tensor<4x4x1xi32, #linear>
      %86 = tt.bitcast %84 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %87 = arith.addi %85, %cst_28 : tensor<4x4x1xi32, #linear>
      %88 = arith.addi %86, %cst_1 : tensor<4x4x1xi32, #blocked>
      %89 = tt.bitcast %87 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xi32, #linear>
      %90 = tt.bitcast %88 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %91 = arith.andi %89, %cst_27 : tensor<4x4x1xi32, #linear>
      %92 = arith.andi %90, %cst_4 : tensor<4x4x1xi32, #blocked>
      %93 = tt.bitcast %91 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xf32, #linear>
      %94 = tt.bitcast %92 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xf32, #blocked>
      %95 = math.log2 %93 : tensor<4x4x1xf32, #linear>
      %96 = math.log2 %94 : tensor<4x4x1xf32, #blocked>
      %97 = math.floor %95 : tensor<4x4x1xf32, #linear>
      %98 = math.floor %96 : tensor<4x4x1xf32, #blocked>
      %99 = arith.subf %97, %cst_26 : tensor<4x4x1xf32, #linear>
      %100 = arith.subf %98, %cst_5 : tensor<4x4x1xf32, #blocked>
      %101 = tt.clampf %99, %cst_25, %cst_24, propagateNan = none : tensor<4x4x1xf32, #linear>
      %102 = tt.clampf %100, %cst_18, %cst_23, propagateNan = none : tensor<4x4x1xf32, #blocked>
      %103 = arith.fptoui %101 : tensor<4x4x1xf32, #linear> to tensor<4x4x1xi8, #linear>
      %104 = arith.fptoui %102 : tensor<4x4x1xf32, #blocked> to tensor<4x4x1xi8, #blocked>
      %105 = arith.addi %103, %cst_29 : tensor<4x4x1xi8, #linear>
      %106 = arith.addi %104, %cst : tensor<4x4x1xi8, #blocked>
      %107 = arith.subf %cst_6, %102 : tensor<4x4x1xf32, #blocked>
      %108 = math.exp2 %107 : tensor<4x4x1xf32, #blocked>
      %109 = arith.extf %78 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %110 = tt.broadcast %108 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x32xf32, #blocked>
      %111 = arith.mulf %109, %110 : tensor<4x4x32xf32, #blocked>
      %112 = tt.bitcast %111 : tensor<4x4x32xf32, #blocked> -> tensor<4x4x32xi32, #blocked>
      %113 = arith.andi %112, %cst_7 : tensor<4x4x32xi32, #blocked>
      %114 = arith.shrui %112, %cst_8 : tensor<4x4x32xi32, #blocked>
      %115 = arith.andi %114, %cst_9 : tensor<4x4x32xi32, #blocked>
      %116 = arith.andi %112, %cst_10 : tensor<4x4x32xi32, #blocked>
      %117 = arith.addi %115, %cst_11 : tensor<4x4x32xi32, #blocked>
      %118 = arith.subi %cst_12, %117 : tensor<4x4x32xi32, #blocked>
      %119 = arith.cmpi ult, %115, %cst_12 : tensor<4x4x32xi32, #blocked>
      %120 = arith.shrui %116, %cst_11 : tensor<4x4x32xi32, #blocked>
      %121 = arith.ori %120, %cst_13 : tensor<4x4x32xi32, #blocked>
      %122 = arith.shrui %121, %118 : tensor<4x4x32xi32, #blocked>
      %123 = arith.select %119, %122, %116 : tensor<4x4x32xi1, #blocked>, tensor<4x4x32xi32, #blocked>
      %124 = arith.maxui %115, %cst_14 : tensor<4x4x32xi32, #blocked>
      %125 = arith.subi %124, %cst_14 : tensor<4x4x32xi32, #blocked>
      %126 = arith.shli %125, %cst_15 : tensor<4x4x32xi32, #blocked>
      %127 = arith.shrui %123, %cst_16 : tensor<4x4x32xi32, #blocked>
      %128 = arith.ori %126, %127 : tensor<4x4x32xi32, #blocked>
      %129 = arith.addi %128, %cst_11 : tensor<4x4x32xi32, #blocked>
      %130 = arith.shrui %129, %cst_11 : tensor<4x4x32xi32, #blocked>
      %131 = arith.minui %130, %cst_22 : tensor<4x4x32xi32, #blocked>
      %132 = arith.shrui %113, %cst_17 : tensor<4x4x32xi32, #blocked>
      %133 = arith.ori %132, %131 : tensor<4x4x32xi32, #blocked>
      %134 = arith.trunci %133 : tensor<4x4x32xi32, #blocked> to tensor<4x4x32xi8, #blocked>
      %135 = tt.reshape %134 : tensor<4x4x32xi8, #blocked> -> tensor<4x4x16x2xi8, #blocked7>
      %outLHS, %outRHS = tt.split %135 : tensor<4x4x16x2xi8, #blocked7> -> tensor<4x4x16xi8, #blocked2>
      %136 = arith.shli %outRHS, %cst_2 : tensor<4x4x16xi8, #blocked2>
      %137 = arith.ori %outLHS, %136 : tensor<4x4x16xi8, #blocked2>
      %138 = tt.reshape %137 : tensor<4x4x16xi8, #blocked2> -> tensor<4x64xi8, #blocked8>
      %139 = tt.reshape %105 : tensor<4x4x1xi8, #linear> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %140 = tt.reshape %106 : tensor<4x4x1xi8, #blocked> -> tensor<4x4xi8, #linear2>
      %141 = ttg.convert_layout %140 : tensor<4x4xi8, #linear2> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %142 = arith.extui %141 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>> to tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %143 = arith.shli %142, %cst_30 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %144 = tt.bitcast %143 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %145 = tt.expand_dims %144 {axis = 2 : i32} : tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xbf16, #blocked>
      %146 = tt.broadcast %145 : tensor<4x4x1xbf16, #blocked> -> tensor<4x4x32xbf16, #blocked>
      %147 = tt.reshape %146 : tensor<4x4x32xbf16, #blocked> -> tensor<4x128xbf16, #blocked1>
      %148 = amdgpu.scaled_upcast_fp4 %138 scale %147 {axis = 1 : i32} : tensor<4x64xi8, #blocked8>, tensor<4x128xbf16, #blocked1> -> tensor<4x128xbf16, #blocked1>
      %149 = arith.cmpi eq, %139, %cst_31 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %150 = tt.expand_dims %149 {axis = 2 : i32} : tensor<4x4xi1, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xi1, #blocked>
      %151 = tt.broadcast %150 : tensor<4x4x1xi1, #blocked> -> tensor<4x4x32xi1, #blocked>
      %152 = tt.reshape %151 : tensor<4x4x32xi1, #blocked> -> tensor<4x128xi1, #blocked1>
      %153 = arith.select %152, %cst_0, %148 : tensor<4x128xi1, #blocked1>, tensor<4x128xbf16, #blocked1>
      %154 = ttg.local_alloc %153 : (tensor<4x128xbf16, #blocked1>) -> !ttg.memdesc<4x128xbf16, #shared, #smem>
      %155 = ttg.local_load %154 : !ttg.memdesc<4x128xbf16, #shared, #smem> -> tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>>
      %156 = arith.extui %70 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>> to tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %157 = arith.shli %156, %cst_19 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %158 = tt.bitcast %157 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %159 = tt.expand_dims %158 {axis = 2 : i32} : tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xbf16, #blocked4>
      %160 = tt.broadcast %159 : tensor<4x32x1xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked4>
      %161 = tt.trans %160 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked9>
      %162 = tt.reshape %161 : tensor<4x32x32xbf16, #blocked9> -> tensor<128x32xbf16, #blocked5>
      %163 = amdgpu.scaled_upcast_fp4 %77 scale %162 {axis = 0 : i32} : tensor<64x32xi8, #blocked3>, tensor<128x32xbf16, #blocked5> -> tensor<128x32xbf16, #blocked5>
      %164 = arith.cmpi eq, %70, %cst_20 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %165 = tt.expand_dims %164 {axis = 2 : i32} : tensor<4x32xi1, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xi1, #blocked4>
      %166 = tt.broadcast %165 : tensor<4x32x1xi1, #blocked4> -> tensor<4x32x32xi1, #blocked4>
      %167 = tt.trans %166 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xi1, #blocked4> -> tensor<4x32x32xi1, #blocked9>
      %168 = tt.reshape %167 : tensor<4x32x32xi1, #blocked9> -> tensor<128x32xi1, #blocked5>
      %169 = arith.select %168, %cst_21, %163 : tensor<128x32xi1, #blocked5>, tensor<128x32xbf16, #blocked5>
      %170 = ttg.local_alloc %169 : (tensor<128x32xbf16, #blocked5>) -> !ttg.memdesc<128x32xbf16, #shared, #smem>
      %171 = ttg.local_load %170 : !ttg.memdesc<128x32xbf16, #shared, #smem> -> tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>
      %172 = tt.dot %155, %171, %cst_3 : tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>> * tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> -> tensor<4x32xf32, #blocked3>
      %173 = arith.addf %172, %cst_3 : tensor<4x32xf32, #blocked3>
      %174 = arith.truncf %173 : tensor<4x32xf32, #blocked3> to tensor<4x32xbf16, #blocked3>
      %175 = arith.extsi %13 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %176 = arith.extsi %11 : i32 to i64
      %177 = tt.splat %176 : i64 -> tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %178 = arith.addi %177, %175 : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %179 = arith.extsi %32 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> to tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %180 = arith.extsi %30 : i32 to i64
      %181 = tt.splat %180 : i64 -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %182 = arith.addi %181, %179 : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %183 = arith.muli %7, %6 : i64
      %184 = tt.addptr %arg2, %183 : !tt.ptr<bf16>, i64
      %185 = tt.expand_dims %178 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %186 = arith.extsi %arg13 : i32 to i64
      %187 = tt.expand_dims %175 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %188 = arith.muli %186, %176 : i64
      %189 = tt.splat %186 : i64 -> tensor<4x1xi64, #blocked3>
      %190 = arith.muli %189, %187 : tensor<4x1xi64, #blocked3>
      %191 = tt.addptr %184, %188 : !tt.ptr<bf16>, i64
      %192 = tt.expand_dims %182 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %193 = tt.broadcast %190 : tensor<4x1xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %194 = tt.expand_dims %179 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %195 = tt.broadcast %194 : tensor<1x32xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %196 = tt.addptr %191, %180 : !tt.ptr<bf16>, i64
      %197 = arith.addi %195, %193 : tensor<4x32xi64, #blocked3>
      %198 = arith.extsi %arg4 : i32 to i64
      %199 = tt.splat %198 : i64 -> tensor<4x1xi64, #blocked3>
      %200 = arith.cmpi slt, %185, %199 : tensor<4x1xi64, #blocked3>
      %201 = arith.extsi %arg5 : i32 to i64
      %202 = tt.splat %201 : i64 -> tensor<1x32xi64, #blocked3>
      %203 = arith.cmpi slt, %192, %202 : tensor<1x32xi64, #blocked3>
      %204 = tt.broadcast %200 : tensor<4x1xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %205 = tt.broadcast %203 : tensor<1x32xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %206 = arith.andi %204, %205 : tensor<4x32xi1, #blocked3>
      %207 = tt.splat %196 : !tt.ptr<bf16> -> tensor<4x32x!tt.ptr<bf16>, #blocked3>
      %208 = tt.addptr %207, %197 : tensor<4x32x!tt.ptr<bf16>, #blocked3>, tensor<4x32xi64, #blocked3>
      tt.store %208, %174, %206 : tensor<4x32x!tt.ptr<bf16>, #blocked3>
    }
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(optimize-amd-lds-usage{lds-limit=0 target-arch=gfx950}, triton-scf-to-cf, convert-index-to-llvm{index-bitwidth=0}, allocate-amdgpu-shared-memory, convert-triton-amdgpu-to-llvm{arch=gfx950 ftz=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-cf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwidth=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info, convert-builtin-func-to-llvm{ftz=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_root/fu/cfuiinc534jrjpi5zoq4wqfuuulfwenxxsmmewxjp6ty4csth4xf.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_root/fu/cfuiinc534jrjpi5zoq4wqfuuulfwenxxsmmewxjp6ty4csth4xf.py:18:0: note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     Kernel for computing the matmul C = A x B.
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bn': 'i32', 'stride_bk': 'constexpr', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 2, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank2]:E1113 09:51:30.684000 205 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
ler_TP7: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
#blocked = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_batched_gemm_afp4_wfp4_pre_quant_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32) attributes {noinline = false} {
    %cst = arith.constant dense<127> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128xbf16, #blocked1>
    %cst_1 = arith.constant dense<2097152> : tensor<4x4x1xi32, #blocked>
    %cst_2 = arith.constant dense<4> : tensor<4x4x16xi8, #blocked2>
    %c31_i32 = arith.constant 31 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x32xf32, #blocked3>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %true = arith.constant true
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #blocked>
    %cst_5 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_7 = arith.constant dense<-2147483648> : tensor<4x4x32xi32, #blocked>
    %cst_8 = arith.constant dense<23> : tensor<4x4x32xi32, #blocked>
    %cst_9 = arith.constant dense<255> : tensor<4x4x32xi32, #blocked>
    %cst_10 = arith.constant dense<8388607> : tensor<4x4x32xi32, #blocked>
    %cst_11 = arith.constant dense<1> : tensor<4x4x32xi32, #blocked>
    %cst_12 = arith.constant dense<127> : tensor<4x4x32xi32, #blocked>
    %cst_13 = arith.constant dense<4194304> : tensor<4x4x32xi32, #blocked>
    %cst_14 = arith.constant dense<126> : tensor<4x4x32xi32, #blocked>
    %cst_15 = arith.constant dense<2> : tensor<4x4x32xi32, #blocked>
    %cst_16 = arith.constant dense<21> : tensor<4x4x32xi32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>> -> tensor<64x1xi32, #blocked6>
      %45 = arith.extsi %44 : tensor<64x1xi32, #blocked6> to tensor<64x1xi64, #blocked6>
      %46 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>> -> tensor<1x32xi32, #blocked6>
      %47 = tt.splat %arg10 : i32 -> tensor<1x32xi32, #blocked6>
      %48 = arith.muli %46, %47 : tensor<1x32xi32, #blocked6>
      %49 = arith.extsi %48 : tensor<1x32xi32, #blocked6> to tensor<1x32xi64, #blocked6>
      %50 = tt.broadcast %45 : tensor<64x1xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %51 = tt.broadcast %49 : tensor<1x32xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %52 = arith.addi %50, %51 : tensor<64x32xi64, #blocked6>
      %53 = tt.addptr %arg1, %42 : !tt.ptr<i8>, i64
      %54 = arith.extsi %arg14 : i32 to i64
      %55 = arith.muli %7, %54 : i64
      %56 = tt.addptr %arg3, %55 : !tt.ptr<i8>, i64
      %57 = tt.expand_dims %41 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<32x1xi32, #linear1>
      %58 = tt.splat %arg15 : i32 -> tensor<32x1xi32, #linear1>
      %59 = arith.muli %57, %58 : tensor<32x1xi32, #linear1>
      %60 = arith.extsi %59 : tensor<32x1xi32, #linear1> to tensor<32x1xi64, #linear1>
      %61 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>>
      %62 = tt.broadcast %60 : tensor<32x1xi64, #linear1> -> tensor<32x4xi64, #linear1>
      %63 = tt.expand_dims %61 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>> -> tensor<1x4xi32, #linear1>
      %64 = tt.broadcast %63 : tensor<1x4xi32, #linear1> -> tensor<32x4xi32, #linear1>
      %65 = arith.extsi %64 : tensor<32x4xi32, #linear1> to tensor<32x4xi64, #linear1>
      %66 = arith.addi %65, %62 : tensor<32x4xi64, #linear1>
      %67 = tt.splat %56 : !tt.ptr<i8> -> tensor<32x4x!tt.ptr<i8>, #linear1>
      %68 = tt.addptr %67, %66 : tensor<32x4x!tt.ptr<i8>, #linear1>, tensor<32x4xi64, #linear1>
      %69 = tt.load %68 : tensor<32x4x!tt.ptr<i8>, #linear1>
      %70 = tt.trans %69 {order = array<i32: 1, 0>} : tensor<32x4xi8, #linear1> -> tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %71 = tt.splat %29 : !tt.ptr<bf16> -> tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %72 = tt.addptr %71, %28 : tensor<4x128x!tt.ptr<bf16>, #blocked1>, tensor<4x128xi64, #blocked1>
      %73 = tt.load %72 : tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %74 = tt.splat %53 : !tt.ptr<i8> -> tensor<64x32x!tt.ptr<i8>, #blocked6>
      %75 = tt.addptr %74, %52 : tensor<64x32x!tt.ptr<i8>, #blocked6>, tensor<64x32xi64, #blocked6>
      %76 = tt.load %75 cacheModifier = cg : tensor<64x32x!tt.ptr<i8>, #blocked6>
      %77 = ttg.convert_layout %76 : tensor<64x32xi8, #blocked6> -> tensor<64x32xi8, #blocked3>
      %78 = tt.reshape %73 : tensor<4x128xbf16, #blocked1> -> tensor<4x4x32xbf16, #blocked>
      %79 = math.absf %78 : tensor<4x4x32xbf16, #blocked>
      %80 = arith.extf %79 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %81 = "tt.reduce"(%80) <{axis = 2 : i32}> ({
      ^bb0(%arg16: f32, %arg17: f32):
        %209 = arith.maxnumf %arg16, %arg17 : f32
        tt.reduce.return %209 : f32
      }) : (tensor<4x4x32xf32, #blocked>) -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>>
      %82 = ttg.convert_layout %81 : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>>
      %83 = tt.expand_dims %82 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>> -> tensor<4x4x1xf32, #linear>
      %84 = tt.expand_dims %81 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xf32, #blocked>
      %85 = tt.bitcast %83 : tensor<4x4x1xf32, #linear> -> tensor<4x4x1xi32, #linear>
      %86 = tt.bitcast %84 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %87 = arith.addi %85, %cst_28 : tensor<4x4x1xi32, #linear>
      %88 = arith.addi %86, %cst_1 : tensor<4x4x1xi32, #blocked>
      %89 = tt.bitcast %87 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xi32, #linear>
      %90 = tt.bitcast %88 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %91 = arith.andi %89, %cst_27 : tensor<4x4x1xi32, #linear>
      %92 = arith.andi %90, %cst_4 : tensor<4x4x1xi32, #blocked>
      %93 = tt.bitcast %91 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xf32, #linear>
      %94 = tt.bitcast %92 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xf32, #blocked>
      %95 = math.log2 %93 : tensor<4x4x1xf32, #linear>
      %96 = math.log2 %94 : tensor<4x4x1xf32, #blocked>
      %97 = math.floor %95 : tensor<4x4x1xf32, #linear>
      %98 = math.floor %96 : tensor<4x4x1xf32, #blocked>
      %99 = arith.subf %97, %cst_26 : tensor<4x4x1xf32, #linear>
      %100 = arith.subf %98, %cst_5 : tensor<4x4x1xf32, #blocked>
      %101 = tt.clampf %99, %cst_25, %cst_24, propagateNan = none : tensor<4x4x1xf32, #linear>
      %102 = tt.clampf %100, %cst_18, %cst_23, propagateNan = none : tensor<4x4x1xf32, #blocked>
      %103 = arith.fptoui %101 : tensor<4x4x1xf32, #linear> to tensor<4x4x1xi8, #linear>
      %104 = arith.fptoui %102 : tensor<4x4x1xf32, #blocked> to tensor<4x4x1xi8, #blocked>
      %105 = arith.addi %103, %cst_29 : tensor<4x4x1xi8, #linear>
      %106 = arith.addi %104, %cst : tensor<4x4x1xi8, #blocked>
      %107 = arith.subf %cst_6, %102 : tensor<4x4x1xf32, #blocked>
      %108 = math.exp2 %107 : tensor<4x4x1xf32, #blocked>
      %109 = arith.extf %78 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %110 = tt.broadcast %108 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x32xf32, #blocked>
      %111 = arith.mulf %109, %110 : tensor<4x4x32xf32, #blocked>
      %112 = tt.bitcast %111 : tensor<4x4x32xf32, #blocked> -> tensor<4x4x32xi32, #blocked>
      %113 = arith.andi %112, %cst_7 : tensor<4x4x32xi32, #blocked>
      %114 = arith.shrui %112, %cst_8 : tensor<4x4x32xi32, #blocked>
      %115 = arith.andi %114, %cst_9 : tensor<4x4x32xi32, #blocked>
      %116 = arith.andi %112, %cst_10 : tensor<4x4x32xi32, #blocked>
      %117 = arith.addi %115, %cst_11 : tensor<4x4x32xi32, #blocked>
      %118 = arith.subi %cst_12, %117 : tensor<4x4x32xi32, #blocked>
      %119 = arith.cmpi ult, %115, %cst_12 : tensor<4x4x32xi32, #blocked>
      %120 = arith.shrui %116, %cst_11 : tensor<4x4x32xi32, #blocked>
      %121 = arith.ori %120, %cst_13 : tensor<4x4x32xi32, #blocked>
      %122 = arith.shrui %121, %118 : tensor<4x4x32xi32, #blocked>
      %123 = arith.select %119, %122, %116 : tensor<4x4x32xi1, #blocked>, tensor<4x4x32xi32, #blocked>
      %124 = arith.maxui %115, %cst_14 : tensor<4x4x32xi32, #blocked>
      %125 = arith.subi %124, %cst_14 : tensor<4x4x32xi32, #blocked>
      %126 = arith.shli %125, %cst_15 : tensor<4x4x32xi32, #blocked>
      %127 = arith.shrui %123, %cst_16 : tensor<4x4x32xi32, #blocked>
      %128 = arith.ori %126, %127 : tensor<4x4x32xi32, #blocked>
      %129 = arith.addi %128, %cst_11 : tensor<4x4x32xi32, #blocked>
      %130 = arith.shrui %129, %cst_11 : tensor<4x4x32xi32, #blocked>
      %131 = arith.minui %130, %cst_22 : tensor<4x4x32xi32, #blocked>
      %132 = arith.shrui %113, %cst_17 : tensor<4x4x32xi32, #blocked>
      %133 = arith.ori %132, %131 : tensor<4x4x32xi32, #blocked>
      %134 = arith.trunci %133 : tensor<4x4x32xi32, #blocked> to tensor<4x4x32xi8, #blocked>
      %135 = tt.reshape %134 : tensor<4x4x32xi8, #blocked> -> tensor<4x4x16x2xi8, #blocked7>
      %outLHS, %outRHS = tt.split %135 : tensor<4x4x16x2xi8, #blocked7> -> tensor<4x4x16xi8, #blocked2>
      %136 = arith.shli %outRHS, %cst_2 : tensor<4x4x16xi8, #blocked2>
      %137 = arith.ori %outLHS, %136 : tensor<4x4x16xi8, #blocked2>
      %138 = tt.reshape %137 : tensor<4x4x16xi8, #blocked2> -> tensor<4x64xi8, #blocked8>
      %139 = tt.reshape %105 : tensor<4x4x1xi8, #linear> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %140 = tt.reshape %106 : tensor<4x4x1xi8, #blocked> -> tensor<4x4xi8, #linear2>
      %141 = ttg.convert_layout %140 : tensor<4x4xi8, #linear2> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %142 = arith.extui %141 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>> to tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %143 = arith.shli %142, %cst_30 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %144 = tt.bitcast %143 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %145 = tt.expand_dims %144 {axis = 2 : i32} : tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xbf16, #blocked>
      %146 = tt.broadcast %145 : tensor<4x4x1xbf16, #blocked> -> tensor<4x4x32xbf16, #blocked>
      %147 = tt.reshape %146 : tensor<4x4x32xbf16, #blocked> -> tensor<4x128xbf16, #blocked1>
      %148 = amdgpu.scaled_upcast_fp4 %138 scale %147 {axis = 1 : i32} : tensor<4x64xi8, #blocked8>, tensor<4x128xbf16, #blocked1> -> tensor<4x128xbf16, #blocked1>
      %149 = arith.cmpi eq, %139, %cst_31 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %150 = tt.expand_dims %149 {axis = 2 : i32} : tensor<4x4xi1, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xi1, #blocked>
      %151 = tt.broadcast %150 : tensor<4x4x1xi1, #blocked> -> tensor<4x4x32xi1, #blocked>
      %152 = tt.reshape %151 : tensor<4x4x32xi1, #blocked> -> tensor<4x128xi1, #blocked1>
      %153 = arith.select %152, %cst_0, %148 : tensor<4x128xi1, #blocked1>, tensor<4x128xbf16, #blocked1>
      %154 = ttg.local_alloc %153 : (tensor<4x128xbf16, #blocked1>) -> !ttg.memdesc<4x128xbf16, #shared, #smem>
      %155 = ttg.local_load %154 : !ttg.memdesc<4x128xbf16, #shared, #smem> -> tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>>
      %156 = arith.extui %70 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>> to tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %157 = arith.shli %156, %cst_19 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %158 = tt.bitcast %157 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %159 = tt.expand_dims %158 {axis = 2 : i32} : tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xbf16, #blocked4>
      %160 = tt.broadcast %159 : tensor<4x32x1xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked4>
      %161 = tt.trans %160 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked9>
      %162 = tt.reshape %161 : tensor<4x32x32xbf16, #blocked9> -> tensor<128x32xbf16, #blocked5>
      %163 = amdgpu.scaled_upcast_fp4 %77 scale %162 {axis = 0 : i32} : tensor<64x32xi8, #blocked3>, tensor<128x32xbf16, #blocked5> -> tensor<128x32xbf16, #blocked5>
      %164 = arith.cmpi eq, %70, %cst_20 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %165 = tt.expand_dims %164 {axis = 2 : i32} : tensor<4x32xi1, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xi1, #blocked4>
      %166 = tt.broadcast %165 : tensor<4x32x1xi1, #blocked4> -> tensor<4x32x32xi1, #blocked4>
      %167 = tt.trans %166 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xi1, #blocked4> -> tensor<4x32x32xi1, #blocked9>
      %168 = tt.reshape %167 : tensor<4x32x32xi1, #blocked9> -> tensor<128x32xi1, #blocked5>
      %169 = arith.select %168, %cst_21, %163 : tensor<128x32xi1, #blocked5>, tensor<128x32xbf16, #blocked5>
      %170 = ttg.local_alloc %169 : (tensor<128x32xbf16, #blocked5>) -> !ttg.memdesc<128x32xbf16, #shared, #smem>
      %171 = ttg.local_load %170 : !ttg.memdesc<128x32xbf16, #shared, #smem> -> tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>
      %172 = tt.dot %155, %171, %cst_3 : tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>> * tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> -> tensor<4x32xf32, #blocked3>
      %173 = arith.addf %172, %cst_3 : tensor<4x32xf32, #blocked3>
      %174 = arith.truncf %173 : tensor<4x32xf32, #blocked3> to tensor<4x32xbf16, #blocked3>
      %175 = arith.extsi %13 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %176 = arith.extsi %11 : i32 to i64
      %177 = tt.splat %176 : i64 -> tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %178 = arith.addi %177, %175 : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %179 = arith.extsi %32 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> to tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %180 = arith.extsi %30 : i32 to i64
      %181 = tt.splat %180 : i64 -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %182 = arith.addi %181, %179 : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %183 = arith.muli %7, %6 : i64
      %184 = tt.addptr %arg2, %183 : !tt.ptr<bf16>, i64
      %185 = tt.expand_dims %178 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %186 = arith.extsi %arg13 : i32 to i64
      %187 = tt.expand_dims %175 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %188 = arith.muli %186, %176 : i64
      %189 = tt.splat %186 : i64 -> tensor<4x1xi64, #blocked3>
      %190 = arith.muli %189, %187 : tensor<4x1xi64, #blocked3>
      %191 = tt.addptr %184, %188 : !tt.ptr<bf16>, i64
      %192 = tt.expand_dims %182 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %193 = tt.broadcast %190 : tensor<4x1xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %194 = tt.expand_dims %179 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %195 = tt.broadcast %194 : tensor<1x32xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %196 = tt.addptr %191, %180 : !tt.ptr<bf16>, i64
      %197 = arith.addi %195, %193 : tensor<4x32xi64, #blocked3>
      %198 = arith.extsi %arg4 : i32 to i64
      %199 = tt.splat %198 : i64 -> tensor<4x1xi64, #blocked3>
      %200 = arith.cmpi slt, %185, %199 : tensor<4x1xi64, #blocked3>
      %201 = arith.extsi %arg5 : i32 to i64
      %202 = tt.splat %201 : i64 -> tensor<1x32xi64, #blocked3>
      %203 = arith.cmpi slt, %192, %202 : tensor<1x32xi64, #blocked3>
      %204 = tt.broadcast %200 : tensor<4x1xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %205 = tt.broadcast %203 : tensor<1x32xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %206 = arith.andi %204, %205 : tensor<4x32xi1, #blocked3>
      %207 = tt.splat %196 : !tt.ptr<bf16> -> tensor<4x32x!tt.ptr<bf16>, #blocked3>
      %208 = tt.addptr %207, %197 : tensor<4x32x!tt.ptr<bf16>, #blocked3>, tensor<4x32xi64, #blocked3>
      tt.store %208, %174, %206 : tensor<4x32x!tt.ptr<bf16>, #blocked3>
    }
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(optimize-amd-lds-usage{lds-limit=0 target-arch=gfx950}, triton-scf-to-cf, convert-index-to-llvm{index-bitwidth=0}, allocate-amdgpu-shared-memory, convert-triton-amdgpu-to-llvm{arch=gfx950 ftz=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-cf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwidth=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info, convert-builtin-func-to-llvm{ftz=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_root/m2/cm2yvkjklqde7xagixdvc2vu4nfxl767zo5ngriuvz6m4pec2ya3.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_root/m2/cm2yvkjklqde7xagixdvc2vu4nfxl767zo5ngriuvz6m4pec2ya3.py:18:0: note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     Kernel for computing the matmul C = A x B.
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bn': 'i32', 'stride_bk': 'constexpr', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 7, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank7]:E1113 09:51:30.702000 210 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
ler_TP1: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
#blocked = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_batched_gemm_afp4_wfp4_pre_quant_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32) attributes {noinline = false} {
    %cst = arith.constant dense<127> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128xbf16, #blocked1>
    %cst_1 = arith.constant dense<2097152> : tensor<4x4x1xi32, #blocked>
    %cst_2 = arith.constant dense<4> : tensor<4x4x16xi8, #blocked2>
    %c31_i32 = arith.constant 31 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x32xf32, #blocked3>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %true = arith.constant true
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #blocked>
    %cst_5 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_7 = arith.constant dense<-2147483648> : tensor<4x4x32xi32, #blocked>
    %cst_8 = arith.constant dense<23> : tensor<4x4x32xi32, #blocked>
    %cst_9 = arith.constant dense<255> : tensor<4x4x32xi32, #blocked>
    %cst_10 = arith.constant dense<8388607> : tensor<4x4x32xi32, #blocked>
    %cst_11 = arith.constant dense<1> : tensor<4x4x32xi32, #blocked>
    %cst_12 = arith.constant dense<127> : tensor<4x4x32xi32, #blocked>
    %cst_13 = arith.constant dense<4194304> : tensor<4x4x32xi32, #blocked>
    %cst_14 = arith.constant dense<126> : tensor<4x4x32xi32, #blocked>
    %cst_15 = arith.constant dense<2> : tensor<4x4x32xi32, #blocked>
    %cst_16 = arith.constant dense<21> : tensor<4x4x32xi32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>> -> tensor<64x1xi32, #blocked6>
      %45 = arith.extsi %44 : tensor<64x1xi32, #blocked6> to tensor<64x1xi64, #blocked6>
      %46 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>> -> tensor<1x32xi32, #blocked6>
      %47 = tt.splat %arg10 : i32 -> tensor<1x32xi32, #blocked6>
      %48 = arith.muli %46, %47 : tensor<1x32xi32, #blocked6>
      %49 = arith.extsi %48 : tensor<1x32xi32, #blocked6> to tensor<1x32xi64, #blocked6>
      %50 = tt.broadcast %45 : tensor<64x1xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %51 = tt.broadcast %49 : tensor<1x32xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %52 = arith.addi %50, %51 : tensor<64x32xi64, #blocked6>
      %53 = tt.addptr %arg1, %42 : !tt.ptr<i8>, i64
      %54 = arith.extsi %arg14 : i32 to i64
      %55 = arith.muli %7, %54 : i64
      %56 = tt.addptr %arg3, %55 : !tt.ptr<i8>, i64
      %57 = tt.expand_dims %41 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<32x1xi32, #linear1>
      %58 = tt.splat %arg15 : i32 -> tensor<32x1xi32, #linear1>
      %59 = arith.muli %57, %58 : tensor<32x1xi32, #linear1>
      %60 = arith.extsi %59 : tensor<32x1xi32, #linear1> to tensor<32x1xi64, #linear1>
      %61 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>>
      %62 = tt.broadcast %60 : tensor<32x1xi64, #linear1> -> tensor<32x4xi64, #linear1>
      %63 = tt.expand_dims %61 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>> -> tensor<1x4xi32, #linear1>
      %64 = tt.broadcast %63 : tensor<1x4xi32, #linear1> -> tensor<32x4xi32, #linear1>
      %65 = arith.extsi %64 : tensor<32x4xi32, #linear1> to tensor<32x4xi64, #linear1>
      %66 = arith.addi %65, %62 : tensor<32x4xi64, #linear1>
      %67 = tt.splat %56 : !tt.ptr<i8> -> tensor<32x4x!tt.ptr<i8>, #linear1>
      %68 = tt.addptr %67, %66 : tensor<32x4x!tt.ptr<i8>, #linear1>, tensor<32x4xi64, #linear1>
      %69 = tt.load %68 : tensor<32x4x!tt.ptr<i8>, #linear1>
      %70 = tt.trans %69 {order = array<i32: 1, 0>} : tensor<32x4xi8, #linear1> -> tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %71 = tt.splat %29 : !tt.ptr<bf16> -> tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %72 = tt.addptr %71, %28 : tensor<4x128x!tt.ptr<bf16>, #blocked1>, tensor<4x128xi64, #blocked1>
      %73 = tt.load %72 : tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %74 = tt.splat %53 : !tt.ptr<i8> -> tensor<64x32x!tt.ptr<i8>, #blocked6>
      %75 = tt.addptr %74, %52 : tensor<64x32x!tt.ptr<i8>, #blocked6>, tensor<64x32xi64, #blocked6>
      %76 = tt.load %75 cacheModifier = cg : tensor<64x32x!tt.ptr<i8>, #blocked6>
      %77 = ttg.convert_layout %76 : tensor<64x32xi8, #blocked6> -> tensor<64x32xi8, #blocked3>
      %78 = tt.reshape %73 : tensor<4x128xbf16, #blocked1> -> tensor<4x4x32xbf16, #blocked>
      %79 = math.absf %78 : tensor<4x4x32xbf16, #blocked>
      %80 = arith.extf %79 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %81 = "tt.reduce"(%80) <{axis = 2 : i32}> ({
      ^bb0(%arg16: f32, %arg17: f32):
        %209 = arith.maxnumf %arg16, %arg17 : f32
        tt.reduce.return %209 : f32
      }) : (tensor<4x4x32xf32, #blocked>) -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>>
      %82 = ttg.convert_layout %81 : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>>
      %83 = tt.expand_dims %82 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>> -> tensor<4x4x1xf32, #linear>
      %84 = tt.expand_dims %81 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xf32, #blocked>
      %85 = tt.bitcast %83 : tensor<4x4x1xf32, #linear> -> tensor<4x4x1xi32, #linear>
      %86 = tt.bitcast %84 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %87 = arith.addi %85, %cst_28 : tensor<4x4x1xi32, #linear>
      %88 = arith.addi %86, %cst_1 : tensor<4x4x1xi32, #blocked>
      %89 = tt.bitcast %87 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xi32, #linear>
      %90 = tt.bitcast %88 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %91 = arith.andi %89, %cst_27 : tensor<4x4x1xi32, #linear>
      %92 = arith.andi %90, %cst_4 : tensor<4x4x1xi32, #blocked>
      %93 = tt.bitcast %91 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xf32, #linear>
      %94 = tt.bitcast %92 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xf32, #blocked>
      %95 = math.log2 %93 : tensor<4x4x1xf32, #linear>
      %96 = math.log2 %94 : tensor<4x4x1xf32, #blocked>
      %97 = math.floor %95 : tensor<4x4x1xf32, #linear>
      %98 = math.floor %96 : tensor<4x4x1xf32, #blocked>
      %99 = arith.subf %97, %cst_26 : tensor<4x4x1xf32, #linear>
      %100 = arith.subf %98, %cst_5 : tensor<4x4x1xf32, #blocked>
      %101 = tt.clampf %99, %cst_25, %cst_24, propagateNan = none : tensor<4x4x1xf32, #linear>
      %102 = tt.clampf %100, %cst_18, %cst_23, propagateNan = none : tensor<4x4x1xf32, #blocked>
      %103 = arith.fptoui %101 : tensor<4x4x1xf32, #linear> to tensor<4x4x1xi8, #linear>
      %104 = arith.fptoui %102 : tensor<4x4x1xf32, #blocked> to tensor<4x4x1xi8, #blocked>
      %105 = arith.addi %103, %cst_29 : tensor<4x4x1xi8, #linear>
      %106 = arith.addi %104, %cst : tensor<4x4x1xi8, #blocked>
      %107 = arith.subf %cst_6, %102 : tensor<4x4x1xf32, #blocked>
      %108 = math.exp2 %107 : tensor<4x4x1xf32, #blocked>
      %109 = arith.extf %78 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %110 = tt.broadcast %108 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x32xf32, #blocked>
      %111 = arith.mulf %109, %110 : tensor<4x4x32xf32, #blocked>
      %112 = tt.bitcast %111 : tensor<4x4x32xf32, #blocked> -> tensor<4x4x32xi32, #blocked>
      %113 = arith.andi %112, %cst_7 : tensor<4x4x32xi32, #blocked>
      %114 = arith.shrui %112, %cst_8 : tensor<4x4x32xi32, #blocked>
      %115 = arith.andi %114, %cst_9 : tensor<4x4x32xi32, #blocked>
      %116 = arith.andi %112, %cst_10 : tensor<4x4x32xi32, #blocked>
      %117 = arith.addi %115, %cst_11 : tensor<4x4x32xi32, #blocked>
      %118 = arith.subi %cst_12, %117 : tensor<4x4x32xi32, #blocked>
      %119 = arith.cmpi ult, %115, %cst_12 : tensor<4x4x32xi32, #blocked>
      %120 = arith.shrui %116, %cst_11 : tensor<4x4x32xi32, #blocked>
      %121 = arith.ori %120, %cst_13 : tensor<4x4x32xi32, #blocked>
      %122 = arith.shrui %121, %118 : tensor<4x4x32xi32, #blocked>
      %123 = arith.select %119, %122, %116 : tensor<4x4x32xi1, #blocked>, tensor<4x4x32xi32, #blocked>
      %124 = arith.maxui %115, %cst_14 : tensor<4x4x32xi32, #blocked>
      %125 = arith.subi %124, %cst_14 : tensor<4x4x32xi32, #blocked>
      %126 = arith.shli %125, %cst_15 : tensor<4x4x32xi32, #blocked>
      %127 = arith.shrui %123, %cst_16 : tensor<4x4x32xi32, #blocked>
      %128 = arith.ori %126, %127 : tensor<4x4x32xi32, #blocked>
      %129 = arith.addi %128, %cst_11 : tensor<4x4x32xi32, #blocked>
      %130 = arith.shrui %129, %cst_11 : tensor<4x4x32xi32, #blocked>
      %131 = arith.minui %130, %cst_22 : tensor<4x4x32xi32, #blocked>
      %132 = arith.shrui %113, %cst_17 : tensor<4x4x32xi32, #blocked>
      %133 = arith.ori %132, %131 : tensor<4x4x32xi32, #blocked>
      %134 = arith.trunci %133 : tensor<4x4x32xi32, #blocked> to tensor<4x4x32xi8, #blocked>
      %135 = tt.reshape %134 : tensor<4x4x32xi8, #blocked> -> tensor<4x4x16x2xi8, #blocked7>
      %outLHS, %outRHS = tt.split %135 : tensor<4x4x16x2xi8, #blocked7> -> tensor<4x4x16xi8, #blocked2>
      %136 = arith.shli %outRHS, %cst_2 : tensor<4x4x16xi8, #blocked2>
      %137 = arith.ori %outLHS, %136 : tensor<4x4x16xi8, #blocked2>
      %138 = tt.reshape %137 : tensor<4x4x16xi8, #blocked2> -> tensor<4x64xi8, #blocked8>
      %139 = tt.reshape %105 : tensor<4x4x1xi8, #linear> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %140 = tt.reshape %106 : tensor<4x4x1xi8, #blocked> -> tensor<4x4xi8, #linear2>
      %141 = ttg.convert_layout %140 : tensor<4x4xi8, #linear2> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %142 = arith.extui %141 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>> to tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %143 = arith.shli %142, %cst_30 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %144 = tt.bitcast %143 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %145 = tt.expand_dims %144 {axis = 2 : i32} : tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xbf16, #blocked>
      %146 = tt.broadcast %145 : tensor<4x4x1xbf16, #blocked> -> tensor<4x4x32xbf16, #blocked>
      %147 = tt.reshape %146 : tensor<4x4x32xbf16, #blocked> -> tensor<4x128xbf16, #blocked1>
      %148 = amdgpu.scaled_upcast_fp4 %138 scale %147 {axis = 1 : i32} : tensor<4x64xi8, #blocked8>, tensor<4x128xbf16, #blocked1> -> tensor<4x128xbf16, #blocked1>
      %149 = arith.cmpi eq, %139, %cst_31 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %150 = tt.expand_dims %149 {axis = 2 : i32} : tensor<4x4xi1, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xi1, #blocked>
      %151 = tt.broadcast %150 : tensor<4x4x1xi1, #blocked> -> tensor<4x4x32xi1, #blocked>
      %152 = tt.reshape %151 : tensor<4x4x32xi1, #blocked> -> tensor<4x128xi1, #blocked1>
      %153 = arith.select %152, %cst_0, %148 : tensor<4x128xi1, #blocked1>, tensor<4x128xbf16, #blocked1>
      %154 = ttg.local_alloc %153 : (tensor<4x128xbf16, #blocked1>) -> !ttg.memdesc<4x128xbf16, #shared, #smem>
      %155 = ttg.local_load %154 : !ttg.memdesc<4x128xbf16, #shared, #smem> -> tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>>
      %156 = arith.extui %70 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>> to tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %157 = arith.shli %156, %cst_19 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %158 = tt.bitcast %157 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %159 = tt.expand_dims %158 {axis = 2 : i32} : tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xbf16, #blocked4>
      %160 = tt.broadcast %159 : tensor<4x32x1xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked4>
      %161 = tt.trans %160 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked9>
      %162 = tt.reshape %161 : tensor<4x32x32xbf16, #blocked9> -> tensor<128x32xbf16, #blocked5>
      %163 = amdgpu.scaled_upcast_fp4 %77 scale %162 {axis = 0 : i32} : tensor<64x32xi8, #blocked3>, tensor<128x32xbf16, #blocked5> -> tensor<128x32xbf16, #blocked5>
      %164 = arith.cmpi eq, %70, %cst_20 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %165 = tt.expand_dims %164 {axis = 2 : i32} : tensor<4x32xi1, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xi1, #blocked4>
      %166 = tt.broadcast %165 : tensor<4x32x1xi1, #blocked4> -> tensor<4x32x32xi1, #blocked4>
      %167 = tt.trans %166 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xi1, #blocked4> -> tensor<4x32x32xi1, #blocked9>
      %168 = tt.reshape %167 : tensor<4x32x32xi1, #blocked9> -> tensor<128x32xi1, #blocked5>
      %169 = arith.select %168, %cst_21, %163 : tensor<128x32xi1, #blocked5>, tensor<128x32xbf16, #blocked5>
      %170 = ttg.local_alloc %169 : (tensor<128x32xbf16, #blocked5>) -> !ttg.memdesc<128x32xbf16, #shared, #smem>
      %171 = ttg.local_load %170 : !ttg.memdesc<128x32xbf16, #shared, #smem> -> tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>
      %172 = tt.dot %155, %171, %cst_3 : tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>> * tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> -> tensor<4x32xf32, #blocked3>
      %173 = arith.addf %172, %cst_3 : tensor<4x32xf32, #blocked3>
      %174 = arith.truncf %173 : tensor<4x32xf32, #blocked3> to tensor<4x32xbf16, #blocked3>
      %175 = arith.extsi %13 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %176 = arith.extsi %11 : i32 to i64
      %177 = tt.splat %176 : i64 -> tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %178 = arith.addi %177, %175 : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %179 = arith.extsi %32 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> to tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %180 = arith.extsi %30 : i32 to i64
      %181 = tt.splat %180 : i64 -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %182 = arith.addi %181, %179 : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %183 = arith.muli %7, %6 : i64
      %184 = tt.addptr %arg2, %183 : !tt.ptr<bf16>, i64
      %185 = tt.expand_dims %178 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %186 = arith.extsi %arg13 : i32 to i64
      %187 = tt.expand_dims %175 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %188 = arith.muli %186, %176 : i64
      %189 = tt.splat %186 : i64 -> tensor<4x1xi64, #blocked3>
      %190 = arith.muli %189, %187 : tensor<4x1xi64, #blocked3>
      %191 = tt.addptr %184, %188 : !tt.ptr<bf16>, i64
      %192 = tt.expand_dims %182 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %193 = tt.broadcast %190 : tensor<4x1xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %194 = tt.expand_dims %179 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %195 = tt.broadcast %194 : tensor<1x32xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %196 = tt.addptr %191, %180 : !tt.ptr<bf16>, i64
      %197 = arith.addi %195, %193 : tensor<4x32xi64, #blocked3>
      %198 = arith.extsi %arg4 : i32 to i64
      %199 = tt.splat %198 : i64 -> tensor<4x1xi64, #blocked3>
      %200 = arith.cmpi slt, %185, %199 : tensor<4x1xi64, #blocked3>
      %201 = arith.extsi %arg5 : i32 to i64
      %202 = tt.splat %201 : i64 -> tensor<1x32xi64, #blocked3>
      %203 = arith.cmpi slt, %192, %202 : tensor<1x32xi64, #blocked3>
      %204 = tt.broadcast %200 : tensor<4x1xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %205 = tt.broadcast %203 : tensor<1x32xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %206 = arith.andi %204, %205 : tensor<4x32xi1, #blocked3>
      %207 = tt.splat %196 : !tt.ptr<bf16> -> tensor<4x32x!tt.ptr<bf16>, #blocked3>
      %208 = tt.addptr %207, %197 : tensor<4x32x!tt.ptr<bf16>, #blocked3>, tensor<4x32xi64, #blocked3>
      tt.store %208, %174, %206 : tensor<4x32x!tt.ptr<bf16>, #blocked3>
    }
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(optimize-amd-lds-usage{lds-limit=0 target-arch=gfx950}, triton-scf-to-cf, convert-index-to-llvm{index-bitwidth=0}, allocate-amdgpu-shared-memory, convert-triton-amdgpu-to-llvm{arch=gfx950 ftz=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-cf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwidth=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info, convert-builtin-func-to-llvm{ftz=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_root/cw/ccwd4zrgrlks5y5xocpe5ta7oml6i6mmmvnpji7xscgl7kgbntfh.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_root/cw/ccwd4zrgrlks5y5xocpe5ta7oml6i6mmmvnpji7xscgl7kgbntfh.py:18:0: note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     Kernel for computing the matmul C = A x B.
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bn': 'i32', 'stride_bk': 'constexpr', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 1, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank1]:E1113 09:51:30.710000 204 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
ler_TP0: /app/triton/third_party/amd/lib/TritonAMDGPUDialectToLLVM/ScaledUpcastToLLVM.cpp:73: virtual llvm::LogicalResult {anonymous}::ScaledUpcastFp4Pattern::matchAndRewrite(mlir::triton::amdgpu::ScaledUpcastFp4Op, mlir::ConvertOpToLLVMPattern<mlir::triton::amdgpu::ScaledUpcastFp4Op>::OpAdaptor, mlir::ConversionPatternRewriter&) const: Assertion `inputVals.size() % 4 == 0' failed.
#blocked = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1, 1], threadsPerWarp = [1, 4, 16], warpsPerCTA = [4, 1, 1], order = [2, 1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1, 2], threadsPerWarp = [1, 32, 2], warpsPerCTA = [1, 1, 4], order = [1, 2, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [2, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 1, 1, 2], threadsPerWarp = [1, 4, 16, 1], warpsPerCTA = [4, 1, 1, 1], order = [3, 2, 1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 2, 1], threadsPerWarp = [1, 2, 32], warpsPerCTA = [1, 4, 1], order = [2, 1, 0]}>
#linear = #ttg.linear<{register = [], lane = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 1, 0], [0, 2, 0]], warp = [[1, 0, 0], [2, 0, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 0]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 0]], lane = [[0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [0, 2]], warp = [[1, 0], [2, 0]], block = []}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_batched_gemm_afp4_wfp4_pre_quant_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32) attributes {noinline = false} {
    %cst = arith.constant dense<127> : tensor<4x4x1xi8, #blocked>
    %cst_0 = arith.constant dense<0x7FC0> : tensor<4x128xbf16, #blocked1>
    %cst_1 = arith.constant dense<2097152> : tensor<4x4x1xi32, #blocked>
    %cst_2 = arith.constant dense<4> : tensor<4x4x16xi8, #blocked2>
    %c31_i32 = arith.constant 31 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x32xf32, #blocked3>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %true = arith.constant true
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #blocked>
    %cst_5 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<4x4x1xf32, #blocked>
    %cst_7 = arith.constant dense<-2147483648> : tensor<4x4x32xi32, #blocked>
    %cst_8 = arith.constant dense<23> : tensor<4x4x32xi32, #blocked>
    %cst_9 = arith.constant dense<255> : tensor<4x4x32xi32, #blocked>
    %cst_10 = arith.constant dense<8388607> : tensor<4x4x32xi32, #blocked>
    %cst_11 = arith.constant dense<1> : tensor<4x4x32xi32, #blocked>
    %cst_12 = arith.constant dense<127> : tensor<4x4x32xi32, #blocked>
    %cst_13 = arith.constant dense<4194304> : tensor<4x4x32xi32, #blocked>
    %cst_14 = arith.constant dense<126> : tensor<4x4x32xi32, #blocked>
    %cst_15 = arith.constant dense<2> : tensor<4x4x32xi32, #blocked>
    %cst_16 = arith.constant dense<21> : tensor<4x4x32xi32, #blocked>
    %cst_17 = arith.constant dense<28> : tensor<4x4x32xi32, #blocked>
    %cst_18 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_19 = arith.constant dense<7> : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_20 = arith.constant dense<-1> : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
    %cst_21 = arith.constant dense<0x7FC0> : tensor<128x32xbf16, #blocked5>
    %cst_22 = arith.constant dense<7> : tensor<4x4x32xi32, #blocked>
    %cst_23 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #blocked>
    %cst_24 = arith.constant dense<1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_25 = arith.constant dense<-1.270000e+02> : tensor<4x4x1xf32, #linear>
    %cst_26 = arith.constant dense<2.000000e+00> : tensor<4x4x1xf32, #linear>
    %cst_27 = arith.constant dense<-8388608> : tensor<4x4x1xi32, #linear>
    %cst_28 = arith.constant dense<2097152> : tensor<4x4x1xi32, #linear>
    %cst_29 = arith.constant dense<127> : tensor<4x4x1xi8, #linear>
    %cst_30 = arith.constant dense<7> : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
    %cst_31 = arith.constant dense<-1> : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = arith.addi %arg5, %c31_i32 : i32
    %3 = arith.divsi %2, %c32_i32 : i32
    %4 = arith.extsi %arg7 : i32 to i64
    %5 = arith.extsi %arg9 : i32 to i64
    %6 = arith.extsi %arg11 : i32 to i64
    %7 = arith.extsi %0 : i32 to i64
    %8 = arith.divsi %1, %3 : i32
    %9 = arith.remsi %1, %3 : i32
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %10 = arith.cmpi sgt, %arg6, %c0_i32 : i32
    scf.if %10 {
      %11 = arith.muli %8, %c4_i32 : i32
      %12 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %13 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %14 = tt.splat %11 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %15 = arith.addi %14, %12 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %16 = tt.splat %arg4 : i32 -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %17 = arith.remsi %15, %16 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
      %18 = arith.muli %7, %4 : i64
      %19 = tt.expand_dims %17 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<4x1xi32, #blocked1>
      %20 = tt.splat %arg8 : i32 -> tensor<4x1xi32, #blocked1>
      %21 = arith.muli %19, %20 : tensor<4x1xi32, #blocked1>
      %22 = arith.extsi %21 : tensor<4x1xi32, #blocked1> to tensor<4x1xi64, #blocked1>
      %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
      %24 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xi32, #blocked1>
      %25 = arith.extsi %24 : tensor<1x128xi32, #blocked1> to tensor<1x128xi64, #blocked1>
      %26 = tt.broadcast %22 : tensor<4x1xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %27 = tt.broadcast %25 : tensor<1x128xi64, #blocked1> -> tensor<4x128xi64, #blocked1>
      %28 = arith.addi %26, %27 : tensor<4x128xi64, #blocked1>
      %29 = tt.addptr %arg0, %18 : !tt.ptr<bf16>, i64
      %30 = arith.muli %9, %c32_i32 : i32
      %31 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %32 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %33 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %34 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %35 = tt.splat %30 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %36 = arith.addi %34, %31 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %37 = arith.addi %35, %33 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %38 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %39 = tt.splat %arg5 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %40 = arith.remsi %36, %38 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>>
      %41 = arith.remsi %37, %39 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>>
      %42 = arith.muli %7, %5 : i64
      %43 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked6}>> -> tensor<64x1xi32, #blocked6>
      %45 = arith.extsi %44 : tensor<64x1xi32, #blocked6> to tensor<64x1xi64, #blocked6>
      %46 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked6}>> -> tensor<1x32xi32, #blocked6>
      %47 = tt.splat %arg10 : i32 -> tensor<1x32xi32, #blocked6>
      %48 = arith.muli %46, %47 : tensor<1x32xi32, #blocked6>
      %49 = arith.extsi %48 : tensor<1x32xi32, #blocked6> to tensor<1x32xi64, #blocked6>
      %50 = tt.broadcast %45 : tensor<64x1xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %51 = tt.broadcast %49 : tensor<1x32xi64, #blocked6> -> tensor<64x32xi64, #blocked6>
      %52 = arith.addi %50, %51 : tensor<64x32xi64, #blocked6>
      %53 = tt.addptr %arg1, %42 : !tt.ptr<i8>, i64
      %54 = arith.extsi %arg14 : i32 to i64
      %55 = arith.muli %7, %54 : i64
      %56 = tt.addptr %arg3, %55 : !tt.ptr<i8>, i64
      %57 = tt.expand_dims %41 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<32x1xi32, #linear1>
      %58 = tt.splat %arg15 : i32 -> tensor<32x1xi32, #linear1>
      %59 = arith.muli %57, %58 : tensor<32x1xi32, #linear1>
      %60 = arith.extsi %59 : tensor<32x1xi32, #linear1> to tensor<32x1xi64, #linear1>
      %61 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>>
      %62 = tt.broadcast %60 : tensor<32x1xi64, #linear1> -> tensor<32x4xi64, #linear1>
      %63 = tt.expand_dims %61 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #linear1}>> -> tensor<1x4xi32, #linear1>
      %64 = tt.broadcast %63 : tensor<1x4xi32, #linear1> -> tensor<32x4xi32, #linear1>
      %65 = arith.extsi %64 : tensor<32x4xi32, #linear1> to tensor<32x4xi64, #linear1>
      %66 = arith.addi %65, %62 : tensor<32x4xi64, #linear1>
      %67 = tt.splat %56 : !tt.ptr<i8> -> tensor<32x4x!tt.ptr<i8>, #linear1>
      %68 = tt.addptr %67, %66 : tensor<32x4x!tt.ptr<i8>, #linear1>, tensor<32x4xi64, #linear1>
      %69 = tt.load %68 : tensor<32x4x!tt.ptr<i8>, #linear1>
      %70 = tt.trans %69 {order = array<i32: 1, 0>} : tensor<32x4xi8, #linear1> -> tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %71 = tt.splat %29 : !tt.ptr<bf16> -> tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %72 = tt.addptr %71, %28 : tensor<4x128x!tt.ptr<bf16>, #blocked1>, tensor<4x128xi64, #blocked1>
      %73 = tt.load %72 : tensor<4x128x!tt.ptr<bf16>, #blocked1>
      %74 = tt.splat %53 : !tt.ptr<i8> -> tensor<64x32x!tt.ptr<i8>, #blocked6>
      %75 = tt.addptr %74, %52 : tensor<64x32x!tt.ptr<i8>, #blocked6>, tensor<64x32xi64, #blocked6>
      %76 = tt.load %75 cacheModifier = cg : tensor<64x32x!tt.ptr<i8>, #blocked6>
      %77 = ttg.convert_layout %76 : tensor<64x32xi8, #blocked6> -> tensor<64x32xi8, #blocked3>
      %78 = tt.reshape %73 : tensor<4x128xbf16, #blocked1> -> tensor<4x4x32xbf16, #blocked>
      %79 = math.absf %78 : tensor<4x4x32xbf16, #blocked>
      %80 = arith.extf %79 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %81 = "tt.reduce"(%80) <{axis = 2 : i32}> ({
      ^bb0(%arg16: f32, %arg17: f32):
        %209 = arith.maxnumf %arg16, %arg17 : f32
        tt.reduce.return %209 : f32
      }) : (tensor<4x4x32xf32, #blocked>) -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>>
      %82 = ttg.convert_layout %81 : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>>
      %83 = tt.expand_dims %82 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #linear}>> -> tensor<4x4x1xf32, #linear>
      %84 = tt.expand_dims %81 {axis = 2 : i32} : tensor<4x4xf32, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xf32, #blocked>
      %85 = tt.bitcast %83 : tensor<4x4x1xf32, #linear> -> tensor<4x4x1xi32, #linear>
      %86 = tt.bitcast %84 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %87 = arith.addi %85, %cst_28 : tensor<4x4x1xi32, #linear>
      %88 = arith.addi %86, %cst_1 : tensor<4x4x1xi32, #blocked>
      %89 = tt.bitcast %87 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xi32, #linear>
      %90 = tt.bitcast %88 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xi32, #blocked>
      %91 = arith.andi %89, %cst_27 : tensor<4x4x1xi32, #linear>
      %92 = arith.andi %90, %cst_4 : tensor<4x4x1xi32, #blocked>
      %93 = tt.bitcast %91 : tensor<4x4x1xi32, #linear> -> tensor<4x4x1xf32, #linear>
      %94 = tt.bitcast %92 : tensor<4x4x1xi32, #blocked> -> tensor<4x4x1xf32, #blocked>
      %95 = math.log2 %93 : tensor<4x4x1xf32, #linear>
      %96 = math.log2 %94 : tensor<4x4x1xf32, #blocked>
      %97 = math.floor %95 : tensor<4x4x1xf32, #linear>
      %98 = math.floor %96 : tensor<4x4x1xf32, #blocked>
      %99 = arith.subf %97, %cst_26 : tensor<4x4x1xf32, #linear>
      %100 = arith.subf %98, %cst_5 : tensor<4x4x1xf32, #blocked>
      %101 = tt.clampf %99, %cst_25, %cst_24, propagateNan = none : tensor<4x4x1xf32, #linear>
      %102 = tt.clampf %100, %cst_18, %cst_23, propagateNan = none : tensor<4x4x1xf32, #blocked>
      %103 = arith.fptoui %101 : tensor<4x4x1xf32, #linear> to tensor<4x4x1xi8, #linear>
      %104 = arith.fptoui %102 : tensor<4x4x1xf32, #blocked> to tensor<4x4x1xi8, #blocked>
      %105 = arith.addi %103, %cst_29 : tensor<4x4x1xi8, #linear>
      %106 = arith.addi %104, %cst : tensor<4x4x1xi8, #blocked>
      %107 = arith.subf %cst_6, %102 : tensor<4x4x1xf32, #blocked>
      %108 = math.exp2 %107 : tensor<4x4x1xf32, #blocked>
      %109 = arith.extf %78 : tensor<4x4x32xbf16, #blocked> to tensor<4x4x32xf32, #blocked>
      %110 = tt.broadcast %108 : tensor<4x4x1xf32, #blocked> -> tensor<4x4x32xf32, #blocked>
      %111 = arith.mulf %109, %110 : tensor<4x4x32xf32, #blocked>
      %112 = tt.bitcast %111 : tensor<4x4x32xf32, #blocked> -> tensor<4x4x32xi32, #blocked>
      %113 = arith.andi %112, %cst_7 : tensor<4x4x32xi32, #blocked>
      %114 = arith.shrui %112, %cst_8 : tensor<4x4x32xi32, #blocked>
      %115 = arith.andi %114, %cst_9 : tensor<4x4x32xi32, #blocked>
      %116 = arith.andi %112, %cst_10 : tensor<4x4x32xi32, #blocked>
      %117 = arith.addi %115, %cst_11 : tensor<4x4x32xi32, #blocked>
      %118 = arith.subi %cst_12, %117 : tensor<4x4x32xi32, #blocked>
      %119 = arith.cmpi ult, %115, %cst_12 : tensor<4x4x32xi32, #blocked>
      %120 = arith.shrui %116, %cst_11 : tensor<4x4x32xi32, #blocked>
      %121 = arith.ori %120, %cst_13 : tensor<4x4x32xi32, #blocked>
      %122 = arith.shrui %121, %118 : tensor<4x4x32xi32, #blocked>
      %123 = arith.select %119, %122, %116 : tensor<4x4x32xi1, #blocked>, tensor<4x4x32xi32, #blocked>
      %124 = arith.maxui %115, %cst_14 : tensor<4x4x32xi32, #blocked>
      %125 = arith.subi %124, %cst_14 : tensor<4x4x32xi32, #blocked>
      %126 = arith.shli %125, %cst_15 : tensor<4x4x32xi32, #blocked>
      %127 = arith.shrui %123, %cst_16 : tensor<4x4x32xi32, #blocked>
      %128 = arith.ori %126, %127 : tensor<4x4x32xi32, #blocked>
      %129 = arith.addi %128, %cst_11 : tensor<4x4x32xi32, #blocked>
      %130 = arith.shrui %129, %cst_11 : tensor<4x4x32xi32, #blocked>
      %131 = arith.minui %130, %cst_22 : tensor<4x4x32xi32, #blocked>
      %132 = arith.shrui %113, %cst_17 : tensor<4x4x32xi32, #blocked>
      %133 = arith.ori %132, %131 : tensor<4x4x32xi32, #blocked>
      %134 = arith.trunci %133 : tensor<4x4x32xi32, #blocked> to tensor<4x4x32xi8, #blocked>
      %135 = tt.reshape %134 : tensor<4x4x32xi8, #blocked> -> tensor<4x4x16x2xi8, #blocked7>
      %outLHS, %outRHS = tt.split %135 : tensor<4x4x16x2xi8, #blocked7> -> tensor<4x4x16xi8, #blocked2>
      %136 = arith.shli %outRHS, %cst_2 : tensor<4x4x16xi8, #blocked2>
      %137 = arith.ori %outLHS, %136 : tensor<4x4x16xi8, #blocked2>
      %138 = tt.reshape %137 : tensor<4x4x16xi8, #blocked2> -> tensor<4x64xi8, #blocked8>
      %139 = tt.reshape %105 : tensor<4x4x1xi8, #linear> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %140 = tt.reshape %106 : tensor<4x4x1xi8, #blocked> -> tensor<4x4xi8, #linear2>
      %141 = ttg.convert_layout %140 : tensor<4x4xi8, #linear2> -> tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %142 = arith.extui %141 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>> to tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %143 = arith.shli %142, %cst_30 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %144 = tt.bitcast %143 : tensor<4x4xi16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>>
      %145 = tt.expand_dims %144 {axis = 2 : i32} : tensor<4x4xbf16, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xbf16, #blocked>
      %146 = tt.broadcast %145 : tensor<4x4x1xbf16, #blocked> -> tensor<4x4x32xbf16, #blocked>
      %147 = tt.reshape %146 : tensor<4x4x32xbf16, #blocked> -> tensor<4x128xbf16, #blocked1>
      %148 = amdgpu.scaled_upcast_fp4 %138 scale %147 {axis = 1 : i32} : tensor<4x64xi8, #blocked8>, tensor<4x128xbf16, #blocked1> -> tensor<4x128xbf16, #blocked1>
      %149 = arith.cmpi eq, %139, %cst_31 : tensor<4x4xi8, #ttg.slice<{dim = 2, parent = #blocked}>>
      %150 = tt.expand_dims %149 {axis = 2 : i32} : tensor<4x4xi1, #ttg.slice<{dim = 2, parent = #blocked}>> -> tensor<4x4x1xi1, #blocked>
      %151 = tt.broadcast %150 : tensor<4x4x1xi1, #blocked> -> tensor<4x4x32xi1, #blocked>
      %152 = tt.reshape %151 : tensor<4x4x32xi1, #blocked> -> tensor<4x128xi1, #blocked1>
      %153 = arith.select %152, %cst_0, %148 : tensor<4x128xi1, #blocked1>, tensor<4x128xbf16, #blocked1>
      %154 = ttg.local_alloc %153 : (tensor<4x128xbf16, #blocked1>) -> !ttg.memdesc<4x128xbf16, #shared, #smem>
      %155 = ttg.local_load %154 : !ttg.memdesc<4x128xbf16, #shared, #smem> -> tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>>
      %156 = arith.extui %70 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>> to tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %157 = arith.shli %156, %cst_19 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %158 = tt.bitcast %157 : tensor<4x32xi16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %159 = tt.expand_dims %158 {axis = 2 : i32} : tensor<4x32xbf16, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xbf16, #blocked4>
      %160 = tt.broadcast %159 : tensor<4x32x1xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked4>
      %161 = tt.trans %160 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xbf16, #blocked4> -> tensor<4x32x32xbf16, #blocked9>
      %162 = tt.reshape %161 : tensor<4x32x32xbf16, #blocked9> -> tensor<128x32xbf16, #blocked5>
      %163 = amdgpu.scaled_upcast_fp4 %77 scale %162 {axis = 0 : i32} : tensor<64x32xi8, #blocked3>, tensor<128x32xbf16, #blocked5> -> tensor<128x32xbf16, #blocked5>
      %164 = arith.cmpi eq, %70, %cst_20 : tensor<4x32xi8, #ttg.slice<{dim = 2, parent = #blocked4}>>
      %165 = tt.expand_dims %164 {axis = 2 : i32} : tensor<4x32xi1, #ttg.slice<{dim = 2, parent = #blocked4}>> -> tensor<4x32x1xi1, #blocked4>
      %166 = tt.broadcast %165 : tensor<4x32x1xi1, #blocked4> -> tensor<4x32x32xi1, #blocked4>
      %167 = tt.trans %166 {order = array<i32: 0, 2, 1>} : tensor<4x32x32xi1, #blocked4> -> tensor<4x32x32xi1, #blocked9>
      %168 = tt.reshape %167 : tensor<4x32x32xi1, #blocked9> -> tensor<128x32xi1, #blocked5>
      %169 = arith.select %168, %cst_21, %163 : tensor<128x32xi1, #blocked5>, tensor<128x32xbf16, #blocked5>
      %170 = ttg.local_alloc %169 : (tensor<128x32xbf16, #blocked5>) -> !ttg.memdesc<128x32xbf16, #shared, #smem>
      %171 = ttg.local_load %170 : !ttg.memdesc<128x32xbf16, #shared, #smem> -> tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>
      %172 = tt.dot %155, %171, %cst_3 : tensor<4x128xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked3}>> * tensor<128x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> -> tensor<4x32xf32, #blocked3>
      %173 = arith.addf %172, %cst_3 : tensor<4x32xf32, #blocked3>
      %174 = arith.truncf %173 : tensor<4x32xf32, #blocked3> to tensor<4x32xbf16, #blocked3>
      %175 = arith.extsi %13 : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %176 = arith.extsi %11 : i32 to i64
      %177 = tt.splat %176 : i64 -> tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %178 = arith.addi %177, %175 : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %179 = arith.extsi %32 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> to tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %180 = arith.extsi %30 : i32 to i64
      %181 = tt.splat %180 : i64 -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %182 = arith.addi %181, %179 : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>>
      %183 = arith.muli %7, %6 : i64
      %184 = tt.addptr %arg2, %183 : !tt.ptr<bf16>, i64
      %185 = tt.expand_dims %178 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %186 = arith.extsi %arg13 : i32 to i64
      %187 = tt.expand_dims %175 {axis = 1 : i32} : tensor<4xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<4x1xi64, #blocked3>
      %188 = arith.muli %186, %176 : i64
      %189 = tt.splat %186 : i64 -> tensor<4x1xi64, #blocked3>
      %190 = arith.muli %189, %187 : tensor<4x1xi64, #blocked3>
      %191 = tt.addptr %184, %188 : !tt.ptr<bf16>, i64
      %192 = tt.expand_dims %182 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %193 = tt.broadcast %190 : tensor<4x1xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %194 = tt.expand_dims %179 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x32xi64, #blocked3>
      %195 = tt.broadcast %194 : tensor<1x32xi64, #blocked3> -> tensor<4x32xi64, #blocked3>
      %196 = tt.addptr %191, %180 : !tt.ptr<bf16>, i64
      %197 = arith.addi %195, %193 : tensor<4x32xi64, #blocked3>
      %198 = arith.extsi %arg4 : i32 to i64
      %199 = tt.splat %198 : i64 -> tensor<4x1xi64, #blocked3>
      %200 = arith.cmpi slt, %185, %199 : tensor<4x1xi64, #blocked3>
      %201 = arith.extsi %arg5 : i32 to i64
      %202 = tt.splat %201 : i64 -> tensor<1x32xi64, #blocked3>
      %203 = arith.cmpi slt, %192, %202 : tensor<1x32xi64, #blocked3>
      %204 = tt.broadcast %200 : tensor<4x1xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %205 = tt.broadcast %203 : tensor<1x32xi1, #blocked3> -> tensor<4x32xi1, #blocked3>
      %206 = arith.andi %204, %205 : tensor<4x32xi1, #blocked3>
      %207 = tt.splat %196 : !tt.ptr<bf16> -> tensor<4x32x!tt.ptr<bf16>, #blocked3>
      %208 = tt.addptr %207, %197 : tensor<4x32x!tt.ptr<bf16>, #blocked3>, tensor<4x32xi64, #blocked3>
      tt.store %208, %174, %206 : tensor<4x32x!tt.ptr<bf16>, #blocked3>
    }
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(optimize-amd-lds-usage{lds-limit=0 target-arch=gfx950}, triton-scf-to-cf, convert-index-to-llvm{index-bitwidth=0}, allocate-amdgpu-shared-memory, convert-triton-amdgpu-to-llvm{arch=gfx950 ftz=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-cf-to-llvm{index-bitwidth=0}, convert-arith-to-llvm{index-bitwidth=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info, convert-builtin-func-to-llvm{ftz=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_root/5s/c5sqzx55uwvuzsvm5c7zd4qji4rge7a3hkhdij4t4ybltxzoj4cz.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_root/5s/c5sqzx55uwvuzsvm5c7zd4qji4rge7a3hkhdij4t4ybltxzoj4cz.py:18:0: note: Pipeline failed while executing [`ConvertTritonAMDGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Triton compilation failed: _batched_gemm_afp4_wfp4_pre_quant_kernel_0
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] def _batched_gemm_afp4_wfp4_pre_quant_kernel(
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     a_ptr,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_ptr,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     c_ptr,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     b_scales_ptr,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     M,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     N,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     K,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_am,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ak,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bn,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bk,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ck,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cm,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cn,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsb,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsn,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bsk,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Meta-parameters
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_M: tl.constexpr,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_N: tl.constexpr,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     BLOCK_SIZE_K: tl.constexpr,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GROUP_SIZE_M: tl.constexpr,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     NUM_KSPLIT: tl.constexpr,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SPLITK_BLOCK_SIZE: tl.constexpr,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     EVEN_K: tl.constexpr,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     GRID_MN: tl.constexpr,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     cache_modifier: tl.constexpr,
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] ):
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     Kernel for computing the matmul C = A x B.
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A and B inputs are in the microscale fp4 (mxfp4) format.
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A_scales and B_scales are in e8m0 format.
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     A has shape (M, K), B has shape (K, N) and C has shape (M, N)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     """
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ab > 0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_am > 0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_ak > 0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bb > 0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bk > 0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bn > 0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cb > 0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cm > 0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_cn > 0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsb > 0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsk > 0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(stride_bsn > 0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # -----------------------------------------------------------
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Map program ids `pid` to the block of C it should compute.
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # This is done in a grouped ordering to promote L2 data reuse.
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.program_id(axis=0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_unified = tl.program_id(axis=1)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_k = pid_unified % NUM_KSPLIT
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid = pid_unified // NUM_KSPLIT
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Cast batch id and batch dimension strides to int64 to avoid int32 overflow during offset calculation
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # Note: If you're attempting to cast strides to int64 to prevent integer overflow, use `tl.cast` instead of `.to()`.
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # See https://github.com/ROCm/aiter/pull/597 for rationale
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_ab = tl.cast(stride_ab, tl.int64)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_bb = tl.cast(stride_bb, tl.int64)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stride_cb = tl.cast(stride_cb, tl.int64)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pid_batch = tl.cast(pid_batch, tl.int64)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if NUM_KSPLIT == 1:
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         remap_xcd(pid, GRID_MN)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m, pid_n = pid_grid(pid, num_pid_m, num_pid_n, GROUP_SIZE_M=GROUP_SIZE_M)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     else:
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_m = pid // num_pid_n
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         pid_n = pid % num_pid_n
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_batch >= 0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_m >= 0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     tl.assume(pid_n >= 0)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     # We assume 32 elements along K share the same scale.
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     SCALE_GROUP_SIZE: tl.constexpr = 32
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     if (pid_k * SPLITK_BLOCK_SIZE // 2) < K:
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         num_k_iter = tl.cdiv(SPLITK_BLOCK_SIZE // 2, BLOCK_SIZE_K // 2)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for first block of A and B input matrices
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_bf16 = tl.arange(0, BLOCK_SIZE_K)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split_bf16 = pid_k * SPLITK_BLOCK_SIZE + offs_k_bf16
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         a_ptrs = a_ptr + (
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_ab
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_am[:, None] * stride_am
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split_bf16[None, :] * stride_ak
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k = tl.arange(0, BLOCK_SIZE_K // 2)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_k_split = pid_k * (SPLITK_BLOCK_SIZE // 2) + offs_k
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_ptrs = b_ptr + (
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             pid_batch * stride_bb
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_k_split[:, None] * stride_bk
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[None, :] * stride_bn
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Create pointers for the first block of A and B scales
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_ks = (pid_k * (SPLITK_BLOCK_SIZE // SCALE_GROUP_SIZE)) + tl.arange(
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             0, BLOCK_SIZE_K // SCALE_GROUP_SIZE
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # B scales are N x K even though B operand is K x N.
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         b_scale_ptrs = (
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales_ptr
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_bsb
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_bn[:, None] * stride_bsn
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + offs_ks[None, :] * stride_bsk
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         for k in range(pid_k * num_k_iter, (pid_k + 1) * num_k_iter):
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scales = tl.load(b_scale_ptrs)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # a_scales = tl.full((BLOCK_SIZE_M, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # b_scales = tl.full((BLOCK_SIZE_N, BLOCK_SIZE_K//SCALE_GROUP_SIZE), 127, dtype=tl.uint8)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Load the next block of A and B, generate a mask by checking the K dimension.
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # If it is out of bounds, set it to 0.
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             if EVEN_K:
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(a_ptrs)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(b_ptrs, cache_modifier=cache_modifier)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             else:
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 a_bf16 = tl.load(
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 b = tl.load(
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                     b_ptrs, mask=offs_k[:, None] < K - k * (BLOCK_SIZE_K // 2), other=0
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]                 )
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a, a_scales = _mxfp4_quant_op(a_bf16, BLOCK_SIZE_K, BLOCK_SIZE_M, 32)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             accumulator += tl.dot_scaled(a, a_scales, "e2m1", b, b_scales, "e2m1")
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             # Advance the ptrs to the next K block.
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             a_ptrs += BLOCK_SIZE_K * stride_ak
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             b_scale_ptrs += (BLOCK_SIZE_K // SCALE_GROUP_SIZE) * stride_bsk
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c = accumulator.to(c_ptr.type.element_ty)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         # Write back the block of the output matrix C with masks.
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_ptrs = (
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             c_ptr
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_batch * stride_cb
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cm * offs_cm[:, None]
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + stride_cn * offs_cn[None, :]
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]             + pid_k * stride_ck
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         )
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]         tl.store(c_ptrs, c, mask=c_mask)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] 
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] metadata: {'signature': {'a_ptr': '*bf16', 'b_ptr': '*u8', 'c_ptr': '*bf16', 'b_scales_ptr': '*u8', 'M': 'i32', 'N': 'i32', 'K': 'i32', 'stride_ab': 'i32', 'stride_am': 'i32', 'stride_ak': 'constexpr', 'stride_bb': 'i32', 'stride_bn': 'i32', 'stride_bk': 'constexpr', 'stride_cb': 'i32', 'stride_ck': 'i32', 'stride_cm': 'i32', 'stride_cn': 'constexpr', 'stride_bsb': 'i32', 'stride_bsn': 'i32', 'stride_bsk': 'constexpr', 'BLOCK_SIZE_M': 'constexpr', 'BLOCK_SIZE_N': 'constexpr', 'BLOCK_SIZE_K': 'constexpr', 'GROUP_SIZE_M': 'constexpr', 'NUM_KSPLIT': 'constexpr', 'SPLITK_BLOCK_SIZE': 'constexpr', 'EVEN_K': 'constexpr', 'GRID_MN': 'constexpr', 'cache_modifier': 'constexpr'}, 'device': 0, 'constants': {'stride_ak': 1, 'stride_bk': 1, 'stride_cn': 1, 'stride_bsk': 1, 'BLOCK_SIZE_M': 4, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 1, 'NUM_KSPLIT': 1, 'SPLITK_BLOCK_SIZE': 128, 'EVEN_K': True, 'GRID_MN': 64, 'cache_modifier': '.cg'}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]], (13,): [['tt.divisibility', 16]], (14,): [['tt.divisibility', 16]], (15,): [['tt.divisibility', 16]], (17,): [['tt.divisibility', 16]]}], 'device_type': 'hip', 'num_warps': 4, 'num_stages': 1, 'debug': False, 'cc': 'gfx950'}
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] Traceback (most recent call last):
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     binary = triton.compile(*compile_args, **compile_kwargs)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     next_module = compile_ir(module, metadata)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]   File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0]     pm.run(mod)
[rank0]:E1113 09:51:30.781000 203 torch/_inductor/runtime/triton_heuristics.py:750] [22/0] RuntimeError: PassManager::run failed
Capturing batches (bs=16 avail_mem=90.15 GB):   0%|          | 0/6 [00:04<?, ?it/s]
[aiter] Registering 0 cuda graph addresses
[aiter] Registering 0 cuda graph addresses
[2025-11-13 09:51:30 TP7] Registering 0 cuda graph addresses
[2025-11-13 09:51:30 TP0] Registering 0 cuda graph addresses
[aiter] Registering 0 cuda graph addresses
[2025-11-13 09:51:30 TP3] Registering 0 cuda graph addresses
[aiter] Registering 0 cuda graph addresses
[2025-11-13 09:51:30 TP5] Registering 0 cuda graph addresses
[aiter] Registering 0 cuda graph addresses
[2025-11-13 09:51:30 TP2] Registering 0 cuda graph addresses
[aiter] Registering 0 cuda graph addresses
[2025-11-13 09:51:30 TP4] Registering 0 cuda graph addresses
[aiter] Registering 0 cuda graph addresses
[2025-11-13 09:51:30 TP6] Registering 0 cuda graph addresses
[aiter] Registering 0 cuda graph addresses
[2025-11-13 09:51:30 TP1] Registering 0 cuda graph addresses
[2025-11-13 09:51:31 TP6] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 381, in __init__
    self.capture()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 520, in capture
    ) = self.capture_one_batch_size(bs, forward)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 699, in capture_one_batch_size
    run_once()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 686, in run_once
    logits_output_or_pp_proxy_tensors = forward(
  File "/opt/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 817, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 979, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 963, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1646, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1506, in codegen_and_compile
    compiled_module = graph.compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2318, in compile_to_module
    return self._compile_to_module()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2328, in _compile_to_module
    mod = self._compile_to_module_lines(wrapper_code)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2396, in _compile_to_module_lines
    mod = PyCodeCache.load_by_key_path(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3462, in load_by_key_path
    mod = _reload_python_module(key, path, set_sys_modules=in_toplevel)
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 31, in _reload_python_module
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_root/7o/c7orvfngu6yyqzh5tldkayjbanqaedd2y3djslnnvgiul2cj5zrn.py", line 38, in <module>
    _batched_gemm_afp4_wfp4_pre_quant_kernel_0 = async_compile.triton('_batched_gemm_afp4_wfp4_pre_quant_kernel', '''
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 486, in triton
    kernel.precompile(
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 437, in precompile
    self._precompile_worker()
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 459, in _precompile_worker
    compile_results.append(self._precompile_config(c))
  File "/opt/venv/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 748, in _precompile_config
    binary = triton.compile(*compile_args, **compile_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/triton/compiler/compiler.py", line 322, in compile
    next_module = compile_ir(module, metadata)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 450, in <lambda>
    stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
  File "/opt/venv/lib/python3.10/site-packages/triton/backends/amd/compiler.py", line 325, in make_llir
    pm.run(mod)
torch._inductor.exc.InductorError: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2703, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 312, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 237, in __init__
    self._model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 323, in __init__
    self.initialize(min_per_gpu_memory)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 490, in initialize
    self.init_device_graphs()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2006, in init_device_graphs
    self.graph_runner = graph_runners[self.device](self)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 383, in __init__
    raise Exception(
Exception: Capture cuda graph failed: RuntimeError: PassManager::run failed

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

Possible solutions:
1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
3. disable torch compile by not using --enable-torch-compile
4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 


[2025-11-13 09:51:31] Received sigquit from a child process. It usually means the child failed.
